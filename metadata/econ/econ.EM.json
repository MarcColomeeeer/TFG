[
    {
        "title": "Fixed Effect Estimation of Large T Panel Data Models",
        "authors": [
            "Iván Fernández-Val",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  This article reviews recent advances in fixed effect estimation of panel data\nmodels for long panels, where the number of time periods is relatively large.\nWe focus on semiparametric models with unobserved individual and time effects,\nwhere the distribution of the outcome variable conditional on covariates and\nunobserved effects is specified parametrically, while the distribution of the\nunobserved effects is left unrestricted. Compared to existing reviews on long\npanels (Arellano and Hahn 2007; a section in Arellano and Bonhomme 2011) we\ndiscuss models with both individual and time effects, split-panel Jackknife\nbias corrections, unbalanced panels, distribution and quantile effects, and\nother extensions. Understanding and correcting the incidental parameter bias\ncaused by the estimation of many fixed effects is our main focus, and the\nunifying theme is that the order of this bias is given by the simple formula\np/n for all models discussed, with p the number of estimated parameters and n\nthe total sample size.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08980v2"
    },
    {
        "title": "Bounds On Treatment Effects On Transitions",
        "authors": [
            "Johan Vikström",
            "Geert Ridder",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  This paper considers the identification of treatment effects on conditional\ntransition probabilities. We show that even under random assignment only the\ninstantaneous average treatment effect is point identified. Since treated and\ncontrol units drop out at different rates, randomization only ensures the\ncomparability of treatment and controls at the time of randomization, so that\nlong-run average treatment effects are not point identified. Instead we derive\ninformative bounds on these average treatment effects. Our bounds do not impose\n(semi)parametric restrictions, for example, proportional hazards. We also\nexplore various assumptions such as monotone treatment response, common shocks\nand positively correlated outcomes that tighten the bounds.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08981v1"
    },
    {
        "title": "Inference on Estimators defined by Mathematical Programming",
        "authors": [
            "Yu-Wei Hsieh",
            "Xiaoxia Shi",
            "Matthew Shum"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  We propose an inference procedure for estimators defined by mathematical\nprogramming problems, focusing on the important special cases of linear\nprogramming (LP) and quadratic programming (QP). In these settings, the\ncoefficients in both the objective function and the constraints of the\nmathematical programming problem may be estimated from data and hence involve\nsampling error. Our inference approach exploits the characterization of the\nsolutions to these programming problems by complementarity conditions; by doing\nso, we can transform the problem of doing inference on the solution of a\nconstrained optimization problem (a non-standard inference problem) into one\ninvolving inference based on a set of inequalities with pre-estimated\ncoefficients, which is much better understood. We evaluate the performance of\nour procedure in several Monte Carlo simulations and an empirical application\nto the classic portfolio selection problem in finance.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.09115v1"
    },
    {
        "title": "Sharp bounds and testability of a Roy model of STEM major choices",
        "authors": [
            "Ismael Mourifie",
            "Marc Henry",
            "Romuald Meango"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  We analyze the empirical content of the Roy model, stripped down to its\nessential features, namely sector specific unobserved heterogeneity and\nself-selection on the basis of potential outcomes. We characterize sharp bounds\non the joint distribution of potential outcomes and testable implications of\nthe Roy self-selection model under an instrumental constraint on the joint\ndistribution of potential outcomes we call stochastically monotone instrumental\nvariable (SMIV). We show that testing the Roy model selection is equivalent to\ntesting stochastic monotonicity of observed outcomes relative to the\ninstrument. We apply our sharp bounds to the derivation of a measure of\ndeparture from Roy self-selection to identify values of observable\ncharacteristics that induce the most costly misallocation of talent and sector\nand are therefore prime targets for intervention. Special emphasis is put on\nthe case of binary outcomes, which has received little attention in the\nliterature to date. For richer sets of outcomes, we emphasize the distinction\nbetween pointwise sharp bounds and functional sharp bounds, and its importance,\nwhen constructing sharp bounds on functional features, such as inequality\nmeasures. We analyze a Roy model of college major choice in Canada and Germany\nwithin this framework, and we take a new look at the under-representation of\nwomen in~STEM.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.09284v2"
    },
    {
        "title": "Zero-rating of Content and its Effect on the Quality of Service in the\n  Internet",
        "authors": [
            "Manjesh K. Hanawal",
            "Fehmina Malik",
            "Yezekael Hayel"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  The ongoing net neutrality debate has generated a lot of heated discussions\non whether or not monetary interactions should be regulated between content and\naccess providers. Among the several topics discussed, `differential pricing'\nhas recently received attention due to `zero-rating' platforms proposed by some\nservice providers. In the differential pricing scheme, Internet Service\nProviders (ISPs) can exempt data access charges for on content from certain CPs\n(zero-rated) while no exemption is on content from other CPs. This allows the\npossibility for Content Providers (CPs) to make `sponsorship' agreements to\nzero-rate their content and attract more user traffic. In this paper, we study\nthe effect of differential pricing on various players in the Internet. We first\nconsider a model with a monopolistic ISP and multiple CPs where users select\nCPs based on the quality of service (QoS) and data access charges. We show that\nin a differential pricing regime 1) a CP offering low QoS can make have higher\nsurplus than a CP offering better QoS through sponsorships. 2) Overall QoS\n(mean delay) for end users can degrade under differential pricing schemes. In\nthe oligopolistic market with multiple ISPs, users tend to select the ISP with\nlowest ISP resulting in same type of conclusions as in the monopolistic market.\nWe then study how differential pricing effects the revenue of ISPs.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.09334v2"
    },
    {
        "title": "Quasi-random Monte Carlo application in CGE systematic sensitivity\n  analysis",
        "authors": [
            "Theodoros Chatzivasileiadis"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  The uncertainty and robustness of Computable General Equilibrium models can\nbe assessed by conducting a Systematic Sensitivity Analysis. Different methods\nhave been used in the literature for SSA of CGE models such as Gaussian\nQuadrature and Monte Carlo methods. This paper explores the use of Quasi-random\nMonte Carlo methods based on the Halton and Sobol' sequences as means to\nimprove the efficiency over regular Monte Carlo SSA, thus reducing the\ncomputational requirements of the SSA. The findings suggest that by using\nlow-discrepancy sequences, the number of simulations required by the regular MC\nSSA methods can be notably reduced, hence lowering the computational time\nrequired for SSA of CGE models.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.09755v1"
    },
    {
        "title": "Estimation of Peer Effects in Endogenous Social Networks: Control\n  Function Approach",
        "authors": [
            "Ida Johnsson",
            "Hyungsik Roger Moon"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  We propose a method of estimating the linear-in-means model of peer effects\nin which the peer group, defined by a social network, is endogenous in the\noutcome equation for peer effects. Endogeneity is due to unobservable\nindividual characteristics that influence both link formation in the network\nand the outcome of interest. We propose two estimators of the peer effect\nequation that control for the endogeneity of the social connections using a\ncontrol function approach. We leave the functional form of the control function\nunspecified and treat it as unknown. To estimate the model, we use a sieve\nsemiparametric approach, and we establish asymptotics of the semiparametric\nestimator.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.10024v3"
    },
    {
        "title": "Forecasting with Dynamic Panel Data Models",
        "authors": [
            "Laura Liu",
            "Hyungsik Roger Moon",
            "Frank Schorfheide"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  This paper considers the problem of forecasting a collection of short time\nseries using cross sectional information in panel data. We construct point\npredictors using Tweedie's formula for the posterior mean of heterogeneous\ncoefficients under a correlated random effects distribution. This formula\nutilizes cross-sectional information to transform the unit-specific (quasi)\nmaximum likelihood estimator into an approximation of the posterior mean under\na prior distribution that equals the population distribution of the random\ncoefficients. We show that the risk of a predictor based on a non-parametric\nestimate of the Tweedie correction is asymptotically equivalent to the risk of\na predictor that treats the correlated-random-effects distribution as known\n(ratio-optimality). Our empirical Bayes predictor performs well compared to\nvarious competitors in a Monte Carlo study. In an empirical application we use\nthe predictor to forecast revenues for a large panel of bank holding companies\nand compare forecasts that condition on actual and severely adverse\nmacroeconomic conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.10193v1"
    },
    {
        "title": "Inference for VARs Identified with Sign Restrictions",
        "authors": [
            "Eleonora Granziera",
            "Hyungsik Roger Moon",
            "Frank Schorfheide"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  There is a fast growing literature that set-identifies structural vector\nautoregressions (SVARs) by imposing sign restrictions on the responses of a\nsubset of the endogenous variables to a particular structural shock\n(sign-restricted SVARs). Most methods that have been used to construct\npointwise coverage bands for impulse responses of sign-restricted SVARs are\njustified only from a Bayesian perspective. This paper demonstrates how to\nformulate the inference problem for sign-restricted SVARs within a\nmoment-inequality framework. In particular, it develops methods of constructing\nconfidence bands for impulse response functions of sign-restricted SVARs that\nare valid from a frequentist perspective. The paper also provides a comparison\nof frequentist and Bayesian coverage bands in the context of an empirical\napplication - the former can be substantially wider than the latter.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.10196v2"
    },
    {
        "title": "Heterogeneous Employment Effects of Job Search Programmes: A Machine\n  Learning Approach",
        "authors": [
            "Michael Knaus",
            "Michael Lechner",
            "Anthony Strittmatter"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  We systematically investigate the effect heterogeneity of job search\nprogrammes for unemployed workers. To investigate possibly heterogeneous\nemployment effects, we combine non-experimental causal empirical models with\nLasso-type estimators. The empirical analyses are based on rich administrative\ndata from Swiss social security records. We find considerable heterogeneities\nonly during the first six months after the start of training. Consistent with\nprevious results of the literature, unemployed persons with fewer employment\nopportunities profit more from participating in these programmes. Furthermore,\nwe also document heterogeneous employment effects by residence status. Finally,\nwe show the potential of easy-to-implement programme participation rules for\nimproving average employment effects of these active labour market programmes.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.10279v2"
    },
    {
        "title": "Startups and Stanford University",
        "authors": [
            "Hervé Lebret"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Startups have become in less than 50 years a major component of innovation\nand economic growth. Silicon Valley has been the place where the startup\nphenomenon was the most obvious and Stanford University was a major component\nof that success. Companies such as Google, Yahoo, Sun Microsystems, Cisco,\nHewlett Packard had very strong links with Stanford but even these vary famous\nsuccess stories cannot fully describe the richness and diversity of the\nStanford entrepreneurial activity. This report explores the dynamics of more\nthan 5000 companies founded by Stanford University alumni and staff, through\ntheir value creation, their field of activities, their growth patterns and\nmore. The report also explores some features of the founders of these companies\nsuch as their academic background or the number of years between their Stanford\nexperience and their company creation.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.00644v1"
    },
    {
        "title": "Equity in Startups",
        "authors": [
            "Hervé Lebret"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Startups have become in less than 50 years a major component of innovation\nand economic growth. An important feature of the startup phenomenon has been\nthe wealth created through equity in startups to all stakeholders. These\ninclude the startup founders, the investors, and also the employees through the\nstock-option mechanism and universities through licenses of intellectual\nproperty. In the employee group, the allocation to important managers like the\nchief executive, vice-presidents and other officers, and independent board\nmembers is also analyzed. This report analyzes how equity was allocated in more\nthan 400 startups, most of which had filed for an initial public offering. The\nauthor has the ambition of informing a general audience about best practice in\nequity split, in particular in Silicon Valley, the central place for startup\ninnovation.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.00661v1"
    },
    {
        "title": "Identifying the Effects of a Program Offer with an Application to Head\n  Start",
        "authors": [
            "Vishal Kamat"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  I propose a treatment selection model that introduces unobserved\nheterogeneity in both choice sets and preferences to evaluate the average\neffects of a program offer. I show how to exploit the model structure to define\nparameters capturing these effects and then computationally characterize their\nidentified sets under instrumental variable variation in choice sets. I\nillustrate these tools by analyzing the effects of providing an offer to the\nHead Start preschool program using data from the Head Start Impact Study. I\nfind that such a policy affects a large number of children who take up the\noffer, and that they subsequently have positive effects on test scores. These\neffects arise from children who do not have any preschool as an outside option.\nA cost-benefit analysis reveals that the earning benefits associated with the\ntest score gains can be large and outweigh the net costs associated with offer\ntake up.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.02048v6"
    },
    {
        "title": "Identification and Estimation of Spillover Effects in Randomized\n  Experiments",
        "authors": [
            "Gonzalo Vazquez-Bare"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  I study identification, estimation and inference for spillover effects in\nexperiments where units' outcomes may depend on the treatment assignments of\nother units within a group. I show that the commonly-used reduced-form\nlinear-in-means regression identifies a weighted sum of spillover effects with\nsome negative weights, and that the difference in means between treated and\ncontrols identifies a combination of direct and spillover effects entering with\ndifferent signs. I propose nonparametric estimators for average direct and\nspillover effects that overcome these issues and are consistent and\nasymptotically normal under a precise relationship between the number of\nparameters of interest, the total sample size and the treatment assignment\nmechanism. These findings are illustrated using data from a conditional cash\ntransfer program and with simulations. The empirical results reveal the\npotential pitfalls of failing to flexibly account for spillover effects in\npolicy evaluation: the estimated difference in means and the reduced-form\nlinear-in-means coefficients are all close to zero and statistically\ninsignificant, whereas the nonparametric estimators I propose reveal large,\nnonlinear and significant spillover effects.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.02745v8"
    },
    {
        "title": "Measuring Price Discovery between Nearby and Deferred Contracts in\n  Storable and Non-Storable Commodity Futures Markets",
        "authors": [
            "Zhepeng Hu",
            "Mindy Mallory",
            "Teresa Serra",
            "Philip Garcia"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Futures market contracts with varying maturities are traded concurrently and\nthe speed at which they process information is of value in understanding the\npricing discovery process. Using price discovery measures, including Putnins\n(2013) information leadership share and intraday data, we quantify the\nproportional contribution of price discovery between nearby and deferred\ncontracts in the corn and live cattle futures markets. Price discovery is more\nsystematic in the corn than in the live cattle market. On average, nearby\ncontracts lead all deferred contracts in price discovery in the corn market,\nbut have a relatively less dominant role in the live cattle market. In both\nmarkets, the nearby contract loses dominance when its relative volume share\ndips below 50%, which occurs about 2-3 weeks before expiration in corn and 5-6\nweeks before expiration in live cattle. Regression results indicate that the\nshare of price discovery is most closely linked to trading volume but is also\naffected, to far less degree, by time to expiration, backwardation, USDA\nannouncements and market crashes. The effects of these other factors vary\nbetween the markets which likely reflect the difference in storability as well\nas other market-related characteristics.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.03506v1"
    },
    {
        "title": "Economic Complexity Unfolded: Interpretable Model for the Productive\n  Structure of Economies",
        "authors": [
            "Zoran Utkovski",
            "Melanie F. Pradier",
            "Viktor Stojkoski",
            "Fernando Perez-Cruz",
            "Ljupco Kocarev"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Economic complexity reflects the amount of knowledge that is embedded in the\nproductive structure of an economy. It resides on the premise of hidden\ncapabilities - fundamental endowments underlying the productive structure. In\ngeneral, measuring the capabilities behind economic complexity directly is\ndifficult, and indirect measures have been suggested which exploit the fact\nthat the presence of the capabilities is expressed in a country's mix of\nproducts. We complement these studies by introducing a probabilistic framework\nwhich leverages Bayesian non-parametric techniques to extract the dominant\nfeatures behind the comparative advantage in exported products. Based on\neconomic evidence and trade data, we place a restricted Indian Buffet Process\non the distribution of countries' capability endowment, appealing to a culinary\nmetaphor to model the process of capability acquisition. The approach comes\nwith a unique level of interpretability, as it produces a concise and\neconomically plausible description of the instantiated capabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.07327v2"
    },
    {
        "title": "The Research on the Stagnant Development of Shantou Special Economic\n  Zone Under Reform and Opening-Up Policy",
        "authors": [
            "Bowen Cai"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  This study briefly introduces the development of Shantou Special Economic\nZone under Reform and Opening-Up Policy from 1980 through 2016 with a focus on\npolicy making issues and its influences on local economy. This paper is divided\ninto two parts, 1980 to 1991, 1992 to 2016 in accordance with the separation of\nthe original Shantou District into three cities: Shantou, Chaozhou and Jieyang\nin the end of 1991. This study analyzes the policy making issues in the\nseparation of the original Shantou District, the influences of the policy on\nShantou's economy after separation, the possibility of merging the three cities\ninto one big new economic district in the future and reasons that lead to the\nstagnant development of Shantou in recent 20 years. This paper uses statistical\nlongitudinal analysis in analyzing economic problems with applications of\nnon-parametric statistics through generalized additive model and time series\nforecasting methods. The paper is authored by Bowen Cai solely, who is the\ngraduate student in the PhD program of Applied and Computational Mathematics\nand Statistics at the University of Notre Dame with concentration in big data\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.08877v1"
    },
    {
        "title": "Constructive Identification of Heterogeneous Elasticities in the\n  Cobb-Douglas Production Function",
        "authors": [
            "Tong Li",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  This paper presents the identification of heterogeneous elasticities in the\nCobb-Douglas production function. The identification is constructive with\nclosed-form formulas for the elasticity with respect to each input for each\nfirm. We propose that the flexible input cost ratio plays the role of a control\nfunction under \"non-collinear heterogeneity\" between elasticities with respect\nto two flexible inputs. The ex ante flexible input cost share can be used to\nidentify the elasticities with respect to flexible inputs for each firm. The\nelasticities with respect to labor and capital can be subsequently identified\nfor each firm under the timing assumption admitting the functional\nindependence.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.10031v1"
    },
    {
        "title": "Identification of and correction for publication bias",
        "authors": [
            "Isaiah Andrews",
            "Maximilian Kasy"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Some empirical results are more likely to be published than others. Such\nselective publication leads to biased estimates and distorted inference. This\npaper proposes two approaches for identifying the conditional probability of\npublication as a function of a study's results, the first based on systematic\nreplication studies and the second based on meta-studies. For known conditional\npublication probabilities, we propose median-unbiased estimators and associated\nconfidence sets that correct for selective publication. We apply our methods to\nrecent large-scale replication studies in experimental economics and\npsychology, and to meta-studies of the effects of minimum wages and de-worming\nprograms.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.10527v1"
    },
    {
        "title": "The Effect of Partisanship and Political Advertising on Close Family\n  Ties",
        "authors": [
            "M. Keith Chen",
            "Ryne Rohla"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Research on growing American political polarization and antipathy primarily\nstudies public institutions and political processes, ignoring private effects\nincluding strained family ties. Using anonymized smartphone-location data and\nprecinct-level voting, we show that Thanksgiving dinners attended by\nopposing-party precinct residents were 30-50 minutes shorter than same-party\ndinners. This decline from a mean of 257 minutes survives extensive spatial and\ndemographic controls. Dinner reductions in 2016 tripled for travelers from\nmedia markets with heavy political advertising --- an effect not observed in\n2015 --- implying a relationship to election-related behavior. Effects appear\nasymmetric: while fewer Democratic-precinct residents traveled in 2016 than\n2015, political differences shortened Thanksgiving dinners more among\nRepublican-precinct residents. Nationwide, 34 million person-hours of\ncross-partisan Thanksgiving discourse were lost in 2016 to partisan effects.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.10602v2"
    },
    {
        "title": "Aggregating Google Trends: Multivariate Testing and Analysis",
        "authors": [
            "Stephen L. France",
            "Yuying Shi"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Web search data are a valuable source of business and economic information.\nPrevious studies have utilized Google Trends web search data for economic\nforecasting. We expand this work by providing algorithms to combine and\naggregate search volume data, so that the resulting data is both consistent\nover time and consistent between data series. We give a brand equity example,\nwhere Google Trends is used to analyze shopping data for 100 top ranked brands\nand these data are used to nowcast economic variables. We describe the\nimportance of out of sample prediction and show how principal component\nanalysis (PCA) can be used to improve the signal to noise ratio and prevent\noverfitting in nowcasting models. We give a finance example, where exploratory\ndata analysis and classification is used to analyze the relationship between\nGoogle Trends searches and stock prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.03152v2"
    },
    {
        "title": "Set Identified Dynamic Economies and Robustness to Misspecification",
        "authors": [
            "Andreas Tryphonides"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  We propose a new inferential methodology for dynamic economies that is robust\nto misspecification of the mechanism generating frictions. Economies with\nfrictions are treated as perturbations of a frictionless economy that are\nconsistent with a variety of mechanisms. We derive a representation for the law\nof motion for such economies and we characterize parameter set identification.\nWe derive a link from model aggregate predictions to distributional information\ncontained in qualitative survey data and specify conditions under which the\nidentified set is refined. The latter is used to semi-parametrically estimate\ndistortions due to frictions in macroeconomic variables. Based on these\nestimates, we propose a novel test for complete models. Using consumer and\nbusiness survey data collected by the European Commission, we apply our method\nto estimate distortions due to financial frictions in the Spanish economy. We\ninvestigate the implications of these estimates for the adequacy of the\nstandard model of financial frictions SW-BGG (Smets and Wouters (2007),\nBernanke, Gertler, and Gilchrist (1999)).\n",
        "pdf_link": "http://arxiv.org/pdf/1712.03675v2"
    },
    {
        "title": "Cointegration in functional autoregressive processes",
        "authors": [
            "Massimo Franchi",
            "Paolo Paruolo"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  This paper defines the class of $\\mathcal{H}$-valued autoregressive (AR)\nprocesses with a unit root of finite type, where $\\mathcal{H}$ is an infinite\ndimensional separable Hilbert space, and derives a generalization of the\nGranger-Johansen Representation Theorem valid for any integration order\n$d=1,2,\\dots$. An existence theorem shows that the solution of an AR with a\nunit root of finite type is necessarily integrated of some finite integer $d$\nand displays a common trends representation with a finite number of common\nstochastic trends of the type of (cumulated) bilateral random walks and an\ninfinite dimensional cointegrating space. A characterization theorem clarifies\nthe connections between the structure of the AR operators and $(i)$ the order\nof integration, $(ii)$ the structure of the attractor space and the\ncointegrating space, $(iii)$ the expression of the cointegrating relations, and\n$(iv)$ the Triangular representation of the process. Except for the fact that\nthe number of cointegrating relations that are integrated of order 0 is\ninfinite, the representation of $\\mathcal{H}$-valued ARs with a unit root of\nfinite type coincides with that of usual finite dimensional VARs, which\ncorresponds to the special case $\\mathcal{H}=\\mathbb{R}^p$.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.07522v2"
    },
    {
        "title": "Simultaneous Confidence Intervals for High-dimensional Linear Models\n  with Many Endogenous Variables",
        "authors": [
            "Alexandre Belloni",
            "Christian Hansen",
            "Whitney Newey"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  High-dimensional linear models with endogenous variables play an increasingly\nimportant role in recent econometric literature. In this work we allow for\nmodels with many endogenous variables and many instrument variables to achieve\nidentification. Because of the high-dimensionality in the second stage,\nconstructing honest confidence regions with asymptotically correct coverage is\nnon-trivial. Our main contribution is to propose estimators and confidence\nregions that would achieve that. The approach relies on moment conditions that\nhave an additional orthogonal property with respect to nuisance parameters.\nMoreover, estimation of high-dimension nuisance parameters is carried out via\nnew pivotal procedures. In order to achieve simultaneously valid confidence\nregions we use a multiplier bootstrap procedure to compute critical values and\nestablish its validity.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.08102v4"
    },
    {
        "title": "Resource Abundance and Life Expectancy",
        "authors": [
            "Bahram Sanginabadi"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  This paper investigates the impacts of major natural resource discoveries\nsince 1960 on life expectancy in the nations that they were resource poor prior\nto the discoveries. Previous literature explains the relation between nations\nwealth and life expectancy, but it has been silent about the impacts of\nresource discoveries on life expectancy. We attempt to fill this gap in this\nstudy. An important advantage of this study is that as the previous researchers\nargued resource discovery could be an exogenous variable. We use longitudinal\ndata from 1960 to 2014 and we apply three modern empirical methods including\nDifference-in-Differences, Event studies, and Synthetic Control approach, to\ninvestigate the main question of the research which is 'how resource\ndiscoveries affect life expectancy?'. The findings show that resource\ndiscoveries in Ecuador, Yemen, Oman, and Equatorial Guinea have positive and\nsignificant impacts on life expectancy, but the effects for the European\ncountries are mostly negative.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.00369v1"
    },
    {
        "title": "Implications of macroeconomic volatility in the Euro area",
        "authors": [
            "Niko Hauzenberger",
            "Maximilian Böck",
            "Michael Pfarrhofer",
            "Anna Stelzer",
            "Gregor Zens"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper we estimate a Bayesian vector autoregressive model with factor\nstochastic volatility in the error term to assess the effects of an uncertainty\nshock in the Euro area. This allows us to treat macroeconomic uncertainty as a\nlatent quantity during estimation. Only a limited number of contributions to\nthe literature estimate uncertainty and its macroeconomic consequences jointly,\nand most are based on single country models. We analyze the special case of a\nshock restricted to the Euro area, where member states are highly related by\nconstruction. We find significant results of a decrease in real activity for\nall countries over a period of roughly a year following an uncertainty shock.\nMoreover, equity prices, short-term interest rates and exports tend to decline,\nwhile unemployment levels increase. Dynamic responses across countries differ\nslightly in magnitude and duration, with Ireland, Slovakia and Greece\nexhibiting different reactions for some macroeconomic fundamentals.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02925v2"
    },
    {
        "title": "A Method for Winning at Lotteries",
        "authors": [
            "Steven D. Moffitt",
            "William T. Ziemba"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We report a new result on lotteries --- that a well-funded syndicate has a\npurely mechanical strategy to achieve expected returns of 10\\% to 25\\% in an\nequiprobable lottery with no take and no carryover pool. We prove that an\noptimal strategy (Nash equilibrium) in a game between the syndicate and other\nplayers consists of betting one of each ticket (the \"trump ticket\"), and extend\nthat result to proportional ticket selection in non-equiprobable lotteries. The\nstrategy can be adjusted to accommodate lottery taxes and carryover pools. No\n\"irrationality\" need be involved for the strategy to succeed --- it requires\nonly that a large group of non-syndicate bettors each choose a few tickets\nindependently.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02958v1"
    },
    {
        "title": "Does it Pay to Buy the Pot in the Canadian 6/49 Lotto? Implications for\n  Lottery Design",
        "authors": [
            "Steven D. Moffitt",
            "William T. Ziemba"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Despite its unusual payout structure, the Canadian 6/49 Lotto is one of the\nfew government sponsored lotteries that has the potential for a favorable\nstrategy we call \"buying the pot.\" By buying the pot we mean that a syndicate\nbuys each ticket in the lottery, ensuring that it holds a jackpot winner. We\nassume that the other bettors independently buy small numbers of tickets. This\npaper presents (1) a formula for the syndicate's expected return, (2)\nconditions under which buying the pot produces a significant positive expected\nreturn, and (3) the implications of these findings for lottery design.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02959v1"
    },
    {
        "title": "Solving Dynamic Discrete Choice Models: Integrated or Expected Value\n  Function?",
        "authors": [
            "Patrick Kofod Mogensen"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Dynamic Discrete Choice Models (DDCMs) are important in the structural\nestimation literature. Since the structural errors are practically always\ncontinuous and unbounded in nature, researchers often use the expected value\nfunction. The idea to solve for the expected value function made solution more\npractical and estimation feasible. However, as we show in this paper, the\nexpected value function is impractical compared to an alternative: the\nintegrated (ex ante) value function. We provide brief descriptions of the\ninefficacy of the former, and benchmarks on actual problems with varying\ncardinality of the state space and number of decisions. Though the two\napproaches solve the same problem in theory, the benchmarks support the claim\nthat the integrated value function is preferred in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.03978v1"
    },
    {
        "title": "Heterogeneous structural breaks in panel data models",
        "authors": [
            "Ryo Okui",
            "Wendun Wang"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper develops a new model and estimation procedure for panel data that\nallows us to identify heterogeneous structural breaks. We model individual\nheterogeneity using a grouped pattern. For each group, we allow common\nstructural breaks in the coefficients. However, the number, timing, and size of\nthese breaks can differ across groups. We develop a hybrid estimation procedure\nof the grouped fixed effects approach and adaptive group fused Lasso. We show\nthat our method can consistently identify the latent group structure, detect\nstructural breaks, and estimate the regression parameters. Monte Carlo results\ndemonstrate the good performance of the proposed method in finite samples. An\nempirical application to the relationship between income and democracy\nillustrates the importance of considering heterogeneous structural breaks.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.04672v2"
    },
    {
        "title": "Characterizing Assumption of Rationality by Incomplete Information",
        "authors": [
            "Shuige Liu"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We characterize common assumption of rationality of 2-person games within an\nincomplete information framework. We use the lexicographic model with\nincomplete information and show that a belief hierarchy expresses common\nassumption of rationality within a complete information framework if and only\nif there is a belief hierarchy within the corresponding incomplete information\nframework that expresses common full belief in caution, rationality, every good\nchoice is supported, and prior belief in the original utility functions.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.04714v1"
    },
    {
        "title": "Quantifying Health Shocks Over the Life Cycle",
        "authors": [
            "Taiyo Fukai",
            "Hidehiko Ichimura",
            "Kyogo Kanazawa"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We first show (1) the importance of investigating health expenditure process\nusing the order two Markov chain model, rather than the standard order one\nmodel, which is widely used in the literature. Markov chain of order two is the\nminimal framework that is capable of distinguishing those who experience a\ncertain health expenditure level for the first time from those who have been\nexperiencing that or other levels for some time. In addition, using the model\nwe show (2) that the probability of encountering a health shock first de-\ncreases until around age 10, and then increases with age, particularly, after\nage 40, (3) that health shock distributions among different age groups do not\ndiffer until their percentiles reach the median range, but that above the\nmedian the health shock distributions of older age groups gradually start to\nfirst-order dominate those of younger groups, and (4) that the persistency of\nhealth shocks also shows a U-shape in relation to age.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.08746v1"
    },
    {
        "title": "Ordered Kripke Model, Permissibility, and Convergence of Probabilistic\n  Kripke Model",
        "authors": [
            "Shuige Liu"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We define a modification of the standard Kripke model, called the ordered\nKripke model, by introducing a linear order on the set of accessible states of\neach state. We first show this model can be used to describe the lexicographic\nbelief hierarchy in epistemic game theory, and perfect rationalizability can be\ncharacterized within this model. Then we show that each ordered Kripke model is\nthe limit of a sequence of standard probabilistic Kripke models with a modified\n(common) belief operator, in the senses of structure and the\n(epsilon-)permissibilities characterized within them.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.08767v1"
    },
    {
        "title": "How Can We Induce More Women to Competitions?",
        "authors": [
            "Masayuki Yagasaki",
            "Mitsunosuke Morishita"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Why women avoid participating in a competition and how can we encourage them\nto participate in it? In this paper, we investigate how social image concerns\naffect women's decision to compete. We first construct a theoretical model and\nshow that participating in a competition, even under affirmative action\npolicies favoring women, is costly for women under public observability since\nit deviates from traditional female gender norms, resulting in women's low\nappearance in competitive environments. We propose and theoretically show that\nintroducing prosocial incentives in the competitive environment is effective\nand robust to public observability since (i) it induces women who are\nintrinsically motivated by prosocial incentives to the competitive environment\nand (ii) it makes participating in a competition not costly for women from\nsocial image point of view. We conduct a laboratory experiment where we\nrandomly manipulate the public observability of decisions to compete and test\nour theoretical predictions. The results of the experiment are fairly\nconsistent with our theoretical predictions. We suggest that when designing\npolicies to promote gender equality in competitive environments, using\nprosocial incentives through company philanthropy or other social\nresponsibility policies, either as substitutes or as complements to traditional\naffirmative action policies, could be promising.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.10518v1"
    },
    {
        "title": "Hyper-rational choice theory",
        "authors": [
            "Madjid Eshaghi Gordji",
            "Gholamreza Askari"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The rational choice theory is based on this idea that people rationally\npursue goals for increasing their personal interests. In most conditions, the\nbehavior of an actor is not independent of the person and others' behavior.\nHere, we present a new concept of rational choice as a hyper-rational choice\nwhich in this concept, the actor thinks about profit or loss of other actors in\naddition to his personal profit or loss and then will choose an action which is\ndesirable to him. We implement the hyper-rational choice to generalize and\nexpand the game theory. Results of this study will help to model the behavior\nof people considering environmental conditions, the kind of behavior\ninteractive, valuation system of itself and others and system of beliefs and\ninternal values of societies. Hyper-rationality helps us understand how human\ndecision makers behave in interactive decisions.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.10520v2"
    },
    {
        "title": "Structural analysis with mixed-frequency data: A MIDAS-SVAR model of US\n  capital flows",
        "authors": [
            "Emanuele Bacchiocchi",
            "Andrea Bastianin",
            "Alessandro Missale",
            "Eduardo Rossi"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We develop a new VAR model for structural analysis with mixed-frequency data.\nThe MIDAS-SVAR model allows to identify structural dynamic links exploiting the\ninformation contained in variables sampled at different frequencies. It also\nprovides a general framework to test homogeneous frequency-based\nrepresentations versus mixed-frequency data models. A set of Monte Carlo\nexperiments suggests that the test performs well both in terms of size and\npower. The MIDAS-SVAR is then used to study how monetary policy and financial\nmarket volatility impact on the dynamics of gross capital inflows to the US.\nWhile no relation is found when using standard quarterly data, exploiting the\nvariability present in the series within the quarter shows that the effect of\nan interest rate shock is greater the longer the time lag between the month of\nthe shock and the end of the quarter\n",
        "pdf_link": "http://arxiv.org/pdf/1802.00793v1"
    },
    {
        "title": "An Experimental Investigation of Preference Misrepresentation in the\n  Residency Match",
        "authors": [
            "Alex Rees-Jones",
            "Samuel Skowronek"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The development and deployment of matching procedures that incentivize\ntruthful preference reporting is considered one of the major successes of\nmarket design research. In this study, we test the degree to which these\nprocedures succeed in eliminating preference misrepresentation. We administered\nan online experiment to 1,714 medical students immediately after their\nparticipation in the medical residency match--a leading field application of\nstrategy-proof market design. When placed in an analogous, incentivized\nmatching task, we find that 23% of participants misrepresent their preferences.\nWe explore the factors that predict preference misrepresentation, including\ncognitive ability, strategic positioning, overconfidence, expectations, advice,\nand trust. We discuss the implications of this behavior for the design of\nallocation mechanisms and the social welfare in markets that use them.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.01990v2"
    },
    {
        "title": "Prediction of Shared Bicycle Demand with Wavelet Thresholding",
        "authors": [
            "J. Christopher Westland",
            "Jian Mou",
            "Dafei Yin"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Consumers are creatures of habit, often periodic, tied to work, shopping and\nother schedules. We analyzed one month of data from the world's largest\nbike-sharing company to elicit demand behavioral cycles, initially using models\nfrom animal tracking that showed large customers fit an Ornstein-Uhlenbeck\nmodel with demand peaks at periodicities of 7, 12, 24 hour and 7-days. Lorenz\ncurves of bicycle demand showed that the majority of customer usage was\ninfrequent, and demand cycles from time-series models would strongly overfit\nthe data yielding unreliable models. Analysis of thresholded wavelets for the\nspace-time tensor of bike-sharing contracts was able to compress the data into\na 56-coefficient model with little loss of information, suggesting that\nbike-sharing demand behavior is exceptionally strong and regular. Improvements\nto predicted demand could be made by adjusting for 'noise' filtered by our\nmodel from air quality and weather information and demand from infrequent\nriders.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.02683v1"
    },
    {
        "title": "A General Method for Demand Inversion",
        "authors": [
            "Lixiong Li"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper describes a numerical method to solve for mean product qualities\nwhich equates the real market share to the market share predicted by a discrete\nchoice model. The method covers a general class of discrete choice model,\nincluding the pure characteristics model in Berry and Pakes(2007) and the\nrandom coefficient logit model in Berry et al.(1995) (hereafter BLP). The\nmethod transforms the original market share inversion problem to an\nunconstrained convex minimization problem, so that any convex programming\nalgorithm can be used to solve the inversion. Moreover, such results also imply\nthat the computational complexity of inverting a demand model should be no more\nthan that of a convex programming problem. In simulation examples, I show the\nmethod outperforms the contraction mapping algorithm in BLP. I also find the\nmethod remains robust in pure characteristics models with near-zero market\nshares.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.04444v3"
    },
    {
        "title": "Knowledge and Unanimous Acceptance of Core Payoffs: An Epistemic\n  Foundation for Cooperative Game Theory",
        "authors": [
            "Shuige Liu"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We provide an epistemic foundation for cooperative games by proof theory via\nstudying the knowledge for players unanimously accepting only core payoffs. We\nfirst transform each cooperative game into a decision problem where a player\ncan accept or reject any payoff vector offered to her based on her knowledge\nabout available cooperation. Then we use a modified KD-system in epistemic\nlogic, which can be regarded as a counterpart of the model for non-cooperative\ngames in Bonanno (2008), (2015), to describe a player's knowledge,\ndecision-making criterion, and reasoning process; especially, a formula called\nC-acceptability is defined to capture the criterion for accepting a core payoff\nvector. Within this syntactical framework, we characterize the core of a\ncooperative game in terms of players' knowledge. Based on that result, we\ndiscuss an epistemic inconsistency behind Debreu-Scarf Theorem, that is, the\nincrease of the number of replicas has invariant requirement on each\nparticipant's knowledge from the aspect of competitive market, while requires\nunbounded epistemic ability players from the aspect of cooperative game.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.04595v4"
    },
    {
        "title": "The dynamic impact of monetary policy on regional housing prices in the\n  US: Evidence based on factor-augmented vector autoregressions",
        "authors": [
            "Manfred M. Fischer",
            "Florian Huber",
            "Michael Pfarrhofer",
            "Petra Staufer-Steinnocher"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this study interest centers on regional differences in the response of\nhousing prices to monetary policy shocks in the US. We address this issue by\nanalyzing monthly home price data for metropolitan regions using a\nfactor-augmented vector autoregression (FAVAR) model. Bayesian model estimation\nis based on Gibbs sampling with Normal-Gamma shrinkage priors for the\nautoregressive coefficients and factor loadings, while monetary policy shocks\nare identified using high-frequency surprises around policy announcements as\nexternal instruments. The empirical results indicate that monetary policy\nactions typically have sizeable and significant positive effects on regional\nhousing prices, revealing differences in magnitude and duration. The largest\neffects are observed in regions located in states on both the East and West\nCoasts, notably California, Arizona and Florida.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.05870v1"
    },
    {
        "title": "On the iterated estimation of dynamic discrete choice games",
        "authors": [
            "Federico A. Bugni",
            "Jackson Bunting"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We study the asymptotic properties of a class of estimators of the structural\nparameters in dynamic discrete choice games. We consider K-stage policy\niteration (PI) estimators, where K denotes the number of policy iterations\nemployed in the estimation. This class nests several estimators proposed in the\nliterature such as those in Aguirregabiria and Mira (2002, 2007), Pesendorfer\nand Schmidt-Dengler (2008), and Pakes et al. (2007). First, we establish that\nthe K-PML estimator is consistent and asymptotically normal for all K. This\ncomplements findings in Aguirregabiria and Mira (2007), who focus on K=1 and K\nlarge enough to induce convergence of the estimator. Furthermore, we show under\ncertain conditions that the asymptotic variance of the K-PML estimator can\nexhibit arbitrary patterns as a function of K. Second, we establish that the\nK-MD estimator is consistent and asymptotically normal for all K. For a\nspecific weight matrix, the K-MD estimator has the same asymptotic distribution\nas the K-PML estimator. Our main result provides an optimal sequence of weight\nmatrices for the K-MD estimator and shows that the optimally weighted K-MD\nestimator has an asymptotic distribution that is invariant to K. The invariance\nresult is especially unexpected given the findings in Aguirregabiria and Mira\n(2007) for K-PML estimators. Our main result implies two new corollaries about\nthe optimal 1-MD estimator (derived by Pesendorfer and Schmidt-Dengler (2008)).\nFirst, the optimal 1-MD estimator is optimal in the class of K-MD estimators.\nIn other words, additional policy iterations do not provide asymptotic\nefficiency gains relative to the optimal 1-MD estimator. Second, the optimal\n1-MD estimator is more or equally asymptotically efficient than any K-PML\nestimator for all K. Finally, the appendix provides appropriate conditions\nunder which the optimal 1-MD estimator is asymptotically efficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.06665v4"
    },
    {
        "title": "Kernel Estimation for Panel Data with Heterogeneous Dynamics",
        "authors": [
            "Ryo Okui",
            "Takahide Yanagi"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper proposes nonparametric kernel-smoothing estimation for panel data\nto examine the degree of heterogeneity across cross-sectional units. We first\nestimate the sample mean, autocovariances, and autocorrelations for each unit\nand then apply kernel smoothing to compute their density functions. The\ndependence of the kernel estimator on bandwidth makes asymptotic bias of very\nhigh order affect the required condition on the relative magnitudes of the\ncross-sectional sample size (N) and the time-series length (T). In particular,\nit makes the condition on N and T stronger and more complicated than those\ntypically observed in the long-panel literature without kernel smoothing. We\nalso consider a split-panel jackknife method to correct bias and construction\nof confidence intervals. An empirical application and Monte Carlo simulations\nillustrate our procedure in finite samples.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.08825v4"
    },
    {
        "title": "Identifying the occurrence or non occurrence of cognitive bias in\n  situations resembling the Monty Hall problem",
        "authors": [
            "Fatemeh Borhani",
            "Edward J. Green"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  People reason heuristically in situations resembling inferential puzzles such\nas Bertrand's box paradox and the Monty Hall problem. The practical\nsignificance of that fact for economic decision making is uncertain because a\ndeparture from sound reasoning may, but does not necessarily, result in a\n\"cognitively biased\" outcome different from what sound reasoning would have\nproduced. Criteria are derived here, applicable to both experimental and\nnon-experimental situations, for heuristic reasoning in an inferential-puzzle\nsituations to result, or not to result, in cognitively bias. In some\nsituations, neither of these criteria is satisfied, and whether or not agents'\nposterior probability assessments or choices are cognitively biased cannot be\ndetermined.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.08935v1"
    },
    {
        "title": "On the solution of the variational optimisation in the rational\n  inattention framework",
        "authors": [
            "Nigar Hashimzade"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  I analyse the solution method for the variational optimisation problem in the\nrational inattention framework proposed by Christopher A. Sims. The solution,\nin general, does not exist, although it may exist in exceptional cases. I show\nthat the solution does not exist for the quadratic and the logarithmic\nobjective functions analysed by Sims (2003, 2006). For a linear-quadratic\nobjective function a solution can be constructed under restrictions on all but\none of its parameters. This approach is, therefore, unlikely to be applicable\nto a wider set of economic models.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.09869v2"
    },
    {
        "title": "A Bayesian panel VAR model to analyze the impact of climate change on\n  high-income economies",
        "authors": [
            "Florian Huber",
            "Tamás Krisztin",
            "Michael Pfarrhofer"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper, we assess the impact of climate shocks on futures markets for\nagricultural commodities and a set of macroeconomic quantities for multiple\nhigh-income economies. To capture relations among countries, markets, and\nclimate shocks, this paper proposes parsimonious methods to estimate\nhigh-dimensional panel VARs. We assume that coefficients associated with\ndomestic lagged endogenous variables arise from a Gaussian mixture model while\nfurther parsimony is achieved using suitable global-local shrinkage priors on\nseveral regions of the parameter space. Our results point towards pronounced\nglobal reactions of key macroeconomic quantities to climate shocks. Moreover,\nthe empirical findings highlight substantial linkages between regionally\nlocated climate shifts and global commodity markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.01554v3"
    },
    {
        "title": "Varying Random Coefficient Models",
        "authors": [
            "Christoph Breunig"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper provides a new methodology to analyze unobserved heterogeneity\nwhen observed characteristics are modeled nonlinearly. The proposed model\nbuilds on varying random coefficients (VRC) that are determined by nonlinear\nfunctions of observed regressors and additively separable unobservables. This\npaper proposes a novel estimator of the VRC density based on weighted sieve\nminimum distance. The main example of sieve bases are Hermite functions which\nyield a numerically stable estimation procedure. This paper shows inference\nresults that go beyond what has been shown in ordinary RC models. We provide in\neach case rates of convergence and also establish pointwise limit theory of\nlinear functionals, where a prominent example is the density of potential\noutcomes. In addition, a multiplier bootstrap procedure is proposed to\nconstruct uniform confidence bands. A Monte Carlo study examines finite sample\nproperties of the estimator and shows that it performs well even when the\nregressors associated to RC are far from being heavy tailed. Finally, the\nmethodology is applied to analyze heterogeneity in income elasticity of demand\nfor housing.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03110v4"
    },
    {
        "title": "Inference on Local Average Treatment Effects for Misclassified Treatment",
        "authors": [
            "Takahide Yanagi"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We develop point-identification for the local average treatment effect when\nthe binary treatment contains a measurement error. The standard instrumental\nvariable estimator is inconsistent for the parameter since the measurement\nerror is non-classical by construction. We correct the problem by identifying\nthe distribution of the measurement error based on the use of an exogenous\nvariable that can even be a binary covariate. The moment conditions derived\nfrom the identification lead to generalized method of moments estimation with\nasymptotically valid inferences. Monte Carlo simulations and an empirical\nillustration demonstrate the usefulness of the proposed procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03349v1"
    },
    {
        "title": "Shapley Value Methods for Attribution Modeling in Online Advertising",
        "authors": [
            "Kaifeng Zhao",
            "Seyed Hanif Mahboobi",
            "Saeed R. Bagheri"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper re-examines the Shapley value methods for attribution analysis in\nthe area of online advertising. As a credit allocation solution in cooperative\ngame theory, Shapley value method directly quantifies the contribution of\nonline advertising inputs to the advertising key performance indicator (KPI)\nacross multiple channels. We simplify its calculation by developing an\nalternative mathematical formulation. The new formula significantly improves\nthe computational efficiency and therefore extends the scope of applicability.\nBased on the simplified formula, we further develop the ordered Shapley value\nmethod. The proposed method is able to take into account the order of channels\nvisited by users. We claim that it provides a more comprehensive insight by\nevaluating the attribution of channels at different stages of user conversion\njourneys. The proposed approaches are illustrated using a real-world online\nadvertising campaign dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.05327v1"
    },
    {
        "title": "Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous\n  Treatment Effects",
        "authors": [
            "Liyang Sun",
            "Sarah Abraham"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  To estimate the dynamic effects of an absorbing treatment, researchers often\nuse two-way fixed effects regressions that include leads and lags of the\ntreatment. We show that in settings with variation in treatment timing across\nunits, the coefficient on a given lead or lag can be contaminated by effects\nfrom other periods, and apparent pretrends can arise solely from treatment\neffects heterogeneity. We propose an alternative estimator that is free of\ncontamination, and illustrate the relative shortcomings of two-way fixed\neffects regressions with leads and lags through an empirical application.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.05785v2"
    },
    {
        "title": "Revisiting the thermal and superthermal two-class distribution of\n  incomes: A critical perspective",
        "authors": [
            "Markus P. A. Schneider"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper offers a two-pronged critique of the empirical investigation of\nthe income distribution performed by physicists over the past decade. Their\nfinding rely on the graphical analysis of the observed distribution of\nnormalized incomes. Two central observations lead to the conclusion that the\nmajority of incomes are exponentially distributed, but neither each individual\npiece of evidence nor their concurrent observation robustly proves that the\nthermal and superthermal mixture fits the observed distribution of incomes\nbetter than reasonable alternatives. A formal analysis using popular measures\nof fit shows that while an exponential distribution with a power-law tail\nprovides a better fit of the IRS income data than the log-normal distribution\n(often assumed by economists), the thermal and superthermal mixture's fit can\nbe improved upon further by adding a log-normal component. The economic\nimplications of the thermal and superthermal distribution of incomes, and the\nexpanded mixture are explored in the paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06341v1"
    },
    {
        "title": "Estimating Treatment Effects in Mover Designs",
        "authors": [
            "Peter Hull"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Researchers increasingly leverage movement across multiple treatments to\nestimate causal effects. While these \"mover regressions\" are often motivated by\na linear constant-effects model, it is not clear what they capture under weaker\nquasi-experimental assumptions. I show that binary treatment mover regressions\nrecover a convex average of four difference-in-difference comparisons and are\nthus causally interpretable under a standard parallel trends assumption.\nEstimates from multiple-treatment models, however, need not be causal without\nstronger restrictions on the heterogeneity of treatment effects and\ntime-varying shocks. I propose a class of two-step estimators to isolate and\ncombine the large set of difference-in-difference quasi-experiments generated\nby a mover design, identifying mover average treatment effects under\nconditional-on-covariate parallel trends and effect homogeneity restrictions. I\ncharacterize the efficient estimators in this class and derive specification\ntests based on the model's overidentifying restrictions. Future drafts will\napply the theory to the Finkelstein et al. (2016) movers design, analyzing the\ncausal effects of geography on healthcare utilization.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06721v1"
    },
    {
        "title": "Transaction Costs in Collective Waste Recovery Systems in the EU",
        "authors": [
            "Shteryo Nozharov"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The study aims to identify the institutional flaws of the current EU waste\nmanagement model by analysing the economic model of extended producer\nresponsibility and collective waste management systems and to create a model\nfor measuring the transaction costs borne by waste recovery organizations. The\nmodel was approbated by analysing the Bulgarian collective waste management\nsystems that have been complying with the EU legislation for the last 10 years.\nThe analysis focuses on waste oils because of their economic importance and the\nlimited number of studies and analyses in this field as the predominant body of\nresearch to date has mainly addressed packaging waste, mixed household waste or\ndiscarded electrical and electronic equipment. The study aims to support the\nprocess of establishing a circular economy in the EU, which was initiated in\n2015.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06792v1"
    },
    {
        "title": "Empirical Equilibrium",
        "authors": [
            "Rodrigo A. Velez",
            "Alexander L. Brown"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We study the foundations of empirical equilibrium, a refinement of Nash\nequilibrium that is based on a non-parametric characterization of empirical\ndistributions of behavior in games (Velez and Brown,2020b arXiv:1907.12408).\nThe refinement can be alternatively defined as those Nash equilibria that do\nnot refute the regular QRE theory of Goeree, Holt, and Palfrey (2005). By\ncontrast, some empirical equilibria may refute monotone additive randomly\ndisturbed payoff models. As a by product, we show that empirical equilibrium\ndoes not coincide with refinements based on approximation by monotone additive\nrandomly disturbed payoff models, and further our understanding of the\nempirical content of these models.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.07986v3"
    },
    {
        "title": "Price Competition with Geometric Brownian motion in Exchange Rate\n  Uncertainty",
        "authors": [
            "Murat Erkoc",
            "Huaqing Wang",
            "Anas Ahmed"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We analyze an operational policy for a multinational manufacturer to hedge\nagainst exchange rate uncertainties and competition. We consider a single\nproduct and single period. Because of long-lead times, the capacity investment\nmust done before the selling season begins when the exchange rate between the\ntwo countries is uncertain. we consider a duopoly competition in the foreign\ncountry. We model the exchange rate as a random variable. We investigate the\nimpact of competition and exchange rate on optimal capacities and optimal\nprices. We show how competition can impact the decision of the home\nmanufacturer to enter the foreign market.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.08153v1"
    },
    {
        "title": "Statistical and Economic Evaluation of Time Series Models for\n  Forecasting Arrivals at Call Centers",
        "authors": [
            "Andrea Bastianin",
            "Marzio Galeotti",
            "Matteo Manera"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Call centers' managers are interested in obtaining accurate point and\ndistributional forecasts of call arrivals in order to achieve an optimal\nbalance between service quality and operating costs. We present a strategy for\nselecting forecast models of call arrivals which is based on three pillars: (i)\nflexibility of the loss function; (ii) statistical evaluation of forecast\naccuracy; (iii) economic evaluation of forecast performance using money\nmetrics. We implement fourteen time series models and seven forecast\ncombination schemes on three series of daily call arrivals. Although we focus\nmainly on point forecasts, we also analyze density forecast evaluation. We show\nthat second moments modeling is important both for point and density\nforecasting and that the simple Seasonal Random Walk model is always\noutperformed by more general specifications. Our results suggest that call\ncenter managers should invest in the use of forecast models which describe both\nfirst and second moments of call arrivals.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.08315v1"
    },
    {
        "title": "Economic inequality and Islamic Charity: An exploratory agent-based\n  modeling approach",
        "authors": [
            "Hossein Sabzian",
            "Alireza Aliahmadi",
            "Adel Azar",
            "Madjid Mirzaee"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Economic inequality is one of the pivotal issues for most of economic and\nsocial policy makers across the world to insure the sustainable economic growth\nand justice. In the mainstream school of economics, namely neoclassical\ntheories, economic issues are dealt with in a mechanistic manner. Such a\nmainstream framework is majorly focused on investigating a socio-economic\nsystem based on an axiomatic scheme where reductionism approach plays a vital\nrole. The major limitations of such theories include unbounded rationality of\neconomic agents, reducing the economic aggregates to a set of predictable\nfactors and lack of attention to adaptability and the evolutionary nature of\neconomic agents. In tackling deficiencies of conventional economic models, in\nthe past two decades, some new approaches have been recruited. One of those\nnovel approaches is the Complex adaptive systems (CAS) framework which has\nshown a very promising performance in action. In contrast to mainstream school,\nunder this framework, the economic phenomena are studied in an organic manner\nwhere the economic agents are supposed to be both boundedly rational and\nadaptive. According to it, the economic aggregates emerge out of the ways\nagents of a system decide and interact. As a powerful way of modeling CASs,\nAgent-based models (ABMs) has found a growing application among academicians\nand practitioners. ABMs show that how simple behavioral rules of agents and\nlocal interactions among them at micro-scale can generate surprisingly complex\npatterns at macro-scale. In this paper, ABMs have been used to show (1) how an\neconomic inequality emerges in a system and to explain (2) how sadaqah as an\nIslamic charity rule can majorly help alleviating the inequality and how\nresource allocation strategies taken by charity entities can accelerate this\nalleviation.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.09284v1"
    },
    {
        "title": "Aide et Croissance dans les pays de l'Union Economique et Mon{é}taire\n  Ouest Africaine (UEMOA) : retour sur une relation controvers{é}e",
        "authors": [
            "Nimonka Bayale"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The main purpose of this paper is to analyze threshold effects of official\ndevelopment assistance (ODA) on economic growth in WAEMU zone countries. To\nachieve this, the study is based on OECD and WDI data covering the period\n1980-2015 and used Hansen's Panel Threshold Regression (PTR) model to\n\"bootstrap\" aid threshold above which its effectiveness is effective. The\nevidence strongly supports the view that the relationship between aid and\neconomic growth is non-linear with a unique threshold which is 12.74% GDP.\nAbove this value, the marginal effect of aid is 0.69 points, \"all things being\nequal to otherwise\". One of the main contribution of this paper is to show that\nWAEMU countries need investments that could be covered by the foreign aid. This\nlater one should be considered just as a complementary resource. Thus, WEAMU\ncountries should continue to strengthen their efforts in internal resource\nmobilization in order to fulfil this need.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.00435v1"
    },
    {
        "title": "Endogenous growth - A dynamic technology augmentation of the Solow model",
        "authors": [
            "Murad Kasim"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper, I endeavour to construct a new model, by extending the classic\nexogenous economic growth model by including a measurement which tries to\nexplain and quantify the size of technological innovation ( A ) endogenously. I\ndo not agree technology is a \"constant\" exogenous variable, because it is\nhumans who create all technological innovations, and it depends on how much\nhuman and physical capital is allocated for its research. I inspect several\npossible approaches to do this, and then I test my model both against sample\nand real world evidence data. I call this method \"dynamic\" because it tries to\nmodel the details in resource allocations between research, labor and capital,\nby affecting each other interactively. In the end, I point out which is the new\nresidual and the parts of the economic growth model which can be further\nimproved.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.00668v1"
    },
    {
        "title": "Optimal Linear Instrumental Variables Approximations",
        "authors": [
            "Juan Carlos Escanciano",
            "Wei Li"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper studies the identification and estimation of the optimal linear\napproximation of a structural regression function. The parameter in the linear\napproximation is called the Optimal Linear Instrumental Variables Approximation\n(OLIVA). This paper shows that a necessary condition for standard inference on\nthe OLIVA is also sufficient for the existence of an IV estimand in a linear\nmodel. The instrument in the IV estimand is unknown and may not be identified.\nA Two-Step IV (TSIV) estimator based on Tikhonov regularization is proposed,\nwhich can be implemented by standard regression routines. We establish the\nasymptotic normality of the TSIV estimator assuming neither completeness nor\nidentification of the instrument. As an important application of our analysis,\nwe robustify the classical Hausman test for exogeneity against misspecification\nof the linear structural model. We also discuss extensions to weighted least\nsquares criteria. Monte Carlo simulations suggest an excellent finite sample\nperformance for the proposed inferences. Finally, in an empirical application\nestimating the elasticity of intertemporal substitution (EIS) with US data, we\nobtain TSIV estimates that are much larger than their standard IV counterparts,\nwith our robust Hausman test failing to reject the null hypothesis of\nexogeneity of real interest rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.03275v3"
    },
    {
        "title": "Sufficient Statistics for Unobserved Heterogeneity in Structural Dynamic\n  Logit Models",
        "authors": [
            "Victor Aguirregabiria",
            "Jiaying Gu",
            "Yao Luo"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We study the identification and estimation of structural parameters in\ndynamic panel data logit models where decisions are forward-looking and the\njoint distribution of unobserved heterogeneity and observable state variables\nis nonparametric, i.e., fixed-effects model. We consider models with two\nendogenous state variables: the lagged decision variable, and the time duration\nin the last choice. This class of models includes as particular cases important\neconomic applications such as models of market entry-exit, occupational choice,\nmachine replacement, inventory and investment decisions, or dynamic demand of\ndifferentiated products. The identification of structural parameters requires a\nsufficient statistic that controls for unobserved heterogeneity not only in\ncurrent utility but also in the continuation value of the forward-looking\ndecision problem. We obtain the minimal sufficient statistic and prove\nidentification of some structural parameters using a conditional likelihood\napproach. We apply this estimator to a machine replacement model.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.04048v1"
    },
    {
        "title": "Density Forecasts in Panel Data Models: A Semiparametric Bayesian\n  Perspective",
        "authors": [
            "Laura Liu"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper constructs individual-specific density forecasts for a panel of\nfirms or households using a dynamic linear model with common and heterogeneous\ncoefficients as well as cross-sectional heteroskedasticity. The panel\nconsidered in this paper features a large cross-sectional dimension N but short\ntime series T. Due to the short T, traditional methods have difficulty in\ndisentangling the heterogeneous parameters from the shocks, which contaminates\nthe estimates of the heterogeneous parameters. To tackle this problem, I assume\nthat there is an underlying distribution of heterogeneous parameters, model\nthis distribution nonparametrically allowing for correlation between\nheterogeneous parameters and initial conditions as well as individual-specific\nregressors, and then estimate this distribution by combining information from\nthe whole panel. Theoretically, I prove that in cross-sectional homoskedastic\ncases, both the estimated common parameters and the estimated distribution of\nthe heterogeneous parameters achieve posterior consistency, and that the\ndensity forecasts asymptotically converge to the oracle forecast.\nMethodologically, I develop a simulation-based posterior sampling algorithm\nspecifically addressing the nonparametric density estimation of unobserved\nheterogeneous parameters. Monte Carlo simulations and an empirical application\nto young firm dynamics demonstrate improvements in density forecasts relative\nto alternative approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.04178v3"
    },
    {
        "title": "Efficiency in Micro-Behaviors and FL Bias",
        "authors": [
            "Kurihara Kazutaka",
            "Yohei Tutiya"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper, we propose a model which simulates odds distributions of\npari-mutuel betting system under two hypotheses on the behavior of bettors: 1.\nThe amount of bets increases very rapidly as the deadline for betting comes\nnear. 2. Each bettor bets on a horse which gives the largest expectation value\nof the benefit. The results can be interpreted as such efficient behaviors do\nnot serve to extinguish the FL bias but even produce stronger FL bias.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.04225v1"
    },
    {
        "title": "The Finite Sample Performance of Treatment Effects Estimators based on\n  the Lasso",
        "authors": [
            "Michael Zimmert"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper contributes to the literature on treatment effects estimation with\nmachine learning inspired methods by studying the performance of different\nestimators based on the Lasso. Building on recent work in the field of\nhigh-dimensional statistics, we use the semiparametric efficient score\nestimation structure to compare different estimators. Alternative weighting\nschemes are considered and their suitability for the incorporation of machine\nlearning estimators is assessed using theoretical arguments and various Monte\nCarlo experiments. Additionally we propose an own estimator based on doubly\nrobust Kernel matching that is argued to be more robust to nuisance parameter\nmisspecification. In the simulation study we verify theory based intuition and\nfind good finite sample properties of alternative weighting scheme estimators\nlike the one we propose.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.05067v1"
    },
    {
        "title": "Data-Driven Investment Decision-Making: Applying Moore's Law and\n  S-Curves to Business Strategies",
        "authors": [
            "Christopher L. Benson",
            "Christopher L. Magee"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper introduces a method for linking technological improvement rates\n(i.e. Moore's Law) and technology adoption curves (i.e. S-Curves). There has\nbeen considerable research surrounding Moore's Law and the generalized versions\napplied to the time dependence of performance for other technologies. The prior\nwork has culminated with methodology for quantitative estimation of\ntechnological improvement rates for nearly any technology. This paper examines\nthe implications of such regular time dependence for performance upon the\ntiming of key events in the technological adoption process. We propose a simple\ncrossover point in performance which is based upon the technological\nimprovement rates and current level differences for target and replacement\ntechnologies. The timing for the cross-over is hypothesized as corresponding to\nthe first 'knee'? in the technology adoption \"S-curve\" and signals when the\nmarket for a given technology will start to be rewarding for innovators. This\nis also when potential entrants are likely to intensely experiment with\nproduct-market fit and when the competition to achieve a dominant design\nbegins. This conceptual framework is then back-tested by examining two\ntechnological changes brought about by the internet, namely music and video\ntransmission. The uncertainty analysis around the cases highlight opportunities\nfor organizations to reduce future technological uncertainty. Overall, the\nresults from the case studies support the reliability and utility of the\nconceptual framework in strategic business decision-making with the caveat that\nwhile technical uncertainty is reduced, it is not eliminated.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06339v1"
    },
    {
        "title": "Happy family of stable marriages",
        "authors": [
            "Gershon Wolansky"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Some aspects of the problem of stable marriage are discussed. There are two\ndistinguished marriage plans: the fully transferable case, where money can be\ntransferred between the participants, and the fully non transferable case where\neach participant has its own rigid preference list regarding the other gender.\nWe continue to discuss intermediate partial transferable cases. Partial\ntransferable plans can be approached as either special cases of cooperative\ngames using the notion of a core, or as a generalization of the cyclical\nmonotonicity property of the fully transferable case (fake promises). We shall\nintroduced these two approaches, and prove the existence of stable marriage for\nthe fully transferable and non-transferable plans.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06687v1"
    },
    {
        "title": "Bitcoin price and its marginal cost of production: support for a\n  fundamental value",
        "authors": [
            "Adam Hayes"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This study back-tests a marginal cost of production model proposed to value\nthe digital currency bitcoin. Results from both conventional regression and\nvector autoregression (VAR) models show that the marginal cost of production\nplays an important role in explaining bitcoin prices, challenging recent\nallegations that bitcoins are essentially worthless. Even with markets pricing\nbitcoin in the thousands of dollars each, the valuation model seems robust. The\ndata show that a price bubble that began in the Fall of 2017 resolved itself in\nearly 2018, converging with the marginal cost model. This suggests that while\nbubbles may appear in the bitcoin market, prices will tend to this bound and\nnot collapse to zero.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.07610v1"
    },
    {
        "title": "Model Selection in Time Series Analysis: Using Information Criteria as\n  an Alternative to Hypothesis Testing",
        "authors": [
            "R. Scott Hacker",
            "Abdulnasser Hatemi-J"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The issue of model selection in applied research is of vital importance.\nSince the true model in such research is not known, which model should be used\nfrom among various potential ones is an empirical question. There might exist\nseveral competitive models. A typical approach to dealing with this is classic\nhypothesis testing using an arbitrarily chosen significance level based on the\nunderlying assumption that a true null hypothesis exists. In this paper we\ninvestigate how successful this approach is in determining the correct model\nfor different data generating processes using time series data. An alternative\napproach based on more formal model selection techniques using an information\ncriterion or cross-validation is suggested and evaluated in the time series\nenvironment via Monte Carlo experiments. This paper also explores the\neffectiveness of deciding what type of general relation exists between two\nvariables (e.g. relation in levels or relation in first differences) using\nvarious strategies based on hypothesis testing and on information criteria with\nthe presence or absence of unit roots.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.08991v1"
    },
    {
        "title": "A Double Machine Learning Approach to Estimate the Effects of Musical\n  Practice on Student's Skills",
        "authors": [
            "Michael C. Knaus"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This study investigates the dose-response effects of making music on youth\ndevelopment. Identification is based on the conditional independence assumption\nand estimation is implemented using a recent double machine learning estimator.\nThe study proposes solutions to two highly practically relevant questions that\narise for these new methods: (i) How to investigate sensitivity of estimates to\ntuning parameter choices in the machine learning part? (ii) How to assess\ncovariate balancing in high-dimensional settings? The results show that\nimprovements in objectively measured cognitive skills require at least medium\nintensity, while improvements in school grades are already observed for low\nintensity of practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.10300v2"
    },
    {
        "title": "Flexible shrinkage in high-dimensional Bayesian spatial autoregressive\n  models",
        "authors": [
            "Michael Pfarrhofer",
            "Philipp Piribauer"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This article introduces two absolutely continuous global-local shrinkage\npriors to enable stochastic variable selection in the context of\nhigh-dimensional matrix exponential spatial specifications. Existing approaches\nas a means to dealing with overparameterization problems in spatial\nautoregressive specifications typically rely on computationally demanding\nBayesian model-averaging techniques. The proposed shrinkage priors can be\nimplemented using Markov chain Monte Carlo methods in a flexible and efficient\nway. A simulation study is conducted to evaluate the performance of each of the\nshrinkage priors. Results suggest that they perform particularly well in\nhigh-dimensional environments, especially when the number of parameters to\nestimate exceeds the number of observations. For an empirical illustration we\nuse pan-European regional economic growth data.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.10822v1"
    },
    {
        "title": "Tilting Approximate Models",
        "authors": [
            "Andreas Tryphonides"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Model approximations are common practice when estimating structural or\nquasi-structural models. The paper considers the econometric properties of\nestimators that utilize projections to reimpose information about the exact\nmodel in the form of conditional moments. The resulting estimator efficiently\ncombines the information provided by the approximate law of motion and the\nmoment conditions. The paper develops the corresponding asymptotic theory and\nprovides simulation evidence that tilting substantially reduces the mean\nsquared error for parameter estimates. It applies the methodology to pricing\nlong-run risks in aggregate consumption in the US, whereas the model is solved\nusing the Campbell and Shiller (1988) approximation. Tilting improves empirical\nfit and results suggest that approximation error is a source of upward bias in\nestimates of risk aversion and downward bias in the elasticity of intertemporal\nsubstitution.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.10869v5"
    },
    {
        "title": "Modeling the residential electricity consumption within a restructured\n  power market",
        "authors": [
            "Chelsea Sun"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The United States' power market is featured by the lack of judicial power at\nthe federal level. The market thus provides a unique testing environment for\nthe market organization structure. At the same time, the econometric modeling\nand forecasting of electricity market consumption become more challenging.\nImport and export, which generally follow simple rules in European countries,\ncan be a result of direct market behaviors. This paper seeks to build a general\nmodel for power consumption and using the model to test several hypotheses.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.11138v2"
    },
    {
        "title": "Estimation and Inference for Policy Relevant Treatment Effects",
        "authors": [
            "Yuya Sasaki",
            "Takuya Ura"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The policy relevant treatment effect (PRTE) measures the average effect of\nswitching from a status-quo policy to a counterfactual policy. Estimation of\nthe PRTE involves estimation of multiple preliminary parameters, including\npropensity scores, conditional expectation functions of the outcome and\ncovariates given the propensity score, and marginal treatment effects. These\npreliminary estimators can affect the asymptotic distribution of the PRTE\nestimator in complicated and intractable manners. In this light, we propose an\northogonal score for double debiased estimation of the PRTE, whereby the\nasymptotic distribution of the PRTE estimator is obtained without any influence\nof preliminary parameter estimators as far as they satisfy mild requirements of\nconvergence rates. To our knowledge, this paper is the first to develop limit\ndistribution theories for inference about the PRTE.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.11503v4"
    },
    {
        "title": "Proxy Controls and Panel Data",
        "authors": [
            "Ben Deaner"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We provide new results for nonparametric identification, estimation, and\ninference of causal effects using `proxy controls': observables that are noisy\nbut informative proxies for unobserved confounding factors. Our analysis\napplies to cross-sectional settings but is particularly well-suited to panel\nmodels. Our identification results motivate a simple and `well-posed'\nnonparametric estimator. We derive convergence rates for the estimator and\nconstruct uniform confidence bands with asymptotically correct size. In panel\nsettings, our methods provide a novel approach to the difficult problem of\nidentification with non-separable, general heterogeneity and fixed $T$. In\npanels, observations from different periods serve as proxies for unobserved\nheterogeneity and our key identifying assumptions follow from restrictions on\nthe serial dependence structure. We apply our methods to two empirical\nsettings. We estimate consumer demand counterfactuals using panel data and we\nestimate causal effects of grade retention on cognitive performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.00283v8"
    },
    {
        "title": "Nonparametric Regression with Selectively Missing Covariates",
        "authors": [
            "Christoph Breunig",
            "Peter Haan"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We consider the problem of regression with selectively observed covariates in\na nonparametric framework. Our approach relies on instrumental variables that\nexplain variation in the latent covariates but have no direct effect on\nselection. The regression function of interest is shown to be a weighted\nversion of observed conditional expectation where the weighting function is a\nfraction of selection probabilities. Nonparametric identification of the\nfractional probability weight (FPW) function is achieved via a partial\ncompleteness assumption. We provide primitive functional form assumptions for\npartial completeness to hold. The identification result is constructive for the\nFPW series estimator. We derive the rate of convergence and also the pointwise\nasymptotic distribution. In both cases, the asymptotic performance of the FPW\nseries estimator does not suffer from the inverse problem which derives from\nthe nonparametric instrumental variable approach. In a Monte Carlo study, we\nanalyze the finite sample properties of our estimator and we compare our\napproach to inverse probability weighting, which can be used alternatively for\nunconditional moment estimation. In the empirical application, we focus on two\ndifferent applications. We estimate the association between income and health\nusing linked data from the SHARE survey and administrative pension information\nand use pension entitlements as an instrument. In the second application we\nrevisit the question how income affects the demand for housing based on data\nfrom the German Socio-Economic Panel Study (SOEP). In this application we use\nregional income information on the residential block level as an instrument. In\nboth applications we show that income is selectively missing and we demonstrate\nthat standard methods that do not account for the nonrandom selection process\nlead to significantly biased estimates for individuals with low income.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.00411v4"
    },
    {
        "title": "Granger causality on horizontal sum of Boolean algebras",
        "authors": [
            "M. Bohdalová",
            "M. Kalina",
            "O. Nánásiová"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The intention of this paper is to discuss the mathematical model of causality\nintroduced by C.W.J. Granger in 1969. The Granger's model of causality has\nbecome well-known and often used in various econometric models describing\ncausal systems, e.g., between commodity prices and exchange rates.\n  Our paper presents a new mathematical model of causality between two measured\nobjects. We have slightly modified the well-known Kolmogorovian probability\nmodel. In particular, we use the horizontal sum of set $\\sigma$-algebras\ninstead of their direct product.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.01654v1"
    },
    {
        "title": "On LASSO for Predictive Regression",
        "authors": [
            "Ji Hyung Lee",
            "Zhentao Shi",
            "Zhan Gao"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Explanatory variables in a predictive regression typically exhibit low signal\nstrength and various degrees of persistence. Variable selection in such a\ncontext is of great importance. In this paper, we explore the pitfalls and\npossibilities of the LASSO methods in this predictive regression framework. In\nthe presence of stationary, local unit root, and cointegrated predictors, we\nshow that the adaptive LASSO cannot asymptotically eliminate all cointegrating\nvariables with zero regression coefficients. This new finding motivates a novel\npost-selection adaptive LASSO, which we call the twin adaptive LASSO (TAlasso),\nto restore variable selection consistency. Accommodating the system of\nheterogeneous regressors, TAlasso achieves the well-known oracle property. In\ncontrast, conventional LASSO fails to attain coefficient estimation consistency\nand variable screening in all components simultaneously. We apply these LASSO\nmethods to evaluate the short- and long-horizon predictability of S\\&P 500\nexcess returns.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.03140v4"
    },
    {
        "title": "Simple Inference on Functionals of Set-Identified Parameters Defined by\n  Linear Moments",
        "authors": [
            "JoonHwan Cho",
            "Thomas M. Russell"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper proposes a new approach to obtain uniformly valid inference for\nlinear functionals or scalar subvectors of a partially identified parameter\ndefined by linear moment inequalities. The procedure amounts to bootstrapping\nthe value functions of randomly perturbed linear programming problems, and does\nnot require the researcher to grid over the parameter space. The low-level\nconditions for uniform validity rely on genericity results for linear programs.\nThe unconventional perturbation approach produces a confidence set with a\ncoverage probability of 1 over the identified set, but obtains exact coverage\non an outer set, is valid under weak assumptions, and is computationally simple\nto implement.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.03180v10"
    },
    {
        "title": "The Incidental Parameters Problem in Testing for Remaining Cross-section\n  Correlation",
        "authors": [
            "Arturas Juodis",
            "Simon Reese"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper we consider the properties of the Pesaran (2004, 2015a) CD test\nfor cross-section correlation when applied to residuals obtained from panel\ndata models with many estimated parameters. We show that the presence of\nperiod-specific parameters leads the CD test statistic to diverge as length of\nthe time dimension of the sample grows. This result holds even if cross-section\ndependence is correctly accounted for and hence constitutes an example of the\nIncidental Parameters Problem. The relevance of this problem is investigated\nboth for the classical Time Fixed Effects estimator as well as the Common\nCorrelated Effects estimator of Pesaran (2006). We suggest a weighted CD test\nstatistic which re-establishes standard normal inference under the null\nhypothesis. Given the widespread use of the CD test statistic to test for\nremaining cross-section correlation, our results have far reaching implications\nfor empirical researchers.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.03715v4"
    },
    {
        "title": "Prices, Profits, Proxies, and Production",
        "authors": [
            "Victor H. Aguiar",
            "Nail Kashaev",
            "Roy Allen"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper studies nonparametric identification and counterfactual bounds for\nheterogeneous firms that can be ranked in terms of productivity. Our approach\nworks when quantities and prices are latent, rendering standard approaches\ninapplicable. Instead, we require observation of profits or other\noptimizing-values such as costs or revenues, and either prices or price proxies\nof flexibly chosen variables. We extend classical duality results for\nprice-taking firms to a setup with discrete heterogeneity, endogeneity, and\nlimited variation in possibly latent prices. Finally, we show that convergence\nresults for nonparametric estimators may be directly converted to convergence\nresults for production sets.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.04697v4"
    },
    {
        "title": "Stochastic Revealed Preferences with Measurement Error",
        "authors": [
            "Victor H. Aguiar",
            "Nail Kashaev"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  A long-standing question about consumer behavior is whether individuals'\nobserved purchase decisions satisfy the revealed preference (RP) axioms of the\nutility maximization theory (UMT). Researchers using survey or experimental\npanel data sets on prices and consumption to answer this question face the\nwell-known problem of measurement error. We show that ignoring measurement\nerror in the RP approach may lead to overrejection of the UMT. To solve this\nproblem, we propose a new statistical RP framework for consumption panel data\nsets that allows for testing the UMT in the presence of measurement error. Our\ntest is applicable to all consumer models that can be characterized by their\nfirst-order conditions. Our approach is nonparametric, allows for unrestricted\nheterogeneity in preferences, and requires only a centering condition on\nmeasurement error. We develop two applications that provide new evidence about\nthe UMT. First, we find support in a survey data set for the dynamic and\ntime-consistent UMT in single-individual households, in the presence of\n\\emph{nonclassical} measurement error in consumption. In the second\napplication, we cannot reject the static UMT in a widely used experimental data\nset in which measurement error in prices is assumed to be the result of price\nmisperception due to the experimental design. The first finding stands in\ncontrast to the conclusions drawn from the deterministic RP test of Browning\n(1989). The second finding reverses the conclusions drawn from the\ndeterministic RP test of Afriat (1967) and Varian (1982).\n",
        "pdf_link": "http://arxiv.org/pdf/1810.05287v2"
    },
    {
        "title": "Using generalized estimating equations to estimate nonlinear models with\n  spatial data",
        "authors": [
            "Cuicui Lu",
            "Weining Wang",
            "Jeffrey M. Wooldridge"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper, we study estimation of nonlinear models with cross sectional\ndata using two-step generalized estimating equations (GEE) in the quasi-maximum\nlikelihood estimation (QMLE) framework. In the interest of improving\nefficiency, we propose a grouping estimator to account for the potential\nspatial correlation in the underlying innovations. We use a Poisson model and a\nNegative Binomial II model for count data and a Probit model for binary\nresponse data to demonstrate the GEE procedure. Under mild weak dependency\nassumptions, results on estimation consistency and asymptotic normality are\nprovided. Monte Carlo simulations show efficiency gain of our approach in\ncomparison of different estimation methods for count data and binary response\ndata. Finally we apply the GEE approach to study the determinants of the inflow\nforeign direct investment (FDI) to China.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.05855v1"
    },
    {
        "title": "A Consistent Heteroskedasticity Robust LM Type Specification Test for\n  Semiparametric Models",
        "authors": [
            "Ivan Korolev"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper develops a consistent heteroskedasticity robust Lagrange\nMultiplier (LM) type specification test for semiparametric conditional mean\nmodels. Consistency is achieved by turning a conditional moment restriction\ninto a growing number of unconditional moment restrictions using series\nmethods. The proposed test statistic is straightforward to compute and is\nasymptotically standard normal under the null. Compared with the earlier\nliterature on series-based specification tests in parametric models, I rely on\nthe projection property of series estimators and derive a different\nnormalization of the test statistic. Compared with the recent test in Gupta\n(2018), I use a different way of accounting for heteroskedasticity. I\ndemonstrate using Monte Carlo studies that my test has superior finite sample\nperformance compared with the existing tests. I apply the test to one of the\nsemiparametric gasoline demand specifications from Yatchew and No (2001) and\nfind no evidence against it.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.07620v3"
    },
    {
        "title": "Treatment Effect Models with Strategic Interaction in Treatment\n  Decisions",
        "authors": [
            "Tadao Hoshino",
            "Takahide Yanagi"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This study considers treatment effect models in which others' treatment\ndecisions can affect both one's own treatment and outcome. Focusing on the case\nof two-player interactions, we formulate treatment decision behavior as a\ncomplete information game with multiple equilibria. Using a latent index\nframework and assuming a stochastic equilibrium selection, we prove that the\nmarginal treatment effect from one's own treatment and that from the partner\nare identifiable on the conditional supports of certain threshold variables\ndetermined through the game model. Based on our constructive identification\nresults, we propose a two-step semiparametric procedure for estimating the\nmarginal treatment effects using series approximation. We show that the\nproposed estimator is uniformly consistent and asymptotically normally\ndistributed. As an empirical illustration, we investigate the impacts of risky\nbehaviors on adolescents' academic performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.08350v11"
    },
    {
        "title": "Probabilistic Forecasting in Day-Ahead Electricity Markets: Simulating\n  Peak and Off-Peak Prices",
        "authors": [
            "Peru Muniain",
            "Florian Ziel"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper we include dependency structures for electricity price\nforecasting and forecasting evaluation. We work with off-peak and peak time\nseries from the German-Austrian day-ahead price, hence we analyze bivariate\ndata. We first estimate the mean of the two time series, and then in a second\nstep we estimate the residuals. The mean equation is estimated by OLS and\nelastic net and the residuals are estimated by maximum likelihood. Our\ncontribution is to include a bivariate jump component on a mean reverting jump\ndiffusion model in the residuals. The models' forecasts are evaluated using\nfour different criteria, including the energy score to measure whether the\ncorrelation structure between the time series is properly included or not. In\nthe results it is observed that the models with bivariate jumps provide better\nresults with the energy score, which means that it is important to consider\nthis structure in order to properly forecast correlated time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.08418v2"
    },
    {
        "title": "Factor-Driven Two-Regime Regression",
        "authors": [
            "Sokbae Lee",
            "Yuan Liao",
            "Myung Hwan Seo",
            "Youngki Shin"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We propose a novel two-regime regression model where regime switching is\ndriven by a vector of possibly unobservable factors. When the factors are\nlatent, we estimate them by the principal component analysis of a panel data\nset. We show that the optimization problem can be reformulated as mixed integer\noptimization, and we present two alternative computational algorithms. We\nderive the asymptotic distribution of the resulting estimator under the scheme\nthat the threshold effect shrinks to zero. In particular, we establish a phase\ntransition that describes the effect of first-stage factor estimation as the\ncross-sectional dimension of panel data increases relative to the time-series\ndimension. Moreover, we develop bootstrap inference and illustrate our methods\nvia numerical studies.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.11109v4"
    },
    {
        "title": "Semiparametrically efficient estimation of the average linear regression\n  function",
        "authors": [
            "Bryan S. Graham",
            "Cristine Campos de Xavier Pinto"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Let Y be an outcome of interest, X a vector of treatment measures, and W a\nvector of pre-treatment control variables. Here X may include (combinations of)\ncontinuous, discrete, and/or non-mutually exclusive \"treatments\". Consider the\nlinear regression of Y onto X in a subpopulation homogenous in W = w (formally\na conditional linear predictor). Let b0(w) be the coefficient vector on X in\nthis regression. We introduce a semiparametrically efficient estimate of the\naverage beta0 = E[b0(W)]. When X is binary-valued (multi-valued) our procedure\nrecovers the (a vector of) average treatment effect(s). When X is\ncontinuously-valued, or consists of multiple non-exclusive treatments, our\nestimand coincides with the average partial effect (APE) of X on Y when the\nunderlying potential response function is linear in X, but otherwise\nheterogenous across agents. When the potential response function takes a\ngeneral nonlinear/heterogenous form, and X is continuously-valued, our\nprocedure recovers a weighted average of the gradient of this response across\nindividuals and values of X. We provide a simple, and semiparametrically\nefficient, method of covariate adjustment for settings with complicated\ntreatment regimes. Our method generalizes familiar methods of covariate\nadjustment used for program evaluation as well as methods of semiparametric\nregression (e.g., the partially linear regression model).\n",
        "pdf_link": "http://arxiv.org/pdf/1810.12511v1"
    },
    {
        "title": "Machine Learning Estimation of Heterogeneous Causal Effects: Empirical\n  Monte Carlo Evidence",
        "authors": [
            "Michael C. Knaus",
            "Michael Lechner",
            "Anthony Strittmatter"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We investigate the finite sample performance of causal machine learning\nestimators for heterogeneous causal effects at different aggregation levels. We\nemploy an Empirical Monte Carlo Study that relies on arguably realistic data\ngeneration processes (DGPs) based on actual data. We consider 24 different\nDGPs, eleven different causal machine learning estimators, and three\naggregation levels of the estimated effects. In the main DGPs, we allow for\nselection into treatment based on a rich set of observable covariates. We\nprovide evidence that the estimators can be categorized into three groups. The\nfirst group performs consistently well across all DGPs and aggregation levels.\nThese estimators have multiple steps to account for the selection into the\ntreatment and the outcome process. The second group shows competitive\nperformance only for particular DGPs. The third group is clearly outperformed\nby the other estimators.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.13237v2"
    },
    {
        "title": "Column Generation Algorithms for Nonparametric Analysis of Random\n  Utility Models",
        "authors": [
            "Bart Smeulders"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Kitamura and Stoye (2014) develop a nonparametric test for linear inequality\nconstraints, when these are are represented as vertices of a polyhedron instead\nof its faces. They implement this test for an application to nonparametric\ntests of Random Utility Models. As they note in their paper, testing such\nmodels is computationally challenging. In this paper, we develop and implement\nmore efficient algorithms, based on column generation, to carry out the test.\nThese improved algorithms allow us to tackle larger datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.01400v1"
    },
    {
        "title": "Doubly Robust Difference-in-Differences Estimators",
        "authors": [
            "Pedro H. C. Sant'Anna",
            "Jun B. Zhao"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This article proposes doubly robust estimators for the average treatment\neffect on the treated (ATT) in difference-in-differences (DID) research\ndesigns. In contrast to alternative DID estimators, the proposed estimators are\nconsistent if either (but not necessarily both) a propensity score or outcome\nregression working models are correctly specified. We also derive the\nsemiparametric efficiency bound for the ATT in DID designs when either panel or\nrepeated cross-section data are available, and show that our proposed\nestimators attain the semiparametric efficiency bound when the working models\nare correctly specified. Furthermore, we quantify the potential efficiency\ngains of having access to panel data instead of repeated cross-section data.\nFinally, by paying articular attention to the estimation method used to\nestimate the nuisance parameters, we show that one can sometimes construct\ndoubly robust DID estimators for the ATT that are also doubly robust for\ninference. Simulation studies and an empirical application illustrate the\ndesirable finite-sample performance of the proposed estimators. Open-source\nsoftware for implementing the proposed policy evaluation tools is available.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.01723v3"
    },
    {
        "title": "Identifying the Effect of Persuasion",
        "authors": [
            "Sung Jae Jun",
            "Sokbae Lee"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper examines a commonly used measure of persuasion whose precise\ninterpretation has been obscure in the literature. By using the potential\noutcome framework, we define the causal persuasion rate by a proper conditional\nprobability of taking the action of interest with a persuasive message\nconditional on not taking the action without the message. We then formally\nstudy identification under empirically relevant data scenarios and show that\nthe commonly adopted measure generally does not estimate, but often overstates,\nthe causal rate of persuasion. We discuss several new parameters of interest\nand provide practical methods for causal inference.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.02276v6"
    },
    {
        "title": "A supreme test for periodic explosive GARCH",
        "authors": [
            "Stefan Richter",
            "Weining Wang",
            "Wei Biao Wu"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We develop a uniform test for detecting and dating explosive behavior of a\nstrictly stationary GARCH$(r,s)$ (generalized autoregressive conditional\nheteroskedasticity) process. Namely, we test the null hypothesis of a globally\nstable GARCH process with constant parameters against an alternative where\nthere is an 'abnormal' period with changed parameter values. During this\nperiod, the change may lead to an explosive behavior of the volatility process.\nIt is assumed that both the magnitude and the timing of the breaks are unknown.\nWe develop a double supreme test for the existence of a break, and then provide\nan algorithm to identify the period of change. Our theoretical results hold\nunder mild moment assumptions on the innovations of the GARCH process.\nTechnically, the existing properties for the QMLE in the GARCH model need to be\nreinvestigated to hold uniformly over all possible periods of change. The key\nresults involve a uniform weak Bahadur representation for the estimated\nparameters, which leads to weak convergence of the test statistic to the\nsupreme of a Gaussian Process. In simulations we show that the test has good\nsize and power for reasonably large time series lengths. We apply the test to\nApple asset returns and Bitcoin returns.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.03475v1"
    },
    {
        "title": "What Is the Value Added by Using Causal Machine Learning Methods in a\n  Welfare Experiment Evaluation?",
        "authors": [
            "Anthony Strittmatter"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Recent studies have proposed causal machine learning (CML) methods to\nestimate conditional average treatment effects (CATEs). In this study, I\ninvestigate whether CML methods add value compared to conventional CATE\nestimators by re-evaluating Connecticut's Jobs First welfare experiment. This\nexperiment entails a mix of positive and negative work incentives. Previous\nstudies show that it is hard to tackle the effect heterogeneity of Jobs First\nby means of CATEs. I report evidence that CML methods can provide support for\nthe theoretical labor supply predictions. Furthermore, I document reasons why\nsome conventional CATE estimators fail and discuss the limitations of CML\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.06533v3"
    },
    {
        "title": "Fuzzy Difference-in-Discontinuities: Identification Theory and\n  Application to the Affordable Care Act",
        "authors": [
            "Hector Galindo-Silva",
            "Nibene Habib Some",
            "Guy Tchuente"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper explores the use of a fuzzy regression discontinuity design where\nmultiple treatments are applied at the threshold. The identification results\nshow that, under the very strong assumption that the change in the probability\nof treatment at the cutoff is equal across treatments, a\ndifference-in-discontinuities estimator identifies the treatment effect of\ninterest. The point estimates of the treatment effect using a simple fuzzy\ndifference-in-discontinuities design are biased if the change in the\nprobability of a treatment applying at the cutoff differs across treatments.\nModifications of the fuzzy difference-in-discontinuities approach that rely on\nmilder assumptions are also proposed. Our results suggest caution is needed\nwhen applying before-and-after methods in the presence of fuzzy\ndiscontinuities. Using data from the National Health Interview Survey, we apply\nthis new identification strategy to evaluate the causal effect of the\nAffordable Care Act (ACA) on older Americans' health care access and\nutilization.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.06537v3"
    },
    {
        "title": "Approximate State Space Modelling of Unobserved Fractional Components",
        "authors": [
            "Tobias Hartl",
            "Roland Weigand"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We propose convenient inferential methods for potentially nonstationary\nmultivariate unobserved components models with fractional integration and\ncointegration. Based on finite-order ARMA approximations in the state space\nrepresentation, maximum likelihood estimation can make use of the EM algorithm\nand related techniques. The approximation outperforms the frequently used\nautoregressive or moving average truncation, both in terms of computational\ncosts and with respect to approximation quality. Monte Carlo simulations reveal\ngood estimation properties of the proposed methods for processes of different\ncomplexity and dimension.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09142v3"
    },
    {
        "title": "Multivariate Fractional Components Analysis",
        "authors": [
            "Tobias Hartl",
            "Roland Weigand"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We propose a setup for fractionally cointegrated time series which is\nformulated in terms of latent integrated and short-memory components. It\naccommodates nonstationary processes with different fractional orders and\ncointegration of different strengths and is applicable in high-dimensional\nsettings. In an application to realized covariance matrices, we find that\northogonal short- and long-memory components provide a reasonable fit and\ncompetitive out-of-sample performance compared to several competing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09149v2"
    },
    {
        "title": "Many Average Partial Effects: with An Application to Text Regression",
        "authors": [
            "Harold D. Chiang"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We study estimation, pointwise and simultaneous inference, and confidence\nintervals for many average partial effects of lasso Logit. Focusing on\nhigh-dimensional, cluster-sampling environments, we propose a new average\npartial effect estimator and explore its asymptotic properties. Practical\npenalty choices compatible with our asymptotic theory are also provided. The\nproposed estimator allow for valid inference without requiring oracle property.\nWe provide easy-to-implement algorithms for cluster-robust high-dimensional\nhypothesis testing and construction of simultaneously valid confidence\nintervals using a multiplier cluster bootstrap. We apply the proposed\nalgorithms to the text regression model of Wu (2018) to examine the presence of\ngendered language on the internet.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09397v5"
    },
    {
        "title": "Functional Sequential Treatment Allocation",
        "authors": [
            "Anders Bredahl Kock",
            "David Preinerstorfer",
            "Bezirgen Veliyev"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Consider a setting in which a policy maker assigns subjects to treatments,\nobserving each outcome before the next subject arrives. Initially, it is\nunknown which treatment is best, but the sequential nature of the problem\npermits learning about the effectiveness of the treatments. While the\nmulti-armed-bandit literature has shed much light on the situation when the\npolicy maker compares the effectiveness of the treatments through their mean,\nmuch less is known about other targets. This is restrictive, because a cautious\ndecision maker may prefer to target a robust location measure such as a\nquantile or a trimmed mean. Furthermore, socio-economic decision making often\nrequires targeting purpose specific characteristics of the outcome\ndistribution, such as its inherent degree of inequality, welfare or poverty. In\nthe present paper we introduce and study sequential learning algorithms when\nthe distributional characteristic of interest is a general functional of the\noutcome distribution. Minimax expected regret optimality results are obtained\nwithin the subclass of explore-then-commit policies, and for the unrestricted\nclass of all policies.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09408v8"
    },
    {
        "title": "Robust Tests for Convergence Clubs",
        "authors": [
            "Luisa Corrado",
            "Melvyn Weeks",
            "Thanasis Stengos",
            "M. Ege Yazgan"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In many applications common in testing for convergence the number of\ncross-sectional units is large and the number of time periods are few. In these\nsituations asymptotic tests based on an omnibus null hypothesis are\ncharacterised by a number of problems. In this paper we propose a multiple\npairwise comparisons method based on an a recursive bootstrap to test for\nconvergence with no prior information on the composition of convergence clubs.\nMonte Carlo simulations suggest that our bootstrap-based test performs well to\ncorrectly identify convergence clubs when compared with other similar tests\nthat rely on asymptotic arguments. Across a potentially large number of\nregions, using both cross-country and regional data for the European Union, we\nfind that the size distortion which afflicts standard tests and results in a\nbias towards finding less convergence, is ameliorated when we utilise our\nbootstrap test.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09518v1"
    },
    {
        "title": "A $t$-test for synthetic controls",
        "authors": [
            "Victor Chernozhukov",
            "Kaspar Wuthrich",
            "Yinchu Zhu"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We propose a practical and robust method for making inferences on average\ntreatment effects estimated by synthetic controls. We develop a $K$-fold\ncross-fitting procedure for bias correction. To avoid the difficult estimation\nof the long-run variance, inference is based on a self-normalized\n$t$-statistic, which has an asymptotically pivotal $t$-distribution. Our\n$t$-test is easy to implement, provably robust against misspecification, and\nvalid with stationary and non-stationary data. It demonstrates an excellent\nsmall sample performance in application-based simulations and performs well\nrelative to alternative methods. We illustrate the usefulness of the $t$-test\nby revisiting the effect of carbon taxes on emissions.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.10820v8"
    },
    {
        "title": "Decentralization Estimators for Instrumental Variable Quantile\n  Regression Models",
        "authors": [
            "Hiroaki Kaido",
            "Kaspar Wuthrich"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The instrumental variable quantile regression (IVQR) model (Chernozhukov and\nHansen, 2005) is a popular tool for estimating causal quantile effects with\nendogenous covariates. However, estimation is complicated by the non-smoothness\nand non-convexity of the IVQR GMM objective function. This paper shows that the\nIVQR estimation problem can be decomposed into a set of conventional quantile\nregression sub-problems which are convex and can be solved efficiently. This\nreformulation leads to new identification results and to fast, easy to\nimplement, and tuning-free estimators that do not require the availability of\nhigh-level \"black box\" optimization routines.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.10925v4"
    },
    {
        "title": "Predicting \"Design Gaps\" in the Market: Deep Consumer Choice Models\n  under Probabilistic Design Constraints",
        "authors": [
            "Alex Burnap",
            "John Hauser"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Predicting future successful designs and corresponding market opportunity is\na fundamental goal of product design firms. There is accordingly a long history\nof quantitative approaches that aim to capture diverse consumer preferences,\nand then translate those preferences to corresponding \"design gaps\" in the\nmarket. We extend this work by developing a deep learning approach to predict\ndesign gaps in the market. These design gaps represent clusters of designs that\ndo not yet exist, but are predicted to be both (1) highly preferred by\nconsumers, and (2) feasible to build under engineering and manufacturing\nconstraints. This approach is tested on the entire U.S. automotive market using\nof millions of real purchase data. We retroactively predict design gaps in the\nmarket, and compare predicted design gaps with actual known successful designs.\nOur preliminary results give evidence it may be possible to predict design\ngaps, suggesting this approach has promise for early identification of market\nopportunity.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.11067v1"
    },
    {
        "title": "Dynamic Models with Robust Decision Makers: Identification and\n  Estimation",
        "authors": [
            "Timothy M. Christensen"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper studies identification and estimation of a class of dynamic models\nin which the decision maker (DM) is uncertain about the data-generating\nprocess. The DM surrounds a benchmark model that he or she fears is\nmisspecified by a set of models. Decisions are evaluated under a worst-case\nmodel delivering the lowest utility among all models in this set. The DM's\nbenchmark model and preference parameters are jointly underidentified. With the\nbenchmark model held fixed, primitive conditions are established for\nidentification of the DM's worst-case model and preference parameters. The key\nstep in the identification analysis is to establish existence and uniqueness of\nthe DM's continuation value function allowing for unbounded statespace and\nunbounded utilities. To do so, fixed-point results are derived for monotone,\nconvex operators that act on a Banach space of thin-tailed functions arising\nnaturally from the structure of the continuation value recursion. The\nfixed-point results are quite general; applications to models with learning and\nRust-type dynamic discrete choice models are also discussed. For estimation, a\nperturbation result is derived which provides a necessary and sufficient\ncondition for consistent estimation of continuation values and the worst-case\nmodel. The result also allows convergence rates of estimators to be\ncharacterized. An empirical application studies an endowment economy where the\nDM's benchmark model may be interpreted as an aggregate of experts' forecasting\nmodels. The application reveals time-variation in the way the DM\npessimistically distorts benchmark probabilities. Consequences for asset\npricing are explored and connections are drawn with the literature on\nmacroeconomic uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.11246v3"
    },
    {
        "title": "Model Selection in Utility-Maximizing Binary Prediction",
        "authors": [
            "Jiun-Hua Su"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The maximum utility estimation proposed by Elliott and Lieli (2013) can be\nviewed as cost-sensitive binary classification; thus, its in-sample overfitting\nissue is similar to that of perceptron learning. A utility-maximizing\nprediction rule (UMPR) is constructed to alleviate the in-sample overfitting of\nthe maximum utility estimation. We establish non-asymptotic upper bounds on the\ndifference between the maximal expected utility and the generalized expected\nutility of the UMPR. Simulation results show that the UMPR with an appropriate\ndata-dependent penalty achieves larger generalized expected utility than common\nestimators in the binary classification if the conditional probability of the\nbinary outcome is misspecified.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.00716v3"
    },
    {
        "title": "Finite Sample Inference for the Maximum Score Estimand",
        "authors": [
            "Adam M. Rosen",
            "Takuya Ura"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We provide a finite sample inference method for the structural parameters of\na semiparametric binary response model under a conditional median restriction\noriginally studied by Manski (1975, 1985). Our inference method is valid for\nany sample size and irrespective of whether the structural parameters are point\nidentified or partially identified, for example due to the lack of a\ncontinuously distributed covariate with large support. Our inference approach\nexploits distributional properties of observable outcomes conditional on the\nobserved sequence of exogenous variables. Moment inequalities conditional on\nthis size n sequence of exogenous covariates are constructed, and the test\nstatistic is a monotone function of violations of sample moment inequalities.\nThe critical value used for inference is provided by the appropriate quantile\nof a known function of n independent Rademacher random variables. We\ninvestigate power properties of the underlying test and provide simulation\nstudies to support the theoretical findings.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.01511v2"
    },
    {
        "title": "Verifying the existence of maximum likelihood estimates for generalized\n  linear models",
        "authors": [
            "Sergio Correia",
            "Paulo Guimarães",
            "Thomas Zylkin"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  A fundamental problem with nonlinear models is that maximum likelihood\nestimates are not guaranteed to exist. Though nonexistence is a well known\nproblem in the binary choice literature, it presents significant challenges for\nother models as well and is not as well understood in more general settings.\nThese challenges are only magnified for models that feature many fixed effects\nand other high-dimensional parameters. We address the current ambiguity\nsurrounding this topic by studying the conditions that govern the existence of\nestimates for (pseudo-)maximum likelihood estimators used to estimate a wide\nclass of generalized linear models (GLMs). We show that some, but not all, of\nthese GLM estimators can still deliver consistent estimates of at least some of\nthe linear parameters when these conditions fail to hold. We also demonstrate\nhow to verify these conditions in models with high-dimensional parameters, such\nas panel data models with multiple levels of fixed effects.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.01633v6"
    },
    {
        "title": "Econometric analysis of potential outcomes time series: instruments,\n  shocks, linearity and the causal response function",
        "authors": [
            "Ashesh Rambachan",
            "Neil Shephard"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Bojinov & Shephard (2019) defined potential outcome time series to\nnonparametrically measure dynamic causal effects in time series experiments.\nFour innovations are developed in this paper: \"instrumental paths,\" treatments\nwhich are \"shocks,\" \"linear potential outcomes\" and the \"causal response\nfunction.\" Potential outcome time series are then used to provide a\nnonparametric causal interpretation of impulse response functions, generalized\nimpulse response functions, local projections and LP-IV.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.01637v3"
    },
    {
        "title": "ppmlhdfe: Fast Poisson Estimation with High-Dimensional Fixed Effects",
        "authors": [
            "Sergio Correia",
            "Paulo Guimarães",
            "Thomas Zylkin"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In this paper we present ppmlhdfe, a new Stata command for estimation of\n(pseudo) Poisson regression models with multiple high-dimensional fixed effects\n(HDFE). Estimation is implemented using a modified version of the iteratively\nreweighted least-squares (IRLS) algorithm that allows for fast estimation in\nthe presence of HDFE. Because the code is built around the reghdfe package, it\nhas similar syntax, supports many of the same functionalities, and benefits\nfrom reghdfe's fast convergence properties for computing high-dimensional least\nsquares problems.\n  Performance is further enhanced by some new techniques we introduce for\naccelerating HDFE-IRLS estimation specifically. ppmlhdfe also implements a\nnovel and more robust approach to check for the existence of (pseudo) maximum\nlikelihood estimates.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.01690v3"
    },
    {
        "title": "The Africa-Dummy: Gone with the Millennium?",
        "authors": [
            "Max Köhler",
            "Stefan Sperlich"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  A fixed effects regression estimator is introduced that can directly identify\nand estimate the Africa-Dummy in one regression step so that its correct\nstandard errors as well as correlations to other coefficients can easily be\nestimated. We can estimate the Nickel bias and found it to be negligibly tiny.\nSemiparametric extensions check whether the Africa-Dummy is simply a result of\nmisspecification of the functional form. In particular, we show that the\nreturns to growth factors are different for Sub-Saharan African countries\ncompared to the rest of the world. For example, returns to population growth\nare positive and beta-convergence is faster. When extending the model to\nidentify the development of the Africa-Dummy over time we see that it has been\nchanging dramatically over time and that the punishment for Sub-Saharan African\ncountries has been decreasing incrementally to reach insignificance around the\nturn of the millennium.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.02357v1"
    },
    {
        "title": "A Varying Coefficient Model for Assessing the Returns to Growth to\n  Account for Poverty and Inequality",
        "authors": [
            "Max Köhler",
            "Stefan Sperlich",
            "Jisu Yoon"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Various papers demonstrate the importance of inequality, poverty and the size\nof the middle class for economic growth. When explaining why these measures of\nthe income distribution are added to the growth regression, it is often\nmentioned that poor people behave different which may translate to the economy\nas a whole. However, simply adding explanatory variables does not reflect this\nbehavior. By a varying coefficient model we show that the returns to growth\ndiffer a lot depending on poverty and inequality. Furthermore, we investigate\nhow these returns differ for the poorer and for the richer part of the\nsocieties. We argue that the differences in the coefficients impede, on the one\nhand, that the means coefficients are informative, and, on the other hand,\nchallenge the credibility of the economic interpretation. In short, we show\nthat, when estimating mean coefficients without accounting for poverty and\ninequality, the estimation is likely to suffer from a serious endogeneity bias.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.02390v1"
    },
    {
        "title": "Inference for First-Price Auctions with Guerre, Perrigne, and Vuong's\n  Estimator",
        "authors": [
            "Jun Ma",
            "Vadim Marmer",
            "Artyom Shneyerov"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We consider inference on the probability density of valuations in the\nfirst-price sealed-bid auctions model within the independent private value\nparadigm. We show the asymptotic normality of the two-step nonparametric\nestimator of Guerre, Perrigne, and Vuong (2000) (GPV), and propose an easily\nimplementable and consistent estimator of the asymptotic variance. We prove the\nvalidity of the pointwise percentile bootstrap confidence intervals based on\nthe GPV estimator. Lastly, we use the intermediate Gaussian approximation\napproach to construct bootstrap-based asymptotically valid uniform confidence\nbands for the density of the valuations.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.06401v1"
    },
    {
        "title": "An Integrated Panel Data Approach to Modelling Economic Growth",
        "authors": [
            "Guohua Feng",
            "Jiti Gao",
            "Bin Peng"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Empirical growth analysis has three major problems --- variable selection,\nparameter heterogeneity and cross-sectional dependence --- which are addressed\nindependently from each other in most studies. The purpose of this study is to\npropose an integrated framework that extends the conventional linear growth\nregression model to allow for parameter heterogeneity and cross-sectional error\ndependence, while simultaneously performing variable selection. We also derive\nthe asymptotic properties of the estimator under both low and high dimensions,\nand further investigate the finite sample performance of the estimator through\nMonte Carlo simulations. We apply the framework to a dataset of 89 countries\nover the period from 1960 to 2014. Our results reveal some cross-country\npatterns not found in previous studies (e.g., \"middle income trap hypothesis\",\n\"natural resources curse hypothesis\", \"religion works via belief, not\npractice\", etc.).\n",
        "pdf_link": "http://arxiv.org/pdf/1903.07948v1"
    },
    {
        "title": "Bayesian MIDAS Penalized Regressions: Estimation, Selection, and\n  Prediction",
        "authors": [
            "Matteo Mogliani",
            "Anna Simoni"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a new approach to mixed-frequency regressions in a\nhigh-dimensional environment that resorts to Group Lasso penalization and\nBayesian techniques for estimation and inference. In particular, to improve the\nprediction properties of the model and its sparse recovery ability, we consider\na Group Lasso with a spike-and-slab prior. Penalty hyper-parameters governing\nthe model shrinkage are automatically tuned via an adaptive MCMC algorithm. We\nestablish good frequentist asymptotic properties of the posterior of the\nin-sample and out-of-sample prediction error, we recover the optimal posterior\ncontraction rate, and we show optimality of the posterior predictive density.\nSimulations show that the proposed models have good selection and forecasting\nperformance in small samples, even when the design matrix presents\ncross-correlation. When applied to forecasting U.S. GDP, our penalized\nregressions can outperform many strong competitors. Results suggest that\nfinancial variables may have some, although very limited, short-term predictive\ncontent.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.08025v3"
    },
    {
        "title": "Identification and Estimation of a Partially Linear Regression Model\n  using Network Data",
        "authors": [
            "Eric Auerbach"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  I study a regression model in which one covariate is an unknown function of a\nlatent driver of link formation in a network. Rather than specify and fit a\nparametric network formation model, I introduce a new method based on matching\npairs of agents with similar columns of the squared adjacency matrix, the ijth\nentry of which contains the number of other agents linked to both agents i and\nj. The intuition behind this approach is that for a large class of network\nformation models the columns of the squared adjacency matrix characterize all\nof the identifiable information about individual linking behavior. In this\npaper, I describe the model, formalize this intuition, and provide consistent\nestimators for the parameters of the regression model. Auerbach (2021)\nconsiders inference and an application to network peer effects.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.09679v3"
    },
    {
        "title": "Ensemble Methods for Causal Effects in Panel Data Settings",
        "authors": [
            "Susan Athey",
            "Mohsen Bayati",
            "Guido Imbens",
            "Zhaonan Qu"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper studies a panel data setting where the goal is to estimate causal\neffects of an intervention by predicting the counterfactual values of outcomes\nfor treated units, had they not received the treatment. Several approaches have\nbeen proposed for this problem, including regression methods, synthetic control\nmethods and matrix completion methods. This paper considers an ensemble\napproach, and shows that it performs better than any of the individual methods\nin several economic datasets. Matrix completion methods are often given the\nmost weight by the ensemble, but this clearly depends on the setting. We argue\nthat ensemble methods present a fruitful direction for further research in the\ncausal panel data setting.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.10079v1"
    },
    {
        "title": "On the Effect of Imputation on the 2SLS Variance",
        "authors": [
            "Helmut Farbmacher",
            "Alexander Kann"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Endogeneity and missing data are common issues in empirical research. We\ninvestigate how both jointly affect inference on causal parameters.\nConventional methods to estimate the variance, which treat the imputed data as\nif it was observed in the first place, are not reliable. We derive the\nasymptotic variance and propose a heteroskedasticity robust variance estimator\nfor two-stage least squares which accounts for the imputation. Monte Carlo\nsimulations support our theoretical findings.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.11004v1"
    },
    {
        "title": "Testing for Differences in Stochastic Network Structure",
        "authors": [
            "Eric Auerbach"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  How can one determine whether a community-level treatment, such as the\nintroduction of a social program or trade shock, alters agents' incentives to\nform links in a network? This paper proposes analogues of a two-sample\nKolmogorov-Smirnov test, widely used in the literature to test the null\nhypothesis of \"no treatment effects\", for network data. It first specifies a\ntesting problem in which the null hypothesis is that two networks are drawn\nfrom the same random graph model. It then describes two randomization tests\nbased on the magnitude of the difference between the networks' adjacency\nmatrices as measured by the $2\\to2$ and $\\infty\\to1$ operator norms. Power\nproperties of the tests are examined analytically, in simulation, and through\ntwo real-world applications. A key finding is that the test based on the\n$\\infty\\to1$ norm can be substantially more powerful than that based on the\n$2\\to2$ norm for the kinds of sparse and degree-heterogeneous networks common\nin economics.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.11117v5"
    },
    {
        "title": "A spatial multinomial logit model for analysing urban expansion",
        "authors": [
            "Tamás Krisztin",
            "Philipp Piribauer",
            "Michael Wögerer"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The paper proposes a Bayesian multinomial logit model to analyse spatial\npatterns of urban expansion. The specification assumes that the log-odds of\neach class follow a spatial autoregressive process. Using recent advances in\nBayesian computing, our model allows for a computationally efficient treatment\nof the spatial multinomial logit model. This allows us to assess spillovers\nbetween regions and across land use classes. In a series of Monte Carlo\nstudies, we benchmark our model against other competing specifications. The\npaper also showcases the performance of the proposed specification using\nEuropean regional data. Our results indicate that spatial dependence plays a\nkey role in land sealing process of cropland and grassland. Moreover, we\nuncover land sealing spillovers across multiple classes of arable land.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.00673v1"
    },
    {
        "title": "Applying Data Synthesis for Longitudinal Business Data across Three\n  Countries",
        "authors": [
            "M. Jahangir Alam",
            "Benoit Dostie",
            "Jörg Drechsler",
            "Lars Vilhuber"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Data on businesses collected by statistical agencies are challenging to\nprotect. Many businesses have unique characteristics, and distributions of\nemployment, sales, and profits are highly skewed. Attackers wishing to conduct\nidentification attacks often have access to much more information than for any\nindividual. As a consequence, most disclosure avoidance mechanisms fail to\nstrike an acceptable balance between usefulness and confidentiality protection.\nDetailed aggregate statistics by geography or detailed industry classes are\nrare, public-use microdata on businesses are virtually inexistant, and access\nto confidential microdata can be burdensome. Synthetic microdata have been\nproposed as a secure mechanism to publish microdata, as part of a broader\ndiscussion of how to provide broader access to such data sets to researchers.\nIn this article, we document an experiment to create analytically valid\nsynthetic data, using the exact same model and methods previously employed for\nthe United States, for data from two different countries: Canada (LEAP) and\nGermany (BHP). We assess utility and protection, and provide an assessment of\nthe feasibility of extending such an approach in a cost-effective way to other\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.02246v1"
    },
    {
        "title": "On the Size Control of the Hybrid Test for Predictive Ability",
        "authors": [
            "Deborah Kim"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We analyze theoretical properties of the hybrid test for superior\npredictability. We demonstrate with a simple example that the test may not be\npointwise asymptotically of level $\\alpha$ at commonly used significance levels\nand may lead to rejection rates over $11\\%$ when the significance level\n$\\alpha$ is $5\\%$. Generalizing this observation, we provide a formal result\nthat pointwise asymptotic invalidity of the hybrid test persists in a setting\nunder reasonable conditions. As an easy alternative, we propose a modified\nhybrid test based on the generalized moment selection method and show that the\nmodified test enjoys pointwise asymptotic validity. Monte Carlo simulations\nsupport the theoretical findings.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.02318v2"
    },
    {
        "title": "An Upper Bound for Functions of Estimators in High Dimensions",
        "authors": [
            "Mehmet Caner",
            "Xu Han"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We provide an upper bound as a random variable for the functions of\nestimators in high dimensions. This upper bound may help establish the rate of\nconvergence of functions in high dimensions. The upper bound random variable\nmay converge faster, slower, or at the same rate as estimators depending on the\nbehavior of the partial derivative of the function. We illustrate this via\nthree examples. The first two examples use the upper bound for testing in high\ndimensions, and third example derives the estimated out-of-sample variance of\nlarge portfolios. All our results allow for a larger number of parameters, p,\nthan the sample size, n.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.02636v1"
    },
    {
        "title": "Identification of Time-Varying Transformation Models with Fixed Effects,\n  with an Application to Unobserved Heterogeneity in Resource Shares",
        "authors": [
            "Irene Botosaru",
            "Chris Muris",
            "Krishna Pendakur"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We provide new results showing identification of a large class of fixed-T\npanel models, where the response variable is an unknown, weakly monotone,\ntime-varying transformation of a latent linear index of fixed effects,\nregressors, and an error term drawn from an unknown stationary distribution.\nOur results identify the transformation, the coefficient on regressors, and\nfeatures of the distribution of the fixed effects. We then develop a\nfull-commitment intertemporal collective household model, where the implied\nquantity demand equations are time-varying functions of a linear index. The\nfixed effects in this index equal logged resource shares, defined as the\nfractions of household expenditure enjoyed by each household member. Using\nBangladeshi data, we show that women's resource shares decline with household\nbudgets and that half of the variation in women's resource shares is due to\nunobserved household-level heterogeneity.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.05507v2"
    },
    {
        "title": "A dynamic ordered logit model with fixed effects",
        "authors": [
            "Chris Muris",
            "Pedro Raposo",
            "Sotiris Vandoros"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study a fixed-$T$ panel data logit model for ordered outcomes that\naccommodates fixed effects and state dependence. We provide identification\nresults for the autoregressive parameter, regression coefficients, and the\nthreshold parameters in this model. Our results require only four observations\non the outcome variable. We provide conditions under which a composite\nconditional maximum likelihood estimator is consistent and asymptotically\nnormal. We use our estimator to explore the determinants of self-reported\nhealth in a panel of European countries over the period 2003-2016. We find\nthat: (i) the autoregressive parameter is positive and analogous to a linear\nAR(1) coefficient of about 0.25, indicating persistence in health status; (ii)\nthe association between income and health becomes insignificant once we control\nfor unobserved heterogeneity and persistence.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.05517v1"
    },
    {
        "title": "Optimal selection of the number of control units in kNN algorithm to\n  estimate average treatment effects",
        "authors": [
            "Andrés Ramírez-Hassan",
            "Raquel Vargas-Correa",
            "Gustavo García",
            "Daniel Londoño"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose a simple approach to optimally select the number of control units\nin k nearest neighbors (kNN) algorithm focusing in minimizing the mean squared\nerror for the average treatment effects. Our approach is non-parametric where\nconfidence intervals for the treatment effects were calculated using asymptotic\nresults with bias correction. Simulation exercises show that our approach gets\nrelative small mean squared errors, and a balance between confidence intervals\nlength and type I error. We analyzed the average treatment effects on treated\n(ATET) of participation in 401(k) plans on accumulated net financial assets\nconfirming significant effects on amount and positive probability of net asset.\nOur optimal k selection produces significant narrower ATET confidence intervals\ncompared with common practice of using k=1.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.06564v1"
    },
    {
        "title": "Analysing a built-in advantage in asymmetric darts contests using causal\n  machine learning",
        "authors": [
            "Daniel Goller"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We analyse a sequential contest with two players in darts where one of the\ncontestants enjoys a technical advantage. Using methods from the causal machine\nlearning literature, we analyse the built-in advantage, which is the\nfirst-mover having potentially more but never less moves. Our empirical\nfindings suggest that the first-mover has an 8.6 percentage points higher\nprobability to win the match induced by the technical advantage. Contestants\nwith low performance measures and little experience have the highest built-in\nadvantage. With regard to the fairness principle that contestants with equal\nabilities should have equal winning probabilities, this contest is ex-ante fair\nin the case of equal built-in advantages for both competitors and a randomized\nstarting right. Nevertheless, the contest design produces unequal probabilities\nof winning for equally skilled contestants because of asymmetries in the\nbuilt-in advantage associated with social pressure for contestants competing at\nhome and away.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.07165v1"
    },
    {
        "title": "Peer effects and endogenous social interactions",
        "authors": [
            "Koen Jochmans"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We introduce an approach to deal with self-selection of peers in the\nlinear-in-means model. Contrary to the existing proposals we do not require to\nspecify a model for how the selection of peers comes about. Rather, we exploit\ntwo restrictions that are inherent to many such specifications to construct\nintuitive instrumental variables. These restrictions are that link decisions\nthat involve a given individual are not all independent of one another, but\nthat they are independent of the link behavior between other pairs of\nindividuals. A two-stage least-squares estimator of the linear-in-means model\nis then readily obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.07886v1"
    },
    {
        "title": "Bounds on Distributional Treatment Effect Parameters using Panel Data\n  with an Application on Job Displacement",
        "authors": [
            "Brantly Callaway"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper develops new techniques to bound distributional treatment effect\nparameters that depend on the joint distribution of potential outcomes -- an\nobject not identified by standard identifying assumptions such as selection on\nobservables or even when treatment is randomly assigned. I show that panel data\nand an additional assumption on the dependence between untreated potential\noutcomes for the treated group over time (i) provide more identifying power for\ndistributional treatment effect parameters than existing bounds and (ii)\nprovide a more plausible set of conditions than existing methods that obtain\npoint identification. I apply these bounds to study heterogeneity in the effect\nof job displacement during the Great Recession. Using standard techniques, I\nfind that workers who were displaced during the Great Recession lost on average\n34\\% of their earnings relative to their counterfactual earnings had they not\nbeen displaced. Using the methods developed in the current paper, I also show\nthat the average effect masks substantial heterogeneity across workers.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.08117v1"
    },
    {
        "title": "A Novel Approach to Predictive Accuracy Testing in Nested Environments",
        "authors": [
            "Jean-Yves Pitarakis"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We introduce a new approach for comparing the predictive accuracy of two\nnested models that bypasses the difficulties caused by the degeneracy of the\nasymptotic variance of forecast error loss differentials used in the\nconstruction of commonly used predictive comparison statistics. Our approach\ncontinues to rely on the out of sample MSE loss differentials between the two\ncompeting models, leads to nuisance parameter free Gaussian asymptotics and is\nshown to remain valid under flexible assumptions that can accommodate\nheteroskedasticity and the presence of mixed predictors (e.g. stationary and\nlocal to unit root). A local power analysis also establishes its ability to\ndetect departures from the null in both stationary and persistent settings.\nSimulations calibrated to common economic and financial applications indicate\nthat our methods have strong power with good size control across commonly\nencountered sample sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.08387v3"
    },
    {
        "title": "Inference for Moment Inequalities: A Constrained Moment Selection\n  Procedure",
        "authors": [
            "Rami V. Tabri",
            "Christopher D. Walker"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Inference in models where the parameter is defined by moment inequalities is\nof interest in many areas of economics. This paper develops a new method for\nimproving the performance of generalized moment selection (GMS) testing\nprocedures in finite-samples. The method modifies GMS tests by tilting the\nempirical distribution in its moment selection step by an amount that maximizes\nthe empirical likelihood subject to the restrictions of the null hypothesis. We\ncharacterize sets of population distributions on which a modified GMS test is\n(i) asymptotically equivalent to its non-modified version to first-order, and\n(ii) superior to its non-modified version according to local power when the\nsample size is large enough. An important feature of the proposed modification\nis that it remains computationally feasible even when the number of moment\ninequalities is large. We report simulation results that show the modified\ntests control size well, and have markedly improved local power over their\nnon-modified counterparts.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.09021v2"
    },
    {
        "title": "Empirical Likelihood Covariate Adjustment for Regression Discontinuity\n  Designs",
        "authors": [
            "Jun Ma",
            "Zhengfei Yu"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper proposes a versatile covariate adjustment method that directly\nincorporates covariate balance in regression discontinuity (RD) designs. The\nnew empirical entropy balancing method reweights the standard local polynomial\nRD estimator by using the entropy balancing weights that minimize the\nKullback--Leibler divergence from the uniform weights while satisfying the\ncovariate balance constraints. Our estimator can be formulated as an empirical\nlikelihood estimator that efficiently incorporates the information from the\ncovariate balance condition as correctly specified over-identifying moment\nrestrictions, and thus has an asymptotic variance no larger than that of the\nstandard estimator without covariates. We demystify the asymptotic efficiency\ngain of Calonico, Cattaneo, Farrell, and Titiunik (2019)'s regression-based\ncovariate-adjusted estimator, as their estimator has the same asymptotic\nvariance as ours. Further efficiency improvement from balancing over sieve\nspaces is possible if our entropy balancing weights are computed using stronger\ncovariate balance constraints that are imposed on functions of covariates. We\nthen show that our method enjoys favorable second-order properties from\nempirical likelihood estimation and inference: the estimator has a small\n(bounded) nonlinearity bias, and the likelihood ratio based confidence set\nadmits a simple analytical correction that can be used to improve coverage\naccuracy. The coverage accuracy of our confidence set is robust against slight\nperturbation to the covariate balance condition, which may happen in cases such\nas data contamination and misspecified \"unaffected\" outcomes used as\ncovariates. The proposed entropy balancing approach for covariate adjustment is\napplicable to other RD-related settings.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.09263v3"
    },
    {
        "title": "Generalized Lee Bounds",
        "authors": [
            "Vira Semenova"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Lee (2009) is a common approach to bound the average causal effect in the\npresence of selection bias, assuming the treatment effect on selection has the\nsame sign for all subjects. This paper generalizes Lee bounds to allow the sign\nof this effect to be identified by pretreatment covariates, relaxing the\nstandard (unconditional) monotonicity to its conditional analog. Asymptotic\ntheory for generalized Lee bounds is proposed in low-dimensional smooth and\nhigh-dimensional sparse designs. The paper also generalizes Lee bounds to\naccommodate multiple outcomes. It characterizes the sharp identified set for\nthe causal parameter and proposes uniform Gaussian inference on the support\nfunction. The estimated bounds achieve nearly point-identification in JobCorps\njob training program (Lee (2009)), where unconditional monotonicity is unlikely\nto hold.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.12720v3"
    },
    {
        "title": "Markov Switching",
        "authors": [
            "Yong Song",
            "Tomasz Woźniak"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Markov switching models are a popular family of models that introduces\ntime-variation in the parameters in the form of their state- or regime-specific\nvalues. Importantly, this time-variation is governed by a discrete-valued\nlatent stochastic process with limited memory. More specifically, the current\nvalue of the state indicator is determined only by the value of the state\nindicator from the previous period, thus the Markov property, and the\ntransition matrix. The latter characterizes the properties of the Markov\nprocess by determining with what probability each of the states can be visited\nnext period, given the state in the current period. This setup decides on the\ntwo main advantages of the Markov switching models. Namely, the estimation of\nthe probability of state occurrences in each of the sample periods by using\nfiltering and smoothing methods and the estimation of the state-specific\nparameters. These two features open the possibility for improved\ninterpretations of the parameters associated with specific regimes combined\nwith the corresponding regime probabilities, as well as for improved\nforecasting performance based on persistent regimes and parameters\ncharacterizing them.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.03598v1"
    },
    {
        "title": "The Effect of Weather Conditions on Fertilizer Applications: A Spatial\n  Dynamic Panel Data Analysis",
        "authors": [
            "Anna Gloria Billè",
            "Marco Rogna"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Given the extreme dependence of agriculture on weather conditions, this paper\nanalyses the effect of climatic variations on this economic sector, by\nconsidering both a huge dataset and a flexible spatio-temporal model\nspecification. In particular, we study the response of N-fertilizer application\nto abnormal weather conditions, while accounting for other relevant control\nvariables. The dataset consists of gridded data spanning over 21 years\n(1993-2013), while the methodological strategy makes use of a spatial dynamic\npanel data (SDPD) model that accounts for both space and time fixed effects,\nbesides dealing with both space and time dependences. Time-invariant short and\nlong term effects, as well as time-varying marginal effects are also properly\ndefined, revealing interesting results on the impact of both GDP and weather\nconditions on fertilizer utilizations. The analysis considers four\nmacro-regions -- Europe, South America, South-East Asia and Africa -- to allow\nfor comparisons among different socio-economic societies. In addition to\nfinding both spatial (in the form of knowledge spillover effects) and temporal\ndependences as well as a good support for the existence of an environmental\nKuznets curve for fertilizer application, the paper shows peculiar responses of\nN-fertilization to deviations from normal weather conditions of moisture for\neach selected region, calling for ad hoc policy interventions.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.03922v2"
    },
    {
        "title": "Identifiability and Estimation of Possibly Non-Invertible SVARMA Models:\n  A New Parametrisation",
        "authors": [
            "Bernd Funovits"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This article deals with parameterisation, identifiability, and maximum\nlikelihood (ML) estimation of possibly non-invertible structural vector\nautoregressive moving average (SVARMA) models driven by independent and\nnon-Gaussian shocks. In contrast to previous literature, the novel\nrepresentation of the MA polynomial matrix using the Wiener-Hopf factorisation\n(WHF) focuses on the multivariate nature of the model, generates insights into\nits structure, and uses this structure for devising optimisation algorithms. In\nparticular, it allows to parameterise the location of determinantal zeros\ninside and outside the unit circle, and it allows for MA zeros at zero, which\ncan be interpreted as informational delays. This is highly relevant for\ndata-driven evaluation of Dynamic Stochastic General Equilibrium (DSGE) models.\nTypically imposed identifying restrictions on the shock transmission matrix as\nwell as on the determinantal root location are made testable. Furthermore, we\nprovide low level conditions for asymptotic normality of the ML estimator and\nanalytic expressions for the score and the information matrix. As application,\nwe estimate the Blanchard and Quah model and show that our method provides\nfurther insights regarding non-invertibility using a standard macroeconometric\nmodel. These and further analyses are implemented in a well documented\nR-package.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.04346v2"
    },
    {
        "title": "The Dimension of the Set of Causal Solutions of Linear Multivariate\n  Rational Expectations Models",
        "authors": [
            "Bernd Funovits"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper analyses the number of free parameters and solutions of the\nstructural difference equation obtained from a linear multivariate rational\nexpectations model. First, it is shown that the number of free parameters\ndepends on the structure of the zeros at zero of a certain matrix polynomial of\nthe structural difference equation and the number of inputs of the rational\nexpectations model. Second, the implications of requiring that some components\nof the endogenous variables be predetermined are analysed. Third, a condition\nfor existence and uniqueness of a causal stationary solution is given.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.04369v1"
    },
    {
        "title": "Long-term prediction intervals of economic time series",
        "authors": [
            "Marek Chudy",
            "Sayar Karmakar",
            "Wei Biao Wu"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We construct long-term prediction intervals for time-aggregated future values\nof univariate economic time series. We propose computational adjustments of the\nexisting methods to improve coverage probability under a small sample\nconstraint. A pseudo-out-of-sample evaluation shows that our methods perform at\nleast as well as selected alternative methods based on model-implied Bayesian\napproaches and bootstrapping. Our most successful method yields prediction\nintervals for eight macroeconomic indicators over a horizon spanning several\ndecades.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.05384v1"
    },
    {
        "title": "Combining Shrinkage and Sparsity in Conjugate Vector Autoregressive\n  Models",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber",
            "Luca Onorante"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Conjugate priors allow for fast inference in large dimensional vector\nautoregressive (VAR) models but, at the same time, introduce the restriction\nthat each equation features the same set of explanatory variables. This paper\nproposes a straightforward means of post-processing posterior estimates of a\nconjugate Bayesian VAR to effectively perform equation-specific covariate\nselection. Compared to existing techniques using shrinkage alone, our approach\ncombines shrinkage and sparsity in both the VAR coefficients and the error\nvariance-covariance matrices, greatly reducing estimation uncertainty in large\ndimensions while maintaining computational tractability. We illustrate our\napproach by means of two applications. The first application uses synthetic\ndata to investigate the properties of the model across different\ndata-generating processes, the second application analyzes the predictive gains\nfrom sparsification in a forecasting exercise for US data.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.08760v2"
    },
    {
        "title": "Estimation and Inference about Tail Features with Tail Censored Data",
        "authors": [
            "Yulong Wang",
            "Zhijie Xiao"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper considers estimation and inference about tail features when the\nobservations beyond some threshold are censored. We first show that ignoring\nsuch tail censoring could lead to substantial bias and size distortion, even if\nthe censored probability is tiny. Second, we propose a new maximum likelihood\nestimator (MLE) based on the Pareto tail approximation and derive its\nasymptotic properties. Third, we provide a small sample modification to the MLE\nby resorting to Extreme Value theory. The MLE with this modification delivers\nexcellent small sample performance, as shown by Monte Carlo simulations. We\nillustrate its empirical relevance by estimating (i) the tail index and the\nextreme quantiles of the US individual earnings with the Current Population\nSurvey dataset and (ii) the tail index of the distribution of macroeconomic\ndisasters and the coefficient of risk aversion using the dataset collected by\nBarro and Urs{\\'u}a (2008). Our new empirical findings are substantially\ndifferent from the existing literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.09982v1"
    },
    {
        "title": "Bayesian Inference in High-Dimensional Time-varying Parameter Models\n  using Integrated Rotated Gaussian Approximations",
        "authors": [
            "Florian Huber",
            "Gary Koop",
            "Michael Pfarrhofer"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Researchers increasingly wish to estimate time-varying parameter (TVP)\nregressions which involve a large number of explanatory variables. Including\nprior information to mitigate over-parameterization concerns has led to many\nusing Bayesian methods. However, Bayesian Markov Chain Monte Carlo (MCMC)\nmethods can be very computationally demanding. In this paper, we develop\ncomputationally efficient Bayesian methods for estimating TVP models using an\nintegrated rotated Gaussian approximation (IRGA). This exploits the fact that\nwhereas constant coefficients on regressors are often important, most of the\nTVPs are often unimportant. Since Gaussian distributions are invariant to\nrotations we can split the the posterior into two parts: one involving the\nconstant coefficients, the other involving the TVPs. Approximate methods are\nused on the latter and, conditional on these, the former are estimated with\nprecision using MCMC methods. In empirical exercises involving artificial data\nand a large macroeconomic data set, we show the accuracy and computational\nbenefits of IRGA methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.10274v1"
    },
    {
        "title": "Estimating Economic Models with Testable Assumptions: Theory and\n  Applications",
        "authors": [
            "Moyu Liao"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper studies the identification, estimation, and hypothesis testing\nproblem in complete and incomplete economic models with testable assumptions.\nTestable assumptions ($A$) give strong and interpretable empirical content to\nthe models but they also carry the possibility that some distribution of\nobserved outcomes may reject these assumptions. A natural way to avoid this is\nto find a set of relaxed assumptions ($\\tilde{A}$) that cannot be rejected by\nany distribution of observed outcome and the identified set of the parameter of\ninterest is not changed when the original assumption is not rejected. The main\ncontribution of this paper is to characterize the properties of such a relaxed\nassumption $\\tilde{A}$ using a generalized definition of refutability and\nconfirmability. I also propose a general method to construct such $\\tilde{A}$.\nA general estimation and inference procedure is proposed and can be applied to\nmost incomplete economic models. I apply my methodology to the instrument\nmonotonicity assumption in Local Average Treatment Effect (LATE) estimation and\nto the sector selection assumption in a binary outcome Roy model of employment\nsector choice. In the LATE application, I use my general method to construct a\nset of relaxed assumptions $\\tilde{A}$ that can never be rejected, and the\nidentified set of LATE is the same as imposing $A$ when $A$ is not rejected.\nLATE is point identified under my extension $\\tilde{A}$ in the LATE\napplication. In the binary outcome Roy model, I use my method of incomplete\nmodels to relax Roy's sector selection assumption and characterize the\nidentified set of the binary potential outcome as a polyhedron.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.10415v3"
    },
    {
        "title": "Hours Worked and the U.S. Distribution of Real Annual Earnings 1976-2019",
        "authors": [
            "Iván Fernández-Val",
            "Franco Peracchi",
            "Aico van Vuuren",
            "Francis Vella"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We examine the impact of annual hours worked on annual earnings by\ndecomposing changes in the real annual earnings distribution into composition,\nstructural and hours effects. We do so via a nonseparable simultaneous model of\nhours, wages and earnings. Using the Current Population Survey for the survey\nyears 1976--2019, we find that changes in the female distribution of annual\nhours of work are important in explaining movements in inequality in female\nannual earnings. This captures the substantial changes in their employment\nbehavior over this period. Movements in the male hours distribution only affect\nthe lower part of their earnings distribution and reflect the sensitivity of\nthese workers' annual hours of work to cyclical factors.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.11211v3"
    },
    {
        "title": "Causal mediation analysis with double machine learning",
        "authors": [
            "Helmut Farbmacher",
            "Martin Huber",
            "Lukáš Lafférs",
            "Henrika Langen",
            "Martin Spindler"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper combines causal mediation analysis with double machine learning to\ncontrol for observed confounders in a data-driven way under a\nselection-on-observables assumption in a high-dimensional setting. We consider\nthe average indirect effect of a binary treatment operating through an\nintermediate variable (or mediator) on the causal path between the treatment\nand the outcome, as well as the unmediated direct effect. Estimation is based\non efficient score functions, which possess a multiple robustness property\nw.r.t. misspecifications of the outcome, mediator, and treatment models. This\nproperty is key for selecting these models by double machine learning, which is\ncombined with data splitting to prevent overfitting in the estimation of the\neffects of interest. We demonstrate that the direct and indirect effect\nestimators are asymptotically normal and root-n consistent under specific\nregularity conditions and investigate the finite sample properties of the\nsuggested methods in a simulation study when considering lasso as machine\nlearner. We also provide an empirical application to the U.S. National\nLongitudinal Survey of Youth, assessing the indirect effect of health insurance\ncoverage on general health operating via routine checkups as mediator, as well\nas the direct effect. We find a moderate short term effect of health insurance\ncoverage on general health which is, however, not mediated by routine checkups.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.12710v6"
    },
    {
        "title": "A Note on the Multi-Agent Contracts in Continuous Time",
        "authors": [
            "Qi Luo",
            "Romesh Saigal"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Dynamic contracts with multiple agents is a classical decentralized\ndecision-making problem with asymmetric information. In this paper, we extend\nthe single-agent dynamic incentive contract model in continuous-time to a\nmulti-agent scheme in finite horizon and allow the terminal reward to be\ndependent on the history of actions and incentives. We first derive a set of\nsufficient conditions for the existence of optimal contracts in the most\ngeneral setting and conditions under which they form a Nash equilibrium. Then\nwe show that the principal's problem can be converted to solving\nHamilton-Jacobi-Bellman (HJB) equation requiring a static Nash equilibrium.\nFinally, we provide a framework to solve this problem by solving partial\ndifferential equations (PDE) derived from backward stochastic differential\nequations (BSDE).\n",
        "pdf_link": "http://arxiv.org/pdf/1710.00377v2"
    },
    {
        "title": "Rate-Optimal Estimation of the Intercept in a Semiparametric\n  Sample-Selection Model",
        "authors": [
            "Chuan Goh"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  This paper presents a new estimator of the intercept of a linear regression\nmodel in cases where the outcome varaible is observed subject to a selection\nrule. The intercept is often in this context of inherent interest; for example,\nin a program evaluation context, the difference between the intercepts in\noutcome equations for participants and non-participants can be interpreted as\nthe difference in average outcomes of participants and their counterfactual\naverage outcomes if they had chosen not to participate. The new estimator can\nunder mild conditions exhibit a rate of convergence in probability equal to\n$n^{-p/(2p+1)}$, where $p\\ge 2$ is an integer that indexes the strength of\ncertain smoothness assumptions. This rate of convergence is shown in this\ncontext to be the optimal rate of convergence for estimation of the intercept\nparameter in terms of a minimax criterion. The new estimator, unlike other\nproposals in the literature, is under mild conditions consistent and\nasymptotically normal with a rate of convergence that is the same regardless of\nthe degree to which selection depends on unobservables in the outcome equation.\nSimulation evidence and an empirical example are included.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.01423v3"
    },
    {
        "title": "A Note on Gale, Kuhn, and Tucker's Reductions of Zero-Sum Games",
        "authors": [
            "Shuige Liu"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Gale, Kuhn and Tucker (1950) introduced two ways to reduce a zero-sum game by\npackaging some strategies with respect to a probability distribution on them.\nIn terms of value, they gave conditions for a desirable reduction. We show that\na probability distribution for a desirable reduction relies on optimal\nstrategies in the original game. Also, we correct an improper example given by\nthem to show that the reverse of a theorem does not hold.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.02326v1"
    },
    {
        "title": "Propensity score matching for multiple treatment levels: A CODA-based\n  contribution",
        "authors": [
            "Hajime Seya",
            "Takahiro Yoshida"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  This study proposes a simple technique for propensity score matching for\nmultiple treatment levels under the strong unconfoundedness assumption with the\nhelp of the Aitchison distance proposed in the field of compositional data\nanalysis (CODA).\n",
        "pdf_link": "http://arxiv.org/pdf/1710.08558v1"
    },
    {
        "title": "Nonparametric Identification in Index Models of Link Formation",
        "authors": [
            "Wayne Yuan Gao"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  We consider an index model of dyadic link formation with a homophily effect\nindex and a degree heterogeneity index. We provide nonparametric identification\nresults in a single large network setting for the potentially nonparametric\nhomophily effect function, the realizations of unobserved individual fixed\neffects and the unknown distribution of idiosyncratic pairwise shocks, up to\nnormalization, for each possible true value of the unknown parameters. We\npropose a novel form of scale normalization on an arbitrary interquantile\nrange, which is not only theoretically robust but also proves particularly\nconvenient for the identification analysis, as quantiles provide direct\nlinkages between the observable conditional probabilities and the unknown index\nvalues. We then use an inductive \"in-fill and out-expansion\" algorithm to\nestablish our main results, and consider extensions to more general settings\nthat allow nonseparable dependence between homophily and degree heterogeneity,\nas well as certain extents of network sparsity and weaker assumptions on the\nsupport of unobserved heterogeneity. As a byproduct, we also propose a concept\ncalled \"modeling equivalence\" as a refinement of \"observational equivalence\",\nand use it to provide a formal discussion about normalization, identification\nand their interplay with counterfactuals.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.11230v5"
    },
    {
        "title": "At What Level Should One Cluster Standard Errors in Paired and\n  Small-Strata Experiments?",
        "authors": [
            "Clément de Chaisemartin",
            "Jaime Ramirez-Cuellar"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In matched-pairs experiments in which one cluster per pair of clusters is\nassigned to treatment, to estimate treatment effects, researchers often regress\ntheir outcome on a treatment indicator and pair fixed effects, clustering\nstandard errors at the unit-ofrandomization level. We show that even if the\ntreatment has no effect, a 5%-level t-test based on this regression will\nwrongly conclude that the treatment has an effect up to 16.5% of the time. To\nfix this problem, researchers should instead cluster standard errors at the\npair level. Using simulations, we show that similar results apply to clustered\nexperiments with small strata.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.00288v10"
    },
    {
        "title": "Indirect Inference for Locally Stationary Models",
        "authors": [
            "David Frazier",
            "Bonsoo Koo"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose the use of indirect inference estimation to conduct inference in\ncomplex locally stationary models. We develop a local indirect inference\nalgorithm and establish the asymptotic properties of the proposed estimator.\nDue to the nonparametric nature of locally stationary models, the resulting\nindirect inference estimator exhibits nonparametric rates of convergence. We\nvalidate our methodology with simulation studies in the confines of a locally\nstationary moving average model and a new locally stationary multiplicative\nstochastic volatility model. Using this indirect inference methodology and the\nnew locally stationary volatility model, we obtain evidence of non-linear,\ntime-varying volatility trends for monthly returns on several Fama-French\nportfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.01768v2"
    },
    {
        "title": "Nonparametric Identification and Estimation with Independent, Discrete\n  Instruments",
        "authors": [
            "Isaac Loh"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In a nonparametric instrumental regression model, we strengthen the\nconventional moment independence assumption towards full statistical\nindependence between instrument and error term. This allows us to prove\nidentification results and develop estimators for a structural function of\ninterest when the instrument is discrete, and in particular binary. When the\nregressor of interest is also discrete with more mass points than the\ninstrument, we state straightforward conditions under which the structural\nfunction is partially identified, and give modified assumptions which imply\npoint identification. These stronger assumptions are shown to hold outside of a\nsmall set of conditional moments of the error term. Estimators for the\nidentified set are given when the structural function is either partially or\npoint identified. When the regressor is continuously distributed, we prove that\nif the instrument induces a sufficiently rich variation in the joint\ndistribution of the regressor and error term then point identification of the\nstructural function is still possible. This approach is relatively tractable,\nand under some standard conditions we demonstrate that our point identifying\nassumption holds on a topologically generic set of density functions for the\njoint distribution of regressor, error, and instrument. Our method also applies\nto a well-known nonparametric quantile regression framework, and we are able to\nstate analogous point identification results in that context.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05231v1"
    },
    {
        "title": "On the Properties of the Synthetic Control Estimator with Many Periods\n  and Many Controls",
        "authors": [
            "Bruno Ferman"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We consider the asymptotic properties of the Synthetic Control (SC) estimator\nwhen both the number of pre-treatment periods and control units are large. If\npotential outcomes follow a linear factor model, we provide conditions under\nwhich the factor loadings of the SC unit converge in probability to the factor\nloadings of the treated unit. This happens when there are weights diluted among\nan increasing number of control units such that a weighted average of the\nfactor loadings of the control units asymptotically reconstructs the factor\nloadings of the treated unit. In this case, the SC estimator is asymptotically\nunbiased even when treatment assignment is correlated with time-varying\nunobservables. This result can be valid even when the number of control units\nis larger than the number of pre-treatment periods.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.06665v5"
    },
    {
        "title": "Shape Matters: Evidence from Machine Learning on Body Shape-Income\n  Relationship",
        "authors": [
            "Suyong Song",
            "Stephen S. Baek"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We study the association between physical appearance and family income using\na novel data which has 3-dimensional body scans to mitigate the issue of\nreporting errors and measurement errors observed in most previous studies. We\napply machine learning to obtain intrinsic features consisting of human body\nand take into account a possible issue of endogenous body shapes. The\nestimation results show that there is a significant relationship between\nphysical appearance and family income and the associations are different across\nthe gender. This supports the hypothesis on the physical attractiveness premium\nand its heterogeneity across the gender.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.06747v1"
    },
    {
        "title": "Sparse structures with LASSO through Principal Components: forecasting\n  GDP components in the short-run",
        "authors": [
            "Saulius Jokubaitis",
            "Dmitrij Celov",
            "Remigijus Leipus"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper aims to examine the use of sparse methods to forecast the real, in\nthe chain-linked volume sense, expenditure components of the US and EU GDP in\nthe short-run sooner than the national institutions of statistics officially\nrelease the data. We estimate current quarter nowcasts along with 1- and\n2-quarter forecasts by bridging quarterly data with available monthly\ninformation announced with a much smaller delay. We solve the\nhigh-dimensionality problem of the monthly dataset by assuming sparse\nstructures of leading indicators, capable of adequately explaining the dynamics\nof analyzed data. For variable selection and estimation of the forecasts, we\nuse the sparse methods - LASSO together with its recent modifications. We\npropose an adjustment that combines LASSO cases with principal components\nanalysis that deemed to improve the forecasting performance. We evaluate\nforecasting performance conducting pseudo-real-time experiments for gross fixed\ncapital formation, private consumption, imports and exports over the sample of\n2005-2019, compared with benchmark ARMA and factor models. The main results\nsuggest that sparse methods can outperform the benchmarks and to identify\nreasonable subsets of explanatory variables. The proposed LASSO-PC modification\nshow further improvement in forecast accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.07992v2"
    },
    {
        "title": "Understanding the explosive trend in EU ETS prices -- fundamentals or\n  speculation?",
        "authors": [
            "Marina Friedrich",
            "Sébastien Fries",
            "Michael Pahle",
            "Ottmar Edenhofer"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In 2018, allowance prices in the EU Emission Trading Scheme (EU ETS)\nexperienced a run-up from persistently low levels in previous years. Regulators\nattribute this to a comprehensive reform in the same year, and are confident\nthe new price level reflects an anticipated tighter supply of allowances. We\nask if this is indeed the case, or if it is an overreaction of the market\ndriven by speculation. We combine several econometric methods - time-varying\ncoefficient regression, formal bubble detection as well as time stamping and\ncrash odds prediction - to juxtapose the regulators' claim versus the\nconcurrent explanation. We find evidence of a long period of explosive\nbehaviour in allowance prices, starting in March 2018 when the reform was\nadopted. Our results suggest that the reform triggered market participants into\nspeculation, and question regulators' confidence in its long-term outcome. This\nhas implications for both the further development of the EU ETS, and the long\nlasting debate about taxes versus emission trading schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.10572v5"
    },
    {
        "title": "Identification of Semiparametric Panel Multinomial Choice Models with\n  Infinite-Dimensional Fixed Effects",
        "authors": [
            "Wayne Yuan Gao",
            "Ming Li"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper proposes a robust method for semiparametric identification and\nestimation in panel multinomial choice models, where we allow for\ninfinite-dimensional fixed effects that enter into consumer utilities in an\nadditively nonseparable way, thus incorporating rich forms of unobserved\nheterogeneity. Our identification strategy exploits multivariate monotonicity\nin parametric indexes, and uses the logical contraposition of an intertemporal\ninequality on choice probabilities to obtain identifying restrictions. We\nprovide a consistent estimation procedure, and demonstrate the practical\nadvantages of our method with Monte Carlo simulations and an empirical\nillustration on popcorn sales with the Nielsen data.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.00085v2"
    },
    {
        "title": "An optimal test for strategic interaction in social and economic network\n  formation between heterogeneous agents",
        "authors": [
            "Andrin Pelican",
            "Bryan S. Graham"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Consider a setting where $N$ players, partitioned into $K$ observable types,\nform a directed network. Agents' preferences over the form of the network\nconsist of an arbitrary network benefit function (e.g., agents may have\npreferences over their network centrality) and a private component which is\nadditively separable in own links. This latter component allows for unobserved\nheterogeneity in the costs of sending and receiving links across agents\n(respectively out- and in- degree heterogeneity) as well as\nhomophily/heterophily across the $K$ types of agents. In contrast, the network\nbenefit function allows agents' preferences over links to vary with the\npresence or absence of links elsewhere in the network (and hence with the link\nformation behavior of their peers). In the null model which excludes the\nnetwork benefit function, links form independently across dyads in the manner\ndescribed by \\cite{Charbonneau_EJ17}. Under the alternative there is\ninterdependence across linking decisions (i.e., strategic interaction). We show\nhow to test the null with power optimized in specific directions. These\nalternative directions include many common models of strategic network\nformation (e.g., \"connections\" models, \"structural hole\" models etc.). Our\nrandom utility specification induces an exponential family structure under the\nnull which we exploit to construct a similar test which exactly controls size\n(despite the the null being a composite one with many nuisance parameters). We\nfurther show how to construct locally best tests for specific alternatives\nwithout making any assumptions about equilibrium selection. To make our tests\nfeasible we introduce a new MCMC algorithm for simulating the null\ndistributions of our test statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.00212v2"
    },
    {
        "title": "Instrumental Variable Quantile Regression",
        "authors": [
            "Victor Chernozhukov",
            "Christian Hansen",
            "Kaspar Wuthrich"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This chapter reviews the instrumental variable quantile regression model of\nChernozhukov and Hansen (2005). We discuss the key conditions used for\nidentification of structural quantile effects within this model which include\nthe availability of instruments and a restriction on the ranks of structural\ndisturbances. We outline several approaches to obtaining point estimates and\nperforming statistical inference for model parameters. Finally, we point to\npossible directions for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.00436v1"
    },
    {
        "title": "A Vector Monotonicity Assumption for Multiple Instruments",
        "authors": [
            "Leonard Goff"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  When a researcher combines multiple instrumental variables for a single\nbinary treatment, the monotonicity assumption of the local average treatment\neffects (LATE) framework can become restrictive: it requires that all units\nshare a common direction of response even when separate instruments are shifted\nin opposing directions. What I call vector monotonicity, by contrast, simply\nassumes treatment uptake to be monotonic in all instruments. I characterize the\nclass of causal parameters that are point identified under vector monotonicity,\nwhen the instruments are binary. This class includes, for example, the average\ntreatment effect among units that are in any way responsive to the collection\nof instruments, or those that are responsive to a given subset of them. The\nidentification results are constructive and yield a simple estimator for the\nidentified treatment effect parameters. An empirical application revisits the\nlabor market returns to college.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.00553v6"
    },
    {
        "title": "Hidden Group Time Profiles: Heterogeneous Drawdown Behaviours in\n  Retirement",
        "authors": [
            "Igor Balnozan",
            "Denzil G. Fiebig",
            "Anthony Asher",
            "Robert Kohn",
            "Scott A. Sisson"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This article investigates retirement decumulation behaviours using the\nGrouped Fixed-Effects (GFE) estimator applied to Australian panel data on\ndrawdowns from phased withdrawal retirement income products. Behaviours\nexhibited by the distinct latent groups identified suggest that retirees may\nadopt simple heuristics determining how they draw down their accumulated\nwealth. Two extensions to the original GFE methodology are proposed: a latent\ngroup label-matching procedure which broadens bootstrap inference to include\nthe time profile estimates, and a modified estimation procedure for models with\ntime-invariant additive fixed effects estimated using unbalanced data.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.01505v2"
    },
    {
        "title": "The role of parallel trends in event study settings: An application to\n  environmental economics",
        "authors": [
            "Michelle Marcus",
            "Pedro H. C. Sant'Anna"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Difference-in-Differences (DID) research designs usually rely on variation of\ntreatment timing such that, after making an appropriate parallel trends\nassumption, one can identify, estimate, and make inference about causal\neffects. In practice, however, different DID procedures rely on different\nparallel trends assumptions (PTA), and recover different causal parameters. In\nthis paper, we focus on staggered DID (also referred as event-studies) and\ndiscuss the role played by the PTA in terms of identification and estimation of\ncausal parameters. We document a ``robustness'' vs. ``efficiency'' trade-off in\nterms of the strength of the underlying PTA, and argue that practitioners\nshould be explicit about these trade-offs whenever using DID procedures. We\npropose new DID estimators that reflect these trade-offs and derived their\nlarge sample properties. We illustrate the practical relevance of these results\nby assessing whether the transition from federal to state management of the\nClean Water Act affects compliance rates.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.01963v1"
    },
    {
        "title": "COVID-19: Tail Risk and Predictive Regressions",
        "authors": [
            "Walter Distaso",
            "Rustam Ibragimov",
            "Alexander Semenov",
            "Anton Skrobotov"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The paper focuses on econometrically justified robust analysis of the effects\nof the COVID-19 pandemic on financial markets in different countries across the\nWorld. It provides the results of robust estimation and inference on predictive\nregressions for returns on major stock indexes in 23 countries in North and\nSouth America, Europe, and Asia incorporating the time series of reported\ninfections and deaths from COVID-19. We also present a detailed study of\npersistence, heavy-tailedness and tail risk properties of the time series of\nthe COVID-19 infections and death rates that motivate the necessity in\napplications of robust inference methods in the analysis. Econometrically\njustified analysis is based on heteroskedasticity and autocorrelation\nconsistent (HAC) inference methods, recently developed robust $t$-statistic\ninference approaches and robust tail index estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02486v3"
    },
    {
        "title": "Decomposing Identification Gains and Evaluating Instrument\n  Identification Power for Partially Identified Average Treatment Effects",
        "authors": [
            "Lina Zhang",
            "David T. Frazier",
            "D. S. Poskitt",
            "Xueyan Zhao"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper examines the identification power of instrumental variables (IVs)\nfor average treatment effect (ATE) in partially identified models. We decompose\nthe ATE identification gains into components of contributions driven by IV\nrelevancy, IV strength, direction and degree of treatment endogeneity, and\nmatching via exogenous covariates. Our decomposition is demonstrated with\ngraphical illustrations, simulation studies and an empirical example of\nchildbearing and women's labour supply. Our analysis offers insights for\nunderstanding the complex role of IVs in ATE identification and for selecting\nIVs in practical policy designs. Simulations also suggest potential uses of our\nanalysis for detecting irrelevant instruments.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02642v3"
    },
    {
        "title": "Two-Stage Maximum Score Estimator",
        "authors": [
            "Wayne Yuan Gao",
            "Sheng Xu",
            "Kan Xu"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper considers the asymptotic theory of a semiparametric M-estimator\nthat is generally applicable to models that satisfy a monotonicity condition in\none or several parametric indexes. We call the estimator two-stage maximum\nscore (TSMS) estimator since our estimator involves a first-stage nonparametric\nregression when applied to the binary choice model of Manski (1975, 1985). We\ncharacterize the asymptotic distribution of the TSMS estimator, which features\nphase transitions depending on the dimension and thus the convergence rate of\nthe first-stage estimation. Effectively, the first-stage nonparametric\nestimator serves as an imperfect smoothing function on a non-smooth criterion\nfunction, leading to the pivotality of the first-stage estimation error with\nrespect to the second-stage convergence rate and asymptotic distribution\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02854v4"
    },
    {
        "title": "Dimension Reduction for High Dimensional Vector Autoregressive Models",
        "authors": [
            "Gianluca Cubadda",
            "Alain Hecq"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper aims to decompose a large dimensional vector autoregessive (VAR)\nmodel into two components, the first one being generated by a small-scale VAR\nand the second one being a white noise sequence. Hence, a reduced number of\ncommon components generates the entire dynamics of the large system through a\nVAR structure. This modelling, which we label as the dimension-reducible VAR,\nextends the common feature approach to high dimensional systems, and it differs\nfrom the dynamic factor model in which the idiosyncratic component can also\nembed a dynamic pattern. We show the conditions under which this decomposition\nexists. We provide statistical tools to detect its presence in the data and to\nestimate the parameters of the underlying small-scale VAR model. Based on our\nmethodology, we propose a novel approach to identify the shock that is\nresponsible for most of the common variability at the business cycle\nfrequencies. We evaluate the practical value of the proposed methods by\nsimulations as well as by an empirical application to a large set of US\neconomic variables.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03361v3"
    },
    {
        "title": "Local Composite Quantile Regression for Regression Discontinuity",
        "authors": [
            "Xiao Huang",
            "Zhaoguo Zhan"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We introduce the local composite quantile regression (LCQR) to causal\ninference in regression discontinuity (RD) designs. Kai et al. (2010) study the\nefficiency property of LCQR, while we show that its nice boundary performance\ntranslates to accurate estimation of treatment effects in RD under a variety of\ndata generating processes. Moreover, we propose a bias-corrected and standard\nerror-adjusted t-test for inference, which leads to confidence intervals with\ngood coverage probabilities. A bandwidth selector is also discussed. For\nillustration, we conduct a simulation study and revisit a classic example from\nLee (2008). A companion R package rdcqr is developed.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03716v3"
    },
    {
        "title": "Exact Computation of Maximum Rank Correlation Estimator",
        "authors": [
            "Youngki Shin",
            "Zvezdomir Todorov"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper we provide a computation algorithm to get a global solution for\nthe maximum rank correlation estimator using the mixed integer programming\n(MIP) approach. We construct a new constrained optimization problem by\ntransforming all indicator functions into binary parameters to be estimated and\nshow that it is equivalent to the original problem. We also consider an\napplication of the best subset rank prediction and show that the original\noptimization problem can be reformulated as MIP. We derive the non-asymptotic\nbound for the tail probability of the predictive performance measure. We\ninvestigate the performance of the MIP algorithm by an empirical example and\nMonte Carlo simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03844v2"
    },
    {
        "title": "Inferring hidden potentials in analytical regions: uncovering crime\n  suspect communities in Medellín",
        "authors": [
            "Alejandro Puerta",
            "Andrés Ramírez-Hassan"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper proposes a Bayesian approach to perform inference regarding the\nsize of hidden populations at analytical region using reported statistics. To\ndo so, we propose a specification taking into account one-sided error\ncomponents and spatial effects within a panel data structure. Our simulation\nexercises suggest good finite sample performance. We analyze rates of crime\nsuspects living per neighborhood in Medell\\'in (Colombia) associated with four\ncrime activities. Our proposal seems to identify hot spots or \"crime\ncommunities\", potential neighborhoods where under-reporting is more severe, and\nalso drivers of crime schools. Statistical evidence suggests a high level of\ninteraction between homicides and drug dealing in one hand, and motorcycle and\ncar thefts on the other hand.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.05360v1"
    },
    {
        "title": "Regularized Solutions to Linear Rational Expectations Models",
        "authors": [
            "Majid M. Al-Sadoon"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper proposes an algorithm for computing regularized solutions to\nlinear rational expectations models. The algorithm allows for regularization\ncross-sectionally as well as across frequencies. A variety of numerical\nexamples illustrate the advantage of regularization.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.05875v3"
    },
    {
        "title": "Capital Flows and the Stabilizing Role of Macroprudential Policies in\n  CESEE",
        "authors": [
            "Markus Eller",
            "Niko Hauzenberger",
            "Florian Huber",
            "Helene Schuberth",
            "Lukas Vashold"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In line with the recent policy discussion on the use of macroprudential\nmeasures to respond to cross-border risks arising from capital flows, this\npaper tries to quantify to what extent macroprudential policies (MPPs) have\nbeen able to stabilize capital flows in Central, Eastern and Southeastern\nEurope (CESEE) -- a region that experienced a substantial boom-bust cycle in\ncapital flows amid the global financial crisis and where policymakers had been\nquite active in adopting MPPs already before that crisis. To study the dynamic\nresponses of capital flows to MPP shocks, we propose a novel regime-switching\nfactor-augmented vector autoregressive (FAVAR) model. It allows to capture\npotential structural breaks in the policy regime and to control -- besides\ndomestic macroeconomic quantities -- for the impact of global factors such as\nthe global financial cycle. Feeding into this model a novel intensity-adjusted\nmacroprudential policy index, we find that tighter MPPs may be effective in\ncontaining domestic private sector credit growth and the volumes of gross\ncapital inflows in a majority of the countries analyzed. However, they do not\nseem to generally shield CESEE countries from capital flow volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.06391v1"
    },
    {
        "title": "Identification and Estimation of A Rational Inattention Discrete Choice\n  Model with Bayesian Persuasion",
        "authors": [
            "Moyu Liao"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper studies the semi-parametric identification and estimation of a\nrational inattention model with Bayesian persuasion. The identification\nrequires the observation of a cross-section of market-level outcomes. The\nempirical content of the model can be characterized by three moment conditions.\nA two-step estimation procedure is proposed to avoid computation complexity in\nthe structural model. In the empirical application, I study the persuasion\neffect of Fox News in the 2000 presidential election. Welfare analysis shows\nthat persuasion will not influence voters with high school education but will\ngenerate higher dispersion in the welfare of voters with a partial college\neducation and decrease the dispersion in the welfare of voters with a bachelors\ndegree.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.08045v1"
    },
    {
        "title": "Fixed Effects Binary Choice Models with Three or More Periods",
        "authors": [
            "Laurent Davezies",
            "Xavier D'Haultfoeuille",
            "Martin Mugnier"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We consider fixed effects binary choice models with a fixed number of periods\n$T$ and regressors without a large support. If the time-varying unobserved\nterms are i.i.d. with known distribution $F$, \\cite{chamberlain2010} shows that\nthe common slope parameter is point identified if and only if $F$ is logistic.\nHowever, he only considers in his proof $T=2$. We show that the result does not\ngeneralize to $T\\geq 3$: the common slope parameter can be identified when $F$\nbelongs to a family including the logit distribution. Identification is based\non a conditional moment restriction. Under restrictions on the covariates,\nthese moment conditions lead to point identification of relative effects. If\n$T=3$ and mild conditions hold, GMM estimators based on these conditional\nmoment restrictions reach the semiparametric efficiency bound. Finally, we\nillustrate our method by revisiting Brender and Drazen (2008).\n",
        "pdf_link": "http://arxiv.org/pdf/2009.08108v4"
    },
    {
        "title": "Semiparametric Testing with Highly Persistent Predictors",
        "authors": [
            "Bas Werker",
            "Bo Zhou"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We address the issue of semiparametric efficiency in the bivariate regression\nproblem with a highly persistent predictor, where the joint distribution of the\ninnovations is regarded an infinite-dimensional nuisance parameter. Using a\nstructural representation of the limit experiment and exploiting invariance\nrelationships therein, we construct invariant point-optimal tests for the\nregression coefficient of interest. This approach naturally leads to a family\nof feasible tests based on the component-wise ranks of the innovations that can\ngain considerable power relative to existing tests under non-Gaussian\ninnovation distributions, while behaving equivalently under Gaussianity. When\nan i.i.d. assumption on the innovations is appropriate for the data at hand,\nour tests exploit the efficiency gains possible. Moreover, we show by\nsimulation that our test remains well behaved under some forms of conditional\nheteroskedasticity.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.08291v1"
    },
    {
        "title": "Inference for Large-Scale Linear Systems with Known Coefficients",
        "authors": [
            "Zheng Fang",
            "Andres Santos",
            "Azeem M. Shaikh",
            "Alexander Torgovitsky"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper considers the problem of testing whether there exists a\nnon-negative solution to a possibly under-determined system of linear equations\nwith known coefficients. This hypothesis testing problem arises naturally in a\nnumber of settings, including random coefficient, treatment effect, and\ndiscrete choice models, as well as a class of linear programming problems. As a\nfirst contribution, we obtain a novel geometric characterization of the null\nhypothesis in terms of identified parameters satisfying an infinite set of\ninequality restrictions. Using this characterization, we devise a test that\nrequires solving only linear programs for its implementation, and thus remains\ncomputationally feasible in the high-dimensional applications that motivate our\nanalysis. The asymptotic size of the proposed test is shown to equal at most\nthe nominal level uniformly over a large class of distributions that permits\nthe number of linear equations to grow with the sample size.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.08568v2"
    },
    {
        "title": "Spillovers of Program Benefits with Missing Network Links",
        "authors": [
            "Lina Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The issue of missing network links in partially observed networks is\nfrequently neglected in empirical studies. This paper addresses this issue when\ninvestigating the spillovers of program benefits in the presence of network\ninteractions. Our method is flexible enough to account for non-i.i.d. missing\nlinks. It relies on two network measures that can be easily constructed based\non the incoming and outgoing links of the same observed network. The treatment\nand spillover effects can be point identified and consistently estimated if\nnetwork degrees are bounded for all units. We also demonstrate the bias\nreduction property of our method if network degrees of some units are\nunbounded. Monte Carlo experiments and a naturalistic simulation on real-world\nnetwork data are implemented to verify the finite-sample performance of our\nmethod. We also re-examine the spillover effects of home computer use on\nchildren's self-empowered learning.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09614v3"
    },
    {
        "title": "On the Existence of Conditional Maximum Likelihood Estimates of the\n  Binary Logit Model with Fixed Effects",
        "authors": [
            "Martin Mugnier"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  By exploiting McFadden (1974)'s results on conditional logit estimation, we\nshow that there exists a one-to-one mapping between existence and uniqueness of\nconditional maximum likelihood estimates of the binary logit model with fixed\neffects and the configuration of data points. Our results extend those in\nAlbert and Anderson (1984) for the cross-sectional case and can be used to\nbuild a simple algorithm that detects spurious estimates in finite samples. As\nan illustration, we exhibit an artificial dataset for which the STATA's command\n\\texttt{clogit} returns spurious estimates.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09998v3"
    },
    {
        "title": "A step-by-step guide to design, implement, and analyze a discrete choice\n  experiment",
        "authors": [
            "Daniel Pérez-Troncoso"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Discrete Choice Experiments (DCE) have been widely used in health economics,\nenvironmental valuation, and other disciplines. However, there is a lack of\nresources disclosing the whole procedure of carrying out a DCE. This document\naims to assist anyone wishing to use the power of DCEs to understand people's\nbehavior by providing a comprehensive guide to the procedure. This guide\ncontains all the code needed to design, implement, and analyze a DCE using only\nfree software.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.11235v1"
    },
    {
        "title": "A Computational Approach to Identification of Treatment Effects for\n  Policy Evaluation",
        "authors": [
            "Sukjin Han",
            "Shenshen Yang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  For counterfactual policy evaluation, it is important to ensure that\ntreatment parameters are relevant to policies in question. This is especially\nchallenging under unobserved heterogeneity, as is well featured in the\ndefinition of the local average treatment effect (LATE). Being intrinsically\nlocal, the LATE is known to lack external validity in counterfactual\nenvironments. This paper investigates the possibility of extrapolating local\ntreatment effects to different counterfactual settings when instrumental\nvariables are only binary. We propose a novel framework to systematically\ncalculate sharp nonparametric bounds on various policy-relevant treatment\nparameters that are defined as weighted averages of the marginal treatment\neffect (MTE). Our framework is flexible enough to fully incorporate statistical\nindependence (rather than mean independence) of instruments and a large menu of\nidentifying assumptions beyond the shape restrictions on the MTE that have been\nconsidered in prior studies. We apply our method to understand the effects of\nmedical insurance policies on the use of medical services.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13861v4"
    },
    {
        "title": "Ill-posed Estimation in High-Dimensional Models with Instrumental\n  Variables",
        "authors": [
            "Christoph Breunig",
            "Enno Mammen",
            "Anna Simoni"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper is concerned with inference about low-dimensional components of a\nhigh-dimensional parameter vector $\\beta^0$ which is identified through\ninstrumental variables. We allow for eigenvalues of the expected outer product\nof included and excluded covariates, denoted by $M$, to shrink to zero as the\nsample size increases. We propose a novel estimator based on desparsification\nof an instrumental variable Lasso estimator, which is a regularized version of\n2SLS with an additional correction term. This estimator converges to $\\beta^0$\nat a rate depending on the mapping properties of $M$ captured by a sparse link\ncondition. Linear combinations of our estimator of $\\beta^0$ are shown to be\nasymptotically normally distributed. Based on consistent covariance estimation,\nour method allows for constructing confidence intervals and statistical tests\nfor single or low-dimensional components of $\\beta^0$. In Monte-Carlo\nsimulations we analyze the finite sample behavior of our estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.00666v2"
    },
    {
        "title": "Asymptotic Refinements of a Misspecification-Robust Bootstrap for\n  Generalized Empirical Likelihood Estimators",
        "authors": [
            "Seojeong Lee"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  I propose a nonparametric iid bootstrap procedure for the empirical\nlikelihood, the exponential tilting, and the exponentially tilted empirical\nlikelihood estimators that achieves asymptotic refinements for t tests and\nconfidence intervals, and Wald tests and confidence regions based on such\nestimators. Furthermore, the proposed bootstrap is robust to model\nmisspecification, i.e., it achieves asymptotic refinements regardless of\nwhether the assumed moment condition model is correctly specified or not. This\nresult is new, because asymptotic refinements of the bootstrap based on these\nestimators have not been established in the literature even under correct model\nspecification. Monte Carlo experiments are conducted in dynamic panel data\nsetting to support the theoretical finding. As an application, bootstrap\nconfidence intervals for the returns to schooling of Hellerstein and Imbens\n(1999) are calculated. The result suggests that the returns to schooling may be\nhigher.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.00953v2"
    },
    {
        "title": "Quasi-Experimental Shift-Share Research Designs",
        "authors": [
            "Kirill Borusyak",
            "Peter Hull",
            "Xavier Jaravel"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Many studies use shift-share (or ``Bartik'') instruments, which average a set\nof shocks with exposure share weights. We provide a new econometric framework\nfor shift-share instrumental variable (SSIV) regressions in which\nidentification follows from the quasi-random assignment of shocks, while\nexposure shares are allowed to be endogenous. The framework is motivated by an\nequivalence result: the orthogonality between a shift-share instrument and an\nunobserved residual can be represented as the orthogonality between the\nunderlying shocks and a shock-level unobservable. SSIV regression coefficients\ncan similarly be obtained from an equivalent shock-level regression, motivating\nshock-level conditions for their consistency. We discuss and illustrate several\npractical insights of this framework in the setting of Autor et al. (2013),\nestimating the effect of Chinese import competition on manufacturing employment\nacross U.S. commuting zones.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01221v9"
    },
    {
        "title": "The Impact of Supervision and Incentive Process in Explaining Wage\n  Profile and Variance",
        "authors": [
            "Nitsa Kasir",
            "Idit Sohlberg"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The implementation of a supervision and incentive process for identical\nworkers may lead to wage variance that stems from employer and employee\noptimization. The harder it is to assess the nature of the labor output, the\nmore important such a process becomes, and the influence of such a process on\nwage development growth. The dynamic model presented in this paper shows that\nan employer will choose to pay a worker a starting wage that is less than what\nhe deserves, resulting in a wage profile that fits the classic profile in the\nhuman-capital literature. The wage profile and wage variance rise at times of\ntechnological advancements, which leads to increased turnover as older workers\nare replaced by younger workers due to a rise in the relative marginal cost of\nthe former.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01332v1"
    },
    {
        "title": "Asymptotic Refinements of a Misspecification-Robust Bootstrap for\n  Generalized Method of Moments Estimators",
        "authors": [
            "Seojeong Lee"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  I propose a nonparametric iid bootstrap that achieves asymptotic refinements\nfor t tests and confidence intervals based on GMM estimators even when the\nmodel is misspecified. In addition, my bootstrap does not require recentering\nthe moment function, which has been considered as critical for GMM. Regardless\nof model misspecification, the proposed bootstrap achieves the same sharp\nmagnitude of refinements as the conventional bootstrap methods which establish\nasymptotic refinements by recentering in the absence of misspecification. The\nkey idea is to link the misspecified bootstrap moment condition to the large\nsample theory of GMM under misspecification of Hall and Inoue (2003). Two\nexamples are provided: Combining data sets and invalid instrumental variables.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01450v1"
    },
    {
        "title": "A Consistent Variance Estimator for 2SLS When Instruments Identify\n  Different LATEs",
        "authors": [
            "Seojeong Lee"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Under treatment effect heterogeneity, an instrument identifies the\ninstrument-specific local average treatment effect (LATE). With multiple\ninstruments, two-stage least squares (2SLS) estimand is a weighted average of\ndifferent LATEs. What is often overlooked in the literature is that the\npostulated moment condition evaluated at the 2SLS estimand does not hold unless\nthose LATEs are the same. If so, the conventional heteroskedasticity-robust\nvariance estimator would be inconsistent, and 2SLS standard errors based on\nsuch estimators would be incorrect. I derive the correct asymptotic\ndistribution, and propose a consistent asymptotic variance estimator by using\nthe result of Hall and Inoue (2003, Journal of Econometrics) on misspecified\nmoment condition models. This can be used to correctly calculate the standard\nerrors regardless of whether there is more than one LATE or not.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01457v1"
    },
    {
        "title": "Leave-out estimation of variance components",
        "authors": [
            "Patrick Kline",
            "Raffaele Saggio",
            "Mikkel Sølvsten"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We propose leave-out estimators of quadratic forms designed for the study of\nlinear models with unrestricted heteroscedasticity. Applications include\nanalysis of variance and tests of linear restrictions in models with many\nregressors. An approximation algorithm is provided that enables accurate\ncomputation of the estimator in very large datasets. We study the large sample\nproperties of our estimator allowing the number of regressors to grow in\nproportion to the number of observations. Consistency is established in a\nvariety of settings where plug-in methods and estimators predicated on\nhomoscedasticity exhibit first-order biases. For quadratic forms of increasing\nrank, the limiting distribution can be represented by a linear combination of\nnormal and non-central $\\chi^2$ random variables, with normality ensuing under\nstrong identification. Standard error estimators are proposed that enable tests\nof linear restrictions and the construction of uniformly valid confidence\nintervals for quadratic forms of interest. We find in Italian social security\nrecords that leave-out estimates of a variance decomposition in a two-way fixed\neffects model of wage determination yield substantially different conclusions\nregarding the relative contribution of workers, firms, and worker-firm sorting\nto wage inequality than conventional methods. Monte Carlo exercises corroborate\nthe accuracy of our asymptotic approximations, with clear evidence of\nnon-normality emerging when worker mobility between blocks of firms is limited.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01494v2"
    },
    {
        "title": "A Quantitative Analysis of Possible Futures of Autonomous Transport",
        "authors": [
            "Christopher L. Benson",
            "Pranav D Sumanth",
            "Alina P Colling"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Autonomous ships (AS) used for cargo transport have gained a considerable\namount of attention in recent years. They promise benefits such as reduced crew\ncosts, increased safety and increased flexibility. This paper explores the\neffects of a faster increase in technological performance in maritime shipping\nachieved by leveraging fast-improving technological domains such as computer\nprocessors, and advanced energy storage. Based on historical improvement rates\nof several modes of transport (Cargo Ships, Air, Rail, Trucking) a simplified\nMarkov-chain Monte-Carlo (MCMC) simulation of an intermodal transport model\n(IMTM) is used to explore the effects of differing technological improvement\nrates for AS. The results show that the annual improvement rates of traditional\nshipping (Ocean Cargo Ships = 2.6%, Air Cargo = 5.5%, Trucking = 0.6%, Rail =\n1.9%, Inland Water Transport = 0.4%) improve at lower rates than technologies\nassociated with automation such as Computer Processors (35.6%), Fuel Cells\n(14.7%) and Automotive Autonomous Hardware (27.9%). The IMTM simulations up to\nthe year 2050 show that the introduction of any mode of autonomous transport\nwill increase competition in lower cost shipping options, but is unlikely to\nsignificantly alter the overall distribution of transport mode costs. Secondly,\nif all forms of transport end up converting to autonomous systems, then the\nuncertainty surrounding the improvement rates yields a complex intermodal\ntransport solution involving several options, all at a much lower cost over\ntime. Ultimately, the research shows a need for more accurate measurement of\ncurrent autonomous transport costs and how they are changing over time.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01696v1"
    },
    {
        "title": "A Growth Model with Unemployment",
        "authors": [
            "Mina Mahmoudi",
            "Mark Pingle"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  A standard growth model is modified in a straightforward way to incorporate\nwhat Keynes (1936) suggests in the \"essence\" of his general theory. The\ntheoretical essence is the idea that exogenous changes in investment cause\nchanges in employment and unemployment. We implement this idea by assuming the\npath for capital growth rate is exogenous in the growth model. The result is a\ngrowth model that can explain both long term trends and fluctuations around the\ntrend. The modified growth model was tested using the U.S. economic data from\n1947 to 2014. The hypothesized inverse relationship between the capital growth\nand changes in unemployment was confirmed, and the structurally estimated model\nfits fluctuations in unemployment reasonably well.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.04228v1"
    },
    {
        "title": "The Role of Agricultural Sector Productivity in Economic Growth: The\n  Case of Iran's Economic Development Plan",
        "authors": [
            "Morteza Tahamipour",
            "Mina Mahmoudi"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This study provides the theoretical framework and empirical model for\nproductivity growth evaluations in agricultural sector as one of the most\nimportant sectors in Iran's economic development plan. We use the Solow\nresidual model to measure the productivity growth share in the value-added\ngrowth of the agricultural sector. Our time series data includes value-added\nper worker, employment, and capital in this sector. The results show that the\naverage total factor productivity growth rate in the agricultural sector is\n-0.72% during 1991-2010. Also, during this period, the share of total factor\nproductivity growth in the value-added growth is -19.6%, while it has been\nforecasted to be 33.8% in the fourth development plan. Considering the\neffective role of capital in the agricultural low productivity, we suggest\napplying productivity management plans (especially in regards of capital\nproductivity) to achieve future growth goals.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.04235v1"
    },
    {
        "title": "Estimating Trade-Related Adjustment Costs in the Agricultural Sector in\n  Iran",
        "authors": [
            "Omid Karami",
            "Mina Mahmoudi"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Tariff liberalization and its impact on tax revenue is an important\nconsideration for developing countries, because they are increasingly facing\nthe difficult task of implementing and harmonizing regional and international\ntrade commitments. The tariff reform and its costs for Iranian government is\none of the issues that are examined in this study. Another goal of this paper\nis, estimating the cost of trade liberalization. On this regard, imports value\nof agricultural sector in Iran in 2010 was analyzed according to two scenarios.\nFor reforming nuisance tariff, a VAT policy is used in both scenarios. In this\nstudy, TRIST method is used. In the first scenario, imports' value decreased to\na level equal to the second scenario and higher tariff revenue will be created.\nThe results show that reducing the average tariff rate does not always result\nin the loss of tariff revenue. This paper is a witness that different forms of\ntariff can generate different amount of income when they have same level of\nliberalization and equal effect on producers. Therefore, using a good tariff\nregime can help a government to generate income when increases social welfare\nby liberalization.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.04238v1"
    },
    {
        "title": "On the relation between Sion's minimax theorem and existence of Nash\n  equilibrium in asymmetric multi-players zero-sum game with only one alien",
        "authors": [
            "Atsuhiro Satoh",
            "Yasuhito Tanaka"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We consider the relation between Sion's minimax theorem for a continuous\nfunction and a Nash equilibrium in an asymmetric multi-players zero-sum game in\nwhich only one player is different from other players, and the game is\nsymmetric for the other players. Then,\n  1. The existence of a Nash equilibrium, which is symmetric for players other\nthan one player, implies Sion's minimax theorem for pairs of this player and\none of other players with symmetry for the other players.\n  2. Sion's minimax theorem for pairs of one player and one of other players\nwith symmetry for the other players implies the existence of a Nash equilibrium\nwhich is symmetric for the other players.\n  Thus, they are equivalent.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07253v1"
    },
    {
        "title": "Cluster-Robust Standard Errors for Linear Regression Models with Many\n  Controls",
        "authors": [
            "Riccardo D'Adamo"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  It is common practice in empirical work to employ cluster-robust standard\nerrors when using the linear regression model to estimate some\nstructural/causal effect of interest. Researchers also often include a large\nset of regressors in their model specification in order to control for observed\nand unobserved confounders. In this paper we develop inference methods for\nlinear regression models with many controls and clustering. We show that\ninference based on the usual cluster-robust standard errors by Liang and Zeger\n(1986) is invalid in general when the number of controls is a non-vanishing\nfraction of the sample size. We then propose a new clustered standard errors\nformula that is robust to the inclusion of many controls and allows to carry\nout valid inference in a variety of high-dimensional linear regression models,\nincluding fixed effects panel data models and the semiparametric partially\nlinear model. Monte Carlo evidence supports our theoretical results and shows\nthat our proposed variance estimator performs well in finite samples. The\nproposed method is also illustrated with an empirical application that\nre-visits Donohue III and Levitt's (2001) study of the impact of abortion on\ncrime.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07314v3"
    },
    {
        "title": "Shift-Share Designs: Theory and Inference",
        "authors": [
            "Rodrigo Adão",
            "Michal Kolesár",
            "Eduardo Morales"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We study inference in shift-share regression designs, such as when a regional\noutcome is regressed on a weighted average of sectoral shocks, using regional\nsector shares as weights. We conduct a placebo exercise in which we estimate\nthe effect of a shift-share regressor constructed with randomly generated\nsectoral shocks on actual labor market outcomes across U.S. Commuting Zones.\nTests based on commonly used standard errors with 5\\% nominal significance\nlevel reject the null of no effect in up to 55\\% of the placebo samples. We use\na stylized economic model to show that this overrejection problem arises\nbecause regression residuals are correlated across regions with similar\nsectoral shares, independently of their geographic location. We derive novel\ninference methods that are valid under arbitrary cross-regional correlation in\nthe regression residuals. We show using popular applications of shift-share\ndesigns that our methods may lead to substantially wider confidence intervals\nin practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07928v5"
    },
    {
        "title": "The transmission of uncertainty shocks on income inequality: State-level\n  evidence from the United States",
        "authors": [
            "Manfred M. Fischer",
            "Florian Huber",
            "Michael Pfarrhofer"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper, we explore the relationship between state-level household\nincome inequality and macroeconomic uncertainty in the United States. Using a\nnovel large-scale macroeconometric model, we shed light on regional disparities\nof inequality responses to a national uncertainty shock. The results suggest\nthat income inequality decreases in most states, with a pronounced degree of\nheterogeneity in terms of shapes and magnitudes of the dynamic responses. By\ncontrast, some few states, mostly located in the West and South census region,\ndisplay increasing levels of income inequality over time. We find that this\ndirectional pattern in responses is mainly driven by the income composition and\nlabor market fundamentals. In addition, forecast error variance decompositions\nallow for a quantitative assessment of the importance of uncertainty shocks in\nexplaining income inequality. The findings highlight that volatility shocks\naccount for a considerable fraction of forecast error variance for most states\nconsidered. Finally, a regression-based analysis sheds light on the driving\nforces behind differences in state-specific inequality responses.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.08278v1"
    },
    {
        "title": "Semiparametrically Point-Optimal Hybrid Rank Tests for Unit Roots",
        "authors": [
            "Bo Zhou",
            "Ramon van den Akker",
            "Bas J. M. Werker"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We propose a new class of unit root tests that exploits invariance properties\nin the Locally Asymptotically Brownian Functional limit experiment associated\nto the unit root model. The invariance structures naturally suggest tests that\nare based on the ranks of the increments of the observations, their average,\nand an assumed reference density for the innovations. The tests are\nsemiparametric in the sense that they are valid, i.e., have the correct\n(asymptotic) size, irrespective of the true innovation density. For a correctly\nspecified reference density, our test is point-optimal and nearly efficient.\nFor arbitrary reference densities, we establish a Chernoff-Savage type result,\ni.e., our test performs as well as commonly used tests under Gaussian\ninnovations but has improved power under other, e.g., fat-tailed or skewed,\ninnovation distributions. To avoid nonparametric estimation, we propose a\nsimplified version of our test that exhibits the same asymptotic properties,\nexcept for the Chernoff-Savage result that we are only able to demonstrate by\nmeans of simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.09304v1"
    },
    {
        "title": "Point-identification in multivariate nonseparable triangular models",
        "authors": [
            "Florian Gunsilius"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this article we introduce a general nonparametric point-identification\nresult for nonseparable triangular models with a multivariate first- and second\nstage. Based on this we prove point-identification of Hedonic models with\nmultivariate heterogeneity and endogenous observable characteristics, extending\nand complementing identification results from the literature which all require\nexogeneity. As an additional application of our theoretical result, we show\nthat the BLP model (Berry et al. 1995) can also be identified without index\nrestrictions.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.09680v1"
    },
    {
        "title": "Asymptotic Theory for Clustered Samples",
        "authors": [
            "Bruce E. Hansen",
            "Seojeong Lee"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We provide a complete asymptotic distribution theory for clustered data with\na large number of independent groups, generalizing the classic laws of large\nnumbers, uniform laws, central limit theory, and clustered covariance matrix\nestimation. Our theory allows for clustered observations with heterogeneous and\nunbounded cluster sizes. Our conditions cleanly nest the classical results for\ni.n.i.d. observations, in the sense that our conditions specialize to the\nclassical conditions under independent sampling. We use this theory to develop\na full asymptotic distribution theory for estimation based on linear\nleast-squares, 2SLS, nonlinear MLE, and nonlinear GMM.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.01497v1"
    },
    {
        "title": "A General Framework for Prediction in Time Series Models",
        "authors": [
            "Eric Beutner",
            "Alexander Heinemann",
            "Stephan Smeekes"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In this paper we propose a general framework to analyze prediction in time\nseries models and show how a wide class of popular time series models satisfies\nthis framework. We postulate a set of high-level assumptions, and formally\nverify these assumptions for the aforementioned time series models. Our\nframework coincides with that of Beutner et al. (2019, arXiv:1710.00643) who\nestablish the validity of conditional confidence intervals for predictions made\nin this framework. The current paper therefore complements the results in\nBeutner et al. (2019, arXiv:1710.00643) by providing practically relevant\napplications of their theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.01622v1"
    },
    {
        "title": "A Bootstrap Test for the Existence of Moments for GARCH Processes",
        "authors": [
            "Alexander Heinemann"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper studies the joint inference on conditional volatility parameters\nand the innovation moments by means of bootstrap to test for the existence of\nmoments for GARCH(p,q) processes. We propose a residual bootstrap to mimic the\njoint distribution of the quasi-maximum likelihood estimators and the empirical\nmoments of the residuals and also prove its validity. A bootstrap-based test\nfor the existence of moments is proposed, which provides asymptotically\ncorrectly-sized tests without losing its consistency property. It is simple to\nimplement and extends to other GARCH-type settings. A simulation study\ndemonstrates the test's size and power properties in finite samples and an\nempirical application illustrates the testing approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.01808v3"
    },
    {
        "title": "Partial Identification in Matching Models for the Marriage Market",
        "authors": [
            "Cristina Gualdani",
            "Shruti Sinha"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We study partial identification of the preference parameters in the\none-to-one matching model with perfectly transferable utilities. We do so\nwithout imposing parametric distributional assumptions on the unobserved\nheterogeneity and with data on one large market. We provide a tractable\ncharacterisation of the identified set under various classes of nonparametric\ndistributional assumptions on the unobserved heterogeneity. Using our\nmethodology, we re-examine some of the relevant questions in the empirical\nliterature on the marriage market, which have been previously studied under the\nLogit assumption. Our results reveal that many findings in the aforementioned\nliterature are primarily driven by such parametric restrictions.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.05610v6"
    },
    {
        "title": "Weak Identification and Estimation of Social Interaction Models",
        "authors": [
            "Guy Tchuente"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The identification of the network effect is based on either group size\nvariation, the structure of the network or the relative position in the\nnetwork. I provide easy-to-verify necessary conditions for identification of\nundirected network models based on the number of distinct eigenvalues of the\nadjacency matrix. Identification of network effects is possible; although in\nmany empirical situations existing identification strategies may require the\nuse of many instruments or instruments that could be strongly correlated with\neach other. The use of highly correlated instruments or many instruments may\nlead to weak identification or many instruments bias. This paper proposes\nregularized versions of the two-stage least squares (2SLS) estimators as a\nsolution to these problems. The proposed estimators are consistent and\nasymptotically normal. A Monte Carlo study illustrates the properties of the\nregularized estimators. An empirical application, assessing a local government\ntax competition model, shows the empirical relevance of using regularization\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.06143v1"
    },
    {
        "title": "Discrete Choice under Risk with Limited Consideration",
        "authors": [
            "Levon Barseghyan",
            "Francesca Molinari",
            "Matthew Thirkettle"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper is concerned with learning decision makers' preferences using data\non observed choices from a finite set of risky alternatives. We propose a\ndiscrete choice model with unobserved heterogeneity in consideration sets and\nin standard risk aversion. We obtain sufficient conditions for the model's\nsemi-nonparametric point identification, including in cases where consideration\ndepends on preferences and on some of the exogenous variables. Our method\nyields an estimator that is easy to compute and is applicable in markets with\nlarge choice sets. We illustrate its properties using a dataset on property\ninsurance purchases.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.06629v3"
    },
    {
        "title": "Estimation and Inference for Synthetic Control Methods with Spillover\n  Effects",
        "authors": [
            "Jianfei Cao",
            "Connor Dowd"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The synthetic control method is often used in treatment effect estimation\nwith panel data where only a few units are treated and a small number of\npost-treatment periods are available. Current estimation and inference\nprocedures for synthetic control methods do not allow for the existence of\nspillover effects, which are plausible in many applications. In this paper, we\nconsider estimation and inference for synthetic control methods, allowing for\nspillover effects. We propose estimators for both direct treatment effects and\nspillover effects and show they are asymptotically unbiased. In addition, we\npropose an inferential procedure and show it is asymptotically unbiased. Our\nestimation and inference procedure applies to cases with multiple treated units\nor periods, and where the underlying factor model is either stationary or\ncointegrated. In simulations, we confirm that the presence of spillovers\nrenders current methods biased and have distorted sizes, whereas our methods\nyield properly sized tests and retain reasonable power. We apply our method to\na classic empirical example that investigates the effect of California's\ntobacco control program as in Abadie et al. (2010) and find evidence of\nspillovers.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.07343v2"
    },
    {
        "title": "Eliciting ambiguity with mixing bets",
        "authors": [
            "Patrick Schmidt"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Preferences for mixing can reveal ambiguity perception and attitude on a\nsingle event. The validity of the approach is discussed for multiple preference\nclasses including maxmin, maxmax, variational, and smooth second-order\npreferences. An experimental implementation suggests that participants perceive\nalmost as much ambiguity for the stock index and actions of other participants\nas for the Ellsberg urn, indicating the importance of ambiguity in real-world\ndecision-making.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.07447v5"
    },
    {
        "title": "Robust Ranking of Happiness Outcomes: A Median Regression Perspective",
        "authors": [
            "Le-Yu Chen",
            "Ekaterina Oparina",
            "Nattavudh Powdthavee",
            "Sorawoot Srisuma"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Ordered probit and logit models have been frequently used to estimate the\nmean ranking of happiness outcomes (and other ordinal data) across groups.\nHowever, it has been recently highlighted that such ranking may not be\nidentified in most happiness applications. We suggest researchers focus on\nmedian comparison instead of the mean. This is because the median rank can be\nidentified even if the mean rank is not. Furthermore, median ranks in probit\nand logit models can be readily estimated using standard statistical softwares.\nThe median ranking, as well as ranking for other quantiles, can also be\nestimated semiparametrically and we provide a new constrained mixed integer\noptimization procedure for implementation. We apply it to estimate a happiness\nequation using General Social Survey data of the US.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.07696v3"
    },
    {
        "title": "Nonparametric Counterfactuals in Random Utility Models",
        "authors": [
            "Yuichi Kitamura",
            "Jörg Stoye"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We bound features of counterfactual choices in the nonparametric random\nutility model of demand, i.e. if observable choices are repeated cross-sections\nand one allows for unrestricted, unobserved heterogeneity. In this setting,\ntight bounds are developed on counterfactual discrete choice probabilities and\non the expectation and c.d.f. of (functionals of) counterfactual stochastic\ndemand.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.08350v2"
    },
    {
        "title": "Counterfactual Inference in Duration Models with Random Censoring",
        "authors": [
            "Jiun-Hua Su"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a counterfactual Kaplan-Meier estimator that incorporates\nexogenous covariates and unobserved heterogeneity of unrestricted\ndimensionality in duration models with random censoring. Under some regularity\nconditions, we establish the joint weak convergence of the proposed\ncounterfactual estimator and the unconditional Kaplan-Meier (1958) estimator.\nApplying the functional delta method, we make inference on the cumulative\nhazard policy effect, that is, the change of duration dependence in response to\na counterfactual policy. We also evaluate the finite sample performance of the\nproposed counterfactual estimation method in a Monte Carlo study.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.08502v1"
    },
    {
        "title": "Robust Principal Component Analysis with Non-Sparse Errors",
        "authors": [
            "Jushan Bai",
            "Junlong Feng"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We show that when a high-dimensional data matrix is the sum of a low-rank\nmatrix and a random error matrix with independent entries, the low-rank\ncomponent can be consistently estimated by solving a convex minimization\nproblem. We develop a new theoretical argument to establish consistency without\nassuming sparsity or the existence of any moments of the error matrix, so that\nfat-tailed continuous random errors such as Cauchy are allowed. The results are\nillustrated by simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.08735v2"
    },
    {
        "title": "Estimation of Dynamic Panel Threshold Model using Stata",
        "authors": [
            "Myung Hwan Seo",
            "Sueyoul Kim",
            "Young-Joo Kim"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We develop a Stata command xthenreg to implement the first-differenced GMM\nestimation of the dynamic panel threshold model, which Seo and Shin (2016,\nJournal of Econometrics 195: 169-186) have proposed. Furthermore, We derive the\nasymptotic variance formula for a kink constrained GMM estimator of the dynamic\nthreshold model and include an estimation algorithm. We also propose a fast\nbootstrap algorithm to implement the bootstrap for the linearity test. The use\nof the command is illustrated through a Monte Carlo simulation and an economic\napplication.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.10318v1"
    },
    {
        "title": "The Empirical Content of Binary Choice Models",
        "authors": [
            "Debopam Bhattacharya"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  An important goal of empirical demand analysis is choice and welfare\nprediction on counterfactual budget sets arising from potential\npolicy-interventions. Such predictions are more credible when made without\narbitrary functional-form/distributional assumptions, and instead based solely\non economic rationality, i.e. that choice is consistent with utility\nmaximization by a heterogeneous population. This paper investigates\nnonparametric economic rationality in the empirically important context of\nbinary choice. We show that under general unobserved heterogeneity, economic\nrationality is equivalent to a pair of Slutsky-like shape-restrictions on\nchoice-probability functions. The forms of these restrictions differ from\nSlutsky-inequalities for continuous goods. Unlike McFadden-Richter's stochastic\nrevealed preference, our shape-restrictions (a) are global, i.e. their forms do\nnot depend on which and how many budget-sets are observed, (b) are closed-form,\nhence easy to impose on parametric/semi/non-parametric models in practical\napplications, and (c) provide computationally simple, theory-consistent bounds\non demand and welfare predictions on counterfactual budget-sets.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.11012v4"
    },
    {
        "title": "Integrability and Identification in Multinomial Choice Models",
        "authors": [
            "Debopam Bhattacharya"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  McFadden's random-utility model of multinomial choice has long been the\nworkhorse of applied research. We establish shape-restrictions under which\nmultinomial choice-probability functions can be rationalized via random-utility\nmodels with nonparametric unobserved heterogeneity and general income-effects.\nWhen combined with an additional restriction, the above conditions are\nequivalent to the canonical Additive Random Utility Model. The\nsufficiency-proof is constructive, and facilitates nonparametric identification\nof preference-distributions without requiring identification-at-infinity type\narguments. A corollary shows that Slutsky-symmetry, a key condition for\nprevious rationalizability results, is equivalent to absence of income-effects.\nOur results imply theory-consistent nonparametric bounds for\nchoice-probabilities on counterfactual budget-sets. They also apply to widely\nused random-coefficient models, upon conditioning on observable choice\ncharacteristics. The theory of partial differential equations plays a key role\nin our analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.11017v4"
    },
    {
        "title": "Robust Nearly-Efficient Estimation of Large Panels with Factor\n  Structures",
        "authors": [
            "Marco Avarucci",
            "Paolo Zaffaroni"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper studies estimation of linear panel regression models with\nheterogeneous coefficients, when both the regressors and the residual contain a\npossibly common, latent, factor structure. Our theory is (nearly) efficient,\nbecause based on the GLS principle, and also robust to the specification of\nsuch factor structure because it does not require any information on the number\nof factors nor estimation of the factor structure itself. We first show how the\nunfeasible GLS estimator not only affords an efficiency improvement but, more\nimportantly, provides a bias-adjusted estimator with the conventional limiting\ndistribution, for situations where the OLS is affected by a first-order bias.\nThe technical challenge resolved in the paper is to show how these properties\nare preserved for a class of feasible GLS estimators in a double-asymptotics\nsetting. Our theory is illustrated by means of Monte Carlo exercises and, then,\nwith an empirical application using individual asset returns and firms'\ncharacteristics data.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.11181v1"
    },
    {
        "title": "Lasso under Multi-way Clustering: Estimation and Post-selection\n  Inference",
        "authors": [
            "Harold D. Chiang",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper studies high-dimensional regression models with lasso when data is\nsampled under multi-way clustering. First, we establish convergence rates for\nthe lasso and post-lasso estimators. Second, we propose a novel inference\nmethod based on a post-double-selection procedure and show its asymptotic\nvalidity. Our procedure can be easily implemented with existing statistical\npackages. Simulation results demonstrate that the proposed procedure works well\nin finite sample. We illustrate the proposed method with a couple of empirical\napplications to development and growth economics.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.02107v3"
    },
    {
        "title": "Regression Discontinuity Design with Multiple Groups for Heterogeneous\n  Causal Effect Estimation",
        "authors": [
            "Takayuki Toda",
            "Ayako Wakano",
            "Takahiro Hoshino"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a new estimation method for heterogeneous causal effects which\nutilizes a regression discontinuity (RD) design for multiple datasets with\ndifferent thresholds. The standard RD design is frequently used in applied\nresearches, but the result is very limited in that the average treatment\neffects is estimable only at the threshold on the running variable. In\napplication studies it is often the case that thresholds are different among\ndatabases from different regions or firms. For example thresholds for\nscholarship differ with states. The proposed estimator based on the augmented\ninverse probability weighted local linear estimator can estimate the average\neffects at an arbitrary point on the running variable between the thresholds\nunder mild conditions, while the method adjust for the difference of the\ndistributions of covariates among datasets. We perform simulations to\ninvestigate the performance of the proposed estimator in the finite samples.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.04443v1"
    },
    {
        "title": "Analyzing Subjective Well-Being Data with Misclassification",
        "authors": [
            "Ekaterina Oparina",
            "Sorawoot Srisuma"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We use novel nonparametric techniques to test for the presence of\nnon-classical measurement error in reported life satisfaction (LS) and study\nthe potential effects from ignoring it. Our dataset comes from Wave 3 of the UK\nUnderstanding Society that is surveyed from 35,000 British households. Our test\nfinds evidence of measurement error in reported LS for the entire dataset as\nwell as for 26 out of 32 socioeconomic subgroups in the sample. We estimate the\njoint distribution of reported and latent LS nonparametrically in order to\nunderstand the mis-reporting behavior. We show this distribution can then be\nused to estimate parametric models of latent LS. We find measurement error bias\nis not severe enough to distort the main drivers of LS. But there is an\nimportant difference that is policy relevant. We find women tend to over-report\ntheir latent LS relative to men. This may help explain the gender puzzle that\nquestions why women are reportedly happier than men despite being worse off on\nobjective outcomes such as income and employment.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.06037v1"
    },
    {
        "title": "Time Series Analysis and Forecasting of the US Housing Starts using\n  Econometric and Machine Learning Model",
        "authors": [
            "Sudiksha Joshi"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In this research paper, I have performed time series analysis and forecasted\nthe monthly value of housing starts for the year 2019 using several econometric\nmethods - ARIMA(X), VARX, (G)ARCH and machine learning algorithms - artificial\nneural networks, ridge regression, K-Nearest Neighbors, and support vector\nregression, and created an ensemble model. The ensemble model stacks the\npredictions from various individual models, and gives a weighted average of all\npredictions. The analyses suggest that the ensemble model has performed the\nbest among all the models as the prediction errors are the lowest, while the\neconometric models have higher error rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.07848v1"
    },
    {
        "title": "Inducing Sparsity and Shrinkage in Time-Varying Parameter Models",
        "authors": [
            "Florian Huber",
            "Gary Koop",
            "Luca Onorante"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Time-varying parameter (TVP) models have the potential to be\nover-parameterized, particularly when the number of variables in the model is\nlarge. Global-local priors are increasingly used to induce shrinkage in such\nmodels. But the estimates produced by these priors can still have appreciable\nuncertainty. Sparsification has the potential to reduce this uncertainty and\nimprove forecasts. In this paper, we develop computationally simple methods\nwhich both shrink and sparsify TVP models. In a simulated data exercise we show\nthe benefits of our shrink-then-sparsify approach in a variety of sparse and\ndense TVP regressions. In a macroeconomic forecasting exercise, we find our\napproach to substantially improve forecast performance relative to shrinkage\nalone.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.10787v2"
    },
    {
        "title": "Local Asymptotic Equivalence of the Bai and Ng (2004) and Moon and\n  Perron (2004) Frameworks for Panel Unit Root Testing",
        "authors": [
            "Oliver Wichert",
            "I. Gaia Becheri",
            "Feike C. Drost",
            "Ramon van den Akker"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper considers unit-root tests in large n and large T heterogeneous\npanels with cross-sectional dependence generated by unobserved factors. We\nreconsider the two prevalent approaches in the literature, that of Moon and\nPerron (2004) and the PANIC setup proposed in Bai and Ng (2004). While these\nhave been considered as completely different setups, we show that, in case of\nGaussian innovations, the frameworks are asymptotically equivalent in the sense\nthat both experiments are locally asymptotically normal (LAN) with the same\ncentral sequence. Using Le Cam's theory of statistical experiments we determine\nthe local asymptotic power envelope and derive an optimal test jointly in both\nsetups. We show that the popular Moon and Perron (2004) and Bai and Ng (2010)\ntests only attain the power envelope in case there is no heterogeneity in the\nlong-run variance of the idiosyncratic components. The new test is\nasymptotically uniformly most powerful irrespective of possible heterogeneity.\nMoreover, it turns out that for any test, satisfying a mild regularity\ncondition, the size and local asymptotic power are the same under both data\ngenerating processes. Thus, applied researchers do not need to decide on one of\nthe two frameworks to conduct unit root tests. Monte-Carlo simulations\ncorroborate our asymptotic results and document significant gains in\nfinite-sample power if the variances of the idiosyncratic shocks differ\nsubstantially among the cross sectional units.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.11184v1"
    },
    {
        "title": "Threshold Regression with Nonparametric Sample Splitting",
        "authors": [
            "Yoonseok Lee",
            "Yulong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper develops a threshold regression model where an unknown\nrelationship between two variables nonparametrically determines the threshold.\nWe allow the observations to be cross-sectionally dependent so that the model\ncan be applied to determine an unknown spatial border for sample splitting over\na random field. We derive the uniform rate of convergence and the nonstandard\nlimiting distribution of the nonparametric threshold estimator. We also obtain\nthe root-n consistency and the asymptotic normality of the regression\ncoefficient estimator. Our model has broad empirical relevance as illustrated\nby estimating the tipping point in social segregation problems as a function of\ndemographic characteristics; and determining metropolitan area boundaries using\nnighttime light intensity collected from satellite imagery. We find that the\nnew empirical results are substantially different from those in the existing\nstudies.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.13140v3"
    },
    {
        "title": "A multifactor regime-switching model for inter-trade durations in the\n  limit order market",
        "authors": [
            "Zhicheng Li",
            "Haipeng Xing",
            "Xinyun Chen"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper studies inter-trade durations in the NASDAQ limit order market and\nfinds that inter-trade durations in ultra-high frequency have two modes. One\nmode is to the order of approximately 10^{-4} seconds, and the other is to the\norder of 1 second. This phenomenon and other empirical evidence suggest that\nthere are two regimes associated with the dynamics of inter-trade durations,\nand the regime switchings are driven by the changes of high-frequency traders\n(HFTs) between providing and taking liquidity. To find how the two modes depend\non information in the limit order book (LOB), we propose a two-state\nmultifactor regime-switching (MF-RSD) model for inter-trade durations, in which\nthe probabilities transition matrices are time-varying and depend on some\nlagged LOB factors. The MF-RSD model has good in-sample fitness and the\nsuperior out-of-sample performance, compared with some benchmark duration\nmodels. Our findings of the effects of LOB factors on the inter-trade durations\nhelp to understand more about the high-frequency market microstructure.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.00764v1"
    },
    {
        "title": "Clustering and External Validity in Randomized Controlled Trials",
        "authors": [
            "Antoine Deeb",
            "Clément de Chaisemartin"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The randomization inference literature studying randomized controlled trials\n(RCTs) assumes that units' potential outcomes are deterministic. This\nassumption is unlikely to hold, as stochastic shocks may take place during the\nexperiment. In this paper, we consider the case of an RCT with individual-level\ntreatment assignment, and we allow for individual-level and cluster-level (e.g.\nvillage-level) shocks. We show that one can draw inference on the ATE\nconditional on the realizations of the cluster-level shocks, using\nheteroskedasticity-robust standard errors, or on the ATE netted out of those\nshocks, using cluster-robust standard errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.01052v7"
    },
    {
        "title": "Bilinear form test statistics for extremum estimation",
        "authors": [
            "Federico Crudu",
            "Felipe Osorio"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper develops a set of test statistics based on bilinear forms in the\ncontext of the extremum estimation framework with particular interest in\nnonlinear hypothesis. We show that the proposed statistic converges to a\nconventional chi-square limit. A Monte Carlo experiment suggests that the test\nstatistic works well in finite samples.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.01410v1"
    },
    {
        "title": "Estimating Large Mixed-Frequency Bayesian VAR Models",
        "authors": [
            "Sebastian Ankargren",
            "Paulina Jonéus"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We discuss the issue of estimating large-scale vector autoregressive (VAR)\nmodels with stochastic volatility in real-time situations where data are\nsampled at different frequencies. In the case of a large VAR with stochastic\nvolatility, the mixed-frequency data warrant an additional step in the already\ncomputationally challenging Markov Chain Monte Carlo algorithm used to sample\nfrom the posterior distribution of the parameters. We suggest the use of a\nfactor stochastic volatility model to capture a time-varying error covariance\nstructure. Because the factor stochastic volatility model renders the equations\nof the VAR conditionally independent, settling for this particular stochastic\nvolatility model comes with major computational benefits. First, we are able to\nimprove upon the mixed-frequency simulation smoothing step by leveraging a\nunivariate and adaptive filtering algorithm. Second, the regression parameters\ncan be sampled equation-by-equation in parallel. These computational features\nof the model alleviate the computational burden and make it possible to move\nthe mixed-frequency VAR to the high-dimensional regime. We illustrate the model\nby an application to US data using our mixed-frequency VAR with 20, 34 and 119\nvariables.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.02231v1"
    },
    {
        "title": "Synthetic Control Inference for Staggered Adoption: Estimating the\n  Dynamic Effects of Board Gender Diversity Policies",
        "authors": [
            "Jianfei Cao",
            "Shirley Lu"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We introduce a synthetic control methodology to study policies with staggered\nadoption. Many policies, such as the board gender quota, are replicated by\nother policy setters at different time frames. Our method estimates the dynamic\naverage treatment effects on the treated using variation introduced by the\nstaggered adoption of policies. Our method gives asymptotically unbiased\nestimators of many interesting quantities and delivers asymptotically valid\ninference. By using the proposed method and national labor data in Europe, we\nfind evidence that quota regulation on board diversity leads to a decrease in\npart-time employment, and an increase in full-time employment for female\nprofessionals.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.06320v1"
    },
    {
        "title": "Econometrics For Decision Making: Building Foundations Sketched By\n  Haavelmo And Wald",
        "authors": [
            "Charles F. Manski"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Haavelmo (1944) proposed a probabilistic structure for econometric modeling,\naiming to make econometrics useful for decision making. His fundamental\ncontribution has become thoroughly embedded in subsequent econometric research,\nyet it could not answer all the deep issues that the author raised. Notably,\nHaavelmo struggled to formalize the implications for decision making of the\nfact that models can at most approximate actuality. In the same period, Wald\n(1939, 1945) initiated his own seminal development of statistical decision\ntheory. Haavelmo favorably cited Wald, but econometrics did not embrace\nstatistical decision theory. Instead, it focused on study of identification,\nestimation, and statistical inference. This paper proposes statistical decision\ntheory as a framework for evaluation of the performance of models in decision\nmaking. I particularly consider the common practice of as-if optimization:\nspecification of a model, point estimation of its parameters, and use of the\npoint estimate to make a decision that would be optimal if the estimate were\naccurate. A central theme is that one should evaluate as-if optimization or any\nother model-based decision rule by its performance across the state space,\nlisting all states of nature that one believes feasible, not across the model\nspace. I apply the theme to prediction and treatment choice. Statistical\ndecision theory is conceptually simple, but application is often challenging.\nAdvancement of computation is the primary task to continue building the\nfoundations sketched by Haavelmo and Wald.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.08726v4"
    },
    {
        "title": "Assessing Inference Methods",
        "authors": [
            "Bruno Ferman"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We analyze different types of simulations that applied researchers may use to\nassess their inference methods. We show that different types of simulations\nvary in many dimensions when considered as inference assessments. Moreover, we\nshow that natural ways of running simulations may lead to misleading\nconclusions, and we propose alternatives. We then provide evidence that even\nsome simple assessments can detect problems in many different settings.\nAlternative assessments that potentially better approximate the true data\ngenerating process may detect problems that simpler assessments would not\ndetect. However, they are not uniformly dominant in this dimension, and may\nimply some costs.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.08772v13"
    },
    {
        "title": "Causal Inference and Data Fusion in Econometrics",
        "authors": [
            "Paul Hünermund",
            "Elias Bareinboim"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Learning about cause and effect is arguably the main goal in applied\neconometrics. In practice, the validity of these causal inferences is\ncontingent on a number of critical assumptions regarding the type of data that\nhas been collected and the substantive knowledge that is available. For\ninstance, unobserved confounding factors threaten the internal validity of\nestimates, data availability is often limited to non-random, selection-biased\nsamples, causal effects need to be learned from surrogate experiments with\nimperfect compliance, and causal knowledge has to be extrapolated across\nstructurally heterogeneous populations. A powerful causal inference framework\nis required to tackle these challenges, which plague most data analysis to\nvarying degrees. Building on the structural approach to causality introduced by\nHaavelmo (1943) and the graph-theoretic framework proposed by Pearl (1995), the\nartificial intelligence (AI) literature has developed a wide array of\ntechniques for causal learning that allow to leverage information from various\nimperfect, heterogeneous, and biased data sources (Bareinboim and Pearl, 2016).\nIn this paper, we discuss recent advances in this literature that have the\npotential to contribute to econometric methodology along three dimensions.\nFirst, they provide a unified and comprehensive framework for causal inference,\nin which the aforementioned problems can be addressed in full generality.\nSecond, due to their origin in AI, they come together with sound, efficient,\nand complete algorithmic criteria for automatization of the corresponding\nidentification task. And third, because of the nonparametric description of\nstructural models that graph-theoretic approaches build on, they combine the\nstrengths of both structural econometrics as well as the potential outcomes\nframework, and thus offer an effective middle ground between these two\nliterature streams.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.09104v4"
    },
    {
        "title": "Temporal-Difference estimation of dynamic discrete choice models",
        "authors": [
            "Karun Adusumilli",
            "Dita Eckardt"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We study the use of Temporal-Difference learning for estimating the\nstructural parameters in dynamic discrete choice models. Our algorithms are\nbased on the conditional choice probability approach but use functional\napproximations to estimate various terms in the pseudo-likelihood function. We\nsuggest two approaches: The first - linear semi-gradient - provides\napproximations to the recursive terms using basis functions. The second -\nApproximate Value Iteration - builds a sequence of approximations to the\nrecursive terms by solving non-parametric estimation problems. Our approaches\nare fast and naturally allow for continuous and/or high-dimensional state\nspaces. Furthermore, they do not require specification of transition densities.\nIn dynamic games, they avoid integrating over other players' actions, further\nheightening the computational advantage. Our proposals can be paired with\npopular existing methods such as pseudo-maximum-likelihood, and we propose\nlocally robust corrections for the latter to achieve parametric rates of\nconvergence. Monte Carlo simulations confirm the properties of our algorithms\nin practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.09509v2"
    },
    {
        "title": "Optimal Dynamic Treatment Regimes and Partial Welfare Ordering",
        "authors": [
            "Sukjin Han"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Dynamic treatment regimes are treatment allocations tailored to heterogeneous\nindividuals. The optimal dynamic treatment regime is a regime that maximizes\ncounterfactual welfare. We introduce a framework in which we can partially\nlearn the optimal dynamic regime from observational data, relaxing the\nsequential randomization assumption commonly employed in the literature but\ninstead using (binary) instrumental variables. We propose the notion of sharp\npartial ordering of counterfactual welfares with respect to dynamic regimes and\nestablish mapping from data to partial ordering via a set of linear programs.\nWe then characterize the identified set of the optimal regime as the set of\nmaximal elements associated with the partial ordering. We relate the notion of\npartial ordering with a more conventional notion of partial identification\nusing topological sorts. Practically, topological sorts can be served as a\npolicy benchmark for a policymaker. We apply our method to understand returns\nto schooling and post-school training as a sequence of treatments by combining\ndata from multiple sources. The framework of this paper can be used beyond the\ncurrent context, e.g., in establishing rankings of multiple treatments or\npolicies across different counterfactual scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10014v4"
    },
    {
        "title": "ResLogit: A residual neural network logit model for data-driven choice\n  modelling",
        "authors": [
            "Melvin Wong",
            "Bilal Farooq"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper presents a novel deep learning-based travel behaviour choice\nmodel.Our proposed Residual Logit (ResLogit) model formulation seamlessly\nintegrates a Deep Neural Network (DNN) architecture into a multinomial logit\nmodel. Recently, DNN models such as the Multi-layer Perceptron (MLP) and the\nRecurrent Neural Network (RNN) have shown remarkable success in modelling\ncomplex and noisy behavioural data. However, econometric studies have argued\nthat machine learning techniques are a `black-box' and difficult to interpret\nfor use in the choice analysis.We develop a data-driven choice model that\nextends the systematic utility function to incorporate non-linear cross-effects\nusing a series of residual layers and using skipped connections to handle model\nidentifiability in estimating a large number of parameters.The model structure\naccounts for cross-effects and choice heterogeneity arising from substitution,\ninteractions with non-chosen alternatives and other effects in a non-linear\nmanner.We describe the formulation, model estimation, interpretability and\nexamine the relative performance and econometric implications of our proposed\nmodel.We present an illustrative example of the model on a classic red/blue bus\nchoice scenario example. For a real-world application, we use a travel mode\nchoice dataset to analyze the model characteristics compared to traditional\nneural networks and Logit formulations.Our findings show that our ResLogit\napproach significantly outperforms MLP models while providing similar\ninterpretability as a Multinomial Logit model.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10058v2"
    },
    {
        "title": "Efficient and Convergent Sequential Pseudo-Likelihood Estimation of\n  Dynamic Discrete Games",
        "authors": [
            "Adam Dearing",
            "Jason R. Blevins"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a new sequential Efficient Pseudo-Likelihood (k-EPL) estimator for\ndynamic discrete choice games of incomplete information. k-EPL considers the\njoint behavior of multiple players simultaneously, as opposed to individual\nresponses to other agents' equilibrium play. This, in addition to reframing the\nproblem from conditional choice probability (CCP) space to value function\nspace, yields a computationally tractable, stable, and efficient estimator. We\nshow that each iteration in the k-EPL sequence is consistent and asymptotically\nefficient, so the first-order asymptotic properties do not vary across\niterations. Furthermore, we show the sequence achieves higher-order equivalence\nto the finite-sample maximum likelihood estimator with iteration and that the\nsequence of estimators converges almost surely to the maximum likelihood\nestimator at a nearly-superlinear rate when the data are generated by any\nregular Markov perfect equilibrium, including equilibria that lead to\ninconsistency of other sequential estimators. When utility is linear in\nparameters, k-EPL iterations are computationally simple, only requiring that\nthe researcher solve linear systems of equations to generate pseudo-regressors\nwhich are used in a static logit/probit regression. Monte Carlo simulations\ndemonstrate the theoretical results and show k-EPL's good performance in finite\nsamples in both small- and large-scale games, even when the game admits\nspurious equilibria in addition to one that generated the data. We apply the\nestimator to study the role of competition in the U.S. wholesale club industry.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10488v6"
    },
    {
        "title": "Recovering Latent Variables by Matching",
        "authors": [
            "Manuel Arellano",
            "Stephane Bonhomme"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose an optimal-transport-based matching method to nonparametrically\nestimate linear models with independent latent variables. The method consists\nin generating pseudo-observations from the latent variables, so that the\nEuclidean distance between the model's predictions and their matched\ncounterparts in the data is minimized. We show that our nonparametric estimator\nis consistent, and we document that it performs well in simulated data. We\napply this method to study the cyclicality of permanent and transitory income\nshocks in the Panel Study of Income Dynamics. We find that the dispersion of\nincome shocks is approximately acyclical, whereas the skewness of permanent\nshocks is procyclical. By comparison, we find that the dispersion and skewness\nof shocks to hourly wages vary little with the business cycle.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.13081v1"
    },
    {
        "title": "Logical Differencing in Dyadic Network Formation Models with\n  Nontransferable Utilities",
        "authors": [
            "Wayne Yuan Gao",
            "Ming Li",
            "Sheng Xu"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper considers a semiparametric model of dyadic network formation under\nnontransferable utilities (NTU). NTU arises frequently in real-world social\ninteractions that require bilateral consent, but by its nature induces additive\nnon-separability. We show how unobserved individual heterogeneity in our model\ncan be canceled out without additive separability, using a novel method we call\nlogical differencing. The key idea is to construct events involving the\nintersection of two mutually exclusive restrictions on the unobserved\nheterogeneity, based on multivariate monotonicity. We provide a consistent\nestimator and analyze its performance via simulation, and apply our method to\nthe Nyakatoke risk-sharing networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.00691v4"
    },
    {
        "title": "Two-Step Estimation of a Strategic Network Formation Model with\n  Clustering",
        "authors": [
            "Geert Ridder",
            "Shuyang Sheng"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper explores strategic network formation under incomplete information\nusing data from a single large network. We allow the utility function to be\nnonseparable in an individual's link choices to capture the spillover effects\nfrom friends in common. In a network with n individuals, the nonseparable\nutility drives an individual to choose between 2^{n-1} overlapping portfolios\nof links. We develop a novel approach that applies the Legendre transform to\nthe utility function so that the optimal decision of an individual can be\nrepresented as a sequence of correlated binary choices. The link dependence\nthat results from the preference for friends in common is captured by an\nauxiliary variable introduced by the Legendre transform. We propose a two-step\nestimator that is consistent and asymptotically normal. We also derive a\nlimiting approximation of the game as n grows large that can help simplify the\ncomputation in very large networks. We apply these methods to favor exchange\nnetworks in rural India and find that the direction of support from a mutual\nlink matters in facilitating favor provision.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.03838v3"
    },
    {
        "title": "A multi-country dynamic factor model with stochastic volatility for euro\n  area business cycle analysis",
        "authors": [
            "Florian Huber",
            "Michael Pfarrhofer",
            "Philipp Piribauer"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper develops a dynamic factor model that uses euro area (EA)\ncountry-specific information on output and inflation to estimate an area-wide\nmeasure of the output gap. Our model assumes that output and inflation can be\ndecomposed into country-specific stochastic trends and a common cyclical\ncomponent. Comovement in the trends is introduced by imposing a factor\nstructure on the shocks to the latent states. We moreover introduce flexible\nstochastic volatility specifications to control for heteroscedasticity in the\nmeasurement errors and innovations to the latent states. Carefully specified\nshrinkage priors allow for pushing the model towards a homoscedastic\nspecification, if supported by the data. Our measure of the output gap closely\ntracks other commonly adopted measures, with small differences in magnitudes\nand timing. To assess whether the model-based output gap helps in forecasting\ninflation, we perform an out-of-sample forecasting exercise. The findings\nindicate that our approach yields superior inflation forecasts, both in terms\nof point and density predictions.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.03935v1"
    },
    {
        "title": "Efficient and Robust Estimation of the Generalized LATE Model",
        "authors": [
            "Haitian Xie"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper studies the estimation of causal parameters in the generalized\nlocal average treatment effect (GLATE) model, a generalization of the classical\nLATE model encompassing multi-valued treatment and instrument. We derive the\nefficient influence function (EIF) and the semiparametric efficiency bound\n(SPEB) for two types of parameters: local average structural function (LASF)\nand local average structural function for the treated (LASF-T). The moment\ncondition generated by the EIF satisfies two robustness properties: double\nrobustness and Neyman orthogonality. Based on the robust moment condition, we\npropose the double/debiased machine learning (DML) estimators for LASF and\nLASF-T. The DML estimator is semiparametric efficient and suitable for high\ndimensional settings. We also propose null-restricted inference methods that\nare robust against weak identification issues. As an empirical application, we\nstudy the effects across different sources of health insurance by applying the\ndeveloped methods to the Oregon Health Insurance Experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.06746v2"
    },
    {
        "title": "Risk Fluctuation Characteristics of Internet Finance: Combining Industry\n  Characteristics with Ecological Value",
        "authors": [
            "Runjie Xu",
            "Chuanmin Mi",
            "Nan Ye",
            "Tom Marshall",
            "Yadong Xiao",
            "Hefan Shuai"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The Internet plays a key role in society and is vital to economic\ndevelopment. Due to the pressure of competition, most technology companies,\nincluding Internet finance companies, continue to explore new markets and new\nbusiness. Funding subsidies and resource inputs have led to significant\nbusiness income tendencies in financial statements. This tendency of business\nincome is often manifested as part of the business loss or long-term\nunprofitability. We propose a risk change indicator (RFR) and compare the risk\nindicator of fourteen representative companies. This model combines extreme\nrisk value with slope, and the combination method is simple and effective. The\nresults of experiment show the potential of this model. The risk volatility of\ntechnology enterprises including Internet finance enterprises is highly\ncyclical, and the risk volatility of emerging Internet fintech companies is\nmuch higher than that of other technology companies.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.09798v1"
    },
    {
        "title": "Frequentist Shrinkage under Inequality Constraints",
        "authors": [
            "Edvard Bakhitov"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper shows how to shrink extremum estimators towards inequality\nconstraints motivated by economic theory. We propose an Inequality Constrained\nShrinkage Estimator (ICSE) which takes the form of a weighted average between\nthe unconstrained and inequality constrained estimators with the data dependent\nweight. The weight drives both the direction and degree of shrinkage. We use a\nlocal asymptotic framework to derive the asymptotic distribution and risk of\nthe ICSE. We provide conditions under which the asymptotic risk of the ICSE is\nstrictly less than that of the unrestricted extremum estimator. The degree of\nshrinkage cannot be consistently estimated under the local asymptotic\nframework. To address this issue, we propose a feasible plug-in estimator and\ninvestigate its finite sample behavior. We also apply our framework to gasoline\ndemand estimation under the Slutsky restriction.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.10586v1"
    },
    {
        "title": "Identifying Preferences when Households are Financially Constrained",
        "authors": [
            "Andreas Tryphonides"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper shows that utilizing information on the extensive margin of\nfinancially constrained households can narrow down the set of admissible\npreferences in a large class of macroeconomic models. Estimates based on\nSpanish aggregate data provide further empirical support for this result and\nsuggest that accounting for this margin can bring estimates closer to\nmicroeconometric evidence. Accounting for financial constraints and the\nextensive margin is shown to matter for empirical asset pricing and quantifying\ndistortions in financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.02010v6"
    },
    {
        "title": "Stocks Vote with Their Feet: Can a Piece of Paper Document Fights the\n  COVID-19 Pandemic?",
        "authors": [
            "J. Su",
            "Q. Zhong"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Assessing the trend of the COVID-19 pandemic and policy effectiveness is\nessential for both policymakers and stock investors, but challenging because\nthe crisis has unfolded with extreme speed and the previous index was not\nsuitable for measuring policy effectiveness for COVID-19. This paper builds an\nindex of policy effectiveness on fighting COVID-19 pandemic, whose building\nmethod is similar to the index of Policy Uncertainty, based on province-level\npaper documents released in China from Jan.1st to Apr.16th of 2020. This paper\nalso studies the relationships among COVID-19 daily confirmed cases, stock\nmarket volatility, and document-based policy effectiveness in China. This paper\nuses the DCC-GARCH model to fit conditional covariance's change rule of\nmulti-series. This paper finally tests four hypotheses, about the time-space\ndifference of policy effectiveness and its overflow effect both on the COVID-19\npandemic and stock market. Through the inner interaction of this triad\nstructure, we can bring forward more specific and scientific suggestions to\nmaintain stability in the stock market at such exceptional times.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.02034v1"
    },
    {
        "title": "Fractional trends in unobserved components models",
        "authors": [
            "Tobias Hartl",
            "Rolf Tschernig",
            "Enzo Weber"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We develop a generalization of unobserved components models that allows for a\nwide range of long-run dynamics by modelling the permanent component as a\nfractionally integrated process. The model does not require stationarity and\ncan be cast in state space form. In a multivariate setup, fractional trends may\nyield a cointegrated system. We derive the Kalman filter estimator for the\ncommon fractionally integrated component and establish consistency and\nasymptotic (mixed) normality of the maximum likelihood estimator. We apply the\nmodel to extract a common long-run component of three US inflation measures,\nwhere we show that the $I(1)$ assumption is likely to be violated for the\ncommon trend.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03988v2"
    },
    {
        "title": "Posterior Probabilities for Lorenz and Stochastic Dominance of\n  Australian Income Distributions",
        "authors": [
            "David Gunawan",
            "William E. Griffiths",
            "Duangkamon Chotikapanich"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Using HILDA data for the years 2001, 2006, 2010, 2014 and 2017, we compute\nposterior probabilities for dominance for all pairwise comparisons of income\ndistributions in these years. The dominance criteria considered are Lorenz\ndominance and first and second order stochastic dominance. The income\ndistributions are estimated using an infinite mixture of gamma density\nfunctions, with posterior probabilities computed as the proportion of Markov\nchain Monte Carlo draws that satisfy the inequalities that define the dominance\ncriteria. We find welfare improvements from 2001 to 2006 and qualified\nimprovements from 2006 to the later three years. Evidence of an ordering\nbetween 2010, 2014 and 2017 cannot be established.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.04870v2"
    },
    {
        "title": "Macroeconomic Forecasting with Fractional Factor Models",
        "authors": [
            "Tobias Hartl"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We combine high-dimensional factor models with fractional integration methods\nand derive models where nonstationary, potentially cointegrated data of\ndifferent persistence is modelled as a function of common fractionally\nintegrated factors. A two-stage estimator, that combines principal components\nand the Kalman filter, is proposed. The forecast performance is studied for a\nhigh-dimensional US macroeconomic data set, where we find that benefits from\nthe fractional factor models can be substantial, as they outperform univariate\nautoregressions, principal components, and the factor-augmented\nerror-correction model.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.04897v1"
    },
    {
        "title": "Fractional trends and cycles in macroeconomic time series",
        "authors": [
            "Tobias Hartl",
            "Rolf Tschernig",
            "Enzo Weber"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We develop a generalization of correlated trend-cycle decompositions that\navoids prior assumptions about the long-run dynamic characteristics by\nmodelling the permanent component as a fractionally integrated process and\nincorporating a fractional lag operator into the autoregressive polynomial of\nthe cyclical component. The model allows for an endogenous estimation of the\nintegration order jointly with the other model parameters and, therefore, no\nprior specification tests with respect to persistence are required. We relate\nthe model to the Beveridge-Nelson decomposition and derive a modified Kalman\nfilter estimator for the fractional components. Identification, consistency,\nand asymptotic normality of the maximum likelihood estimator are shown. For US\nmacroeconomic data we demonstrate that, unlike $I(1)$ correlated unobserved\ncomponents models, the new model estimates a smooth trend together with a cycle\nhitting all NBER recessions. While $I(1)$ unobserved components models yield an\nupward-biased signal-to-noise ratio whenever the integration order of the\ndata-generating mechanism is greater than one, the fractionally integrated\nmodel attributes less variation to the long-run shocks due to the fractional\ntrend specification and a higher variation to the cycle shocks due to the\nfractional lag operator, leading to more persistent cycles and smooth trend\nestimates that reflect macroeconomic common sense.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.05266v2"
    },
    {
        "title": "Moment Conditions for Dynamic Panel Logit Models with Fixed Effects",
        "authors": [
            "Bo E. Honoré",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper investigates the construction of moment conditions in discrete\nchoice panel data with individual specific fixed effects. We describe how to\nsystematically explore the existence of moment conditions that do not depend on\nthe fixed effects, and we demonstrate how to construct them when they exist.\nOur approach is closely related to the numerical \"functional differencing\"\nconstruction in Bonhomme (2012), but our emphasis is to find explicit analytic\nexpressions for the moment functions. We first explain the construction and\ngive examples of such moment conditions in various models. Then, we focus on\nthe dynamic binary choice logit model and explore the implications of the\nmoment conditions for identification and estimation of the model parameters\nthat are common to all individuals.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.05942v7"
    },
    {
        "title": "Irregular Identification of Structural Models with Nonparametric\n  Unobserved Heterogeneity",
        "authors": [
            "Juan Carlos Escanciano"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  One of the most important empirical findings in microeconometrics is the\npervasiveness of heterogeneity in economic behaviour (cf. Heckman 2001). This\npaper shows that cumulative distribution functions and quantiles of the\nnonparametric unobserved heterogeneity have an infinite efficiency bound in\nmany structural economic models of interest. The paper presents a relatively\nsimple check of this fact. The usefulness of the theory is demonstrated with\nseveral relevant examples in economics, including, among others, the proportion\nof individuals with severe long term unemployment duration, the average\nmarginal effect and the proportion of individuals with a positive marginal\neffect in a correlated random coefficient model with heterogenous first-stage\neffects, and the distribution and quantiles of random coefficients in linear,\nbinary and the Mixed Logit models. Monte Carlo simulations illustrate the\nfinite sample implications of our findings for the distribution and quantiles\nof the random coefficients in the Mixed Logit model.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.08611v1"
    },
    {
        "title": "Role models and revealed gender-specific costs of STEM in an extended\n  Roy model of major choice",
        "authors": [
            "Marc Henry",
            "Romuald Meango",
            "Ismael Mourifie"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We derive sharp bounds on the non consumption utility component in an\nextended Roy model of sector selection. We interpret this non consumption\nutility component as a compensating wage differential. The bounds are derived\nunder the assumption that potential utilities in each sector are (jointly)\nstochastically monotone with respect to an observed selection shifter. The\nresearch is motivated by the analysis of women's choice of university major,\ntheir under representation in mathematics intensive fields, and the impact of\nrole models on choices and outcomes. To illustrate our methodology, we\ninvestigate the cost of STEM fields with data from a German graduate survey,\nand using the mother's education level and the proportion of women on the STEM\nfaculty at the time of major choice as selection shifters.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09095v4"
    },
    {
        "title": "Uniform Rates for Kernel Estimators of Weakly Dependent Data",
        "authors": [
            "Juan Carlos Escanciano"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper provides new uniform rate results for kernel estimators of\nabsolutely regular stationary processes that are uniform in the bandwidth and\nin infinite-dimensional classes of dependent variables and regressors. Our\nresults are useful for establishing asymptotic theory for two-step\nsemiparametric estimators in time series models. We apply our results to obtain\nnonparametric estimates and their rates for Expected Shortfall processes.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09951v1"
    },
    {
        "title": "On the Nuisance of Control Variables in Regression Analysis",
        "authors": [
            "Paul Hünermund",
            "Beyers Louw"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Control variables are included in regression analyses to estimate the causal\neffect of a treatment on an outcome. In this paper, we argue that the estimated\neffect sizes of controls are unlikely to have a causal interpretation\nthemselves, though. This is because even valid controls are possibly endogenous\nand represent a combination of several different causal mechanisms operating\njointly on the outcome, which is hard to interpret theoretically. Therefore, we\nrecommend refraining from interpreting marginal effects of controls and\nfocusing on the main variables of interest, for which a plausible\nidentification argument can be established. To prevent erroneous managerial or\npolicy implications, coefficients of control variables should be clearly marked\nas not having a causal interpretation or omitted from regression tables\naltogether. Moreover, we advise against using control variable estimates for\nsubsequent theory building and meta-analyses.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.10314v5"
    },
    {
        "title": "Macroeconomic factors for inflation in Argentine 2013-2019",
        "authors": [
            "Manuel Lopez Galvan"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The aim of this paper is to investigate the use of the Factor Analysis in\norder to identify the role of the relevant macroeconomic variables in driving\nthe inflation. The Macroeconomic predictors that usually affect the inflation\nare summarized using a small number of factors constructed by the principal\ncomponents. This allows us to identify the crucial role of money growth,\ninflation expectation and exchange rate in driving the inflation. Then we use\nthis factors to build econometric models to forecast inflation. Specifically,\nwe use univariate and multivariate models such as classical autoregressive,\nFactor models and FAVAR models. Results of forecasting suggest that models\nwhich incorporate more economic information outperform the benchmark.\nFurthermore, causality test and impulse response are performed in order to\nexamine the short-run dynamics of inflation to shocks in the principal factors.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.11455v1"
    },
    {
        "title": "Bootstrap Inference for Quantile Treatment Effects in Randomized\n  Experiments with Matched Pairs",
        "authors": [
            "Liang Jiang",
            "Xiaobin Liu",
            "Peter C. B. Phillips",
            "Yichong Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper examines methods of inference concerning quantile treatment\neffects (QTEs) in randomized experiments with matched-pairs designs (MPDs).\nStandard multiplier bootstrap inference fails to capture the negative\ndependence of observations within each pair and is therefore conservative.\nAnalytical inference involves estimating multiple functional quantities that\nrequire several tuning parameters. Instead, this paper proposes two bootstrap\nmethods that can consistently approximate the limit distribution of the\noriginal QTE estimator and lessen the burden of tuning parameter choice. Most\nespecially, the inverse propensity score weighted multiplier bootstrap can be\nimplemented without knowledge of pair identities.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.11967v4"
    },
    {
        "title": "Synthetic Control Methods and Big Data",
        "authors": [
            "Daniel Kinn"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Many macroeconomic policy questions may be assessed in a case study\nframework, where the time series of a treated unit is compared to a\ncounterfactual constructed from a large pool of control units. I provide a\ngeneral framework for this setting, tailored to predict the counterfactual by\nminimizing a tradeoff between underfitting (bias) and overfitting (variance).\nThe framework nests recently proposed structural and reduced form machine\nlearning approaches as special cases. Furthermore, difference-in-differences\nwith matching and the original synthetic control are restrictive cases of the\nframework, in general not minimizing the bias-variance objective. Using\nsimulation studies I find that machine learning methods outperform traditional\nmethods when the number of potential controls is large or the treated unit is\nsubstantially different from the controls. Equipped with a toolbox of\napproaches, I revisit a study on the effect of economic liberalisation on\neconomic growth. I find effects for several countries where no effect was found\nin the original study. Furthermore, I inspect how a systematically important\nbank respond to increasing capital requirements by using a large pool of banks\nto estimate the counterfactual. Finally, I assess the effect of a changing\nproduct price on product sales using a novel scanner dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00096v1"
    },
    {
        "title": "An Note on Why Geographically Weighted Regression Overcomes\n  Multidimensional-Kernel-Based Varying-Coefficient Model",
        "authors": [
            "Zihao Yuan"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  It is widely known that geographically weighted regression(GWR) is\nessentially same as varying-coefficient model. In the former research about\nvarying-coefficient model, scholars tend to use multidimensional-kernel-based\nlocally weighted estimation(MLWE) so that information of both distance and\ndirection is considered. However, when we construct the local weight matrix of\ngeographically weighted estimation, distance among the locations in the\nneighbor is the only factor controlling the value of entries of weight matrix.\nIn other word, estimation of GWR is distance-kernel-based. Thus, in this paper,\nunder stationary and limited dependent data with multidimensional subscripts,\nwe analyze the local mean squared properties of without any assumption of the\nform of coefficient functions and compare it with MLWE. According to the\ntheoretical and simulation results, geographically-weighted locally linear\nestimation(GWLE) is asymptotically more efficient than MLWE. Furthermore, a\nrelationship between optimal bandwith selection and design of scale parameters\nis also obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.01402v2"
    },
    {
        "title": "Pricing Mechanism in Information Goods",
        "authors": [
            "Xinming Li",
            "Huaqing Wang"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We study three pricing mechanisms' performance and their effects on the\nparticipants in the data industry from the data supply chain perspective. A\nwin-win pricing strategy for the players in the data supply chain is proposed.\nWe obtain analytical solutions in each pricing mechanism, including the\ndecentralized and centralized pricing, Nash Bargaining pricing, and revenue\nsharing mechanism.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.01530v1"
    },
    {
        "title": "A Nonparametric Approach to Measure the Heterogeneous Spatial\n  Association: Under Spatial Temporal Data",
        "authors": [
            "Zihao Yuan"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Spatial association and heterogeneity are two critical areas in the research\nabout spatial analysis, geography, statistics and so on. Though large amounts\nof outstanding methods has been proposed and studied, there are few of them\ntend to study spatial association under heterogeneous environment.\nAdditionally, most of the traditional methods are based on distance statistic\nand spatial weighted matrix. However, in some abstract spatial situations,\ndistance statistic can not be applied since we can not even observe the\ngeographical locations directly. Meanwhile, under these circumstances, due to\ninvisibility of spatial positions, designing of weight matrix can not\nabsolutely avoid subjectivity. In this paper, a new entropy-based method, which\nis data-driven and distribution-free, has been proposed to help us investigate\nspatial association while fully taking the fact that heterogeneity widely\nexist. Specifically, this method is not bounded with distance statistic or\nweight matrix. Asymmetrical dependence is adopted to reflect the heterogeneity\nin spatial association for each individual and the whole discussion in this\npaper is performed on spatio-temporal data with only assuming stationary\nm-dependent over time.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.02334v2"
    },
    {
        "title": "A study of strategy to the remove and ease TBT for increasing export in\n  GCC6 countries",
        "authors": [
            "YongJae Kim"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The last technical barriers to trade(TBT) between countries are Non-Tariff\nBarriers(NTBs), meaning all trade barriers are possible other than Tariff\nBarriers. And the most typical examples are (TBT), which refer to measure\nTechnical Regulation, Standards, Procedure for Conformity Assessment, Test &\nCertification etc. Therefore, in order to eliminate TBT, WTO has made all\nmembership countries automatically enter into an agreement on TBT\n",
        "pdf_link": "http://arxiv.org/pdf/1803.03394v3"
    },
    {
        "title": "How Smart Are `Water Smart Landscapes'?",
        "authors": [
            "Christa Brelsford",
            "Joshua K. Abbott"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Understanding the effectiveness of alternative approaches to water\nconservation is crucially important for ensuring the security and reliability\nof water services for urban residents. We analyze data from one of the\nlongest-running \"cash for grass\" policies - the Southern Nevada Water\nAuthority's Water Smart Landscapes program, where homeowners are paid to\nreplace grass with xeric landscaping. We use a twelve year long panel dataset\nof monthly water consumption records for 300,000 households in Las Vegas,\nNevada. Utilizing a panel difference-in-differences approach, we estimate the\naverage water savings per square meter of turf removed. We find that\nparticipation in this program reduced the average treated household's\nconsumption by 18 percent. We find no evidence that water savings degrade as\nthe landscape ages, or that water savings per unit area are influenced by the\nvalue of the rebate. Depending on the assumed time horizon of benefits from\nturf removal, we find that the WSL program cost the water authority about $1.62\nper thousand gallons of water saved, which compares favorably to alternative\nmeans of water conservation or supply augmentation.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04593v1"
    },
    {
        "title": "Business Cycles in Economics",
        "authors": [
            "Viktor O. Ledenyov",
            "Dimitri O. Ledenyov"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The business cycles are generated by the oscillating macro-/micro-/nano-\neconomic output variables in the economy of the scale and the scope in the\namplitude/frequency/phase/time domains in the economics. The accurate forward\nlooking assumptions on the business cycles oscillation dynamics can optimize\nthe financial capital investing and/or borrowing by the economic agents in the\ncapital markets. The book's main objective is to study the business cycles in\nthe economy of the scale and the scope, formulating the Ledenyov unified\nbusiness cycles theory in the Ledenyov classic and quantum econodynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.06108v1"
    },
    {
        "title": "Testing for Unobserved Heterogeneous Treatment Effects with\n  Observational Data",
        "authors": [
            "Yu-Chin Hsu",
            "Ta-Cheng Huang",
            "Haiqing Xu"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Unobserved heterogeneous treatment effects have been emphasized in the recent\npolicy evaluation literature (see e.g., Heckman and Vytlacil, 2005). This paper\nproposes a nonparametric test for unobserved heterogeneous treatment effects in\na treatment effect model with a binary treatment assignment, allowing for\nindividuals' self-selection to the treatment. Under the standard local average\ntreatment effects assumptions, i.e., the no defiers condition, we derive\ntestable model restrictions for the hypothesis of unobserved heterogeneous\ntreatment effects. Also, we show that if the treatment outcomes satisfy a\nmonotonicity assumption, these model restrictions are also sufficient. Then, we\npropose a modified Kolmogorov-Smirnov-type test which is consistent and simple\nto implement. Monte Carlo simulations show that our test performs well in\nfinite samples. For illustration, we apply our test to study heterogeneous\ntreatment effects of the Job Training Partnership Act on earnings and the\nimpacts of fertility on family income, where the null hypothesis of homogeneous\ntreatment effects gets rejected in the second case but fails to be rejected in\nthe first application.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.07514v2"
    },
    {
        "title": "Testing Continuity of a Density via g-order statistics in the Regression\n  Discontinuity Design",
        "authors": [
            "Federico A. Bugni",
            "Ivan A. Canay"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In the regression discontinuity design (RDD), it is common practice to assess\nthe credibility of the design by testing the continuity of the density of the\nrunning variable at the cut-off, e.g., McCrary (2008). In this paper we propose\nan approximate sign test for continuity of a density at a point based on the\nso-called g-order statistics, and study its properties under two complementary\nasymptotic frameworks. In the first asymptotic framework, the number q of\nobservations local to the cut-off is fixed as the sample size n diverges to\ninfinity, while in the second framework q diverges to infinity slowly as n\ndiverges to infinity. Under both of these frameworks, we show that the test we\npropose is asymptotically valid in the sense that it has limiting rejection\nprobability under the null hypothesis not exceeding the nominal level. More\nimportantly, the test is easy to implement, asymptotically valid under weaker\nconditions than those used by competing methods, and exhibits finite sample\nvalidity under stronger conditions than those needed for its asymptotic\nvalidity. In a simulation study, we find that the approximate sign test\nprovides good control of the rejection probability under the null hypothesis\nwhile remaining competitive under the alternative hypothesis. We finally apply\nour test to the design in Lee (2008), a well-known application of the RDD to\nstudy incumbency advantage.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.07951v6"
    },
    {
        "title": "Causal Inference for Survival Analysis",
        "authors": [
            "Vikas Ramachandra"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper, we propose the use of causal inference techniques for survival\nfunction estimation and prediction for subgroups of the data, upto individual\nunits. Tree ensemble methods, specifically random forests were modified for\nthis purpose. A real world healthcare dataset was used with about 1800 patients\nwith breast cancer, which has multiple patient covariates as well as disease\nfree survival days (DFS) and a death event binary indicator (y). We use the\ntype of cancer curative intervention as the treatment variable (T=0 or 1,\nbinary treatment case in our example). The algorithm is a 2 step approach. In\nstep 1, we estimate heterogeneous treatment effects using a causalTree with the\nDFS as the dependent variable. Next, in step 2, for each selected leaf of the\ncausalTree with distinctly different average treatment effect (with respect to\nsurvival), we fit a survival forest to all the patients in that leaf, one\nforest each for treatment T=0 as well as T=1 to get estimated patient level\nsurvival curves for each treatment (more generally, any model can be used at\nthis step). Then, we subtract the patient level survival curves to get the\ndifferential survival curve for a given patient, to compare the survival\nfunction as a result of the 2 treatments. The path to a selected leaf also\ngives us the combination of patient features and their values which are\ncausally important for the treatment effect difference at the leaf.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08218v1"
    },
    {
        "title": "Two-way fixed effects estimators with heterogeneous treatment effects",
        "authors": [
            "Clément de Chaisemartin",
            "Xavier D'Haultfœuille"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Linear regressions with period and group fixed effects are widely used to\nestimate treatment effects. We show that they estimate weighted sums of the\naverage treatment effects (ATE) in each group and period, with weights that may\nbe negative. Due to the negative weights, the linear regression coefficient may\nfor instance be negative while all the ATEs are positive. We propose another\nestimator that solves this issue. In the two applications we revisit, it is\nsignificantly different from the linear regression estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08807v7"
    },
    {
        "title": "How does monetary policy affect income inequality in Japan? Evidence\n  from grouped data",
        "authors": [
            "Martin Feldkircher",
            "Kazuhiko Kakamu"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We examine the effects of monetary policy on income inequality in Japan using\na novel econometric approach that jointly estimates the Gini coefficient based\non micro-level grouped data of households and the dynamics of macroeconomic\nquantities. Our results indicate different effects on income inequality for\ndifferent types of households: A monetary tightening increases inequality when\nincome data is based on households whose head is employed (workers'\nhouseholds), while the effect reverses over the medium term when considering a\nbroader definition of households. Differences in the relative strength of the\ntransmission channels can account for this finding. Finally we demonstrate that\nthe proposed joint estimation strategy leads to more informative inference\nwhile results based on the frequently used two-step estimation approach yields\ninconclusive results.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08868v2"
    },
    {
        "title": "Schooling Choice, Labour Market Matching, and Wages",
        "authors": [
            "Jacob Schwartz"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We develop inference for a two-sided matching model where the characteristics\nof agents on one side of the market are endogenous due to pre-matching\ninvestments. The model can be used to measure the impact of frictions in labour\nmarkets using a single cross-section of matched employer-employee data. The\nobserved matching of workers to firms is the outcome of a discrete, two-sided\nmatching process where firms with heterogeneous preferences over education\nsequentially choose workers according to an index correlated with worker\npreferences over firms. The distribution of education arises in equilibrium\nfrom a Bayesian game: workers, knowing the distribution of worker and firm\ntypes, invest in education prior to the matching process. Although the observed\nmatching exhibits strong cross-sectional dependence due to the matching\nprocess, we propose an asymptotically valid inference procedure that combines\ndiscrete choice methods with simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09020v6"
    },
    {
        "title": "Panel Data Analysis with Heterogeneous Dynamics",
        "authors": [
            "Ryo Okui",
            "Takahide Yanagi"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper proposes a model-free approach to analyze panel data with\nheterogeneous dynamic structures across observational units. We first compute\nthe sample mean, autocovariances, and autocorrelations for each unit, and then\nestimate the parameters of interest based on their empirical distributions. We\nthen investigate the asymptotic properties of our estimators using double\nasymptotics and propose split-panel jackknife bias correction and inference\nbased on the cross-sectional bootstrap. We illustrate the usefulness of our\nprocedures by studying the deviation dynamics of the law of one price. Monte\nCarlo simulations confirm that the proposed bias correction is effective and\nyields valid inference in small samples.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09452v2"
    },
    {
        "title": "The Bretton Woods Experience and ERM",
        "authors": [
            "Chris Kirrane"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Historical examination of the Bretton Woods system allows comparisons to be\nmade with the current evolution of the EMS.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00418v1"
    },
    {
        "title": "Maastricht and Monetary Cooperation",
        "authors": [
            "Chris Kirrane"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper describes the opportunities and also the difficulties of EMU with\nregard to international monetary cooperation. Even though the institutional and\nintellectual assistance to the coordination of monetary policy in the EU will\nprobably be strengthened with the EMU, among the shortcomings of the Maastricht\nTreaty concerns the relationship between the founder members and those\ncountries who wish to remain outside monetary union.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00419v1"
    },
    {
        "title": "Stochastic model specification in Markov switching vector error\n  correction models",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber",
            "Michael Pfarrhofer",
            "Thomas O. Zörner"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper proposes a hierarchical modeling approach to perform stochastic\nmodel specification in Markov switching vector error correction models. We\nassume that a common distribution gives rise to the regime-specific regression\ncoefficients. The mean as well as the variances of this distribution are\ntreated as fully stochastic and suitable shrinkage priors are used. These\nshrinkage priors enable to assess which coefficients differ across regimes in a\nflexible manner. In the case of similar coefficients, our model pushes the\nrespective regions of the parameter space towards the common distribution. This\nallows for selecting a parsimonious model while still maintaining sufficient\nflexibility to control for sudden shifts in the parameters, if necessary. We\napply our modeling approach to real-time Euro area data and assume transition\nprobabilities between expansionary and recessionary regimes to be driven by the\ncointegration errors. The results suggest that the regime allocation is\ngoverned by a subset of short-run adjustment coefficients and regime-specific\nvariance-covariance matrices. These findings are complemented by an\nout-of-sample forecast exercise, illustrating the advantages of the model for\npredicting Euro area inflation in real time.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00529v2"
    },
    {
        "title": "Indirect inference through prediction",
        "authors": [
            "Ernesto Carrella",
            "Richard M. Bailey",
            "Jens Koed Madsen"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  By recasting indirect inference estimation as a prediction rather than a\nminimization and by using regularized regressions, we can bypass the three\nmajor problems of estimation: selecting the summary statistics, defining the\ndistance function and minimizing it numerically. By substituting regression\nwith classification we can extend this approach to model selection as well. We\npresent three examples: a statistical fit, the parametrization of a simple real\nbusiness cycle model and heuristics selection in a fishery agent-based model.\nThe outcome is a method that automatically chooses summary statistics, weighs\nthem and use them to parametrize models without running any direct\nminimization.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.01579v1"
    },
    {
        "title": "On the Identifying Content of Instrument Monotonicity",
        "authors": [
            "Vishal Kamat"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper studies the identifying content of the instrument monotonicity\nassumption of Imbens and Angrist (1994) on the distribution of potential\noutcomes in a model with a binary outcome, a binary treatment and an exogenous\nbinary instrument. Specifically, I derive necessary and sufficient conditions\non the distribution of the data under which the identified set for the\ndistribution of potential outcomes when the instrument monotonicity assumption\nis imposed can be a strict subset of that when it is not imposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.01661v2"
    },
    {
        "title": "State-Varying Factor Models of Large Dimensions",
        "authors": [
            "Markus Pelger",
            "Ruoxuan Xiong"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper develops an inferential theory for state-varying factor models of\nlarge dimensions. Unlike constant factor models, loadings are general functions\nof some recurrent state process. We develop an estimator for the latent factors\nand state-varying loadings under a large cross-section and time dimension. Our\nestimator combines nonparametric methods with principal component analysis. We\nderive the rate of convergence and limiting normal distribution for the\nfactors, loadings and common components. In addition, we develop a statistical\ntest for a change in the factor structure in different states. We apply the\nestimator to U.S. Treasury yields and S&P500 stock returns. The systematic\nfactor structure in treasury yields differs in times of booms and recessions as\nwell as in periods of high market volatility. State-varying factors based on\nthe VIX capture significantly more variation and pricing information in\nindividual stocks than constant factor models.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.02248v4"
    },
    {
        "title": "Transaction costs and institutional change of trade litigations in\n  Bulgaria",
        "authors": [
            "Shteryo Nozharov",
            "Petya Koralova-Nozharova"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The methods of new institutional economics for identifying the transaction\ncosts of trade litigations in Bulgaria are used in the current paper. For the\nneeds of the research, an indicative model, measuring this type of costs on\nmicroeconomic level, is applied in the study. The main purpose of the model is\nto forecast the rational behavior of trade litigation parties in accordance\nwith the transaction costs in the process of enforcing the execution of the\nsigned commercial contract. The application of the model is related to the more\naccurate measurement of the transaction costs on microeconomic level, which\nfact could lead to better prediction and management of these costs in order\nmarket efficiency and economic growth to be achieved. In addition, it is made\nan attempt to be analysed the efficiency of the institutional change of the\ncommercial justice system and the impact of the reform of the judicial system\nover the economic turnover. The augmentation or lack of reduction of the\ntransaction costs in trade litigations would mean inefficiency of the reform of\nthe judicial system. JEL Codes: O43, P48, D23, K12\n",
        "pdf_link": "http://arxiv.org/pdf/1807.03034v1"
    },
    {
        "title": "Cancer Risk Messages: A Light Bulb Model",
        "authors": [
            "Ka C. Chan",
            "Ruth F. G. Williams",
            "Christopher T. Lenard",
            "Terence M. Mills"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The meaning of public messages such as \"One in x people gets cancer\" or \"One\nin y people gets cancer by age z\" can be improved. One assumption commonly\ninvoked is that there is no other cause of death, a confusing assumption. We\ndevelop a light bulb model to clarify cumulative risk and we use Markov chain\nmodeling, incorporating the assumption widely in place, to evaluate transition\nprobabilities. Age-progression in the cancer risk is then reported on\nAustralian data. Future modelling can elicit realistic assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.03040v2"
    },
    {
        "title": "Cancer Risk Messages: Public Health and Economic Welfare",
        "authors": [
            "Ruth F. G. Williams",
            "Ka C. Chan",
            "Christopher T. Lenard",
            "Terence M. Mills"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Statements for public health purposes such as \"1 in 2 will get cancer by age\n85\" have appeared in public spaces. The meaning drawn from such statements\naffects economic welfare, not just public health. Both markets and government\nuse risk information on all kinds of risks, useful information can, in turn,\nimprove economic welfare, however inaccuracy can lower it. We adapt the\ncontingency table approach so that a quoted risk is cross-classified with the\nstates of nature. We show that bureaucratic objective functions regarding the\naccuracy of a reported cancer risk can then be stated.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.03045v2"
    },
    {
        "title": "Simulation Modelling of Inequality in Cancer Service Access",
        "authors": [
            "Ka C. Chan",
            "Ruth F. G. Williams",
            "Christopher T. Lenard",
            "Terence M. Mills"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper applies economic concepts from measuring income inequality to an\nexercise in assessing spatial inequality in cancer service access in regional\nareas. We propose a mathematical model for accessing chemotherapy among local\ngovernment areas (LGAs). Our model incorporates a distance factor. With a\nsimulation we report results for a single inequality measure: the Lorenz curve\nis depicted for our illustrative data. We develop this approach in order to\nmove incrementally towards its application to actual data and real-world health\nservice regions. We seek to develop the exercises that can lead policy makers\nto relevant policy information on the most useful data collections to be\ncollected and modeling for cancer service access in regional areas.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.03048v1"
    },
    {
        "title": "Clustering Macroeconomic Time Series",
        "authors": [
            "Iwo Augustyński",
            "Paweł Laskoś-Grabowski"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The data mining technique of time series clustering is well established in\nmany fields. However, as an unsupervised learning method, it requires making\nchoices that are nontrivially influenced by the nature of the data involved.\nThe aim of this paper is to verify usefulness of the time series clustering\nmethod for macroeconomics research, and to develop the most suitable\nmethodology.\n  By extensively testing various possibilities, we arrive at a choice of a\ndissimilarity measure (compression-based dissimilarity measure, or CDM) which\nis particularly suitable for clustering macroeconomic variables. We check that\nthe results are stable in time and reflect large-scale phenomena such as\ncrises. We also successfully apply our findings to analysis of national\neconomies, specifically to identifying their structural relations.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.04004v2"
    },
    {
        "title": "Factor models with many assets: strong factors, weak factors, and the\n  two-pass procedure",
        "authors": [
            "Stanislav Anatolyev",
            "Anna Mikusheva"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper re-examines the problem of estimating risk premia in linear factor\npricing models. Typically, the data used in the empirical literature are\ncharacterized by weakness of some pricing factors, strong cross-sectional\ndependence in the errors, and (moderately) high cross-sectional dimensionality.\nUsing an asymptotic framework where the number of assets/portfolios grows with\nthe time span of the data while the risk exposures of weak factors are\nlocal-to-zero, we show that the conventional two-pass estimation procedure\ndelivers inconsistent estimates of the risk premia. We propose a new estimation\nprocedure based on sample-splitting instrumental variables regression. The\nproposed estimator of risk premia is robust to weak included factors and to the\npresence of strong unaccounted cross-sectional error dependence. We derive the\nmany-asset weak factor asymptotic distribution of the proposed estimator, show\nhow to construct its standard errors, verify its performance in simulations,\nand revisit some empirical studies.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.04094v2"
    },
    {
        "title": "Heterogeneous Effects of Unconventional Monetary Policy on Loan Demand\n  and Supply. Insights from the Bank Lending Survey",
        "authors": [
            "Martin Guth"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper analyzes the bank lending channel and the heterogeneous effects on\nthe euro area, providing evidence that the channel is indeed working. The\nanalysis of the transmission mechanism is based on structural impulse responses\nto an unconventional monetary policy shock on bank loans. The Bank Lending\nSurvey (BLS) is exploited in order to get insights on developments of loan\ndemand and supply. The contribution of this paper is to use country-specific\ndata to analyze the consequences of unconventional monetary policy, instead of\ntaking an aggregate stance by using euro area data. This approach provides a\ndeeper understanding of the bank lending channel and its effects. That is, an\nexpansionary monetary policy shock leads to an increase in loan demand, supply\nand output growth. A small north-south disparity between the countries can be\nobserved.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.04161v1"
    },
    {
        "title": "Analysis of a Dynamic Voluntary Contribution Mechanism Public Good Game",
        "authors": [
            "Dmytro Bogatov"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  I present a dynamic, voluntary contribution mechanism, public good game and\nderive its potential outcomes. In each period, players endogenously determine\ncontribution productivity by engaging in costly investment. The level of\ncontribution productivity carries from period to period, creating a dynamic\nlink between periods. The investment mimics investing in the stock of\ntechnology for producing public goods such as national defense or a clean\nenvironment. After investing, players decide how much of their remaining money\nto contribute to provision of the public good, as in traditional public good\ngames. I analyze three kinds of outcomes of the game: the lowest payoff\noutcome, the Nash Equilibria, and socially optimal behavior. In the lowest\npayoff outcome, all players receive payoffs of zero. Nash Equilibrium occurs\nwhen players invest any amount and contribute all or nothing depending on the\ncontribution productivity. Therefore, there are infinitely many Nash Equilibria\nstrategies. Finally, the socially optimal result occurs when players invest\neverything in early periods, then at some point switch to contributing\neverything. My goal is to discover and explain this point. I use mathematical\nanalysis and computer simulation to derive the results.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.04621v2"
    },
    {
        "title": "A Simple and Efficient Estimation of the Average Treatment Effect in the\n  Presence of Unmeasured Confounders",
        "authors": [
            "Chunrong Ai",
            "Lukang Huang",
            "Zheng Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Wang and Tchetgen Tchetgen (2017) studied identification and estimation of\nthe average treatment effect when some confounders are unmeasured. Under their\nidentification condition, they showed that the semiparametric efficient\ninfluence function depends on five unknown functionals. They proposed to\nparameterize all functionals and estimate the average treatment effect from the\nefficient influence function by replacing the unknown functionals with\nestimated functionals. They established that their estimator is consistent when\ncertain functionals are correctly specified and attains the semiparametric\nefficiency bound when all functionals are correctly specified. In applications,\nit is likely that those functionals could all be misspecified. Consequently\ntheir estimator could be inconsistent or consistent but not efficient. This\npaper presents an alternative estimator that does not require parameterization\nof any of the functionals. We establish that the proposed estimator is always\nconsistent and always attains the semiparametric efficiency bound. A simple and\nintuitive estimator of the asymptotic variance is presented, and a small scale\nsimulation study reveals that the proposed estimation outperforms the existing\nalternatives in finite samples.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.05678v1"
    },
    {
        "title": "Pink Work: Same-Sex Marriage, Employment and Discrimination",
        "authors": [
            "Dario Sansone"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper analyzes how the legalization of same-sex marriage in the U.S.\naffected gay and lesbian couples in the labor market. Results from a\ndifference-in-difference model show that both partners in same-sex couples were\nmore likely to be employed, to have a full-time contract, and to work longer\nhours in states that legalized same-sex marriage. In line with a theoretical\nsearch model of discrimination, suggestive empirical evidence supports the\nhypothesis that marriage equality led to an improvement in employment outcomes\namong gays and lesbians and lower occupational segregation thanks to a decrease\nin discrimination towards sexual minorities.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.06698v1"
    },
    {
        "title": "Quantile-Regression Inference With Adaptive Control of Size",
        "authors": [
            "Juan Carlos Escanciano",
            "Chuan Goh"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Regression quantiles have asymptotic variances that depend on the conditional\ndensities of the response variable given regressors. This paper develops a new\nestimate of the asymptotic variance of regression quantiles that leads any\nresulting Wald-type test or confidence region to behave as well in large\nsamples as its infeasible counterpart in which the true conditional response\ndensities are embedded. We give explicit guidance on implementing the new\nvariance estimator to control adaptively the size of any resulting Wald-type\ntest. Monte Carlo evidence indicates the potential of our approach to deliver\npowerful tests of heterogeneity of quantile treatment effects in covariates\nwith good size performance over different quantile levels, data-generating\nprocesses and sample sizes. We also include an empirical example. Supplementary\nmaterial is available online.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.06977v2"
    },
    {
        "title": "A New Index of Human Capital to Predict Economic Growth",
        "authors": [
            "Henry Laverde",
            "Juan C. Correa",
            "Klaus Jaffe"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The accumulation of knowledge required to produce economic value is a process\nthat often relates to nations economic growth. Such a relationship, however, is\nmisleading when the proxy of such accumulation is the average years of\neducation. In this paper, we show that the predictive power of this proxy\nstarted to dwindle in 1990 when nations schooling began to homogenized. We\npropose a metric of human capital that is less sensitive than average years of\neducation and remains as a significant predictor of economic growth when tested\nwith both cross-section data and panel data. We argue that future research on\neconomic growth will discard educational variables based on quantity as\npredictor given the thresholds that these variables are reaching.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07051v1"
    },
    {
        "title": "Stability in EMU",
        "authors": [
            "Theo Peeters"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The public debt and deficit ceilings of the Maastricht Treaty are the subject\nof recurring controversy. First, there is debate about the role and impact of\nthese criteria in the initial phase of the introduction of the single currency.\nSecondly, it must be specified how these will then be applied, in a permanent\nregime, when the single currency is well established.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07730v1"
    },
    {
        "title": "Asymptotic results under multiway clustering",
        "authors": [
            "Laurent Davezies",
            "Xavier D'Haultfoeuille",
            "Yannick Guyonvarch"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  If multiway cluster-robust standard errors are used routinely in applied\neconomics, surprisingly few theoretical results justify this practice. This\npaper aims to fill this gap. We first prove, under nearly the same conditions\nas with i.i.d. data, the weak convergence of empirical processes under multiway\nclustering. This result implies central limit theorems for sample averages but\nis also key for showing the asymptotic normality of nonlinear estimators such\nas GMM estimators. We then establish consistency of various asymptotic variance\nestimators, including that of Cameron et al. (2011) but also a new estimator\nthat is positive by construction. Next, we show the general consistency, for\nlinear and nonlinear estimators, of the pigeonhole bootstrap, a resampling\nscheme adapted to multiway clustering. Monte Carlo simulations suggest that\ninference based on our two preferred methods may be accurate even with very few\nclusters, and significantly improve upon inference based on Cameron et al.\n(2011).\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07925v2"
    },
    {
        "title": "EMU and ECB Conflicts",
        "authors": [
            "William Mackenzie"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In dynamical framework the conflict between government and the central bank\naccording to the exchange Rate of payment of fixed rates and fixed rates of\nfixed income (EMU) convergence criteria such that the public debt / GDP ratio\nThe method consists of calculating private public debt management in a public\ndebt management system purpose there is no mechanism to allow naturally for\nthis adjustment.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.08097v1"
    },
    {
        "title": "Simple subvector inference on sharp identified set in affine models",
        "authors": [
            "Bulat Gafarov"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper studies a regularized support function estimator for bounds on\ncomponents of the parameter vector in the case in which the identified set is a\npolygon. The proposed regularized estimator has three important properties: (i)\nit has a uniform asymptotic Gaussian limit in the presence of flat faces in the\nabsence of redundant (or overidentifying) constraints (or vice versa); (ii) the\nbias from regularization does not enter the first-order limiting distribution;\n(iii) the estimator remains consistent for sharp (non-enlarged) identified set\nfor the individual components even in the non-regualar case. These properties\nare used to construct \\emph{uniformly valid} confidence sets for an element\n$\\theta_{1}$ of a parameter vector $\\theta\\in\\mathbb{R}^{d}$ that is partially\nidentified by affine moment equality and inequality conditions. The proposed\nconfidence sets can be computed as a solution to a small number of linear and\nconvex quadratic programs, leading to a substantial decrease in computation\ntime and guarantees a global optimum. As a result, the method provides a\nuniformly valid inference in applications in which the dimension of the\nparameter space, $d$, and the number of inequalities, $k$, were previously\ncomputationally unfeasible ($d,k=100$). The proposed approach can be extended\nto construct confidence sets for intersection bounds, to construct joint\npolygon-shaped confidence sets for multiple components of $\\theta$, and to find\nthe set of solutions to a linear program. Inference for coefficients in the\nlinear IV regression model with an interval outcome is used as an illustrative\nexample.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00111v3"
    },
    {
        "title": "Post-Selection Inference in Three-Dimensional Panel Data",
        "authors": [
            "Harold D. Chiang",
            "Joel Rodrigue",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Three-dimensional panel models are widely used in empirical analysis.\nResearchers use various combinations of fixed effects for three-dimensional\npanels. When one imposes a parsimonious model and the true model is rich, then\nit incurs mis-specification biases. When one employs a rich model and the true\nmodel is parsimonious, then it incurs larger standard errors than necessary. It\nis therefore useful for researchers to know correct models. In this light, Lu,\nMiao, and Su (2018) propose methods of model selection. We advance this\nliterature by proposing a method of post-selection inference for regression\nparameters. Despite our use of the lasso technique as means of model selection,\nour assumptions allow for many and even all fixed effects to be nonzero.\nSimulation studies demonstrate that the proposed method is more precise than\nunder-fitting fixed effect estimators, is more efficient than over-fitting\nfixed effect estimators, and allows for as accurate inference as the oracle\nestimator.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00211v2"
    },
    {
        "title": "Counterfactual Sensitivity and Robustness",
        "authors": [
            "Timothy Christensen",
            "Benjamin Connault"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a framework for analyzing the sensitivity of counterfactuals to\nparametric assumptions about the distribution of latent variables in structural\nmodels. In particular, we derive bounds on counterfactuals as the distribution\nof latent variables spans nonparametric neighborhoods of a given parametric\nspecification while other \"structural\" features of the model are maintained.\nOur approach recasts the infinite-dimensional problem of optimizing the\ncounterfactual with respect to the distribution of latent variables (subject to\nmodel constraints) as a finite-dimensional convex program. We also develop an\nMPEC version of our method to further simplify computation in models with\nendogenous parameters (e.g., value functions) defined by equilibrium\nconstraints. We propose plug-in estimators of the bounds and two methods for\ninference. We also show that our bounds converge to the sharp nonparametric\nbounds on counterfactuals as the neighborhood size becomes large. To illustrate\nthe broad applicability of our procedure, we present empirical applications to\nmatching models with transferable utility and dynamic discrete choice models.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00989v4"
    },
    {
        "title": "Matching Points: Supplementing Instruments with Covariates in Triangular\n  Models",
        "authors": [
            "Junlong Feng"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Models with a discrete endogenous variable are typically underidentified when\nthe instrument takes on too few values. This paper presents a new method that\nmatches pairs of covariates and instruments to restore point identification in\nthis scenario in a triangular model. The model consists of a structural\nfunction for a continuous outcome and a selection model for the discrete\nendogenous variable. The structural outcome function must be continuous and\nmonotonic in a scalar disturbance, but it can be nonseparable. The selection\nmodel allows for unrestricted heterogeneity. Global identification is obtained\nunder weak conditions. The paper also provides estimators of the structural\noutcome function. Two empirical examples of the return to education and\nselection into Head Start illustrate the value and limitations of the method.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.01159v3"
    },
    {
        "title": "Fixed Effects Binary Choice Models: Estimation and Inference with Long\n  Panels",
        "authors": [
            "Daniel Czarnowske",
            "Amrei Stammann"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Empirical economists are often deterred from the application of fixed effects\nbinary choice models mainly for two reasons: the incidental parameter problem\nand the computational challenge even in moderately large panels. Using the\nexample of binary choice models with individual and time fixed effects, we show\nhow both issues can be alleviated by combining asymptotic bias corrections with\ncomputational advances. Because unbalancedness is often encountered in applied\nwork, we investigate its consequences on the finite sample properties of\nvarious (bias corrected) estimators. In simulation experiments we find that\nanalytical bias corrections perform particularly well, whereas split-panel\njackknife estimators can be severely biased in unbalanced panels.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.04217v3"
    },
    {
        "title": "Identification of Noncausal Models by Quantile Autoregressions",
        "authors": [
            "Alain Hecq",
            "Li Sun"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a model selection criterion to detect purely causal from purely\nnoncausal models in the framework of quantile autoregressions (QAR). We also\npresent asymptotics for the i.i.d. case with regularly varying distributed\ninnovations in QAR. This new modelling perspective is appealing for\ninvestigating the presence of bubbles in economic and financial time series,\nand is an alternative to approximate maximum likelihood methods. We illustrate\nour analysis using hyperinflation episodes in Latin American countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.05952v1"
    },
    {
        "title": "Distribution Regression in Duration Analysis: an Application to\n  Unemployment Spells",
        "authors": [
            "Miguel A. Delgado",
            "Andrés García-Suaza",
            "Pedro H. C. Sant'Anna"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This article proposes inference procedures for distribution regression models\nin duration analysis using randomly right-censored data. This generalizes\nclassical duration models by allowing situations where explanatory variables'\nmarginal effects freely vary with duration time. The article discusses\napplications to testing uniform restrictions on the varying coefficients,\ninferences on average marginal effects, and others involving conditional\ndistribution estimates. Finite sample properties of the proposed method are\nstudied by means of Monte Carlo experiments. Finally, we apply our proposal to\nstudy the effects of unemployment benefits on unemployment duration.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.06185v2"
    },
    {
        "title": "Complex Network Construction of Internet Financial risk",
        "authors": [
            "Runjie Xu",
            "Chuanmin Mi",
            "Rafal Mierzwiak",
            "Runyu Meng"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Internet finance is a new financial model that applies Internet technology to\npayment, capital borrowing and lending and transaction processing. In order to\nstudy the internal risks, this paper uses the Internet financial risk elements\nas the network node to construct the complex network of Internet financial risk\nsystem. Different from the study of macroeconomic shocks and financial\ninstitution data, this paper mainly adopts the perspective of complex system to\nanalyze the systematic risk of Internet finance. By dividing the entire\nfinancial system into Internet financial subnet, regulatory subnet and\ntraditional financial subnet, the paper discusses the relationship between\ncontagion and contagion among different risk factors, and concludes that risks\nare transmitted externally through the internal circulation of Internet\nfinance, thus discovering potential hidden dangers of systemic risks. The\nresults show that the nodes around the center of the whole system are the main\nobjects of financial risk contagion in the Internet financial network. In\naddition, macro-prudential regulation plays a decisive role in the control of\nthe Internet financial system, and points out the reasons why the current\nregulatory measures are still limited. This paper summarizes a research model\nwhich is still in its infancy, hoping to open up new prospects and directions\nfor us to understand the cascading behaviors of Internet financial risks.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.06640v3"
    },
    {
        "title": "Peer Effects in Random Consideration Sets",
        "authors": [
            "Nail Kashaev",
            "Natalia Lazzati"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We develop a dynamic model of discrete choice that incorporates peer effects\ninto random consideration sets. We characterize the equilibrium behavior and\nstudy the empirical content of the model. In our setup, changes in the choices\nof friends affect the distribution of the consideration sets. We exploit this\nvariation to recover the ranking of preferences, attention mechanisms, and\nnetwork connections. These nonparametric identification results allow\nunrestricted heterogeneity across people and do not rely on the variation of\neither covariates or the set of available options. Our methodology leads to a\nmaximum-likelihood estimator that performs well in simulations. We apply our\nresults to an experimental dataset that has been designed to study the visual\nfocus of attention.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.06742v3"
    },
    {
        "title": "A Generalized Continuous-Multinomial Response Model with a t-distributed\n  Error Kernel",
        "authors": [
            "Subodh Dubey",
            "Prateek Bansal",
            "Ricardo A. Daziano",
            "Erick Guerra"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In multinomial response models, idiosyncratic variations in the indirect\nutility are generally modeled using Gumbel or normal distributions. This study\nmakes a strong case to substitute these thin-tailed distributions with a\nt-distribution. First, we demonstrate that a model with a t-distributed error\nkernel better estimates and predicts preferences, especially in\nclass-imbalanced datasets. Our proposed specification also implicitly accounts\nfor decision-uncertainty behavior, i.e. the degree of certainty that\ndecision-makers hold in their choices relative to the variation in the indirect\nutility of any alternative. Second, after applying a t-distributed error kernel\nin a multinomial response model for the first time, we extend this\nspecification to a generalized continuous-multinomial (GCM) model and derive\nits full-information maximum likelihood estimator. The likelihood involves an\nopen-form expression of the cumulative density function of the multivariate\nt-distribution, which we propose to compute using a combination of the\ncomposite marginal likelihood method and the separation-of-variables approach.\nThird, we establish finite sample properties of the GCM model with a\nt-distributed error kernel (GCM-t) and highlight its superiority over the GCM\nmodel with a normally-distributed error kernel (GCM-N) in a Monte Carlo study.\nFinally, we compare GCM-t and GCM-N in an empirical setting related to\npreferences for electric vehicles (EVs). We observe that accounting for\ndecision-uncertainty behavior in GCM-t results in lower elasticity estimates\nand a higher willingness to pay for improving the EV attributes than those of\nthe GCM-N model. These differences are relevant in making policies to expedite\nthe adoption of EVs.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.08332v3"
    },
    {
        "title": "Location-Sector Analysis of International Profit Shifting on a\n  Multilayer Ownership-Tax Network",
        "authors": [
            "Tembo Nakamoto",
            "Odile Rouhban",
            "Yuichi Ikeda"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Currently all countries including developing countries are expected to\nutilize their own tax revenues and carry out their own development for solving\npoverty in their countries. However, developing countries cannot earn tax\nrevenues like developed countries partly because they do not have effective\ncountermeasures against international tax avoidance. Our analysis focuses on\ntreaty shopping among various ways to conduct international tax avoidance\nbecause tax revenues of developing countries have been heavily damaged through\ntreaty shopping. To analyze the location and sector of conduit firms likely to\nbe used for treaty shopping, we constructed a multilayer ownership-tax network\nand proposed multilayer centrality. Because multilayer centrality can consider\nnot only the value owing in the ownership network but also the withholding tax\nrate, it is expected to grasp precisely the locations and sectors of conduit\nfirms established for the purpose of treaty shopping. Our analysis shows that\nfirms in the sectors of Finance & Insurance and Wholesale & Retail trade etc.\nare involved with treaty shopping. We suggest that developing countries make a\nclause focusing on these sectors in the tax treaties they conclude.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.09165v1"
    },
    {
        "title": "Identification of Regression Models with a Misclassified and Endogenous\n  Binary Regressor",
        "authors": [
            "Hiroyuki Kasahara",
            "Katsumi Shimotsu"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We study identification in nonparametric regression models with a\nmisclassified and endogenous binary regressor when an instrument is correlated\nwith misclassification error. We show that the regression function is\nnonparametrically identified if one binary instrument variable and one binary\ncovariate satisfy the following conditions. The instrumental variable corrects\nendogeneity; the instrumental variable must be correlated with the unobserved\ntrue underlying binary variable, must be uncorrelated with the error term in\nthe outcome equation, but is allowed to be correlated with the\nmisclassification error. The covariate corrects misclassification; this\nvariable can be one of the regressors in the outcome equation, must be\ncorrelated with the unobserved true underlying binary variable, and must be\nuncorrelated with the misclassification error. We also propose a mixture-based\nframework for modeling unobserved heterogeneous treatment effects with a\nmisclassified and endogenous binary regressor and show that treatment effects\ncan be identified if the true treatment effect is related to an observed\nregressor and another observable variable.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.11143v3"
    },
    {
        "title": "Estimation of Conditional Average Treatment Effects with\n  High-Dimensional Data",
        "authors": [
            "Qingliang Fan",
            "Yu-Chin Hsu",
            "Robert P. Lieli",
            "Yichong Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Given the unconfoundedness assumption, we propose new nonparametric\nestimators for the reduced dimensional conditional average treatment effect\n(CATE) function. In the first stage, the nuisance functions necessary for\nidentifying CATE are estimated by machine learning methods, allowing the number\nof covariates to be comparable to or larger than the sample size. The second\nstage consists of a low-dimensional local linear regression, reducing CATE to a\nfunction of the covariate(s) of interest. We consider two variants of the\nestimator depending on whether the nuisance functions are estimated over the\nfull sample or over a hold-out sample. Building on Belloni at al. (2017) and\nChernozhukov et al. (2018), we derive functional limit theory for the\nestimators and provide an easy-to-implement procedure for uniform inference\nbased on the multiplier bootstrap. The empirical application revisits the\neffect of maternal smoking on a baby's birth weight as a function of the\nmother's age.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02399v5"
    },
    {
        "title": "Nonparametric Identification of First-Price Auction with Unobserved\n  Competition: A Density Discontinuity Framework",
        "authors": [
            "Emmanuel Guerre",
            "Yao Luo"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We consider nonparametric identification of independent private value\nfirst-price auction models, in which the analyst only observes winning bids.\nOur benchmark model assumes an exogenous number of bidders $N$. We show that,\nif the bidders observe $N$, the resulting discontinuities in the winning bid\ndensity can be used to identify the distribution of $N$. The private value\ndistribution can be nonparametrically identified in a second step. This\nextends, under testable identification conditions, to the case where $N$ is a\nnumber of potential buyers, who bid with some unknown probability.\nIdentification also holds in presence of additive unobserved heterogeneity\ndrawn from some parametric distributions. A parametric Bayesian estimation\nprocedure is proposed. An application to Shanghai Government IT procurements\nfinds that the imposed three bidders participation rule is not effective. This\ngenerates loss in the range of as large as $10\\%$ of the appraisal budget for\nsmall IT contracts.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.05476v3"
    },
    {
        "title": "Injectivity and the Law of Demand",
        "authors": [
            "Roy Allen"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Establishing that a demand mapping is injective is core first step for a\nvariety of methodologies. When a version of the law of demand holds, global\ninjectivity can be checked by seeing whether the demand mapping is constant\nover any line segments. When we add the assumption of differentiability, we\nobtain necessary and sufficient conditions for injectivity that generalize\nclassical \\cite{gale1965jacobian} conditions for quasi-definite Jacobians.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.05714v1"
    },
    {
        "title": "Forward-Selected Panel Data Approach for Program Evaluation",
        "authors": [
            "Zhentao Shi",
            "Jingyi Huang"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Policy evaluation is central to economic data analysis, but economists mostly\nwork with observational data in view of limited opportunities to carry out\ncontrolled experiments. In the potential outcome framework, the panel data\napproach (Hsiao, Ching and Wan, 2012) constructs the counterfactual by\nexploiting the correlation between cross-sectional units in panel data. The\nchoice of cross-sectional control units, a key step in its implementation, is\nnevertheless unresolved in data-rich environment when many possible controls\nare at the researcher's disposal. We propose the forward selection method to\nchoose control units, and establish validity of the post-selection inference.\nOur asymptotic framework allows the number of possible controls to grow much\nfaster than the time dimension. The easy-to-implement algorithms and their\ntheoretical guarantee extend the panel data approach to big data settings.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.05894v3"
    },
    {
        "title": "A model of discrete choice based on reinforcement learning under\n  short-term memory",
        "authors": [
            "Misha Perepelitsa"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  A family of models of individual discrete choice are constructed by means of\nstatistical averaging of choices made by a subject in a reinforcement learning\nprocess, where the subject has short, k-term memory span. The choice\nprobabilities in these models combine in a non-trivial, non-linear way the\ninitial learning bias and the experience gained through learning. The\nproperties of such models are discussed and, in particular, it is shown that\nprobabilities deviate from Luce's Choice Axiom, even if the initial bias\nadheres to it. Moreover, we shown that the latter property is recovered as the\nmemory span becomes large.\n  Two applications in utility theory are considered. In the first, we use the\ndiscrete choice model to generate binary preference relation on simple\nlotteries. We show that the preferences violate transitivity and independence\naxioms of expected utility theory. Furthermore, we establish the dependence of\nthe preferences on frames, with risk aversion for gains, and risk seeking for\nlosses. Based on these findings we propose next a parametric model of choice\nbased on the probability maximization principle, as a model for deviations from\nexpected utility principle. To illustrate the approach we apply it to the\nclassical problem of demand for insurance.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.06133v1"
    },
    {
        "title": "A Doubly Corrected Robust Variance Estimator for Linear GMM",
        "authors": [
            "Jungbin Hwang",
            "Byunghoon Kang",
            "Seojeong Lee"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a new finite sample corrected variance estimator for the linear\ngeneralized method of moments (GMM) including the one-step, two-step, and\niterated estimators. Our formula additionally corrects for the\nover-identification bias in variance estimation on top of the commonly used\nfinite sample correction of Windmeijer (2005) which corrects for the bias from\nestimating the efficient weight matrix, so is doubly corrected. An important\nfeature of the proposed double correction is that it automatically provides\nrobustness to misspecification of the moment condition. In contrast, the\nconventional variance estimator and the Windmeijer correction are inconsistent\nunder misspecification. That is, the proposed double correction formula\nprovides a convenient way to obtain improved inference under correct\nspecification and robustness against misspecification at the same time.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.07821v2"
    },
    {
        "title": "Nonparametric estimation of causal heterogeneity under high-dimensional\n  confounding",
        "authors": [
            "Michael Zimmert",
            "Michael Lechner"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper considers the practically important case of nonparametrically\nestimating heterogeneous average treatment effects that vary with a limited\nnumber of discrete and continuous covariates in a selection-on-observables\nframework where the number of possible confounders is very large. We propose a\ntwo-step estimator for which the first step is estimated by machine learning.\nWe show that this estimator has desirable statistical properties like\nconsistency, asymptotic normality and rate double robustness. In particular, we\nderive the coupled convergence conditions between the nonparametric and the\nmachine learning steps. We also show that estimating population average\ntreatment effects by averaging the estimated heterogeneous effects is\nsemi-parametrically efficient. The new estimator is an empirical example of the\neffects of mothers' smoking during pregnancy on the resulting birth weight.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.08779v1"
    },
    {
        "title": "Constraint Qualifications in Partial Identification",
        "authors": [
            "Hiroaki Kaido",
            "Francesca Molinari",
            "Jörg Stoye"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The literature on stochastic programming typically restricts attention to\nproblems that fulfill constraint qualifications. The literature on estimation\nand inference under partial identification frequently restricts the geometry of\nidentified sets with diverse high-level assumptions. These superficially appear\nto be different approaches to closely related problems. We extensively analyze\ntheir relation. Among other things, we show that for partial identification\nthrough pure moment inequalities, numerous assumptions from the literature\nessentially coincide with the Mangasarian-Fromowitz constraint qualification.\nThis clarifies the relation between well-known contributions, including within\neconometrics, and elucidates stringency, as well as ease of verification, of\nsome high-level assumptions in seminal papers.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.09103v4"
    },
    {
        "title": "Fixed-k Inference for Conditional Extremal Quantiles",
        "authors": [
            "Yuya Sasaki",
            "Yulong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We develop a new extreme value theory for repeated cross-sectional and panel\ndata to construct asymptotically valid confidence intervals (CIs) for\nconditional extremal quantiles from a fixed number $k$ of nearest-neighbor tail\nobservations. As a by-product, we also construct CIs for extremal quantiles of\ncoefficients in linear random coefficient models. For any fixed $k$, the CIs\nare uniformly valid without parametric assumptions over a set of nonparametric\ndata generating processes associated with various tail indices. Simulation\nstudies show that our CIs exhibit superior small-sample coverage and length\nproperties than alternative nonparametric methods based on asymptotic\nnormality. Applying the proposed method to Natality Vital Statistics, we study\nfactors of extremely low birth weights. We find that signs of major effects are\nthe same as those found in preceding studies based on parametric models, but\nwith different magnitudes.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.00294v3"
    },
    {
        "title": "Bias and Consistency in Three-way Gravity Models",
        "authors": [
            "Martin Weidner",
            "Thomas Zylkin"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We study the incidental parameter problem for the ``three-way'' Poisson\n{Pseudo-Maximum Likelihood} (``PPML'') estimator recently recommended for\nidentifying the effects of trade policies and in other panel data gravity\nsettings. Despite the number and variety of fixed effects involved, we confirm\nPPML is consistent for fixed $T$ and we show it is in fact the only estimator\namong a wide range of PML gravity estimators that is generally consistent in\nthis context when $T$ is fixed. At the same time, asymptotic confidence\nintervals in fixed-$T$ panels are not correctly centered at the true point\nestimates, and cluster-robust variance estimates used to construct standard\nerrors are generally biased as well. We characterize each of these biases\nanalytically and show both numerically and empirically that they are salient\neven for real-data settings with a large number of countries. We also offer\npractical remedies that can be used to obtain more reliable inferences of the\neffects of trade policies and other time-varying gravity variables, which we\nmake available via an accompanying Stata package called ppml_fe_bias.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.01327v6"
    },
    {
        "title": "Inference in Difference-in-Differences: How Much Should We Trust in\n  Independent Clusters?",
        "authors": [
            "Bruno Ferman"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We analyze the challenges for inference in difference-in-differences (DID)\nwhen there is spatial correlation. We present novel theoretical insights and\nempirical evidence on the settings in which ignoring spatial correlation should\nlead to more or less distortions in DID applications. We show that details such\nas the time frame used in the estimation, the choice of the treated and control\ngroups, and the choice of the estimator, are key determinants of distortions\ndue to spatial correlation. We also analyze the feasibility and trade-offs\ninvolved in a series of alternatives to take spatial correlation into account.\nGiven that, we provide relevant recommendations for applied researchers on how\nto mitigate and assess the possibility of inference distortions due to spatial\ncorrelation.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.01782v7"
    },
    {
        "title": "Shrinkage Estimation of Network Spillovers with Factor Structured Errors",
        "authors": [
            "Ayden Higgins",
            "Federico Martellosio"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper explores the estimation of a panel data model with cross-sectional\ninteraction that is flexible both in its approach to specifying the network of\nconnections between cross-sectional units, and in controlling for unobserved\nheterogeneity. It is assumed that there are different sources of information\navailable on a network, which can be represented in the form of multiple\nweights matrices. These matrices may reflect observed links, different measures\nof connectivity, groupings or other network structures, and the number of\nmatrices may be increasing with sample size. A penalised quasi-maximum\nlikelihood estimator is proposed which aims to alleviate the risk of network\nmisspecification by shrinking the coefficients of irrelevant weights matrices\nto exactly zero. Moreover, controlling for unobserved factors in estimation\nprovides a safeguard against the misspecification that might arise from\nunobserved heterogeneity. The asymptotic properties of the estimator are\nderived in a framework where the true value of each parameter remains fixed as\nthe total number of parameters increases. A Monte Carlo simulation is used to\nassess finite sample performance, and in an empirical application the method is\napplied to study the prevalence of network spillovers in determining growth\nrates across countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.02823v4"
    },
    {
        "title": "Identifying Different Definitions of Future in the Assessment of Future\n  Economic Conditions: Application of PU Learning and Text Mining",
        "authors": [
            "Masahiro Kato"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The Economy Watcher Survey, which is a market survey published by the\nJapanese government, contains \\emph{assessments of current and future economic\nconditions} by people from various fields. Although this survey provides\ninsights regarding economic policy for policymakers, a clear definition of the\nword \"future\" in future economic conditions is not provided. Hence, the\nassessments respondents provide in the survey are simply based on their\ninterpretations of the meaning of \"future.\" This motivated us to reveal the\ndifferent interpretations of the future in their judgments of future economic\nconditions by applying weakly supervised learning and text mining. In our\nresearch, we separate the assessments of future economic conditions into\neconomic conditions of the near and distant future using learning from positive\nand unlabeled data (PU learning). Because the dataset includes data from\nseveral periods, we devised new architecture to enable neural networks to\nconduct PU learning based on the idea of multi-task learning to efficiently\nlearn a classifier. Our empirical analysis confirmed that the proposed method\ncould separate the future economic conditions, and we interpreted the\nclassification results to obtain intuitions for policymaking.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.03348v3"
    },
    {
        "title": "Multiway Cluster Robust Double/Debiased Machine Learning",
        "authors": [
            "Harold D. Chiang",
            "Kengo Kato",
            "Yukun Ma",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper investigates double/debiased machine learning (DML) under multiway\nclustered sampling environments. We propose a novel multiway cross fitting\nalgorithm and a multiway DML estimator based on this algorithm. We also develop\na multiway cluster robust standard error formula. Simulations indicate that the\nproposed procedure has favorable finite sample performance. Applying the\nproposed method to market share data for demand analysis, we obtain larger\ntwo-way cluster robust standard errors than non-robust ones.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.03489v3"
    },
    {
        "title": "An Economic Topology of the Brexit vote",
        "authors": [
            "Pawel Dlotko",
            "Lucy Minford",
            "Simon Rudkin",
            "Wanling Qiu"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  A desire to understand the decision of the UK to leave the European Union,\nBrexit, in the referendum of June 2016 has continued to occupy academics, the\nmedia and politicians. Using topological data analysis ball mapper we extract\ninformation from multi-dimensional datasets gathered on Brexit voting and\nregional socio-economic characteristics. While we find broad patterns\nconsistent with extant empirical work, we also evidence that support for Leave\ndrew from a far more homogenous demographic than Remain. Obtaining votes from\nthis concise set was more straightforward for Leave campaigners than was\nRemain's task of mobilising a diverse group to oppose Brexit.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.03490v2"
    },
    {
        "title": "Tree-based Synthetic Control Methods: Consequences of moving the US\n  Embassy",
        "authors": [
            "Nicolaj Søndergaard Mühlbach",
            "Mikkel Slot Nielsen"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We recast the synthetic controls for evaluating policies as a counterfactual\nprediction problem and replace its linear regression with a nonparametric model\ninspired by machine learning. The proposed method enables us to achieve\naccurate counterfactual predictions and we provide theoretical guarantees. We\napply our method to a highly debated policy: the relocation of the US embassy\nto Jerusalem. In Israel and Palestine, we find that the average number of\nweekly conflicts has increased by roughly 103\\% over 48 weeks since the\nrelocation was announced on December 6, 2017. By using conformal inference and\nplacebo tests, we justify our model and find the increase to be statistically\nsignificant.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.03968v3"
    },
    {
        "title": "Matching Estimators with Few Treated and Many Control Observations",
        "authors": [
            "Bruno Ferman"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We analyze the properties of matching estimators when there are few treated,\nbut many control observations. We show that, under standard assumptions, the\nnearest neighbor matching estimator for the average treatment effect on the\ntreated is asymptotically unbiased in this framework. However, when the number\nof treated observations is fixed, the estimator is not consistent, and it is\ngenerally not asymptotically normal. Since standard inference methods are\ninadequate, we propose alternative inference methods, based on the theory of\nrandomization tests under approximate symmetry, that are asymptotically valid\nin this framework. We show that these tests are valid under relatively strong\nassumptions when the number of treated observations is fixed, and under weaker\nassumptions when the number of treated observations increases, but at a lower\nrate relative to the number of control observations.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.05093v4"
    },
    {
        "title": "Quantile regression methods for first-price auctions",
        "authors": [
            "Nathalie Gimenes",
            "Emmanuel Guerre"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The paper proposes a quantile-regression inference framework for first-price\nauctions with symmetric risk-neutral bidders under the independent\nprivate-value paradigm. It is first shown that a private-value quantile\nregression generates a quantile regression for the bids. The private-value\nquantile regression can be easily estimated from the bid quantile regression\nand its derivative with respect to the quantile level. This also allows to test\nfor various specification or exogeneity null hypothesis using the observed bids\nin a simple way. A new local polynomial technique is proposed to estimate the\nlatter over the whole quantile level interval. Plug-in estimation of\nfunctionals is also considered, as needed for the expected revenue or the case\nof CRRA risk-averse bidders, which is amenable to our framework. A\nquantile-regression analysis to USFS timber is found more appropriate than the\nhomogenized-bid methodology and illustrates the contribution of each\nexplanatory variables to the private-value distribution. Linear interactive\nsieve extensions are proposed and studied in the Appendices.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.05542v2"
    },
    {
        "title": "A Consistent LM Type Specification Test for Semiparametric Panel Data\n  Models",
        "authors": [
            "Ivan Korolev"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper develops a consistent series-based specification test for\nsemiparametric panel data models with fixed effects. The test statistic\nresembles the Lagrange Multiplier (LM) test statistic in parametric models and\nis based on a quadratic form in the restricted model residuals. The use of\nseries methods facilitates both estimation of the null model and computation of\nthe test statistic. The asymptotic distribution of the test statistic is\nstandard normal, so that appropriate critical values can easily be computed.\nThe projection property of series estimators allows me to develop a degrees of\nfreedom correction. This correction makes it possible to account for the\nestimation variance and obtain refined asymptotic results. It also\nsubstantially improves the finite sample performance of the test.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.05649v1"
    },
    {
        "title": "Adjusted QMLE for the spatial autoregressive parameter",
        "authors": [
            "Federico Martellosio",
            "Grant Hillier"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  One simple, and often very effective, way to attenuate the impact of nuisance\nparameters on maximum likelihood estimation of a parameter of interest is to\nrecenter the profile score for that parameter. We apply this general principle\nto the quasi-maximum likelihood estimator (QMLE) of the autoregressive\nparameter $\\lambda$ in a spatial autoregression. The resulting estimator for\n$\\lambda$ has better finite sample properties compared to the QMLE for\n$\\lambda$, especially in the presence of a large number of covariates. It can\nalso solve the incidental parameter problem that arises, for example, in social\ninteraction models with network fixed effects, or in spatial panel models with\nindividual or time fixed effects. However, spatial autoregressions present\nspecific challenges for this type of adjustment, because recentering the\nprofile score may cause the adjusted estimate to be outside the usual parameter\nspace for $\\lambda$. Conditions for this to happen are given, and implications\nare discussed. For inference, we propose confidence intervals based on a\nLugannani--Rice approximation to the distribution of the adjusted QMLE of\n$\\lambda$. Based on our simulations, the coverage properties of these intervals\nare excellent even in models with a large number of covariates.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.08141v1"
    },
    {
        "title": "Nonparametric Estimation of the Random Coefficients Model: An Elastic\n  Net Approach",
        "authors": [
            "Florian Heiss",
            "Stephan Hetzenecker",
            "Maximilian Osterhaus"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper investigates and extends the computationally attractive\nnonparametric random coefficients estimator of Fox, Kim, Ryan, and Bajari\n(2011). We show that their estimator is a special case of the nonnegative\nLASSO, explaining its sparse nature observed in many applications. Recognizing\nthis link, we extend the estimator, transforming it to a special case of the\nnonnegative elastic net. The extension improves the estimator's recovery of the\ntrue support and allows for more accurate estimates of the random coefficients'\ndistribution. Our estimator is a generalization of the original estimator and\ntherefore, is guaranteed to have a model fit at least as good as the original\none. A theoretical analysis of both estimators' properties shows that, under\nconditions, our generalized estimator approximates the true distribution more\naccurately. Two Monte Carlo experiments and an application to a travel mode\ndata set illustrate the improved performance of the generalized estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.08434v2"
    },
    {
        "title": "Subspace Clustering for Panel Data with Interactive Effects",
        "authors": [
            "Jiangtao Duan",
            "Wei Gao",
            "Hao Qu",
            "Hon Keung Tony"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In this paper, a statistical model for panel data with unobservable grouped\nfactor structures which are correlated with the regressors and the group\nmembership can be unknown. The factor loadings are assumed to be in different\nsubspaces and the subspace clustering for factor loadings are considered. A\nmethod called least squares subspace clustering estimate (LSSC) is proposed to\nestimate the model parameters by minimizing the least-square criterion and to\nperform the subspace clustering simultaneously. The consistency of the proposed\nsubspace clustering is proved and the asymptotic properties of the estimation\nprocedure are studied under certain conditions. A Monte Carlo simulation study\nis used to illustrate the advantages of the proposed method. Further\nconsiderations for the situations that the number of subspaces for factors, the\ndimension of factors and the dimension of subspaces are unknown are also\ndiscussed. For illustrative purposes, the proposed method is applied to study\nthe linkage between income and democracy across countries while subspace\npatterns of unobserved factors and factor loadings are allowed.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.09928v2"
    },
    {
        "title": "Inference for Linear Conditional Moment Inequalities",
        "authors": [
            "Isaiah Andrews",
            "Jonathan Roth",
            "Ariel Pakes"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We show that moment inequalities in a wide variety of economic applications\nhave a particular linear conditional structure. We use this structure to\nconstruct uniformly valid confidence sets that remain computationally tractable\neven in settings with nuisance parameters. We first introduce least favorable\ncritical values which deliver non-conservative tests if all moments are\nbinding. Next, we introduce a novel conditional inference approach which\nensures a strong form of insensitivity to slack moments. Our recommended\napproach is a hybrid technique which combines desirable aspects of the least\nfavorable and conditional methods. The hybrid approach performs well in\nsimulations calibrated to Wollmann (2018), with favorable power and\ncomputational time comparisons relative to existing alternatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.10062v5"
    },
    {
        "title": "Specification Testing in Nonparametric Instrumental Quantile Regression",
        "authors": [
            "Christoph Breunig"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  There are many environments in econometrics which require nonseparable\nmodeling of a structural disturbance. In a nonseparable model with endogenous\nregressors, key conditions are validity of instrumental variables and\nmonotonicity of the model in a scalar unobservable variable. Under these\nconditions the nonseparable model is equivalent to an instrumental quantile\nregression model. A failure of the key conditions, however, makes instrumental\nquantile regression potentially inconsistent. This paper develops a methodology\nfor testing the hypothesis whether the instrumental quantile regression model\nis correctly specified. Our test statistic is asymptotically normally\ndistributed under correct specification and consistent against any alternative\nmodel. In addition, test statistics to justify the model simplification are\nestablished. Finite sample properties are examined in a Monte Carlo study and\nan empirical illustration is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.10129v1"
    },
    {
        "title": "Goodness-of-Fit Tests based on Series Estimators in Nonparametric\n  Instrumental Regression",
        "authors": [
            "Christoph Breunig"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper proposes several tests of restricted specification in\nnonparametric instrumental regression. Based on series estimators, test\nstatistics are established that allow for tests of the general model against a\nparametric or nonparametric specification as well as a test of exogeneity of\nthe vector of regressors. The tests' asymptotic distributions under correct\nspecification are derived and their consistency against any alternative model\nis shown. Under a sequence of local alternative hypotheses, the asymptotic\ndistributions of the tests is derived. Moreover, uniform consistency is\nestablished over a class of alternatives whose distance to the null hypothesis\nshrinks appropriately as the sample size increases. A Monte Carlo study\nexamines finite sample performance of the test statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.10133v1"
    },
    {
        "title": "Inference in Nonparametric Series Estimation with Specification Searches\n  for the Number of Series Terms",
        "authors": [
            "Byunghoon Kang"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Nonparametric series regression often involves specification search over the\ntuning parameter, i.e., evaluating estimates and confidence intervals with a\ndifferent number of series terms. This paper develops pointwise and uniform\ninferences for conditional mean functions in nonparametric series estimations\nthat are uniform in the number of series terms. As a result, this paper\nconstructs confidence intervals and confidence bands with possibly\ndata-dependent series terms that have valid asymptotic coverage probabilities.\nThis paper also considers a partially linear model setup and develops inference\nmethods for the parametric part uniform in the number of series terms. The\nfinite sample performance of the proposed methods is investigated in various\nsimulation setups as well as in an illustrative example, i.e., the\nnonparametric estimation of the wage elasticity of the expected labor supply\nfrom Blomquist and Newey (2002).\n",
        "pdf_link": "http://arxiv.org/pdf/1909.12162v2"
    },
    {
        "title": "Debiased/Double Machine Learning for Instrumental Variable Quantile\n  Regressions",
        "authors": [
            "Jau-er Chen",
            "Chien-Hsun Huang",
            "Jia-Jyun Tien"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In this study, we investigate estimation and inference on a low-dimensional\ncausal parameter in the presence of high-dimensional controls in an\ninstrumental variable quantile regression. Our proposed econometric procedure\nbuilds on the Neyman-type orthogonal moment conditions of a previous study\nChernozhukov, Hansen and Wuthrich (2018) and is thus relatively insensitive to\nthe estimation of the nuisance parameters. The Monte Carlo experiments show\nthat the estimator copes well with high-dimensional controls. We also apply the\nprocedure to empirically reinvestigate the quantile treatment effect of 401(k)\nparticipation on accumulated wealth.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.12592v3"
    },
    {
        "title": "Regularized Quantile Regression with Interactive Fixed Effects",
        "authors": [
            "Junlong Feng"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper studies large $N$ and large $T$ conditional quantile panel data\nmodels with interactive fixed effects. We propose a nuclear norm penalized\nestimator of the coefficients on the covariates and the low-rank matrix formed\nby the fixed effects. The estimator solves a convex minimization problem, not\nrequiring pre-estimation of the (number of the) fixed effects. It also allows\nthe number of covariates to grow slowly with $N$ and $T$. We derive an error\nbound on the estimator that holds uniformly in quantile level. The order of the\nbound implies uniform consistency of the estimator and is nearly optimal for\nthe low-rank component. Given the error bound, we also propose a consistent\nestimator of the number of fixed effects at any quantile level. To derive the\nerror bound, we develop new theoretical arguments under primitive assumptions\nand new results on random matrices that may be of independent interest. We\ndemonstrate the performance of the estimator via Monte Carlo simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00166v4"
    },
    {
        "title": "Nonparametric Quantile Regressions for Panel Data Models with Large T",
        "authors": [
            "Liang Chen"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper considers panel data models where the conditional quantiles of the\ndependent variables are additively separable as unknown functions of the\nregressors and the individual effects. We propose two estimators of the\nquantile partial effects while controlling for the individual heterogeneity.\nThe first estimator is based on local linear quantile regressions, and the\nsecond is based on local linear smoothed quantile regressions, both of which\nare easy to compute in practice. Within the large T framework, we provide\nsufficient conditions under which the two estimators are shown to be\nasymptotically normally distributed. In particular, for the first estimator, it\nis shown that $N<<T^{2/(d+4)}$ is needed to ignore the incidental parameter\nbiases, where $d$ is the dimension of the regressors. For the second estimator,\nwe are able to derive the analytical expression of the asymptotic biases under\nthe assumption that $N\\approx Th^{d}$, where $h$ is the bandwidth parameter in\nlocal linear approximations. Our theoretical results provide the basis of using\nsplit-panel jackknife for bias corrections. A Monte Carlo simulation shows that\nthe proposed estimators and the bias-correction method perform well in finite\nsamples.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.01824v3"
    },
    {
        "title": "Quantile Factor Models",
        "authors": [
            "Liang Chen",
            "Juan Jose Dolado",
            "Jesus Gonzalo"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Quantile Factor Models (QFM) represent a new class of factor models for\nhigh-dimensional panel data. Unlike Approximate Factor Models (AFM), where only\nlocation-shifting factors can be extracted, QFM also allow to recover\nunobserved factors shifting other relevant parts of the distributions of\nobserved variables. A quantile regression approach, labeled Quantile Factor\nAnalysis (QFA), is proposed to consistently estimate all the quantile-dependent\nfactors and loadings. Their asymptotic distribution is then derived using a\nkernel-smoothed version of the QFA estimators. Two consistent model selection\ncriteria, based on information criteria and rank minimization, are developed to\ndetermine the number of factors at each quantile. Moreover, in contrast to the\nconditions required for the use of Principal Components Analysis in AFM, QFA\nestimation remains valid even when the idiosyncratic errors have heavy-tailed\ndistributions. Three empirical applications (regarding macroeconomic, climate\nand finance panel data) provide evidence that extra factors shifting the\nquantiles other than the means could be relevant in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.02173v2"
    },
    {
        "title": "An Asymptotically F-Distributed Chow Test in the Presence of\n  Heteroscedasticity and Autocorrelation",
        "authors": [
            "Yixiao Sun",
            "Xuexin Wang"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This study proposes a simple, trustworthy Chow test in the presence of\nheteroscedasticity and autocorrelation. The test is based on a series\nheteroscedasticity and autocorrelation robust variance estimator with\njudiciously crafted basis functions. Like the Chow test in a classical normal\nlinear regression, the proposed test employs the standard F distribution as the\nreference distribution, which is justified under fixed-smoothing asymptotics.\nMonte Carlo simulations show that the null rejection probability of the\nasymptotic F test is closer to the nominal level than that of the chi-square\ntest.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.03771v1"
    },
    {
        "title": "Identification in discrete choice models with imperfect information",
        "authors": [
            "Cristina Gualdani",
            "Shruti Sinha"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We study identification of preferences in static single-agent discrete choice\nmodels where decision makers may be imperfectly informed about the state of the\nworld. We leverage the notion of one-player Bayes Correlated Equilibrium by\nBergemann and Morris (2016) to provide a tractable characterization of the\nsharp identified set. We develop a procedure to practically construct the sharp\nidentified set following a sieve approach, and provide sharp bounds on\ncounterfactual outcomes of interest. We use our methodology and data on the\n2017 UK general election to estimate a spatial voting model under weak\nassumptions on agents' information about the returns to voting. Counterfactual\nexercises quantify the consequences of imperfect information on the well-being\nof voters and parties.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.04529v5"
    },
    {
        "title": "Extended MinP Tests for Global and Multiple testing",
        "authors": [
            "Zeng-Hua Lu"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Empirical economic studies often involve multiple propositions or hypotheses,\nwith researchers aiming to assess both the collective and individual evidence\nagainst these propositions or hypotheses. To rigorously assess this evidence,\npractitioners frequently employ tests with quadratic test statistics, such as\n$F$-tests and Wald tests, or tests based on minimum/maximum type test\nstatistics. This paper introduces a combination test that merges these two\nclasses of tests using the minimum $p$-value principle. The proposed test\ncapitalizes on the global power advantages of both constituent tests while\nretaining the benefits of the stepdown procedure from minimum/maximum type\ntests.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.04696v2"
    },
    {
        "title": "A Simple Estimator for Quantile Panel Data Models Using Smoothed\n  Quantile Regressions",
        "authors": [
            "Liang Chen",
            "Yulong Huo"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Canay (2011)'s two-step estimator of quantile panel data models, due to its\nsimple intuition and low computational cost, has been widely used in empirical\nstudies in recent years. In this paper, we revisit the estimator of Canay\n(2011) and point out that in his asymptotic analysis the bias of his estimator\ndue to the estimation of the fixed effects is mistakenly omitted, and that such\nomission will lead to invalid inference on the coefficients. To solve this\nproblem, we propose a similar easy-to-implement estimator based on smoothed\nquantile regressions. The asymptotic distribution of the new estimator is\nestablished and the analytical expression of its asymptotic bias is derived.\nBased on these results, we show how to make asymptotically valid inference\nbased on both analytical and split-panel jackknife bias corrections. Finally,\nfinite sample simulations are used to support our theoretical analysis and to\nillustrate the importance of bias correction in quantile regressions for panel\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.04729v1"
    },
    {
        "title": "Synthetic Controls with Imperfect Pre-Treatment Fit",
        "authors": [
            "Bruno Ferman",
            "Cristine Pinto"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We analyze the properties of the Synthetic Control (SC) and related\nestimators when the pre-treatment fit is imperfect. In this framework, we show\nthat these estimators are generally biased if treatment assignment is\ncorrelated with unobserved confounders, even when the number of pre-treatment\nperiods goes to infinity. Still, we show that a demeaned version of the SC\nmethod can substantially improve in terms of bias and variance relative to the\ndifference-in-difference estimator. We also derive a specification test for the\ndemeaned SC estimator in this setting with imperfect pre-treatment fit. Given\nour theoretical results, we provide practical guidance for applied researchers\non how to justify the use of such estimators in empirical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.08521v2"
    },
    {
        "title": "Robust Inference on Infinite and Growing Dimensional Time Series\n  Regression",
        "authors": [
            "Abhimanyu Gupta",
            "Myung Hwan Seo"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We develop a class of tests for time series models such as multiple\nregression with growing dimension, infinite-order autoregression and\nnonparametric sieve regression. Examples include the Chow test and general\nlinear restriction tests of growing rank $p$. Employing such increasing $p$\nasymptotics, we introduce a new scale correction to conventional test\nstatistics which accounts for a high-order long-run variance (HLV) that emerges\nas $ p $ grows with sample size. We also propose a bias correction via a\nnull-imposed bootstrap to alleviate finite sample bias without sacrificing\npower unduly. A simulation study shows the importance of robustifying testing\nprocedures against the HLV even when $ p $ is moderate. The tests are\nillustrated with an application to the oil regressions in Hamilton (2003).\n",
        "pdf_link": "http://arxiv.org/pdf/1911.08637v4"
    },
    {
        "title": "A Flexible Mixed-Frequency Vector Autoregression with a Steady-State\n  Prior",
        "authors": [
            "Sebastian Ankargren",
            "Måns Unosson",
            "Yukai Yang"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a Bayesian vector autoregressive (VAR) model for mixed-frequency\ndata. Our model is based on the mean-adjusted parametrization of the VAR and\nallows for an explicit prior on the 'steady states' (unconditional means) of\nthe included variables. Based on recent developments in the literature, we\ndiscuss extensions of the model that improve the flexibility of the modeling\napproach. These extensions include a hierarchical shrinkage prior for the\nsteady-state parameters, and the use of stochastic volatility to model\nheteroskedasticity. We put the proposed model to use in a forecast evaluation\nusing US data consisting of 10 monthly and 3 quarterly variables. The results\nshow that the predictive ability typically benefits from using mixed-frequency\ndata, and that improvements can be obtained for both monthly and quarterly\nvariables. We also find that the steady-state prior generally enhances the\naccuracy of the forecasts, and that accounting for heteroskedasticity by means\nof stochastic volatility usually provides additional improvements, although not\nfor all variables.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.09151v1"
    },
    {
        "title": "Uniform inference for value functions",
        "authors": [
            "Sergio Firpo",
            "Antonio F. Galvao",
            "Thomas Parker"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a method to conduct uniform inference for the (optimal) value\nfunction, that is, the function that results from optimizing an objective\nfunction marginally over one of its arguments. Marginal optimization is not\nHadamard differentiable (that is, compactly differentiable) as a map between\nthe spaces of objective and value functions, which is problematic because\nstandard inference methods for nonlinear maps usually rely on Hadamard\ndifferentiability. However, we show that the map from objective function to an\n$L_p$ functional of a value function, for $1 \\leq p \\leq \\infty$, are Hadamard\ndirectionally differentiable. As a result, we establish consistency and weak\nconvergence of nonparametric plug-in estimates of Cram\\'er-von Mises and\nKolmogorov-Smirnov test statistics applied to value functions. For practical\ninference, we develop detailed resampling techniques that combine a bootstrap\nprocedure with estimates of the directional derivatives. In addition, we\nestablish local size control of tests which use the resampling procedure. Monte\nCarlo simulations assess the finite-sample properties of the proposed methods\nand show accurate empirical size and nontrivial power of the procedures.\nFinally, we apply our methods to the evaluation of a job training program using\nbounds for the distribution function of treatment effects.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.10215v7"
    },
    {
        "title": "Topologically Mapping the Macroeconomy",
        "authors": [
            "Pawel Dlotko",
            "Simon Rudkin",
            "Wanling Qiu"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  An understanding of the economic landscape in a world of ever increasing data\nnecessitates representations of data that can inform policy, deepen\nunderstanding and guide future research. Topological Data Analysis offers a set\nof tools which deliver on all three calls. Abstract two-dimensional snapshots\nof multi-dimensional space readily capture non-monotonic relationships, inform\nof similarity between points of interest in parameter space, mapping such to\noutcomes. Specific examples show how some, but not all, countries have returned\nto Great Depression levels, and reappraise the links between real private\ncapital growth and the performance of the economy. Theoretical and empirical\nexpositions alike remind on the dangers of assuming monotonic relationships and\ndiscounting combinations of factors as determinants of outcomes; both dangers\nTopological Data Analysis addresses. Policy-makers can look at outcomes and\ntarget areas of the input space where such are not satisfactory, academics may\nadditionally find evidence to motivate theoretical development, and\npractitioners can gain a rapid and robust base for decision making.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.10476v1"
    },
    {
        "title": "High-Dimensional Forecasting in the Presence of Unit Roots and\n  Cointegration",
        "authors": [
            "Stephan Smeekes",
            "Etienne Wijler"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We investigate how the possible presence of unit roots and cointegration\naffects forecasting with Big Data. As most macroeoconomic time series are very\npersistent and may contain unit roots, a proper handling of unit roots and\ncointegration is of paramount importance for macroeconomic forecasting. The\nhigh-dimensional nature of Big Data complicates the analysis of unit roots and\ncointegration in two ways. First, transformations to stationarity require\nperforming many unit root tests, increasing room for errors in the\nclassification. Second, modelling unit roots and cointegration directly is more\ndifficult, as standard high-dimensional techniques such as factor models and\npenalized regression are not directly applicable to (co)integrated data and\nneed to be adapted. We provide an overview of both issues and review methods\nproposed to address these issues. These methods are also illustrated with two\nempirical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.10552v1"
    },
    {
        "title": "Predicting crashes in oil prices during the COVID-19 pandemic with mixed\n  causal-noncausal models",
        "authors": [
            "Alain Hecq",
            "Elisa Voisin"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper aims at shedding light upon how transforming or detrending a\nseries can substantially impact predictions of mixed causal-noncausal (MAR)\nmodels, namely dynamic processes that depend not only on their lags but also on\ntheir leads. MAR models have been successfully implemented on commodity prices\nas they allow to generate nonlinear features such as locally explosive episodes\n(denoted here as bubbles) in a strictly stationary setting. We consider\nmultiple detrending methods and investigate, using Monte Carlo simulations, to\nwhat extent they preserve the bubble patterns observed in the raw data. MAR\nmodels relies on the dynamics observed in the series alone and does not require\neconomical background to construct a structural model, which can sometimes be\nintricate to specify or which may lack parsimony. We investigate oil prices and\nestimate probabilities of crashes before and during the first 2020 wave of the\nCOVID-19 pandemic. We consider three different mechanical detrending methods\nand compare them to a detrending performed using the level of strategic\npetroleum reserves.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.10916v3"
    },
    {
        "title": "Inference under random limit bootstrap measures",
        "authors": [
            "Giuseppe Cavaliere",
            "Iliyan Georgiev"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Asymptotic bootstrap validity is usually understood as consistency of the\ndistribution of a bootstrap statistic, conditional on the data, for the\nunconditional limit distribution of a statistic of interest. From this\nperspective, randomness of the limit bootstrap measure is regarded as a failure\nof the bootstrap. We show that such limiting randomness does not necessarily\ninvalidate bootstrap inference if validity is understood as control over the\nfrequency of correct inferences in large samples. We first establish sufficient\nconditions for asymptotic bootstrap validity in cases where the unconditional\nlimit distribution of a statistic can be obtained by averaging a (random)\nlimiting bootstrap distribution. Further, we provide results ensuring the\nasymptotic validity of the bootstrap as a tool for conditional inference, the\nleading case being that where a bootstrap distribution estimates consistently a\nconditional (and thus, random) limit distribution of a statistic. We apply our\nframework to several inference problems in econometrics, including linear\nmodels with possibly non-stationary regressors, functional CUSUM statistics,\nconditional Kolmogorov-Smirnov specification tests, the `parameter on the\nboundary' problem and tests for constancy of parameters in dynamic econometric\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.12779v2"
    },
    {
        "title": "Semiparametric Quantile Models for Ascending Auctions with Asymmetric\n  Bidders",
        "authors": [
            "Jayeeta Bhattacharya",
            "Nathalie Gimenes",
            "Emmanuel Guerre"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The paper proposes a parsimonious and flexible semiparametric quantile\nregression specification for asymmetric bidders within the independent private\nvalue framework. Asymmetry is parameterized using powers of a parent private\nvalue distribution, which is generated by a quantile regression specification.\nAs noted in Cantillon (2008) , this covers and extends models used for\nefficient collusion, joint bidding and mergers among homogeneous bidders. The\nspecification can be estimated for ascending auctions using the winning bids\nand the winner's identity. The estimation is in two stage. The asymmetry\nparameters are estimated from the winner's identity using a simple maximum\nlikelihood procedure. The parent quantile regression specification can be\nestimated using simple modifications of Gimenes (2017). Specification testing\nprocedures are also considered. A timber application reveals that weaker\nbidders have $30\\%$ less chances to win the auction than stronger ones. It is\nalso found that increasing participation in an asymmetric ascending auction may\nnot be as beneficial as using an optimal reserve price as would have been\nexpected from a result of BulowKlemperer (1996) valid under symmetry.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.13063v2"
    },
    {
        "title": "Do Public Program Benefits Crowd Out Private Transfers in Developing\n  Countries? A Critical Review of Recent Evidence",
        "authors": [
            "Plamen Nikolov",
            "Matthew Bonci"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Precipitated by rapid globalization, rising inequality, population growth,\nand longevity gains, social protection programs have been on the rise in low-\nand middle-income countries (LMICs) in the last three decades. However, the\nintroduction of public benefits could displace informal mechanisms for\nrisk-protection, which are especially prevalent in LMICs. If the displacement\nof private transfers is considerably large, the expansion of social protection\nprograms could even lead to social welfare loss. In this paper, we critically\nsurvey the recent empirical literature on crowd-out effects in response to\npublic policies, specifically in the context of LMICs. We review and synthesize\npatterns from the behavioral response to various types of social protection\nprograms. Furthermore, we specifically examine for heterogeneous treatment\neffects by important socioeconomic characteristics. We conclude by drawing on\nlessons from our synthesis of studies. If poverty reduction objectives are\nconsidered, along with careful program targeting that accounts for potential\ncrowd-out effects, there may well be a net social gain.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.00737v2"
    },
    {
        "title": "On the plausibility of the latent ignorability assumption",
        "authors": [
            "Martin Huber"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The estimation of the causal effect of an endogenous treatment based on an\ninstrumental variable (IV) is often complicated by attrition, sample selection,\nor non-response in the outcome of interest. To tackle the latter problem, the\nlatent ignorability (LI) assumption imposes that attrition/sample selection is\nindependent of the outcome conditional on the treatment compliance type (i.e.\nhow the treatment behaves as a function of the instrument), the instrument, and\npossibly further observed covariates. As a word of caution, this note formally\ndiscusses the strong behavioral implications of LI in rather standard IV\nmodels. We also provide an empirical illustration based on the Job Corps\nexperimental study, in which the sensitivity of the estimated program effect to\nLI and alternative assumptions about outcome attrition is investigated.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.01703v2"
    },
    {
        "title": "Testing Finite Moment Conditions for the Consistency and the Root-N\n  Asymptotic Normality of the GMM and M Estimators",
        "authors": [
            "Yuya Sasaki",
            "Yulong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Common approaches to inference for structural and reduced-form parameters in\nempirical economic analysis are based on the consistency and the root-n\nasymptotic normality of the GMM and M estimators. The canonical consistency\n(respectively, root-n asymptotic normality) for these classes of estimators\nrequires at least the first (respectively, second) moment of the score to be\nfinite. In this article, we present a method of testing these conditions for\nthe consistency and the root-n asymptotic normality of the GMM and M\nestimators. The proposed test controls size nearly uniformly over the set of\ndata generating processes that are compatible with the null hypothesis.\nSimulation studies support this theoretical result. Applying the proposed test\nto the market share data from the Dominick's Finer Foods retail chain, we find\nthat a common \\textit{ad hoc} procedure to deal with zero market shares in\nanalysis of differentiated products markets results in a failure to satisfy the\nconditions for both the consistency and the root-n asymptotic normality.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.02541v3"
    },
    {
        "title": "The pain of a new idea: Do Late Bloomers response to Extension Service\n  in Rural Ethiopia?",
        "authors": [
            "Alexander Jordan",
            "Marco Guerzoni"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The paper analyses the efficiency of extension programs in the adoption of\nchemical fertilisers in Ethiopia between 1994 and 2004. Fertiliser adoption\nprovides a suitable strategy to ensure and stabilize food production in remote\nvulnerable areas. Extension services programs have a long history in supporting\nthe application of fertiliser. How-ever, their efficiency is questioned. In our\nanalysis, we focus on seven villages with a considerable time lag in fertiliser\ndiffusion. Using matching techniques avoids sample selection bias in the\ncomparison of treated (households received extension service) and controlled\nhouseholds. Additionally to common factors, measures of culture, proxied by\nethnicity and religion, aim to control for potential tensions between extension\nagents and peasants that hamper the efficiency of the program. We find a\nconsiderable impact of extension service on the first fertiliser adoption. The\nimpact is consistent for five of seven villages.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.02846v1"
    },
    {
        "title": "Text as data: a machine learning-based approach to measuring uncertainty",
        "authors": [
            "Rickard Nyman",
            "Paul Ormerod"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The Economic Policy Uncertainty index had gained considerable traction with\nboth academics and policy practitioners. Here, we analyse news feed data to\nconstruct a simple, general measure of uncertainty in the United States using a\nhighly cited machine learning methodology. Over the period January 1996 through\nMay 2020, we show that the series unequivocally Granger-causes the EPU and\nthere is no Granger-causality in the reverse direction\n",
        "pdf_link": "http://arxiv.org/pdf/2006.06457v1"
    },
    {
        "title": "Nonparametric Tests of Tail Behavior in Stochastic Frontier Models",
        "authors": [
            " William",
            "C. Horrace",
            "Yulong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This article studies tail behavior for the error components in the stochastic\nfrontier model, where one component has bounded support on one side, and the\nother has unbounded support on both sides. Under weak assumptions on the error\ncomponents, we derive nonparametric tests that the unbounded component\ndistribution has thin tails and that the component tails are equivalent. The\ntests are useful diagnostic tools for stochastic frontier analysis. A\nsimulation study and an application to a stochastic cost frontier for 6,100 US\nbanks from 1998 to 2005 are provided. The new tests reject the normal or\nLaplace distributional assumptions, which are commonly imposed in the existing\nliterature.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.07780v1"
    },
    {
        "title": "Measuring Macroeconomic Uncertainty: The Labor Channel of Uncertainty\n  from a Cross-Country Perspective",
        "authors": [
            "Andreas Dibiasi",
            "Samad Sarferaz"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper constructs internationally consistent measures of macroeconomic\nuncertainty. Our econometric framework extracts uncertainty from revisions in\ndata obtained from standardized national accounts. Applying our model to\npost-WWII real-time data, we estimate macroeconomic uncertainty for 39\ncountries. The cross-country dimension of our uncertainty data allows us to\nstudy the impact of uncertainty shocks under different employment protection\nlegislation. Our empirical findings suggest that the effects of uncertainty\nshocks are stronger and more persistent in countries with low employment\nprotection compared to countries with high employment protection. These\nempirical findings are in line with a theoretical model under varying firing\ncost.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.09007v2"
    },
    {
        "title": "Flexible Mixture Priors for Large Time-varying Parameter Models",
        "authors": [
            "Niko Hauzenberger"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Time-varying parameter (TVP) models often assume that the TVPs evolve\naccording to a random walk. This assumption, however, might be questionable\nsince it implies that coefficients change smoothly and in an unbounded manner.\nIn this paper, we relax this assumption by proposing a flexible law of motion\nfor the TVPs in large-scale vector autoregressions (VARs). Instead of imposing\na restrictive random walk evolution of the latent states, we carefully design\nhierarchical mixture priors on the coefficients in the state equation. These\npriors effectively allow for discriminating between periods where coefficients\nevolve according to a random walk and times where the TVPs are better\ncharacterized by a stationary stochastic process. Moreover, this approach is\ncapable of introducing dynamic sparsity by pushing small parameter changes\ntowards zero if necessary. The merits of the model are illustrated by means of\ntwo applications. Using synthetic data we show that our approach yields precise\nparameter estimates. When applied to US data, the model reveals interesting\npatterns of low-frequency dynamics in coefficients and forecasts well relative\nto a wide range of competing models.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.10088v2"
    },
    {
        "title": "COVID-19 response needs to broaden financial inclusion to curb the rise\n  in poverty",
        "authors": [
            "Mostak Ahamed",
            "Roxana Gutiérrez-Romero"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The ongoing COVID-19 pandemic risks wiping out years of progress made in\nreducing global poverty. In this paper, we explore to what extent financial\ninclusion could help mitigate the increase in poverty using cross-country data\nacross 78 low- and lower-middle-income countries. Unlike other recent\ncross-country studies, we show that financial inclusion is a key driver of\npoverty reduction in these countries. This effect is not direct, but indirect,\nby mitigating the detrimental effect that inequality has on poverty. Our\nfindings are consistent across all the different measures of poverty used. Our\nforecasts suggest that the world's population living on less than $1.90 per day\ncould increase from 8% to 14% by 2021, pushing nearly 400 million people into\npoverty. However, urgent improvements in financial inclusion could\nsubstantially reduce the impact on poverty.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.10706v1"
    },
    {
        "title": "On the Time Trend of COVID-19: A Panel Data Study",
        "authors": [
            "Chaohua Dong",
            "Jiti Gao",
            "Oliver Linton",
            "Bin Peng"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper, we study the trending behaviour of COVID-19 data at country\nlevel, and draw attention to some existing econometric tools which are\npotentially helpful to understand the trend better in future studies. In our\nempirical study, we find that European countries overall flatten the curves\nmore effectively compared to the other regions, while Asia & Oceania also\nachieve some success, but the situations are not as optimistic elsewhere.\nAfrica and America are still facing serious challenges in terms of managing the\nspread of the virus, and reducing the death rate, although in Africa the virus\nspreads slower and has a lower death rate than the other regions. By comparing\nthe performances of different countries, our results incidentally agree with Gu\net al. (2020), though different approaches and models are considered. For\nexample, both works agree that countries such as USA, UK and Italy perform\nrelatively poorly; on the other hand, Australia, China, Japan, Korea, and\nSingapore perform relatively better.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.11060v2"
    },
    {
        "title": "A Pipeline for Variable Selection and False Discovery Rate Control With\n  an Application in Labor Economics",
        "authors": [
            "Sophie-Charlotte Klose",
            "Johannes Lederer"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We introduce tools for controlled variable selection to economists. In\nparticular, we apply a recently introduced aggregation scheme for false\ndiscovery rate (FDR) control to German administrative data to determine the\nparts of the individual employment histories that are relevant for the career\noutcomes of women. Our results suggest that career outcomes can be predicted\nbased on a small set of variables, such as daily earnings, wage increases in\ncombination with a high level of education, employment status, and working\nexperience.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.12296v2"
    },
    {
        "title": "Locally trimmed least squares: conventional inference in possibly\n  nonstationary models",
        "authors": [
            "Zhishui Hu",
            "Ioannis Kasparis",
            "Qiying Wang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  A novel IV estimation method, that we term Locally Trimmed LS (LTLS), is\ndeveloped which yields estimators with (mixed) Gaussian limit distributions in\nsituations where the data may be weakly or strongly persistent. In particular,\nwe allow for nonlinear predictive type of regressions where the regressor can\nbe stationary short/long memory as well as nonstationary long memory process or\na nearly integrated array. The resultant t-tests have conventional limit\ndistributions (i.e. N(0; 1)) free of (near to unity and long memory) nuisance\nparameters. In the case where the regressor is a fractional process, no\npreliminary estimator for the memory parameter is required. Therefore, the\npractitioner can conduct inference while being agnostic about the exact\ndependence structure in the data. The LTLS estimator is obtained by applying\ncertain chronological trimming to the OLS instrument via the utilisation of\nappropriate kernel functions of time trend variables. The finite sample\nperformance of LTLS based t-tests is investigated with the aid of a simulation\nexperiment. An empirical application to the predictability of stock returns is\nalso provided.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.12595v1"
    },
    {
        "title": "Treatment Effects in Interactive Fixed Effects Models with a Small\n  Number of Time Periods",
        "authors": [
            "Brantly Callaway",
            "Sonia Karami"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper considers identifying and estimating the Average Treatment Effect\non the Treated (ATT) when untreated potential outcomes are generated by an\ninteractive fixed effects model. That is, in addition to time-period and\nindividual fixed effects, we consider the case where there is an unobserved\ntime invariant variable whose effect on untreated potential outcomes may change\nover time and which can therefore cause outcomes (in the absence of\nparticipating in the treatment) to follow different paths for the treated group\nrelative to the untreated group. The models that we consider in this paper\ngeneralize many commonly used models in the treatment effects literature\nincluding difference in differences and individual-specific linear trend\nmodels. Unlike the majority of the literature on interactive fixed effects\nmodels, we do not require the number of time periods to go to infinity to\nconsistently estimate the ATT. Our main identification result relies on having\nthe effect of some time invariant covariate (e.g., race or sex) not vary over\ntime. Using our approach, we show that the ATT can be identified with as few as\nthree time periods and with panel or repeated cross sections data.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.15780v3"
    },
    {
        "title": "Inference in Difference-in-Differences with Few Treated Units and\n  Spatial Correlation",
        "authors": [
            "Luis Alvarez",
            "Bruno Ferman"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We consider the problem of inference in Difference-in-Differences (DID) when\nthere are few treated units and errors are spatially correlated. We first show\nthat, when there is a single treated unit, some existing inference methods\ndesigned for settings with few treated and many control units remain\nasymptotically valid when errors are weakly dependent. However, these methods\nmay be invalid with more than one treated unit. We propose alternatives that\nare asymptotically valid in this setting, even when the relevant distance\nmetric across units is unavailable.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.16997v7"
    },
    {
        "title": "Testable Implications of Multiple Equilibria in Discrete Games with\n  Correlated Types",
        "authors": [
            "Aureo de Paula",
            "Xun Tang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study testable implications of multiple equilibria in discrete games with\nincomplete information. Unlike de Paula and Tang (2012), we allow the players'\nprivate signals to be correlated. In static games, we leverage independence of\nprivate types across games whose equilibrium selection is correlated. In\ndynamic games with serially correlated discrete unobserved heterogeneity, our\ntestable implication builds on the fact that the distribution of a sequence of\nchoices and states are mixtures over equilibria and unobserved heterogeneity.\nThe number of mixture components is a known function of the length of the\nsequence as well as the cardinality of equilibria and unobserved heterogeneity\nsupport. In both static and dynamic cases, these testable implications are\nimplementable using existing statistical tools.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.00787v1"
    },
    {
        "title": "Bull and Bear Markets During the COVID-19 Pandemic",
        "authors": [
            "John M. Maheu",
            "Thomas H. McCurdy",
            "Yong Song"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The COVID-19 pandemic has caused severe disruption to economic and financial\nactivity worldwide. We assess what happened to the aggregate U.S. stock market\nduring this period, including implications for both short and long-horizon\ninvestors. Using the model of Maheu, McCurdy and Song (2012), we provide\nsmoothed estimates and out-of-sample forecasts associated with stock market\ndynamics during the pandemic. We identify bull and bear market regimes\nincluding their bull correction and bear rally components, demonstrate the\nmodel's performance in capturing periods of significant regime change, and\nprovide forecasts that improve risk management and investment decisions. The\npaper concludes with out-of-sample forecasts of market states one year ahead.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.01623v1"
    },
    {
        "title": "Inference in mixed causal and noncausal models with generalized\n  Student's t-distributions",
        "authors": [
            "Francesco Giancaterini",
            "Alain Hecq"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The properties of Maximum Likelihood estimator in mixed causal and noncausal\nmodels with a generalized Student's t error process are reviewed. Several known\nexisting methods are typically not applicable in the heavy-tailed framework. To\nthis end, a new approach to make inference on causal and noncausal parameters\nin finite sample sizes is proposed. It exploits the empirical variance of the\ngeneralized Student's-t, without the existence of population variance. Monte\nCarlo simulations show a good performance of the new variance construction for\nfat tail series. Finally, different existing approaches are compared using\nthree empirical applications: the variation of daily COVID-19 deaths in\nBelgium, the monthly wheat prices, and the monthly inflation rate in Brazil.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.01888v2"
    },
    {
        "title": "Sharp Bounds in the Latent Index Selection Model",
        "authors": [
            "Philip Marx"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  A fundamental question underlying the literature on partial identification\nis: what can we learn about parameters that are relevant for policy but not\nnecessarily point-identified by the exogenous variation we observe? This paper\nprovides an answer in terms of sharp, analytic characterizations and bounds for\nan important class of policy-relevant treatment effects, consisting of marginal\ntreatment effects and linear functionals thereof, in the latent index selection\nmodel as formalized in Vytlacil (2002). The sharp bounds use the full content\nof identified marginal distributions, and analytic derivations rely on the\ntheory of stochastic orders. The proposed methods also make it possible to\nsharply incorporate new auxiliary assumptions on distributions into the latent\nindex selection framework. Empirically, I apply the methods to study the\neffects of Medicaid on emergency room utilization in the Oregon Health\nInsurance Experiment, showing that the predictions from extrapolations based on\na distribution assumption (rank similarity) differ substantively and\nconsistently from existing extrapolations based on a parametric mean assumption\n(linearity). This underscores the value of utilizing the model's full empirical\ncontent in combination with auxiliary assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.02390v2"
    },
    {
        "title": "Asymmetric uncertainty : Nowcasting using skewness in real-time data",
        "authors": [
            "Paul Labonne"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper presents a new way to account for downside and upside risks when\nproducing density nowcasts of GDP growth. The approach relies on modelling\nlocation, scale and shape common factors in real-time macroeconomic data. While\nmovements in the location generate shifts in the central part of the predictive\ndensity, the scale controls its dispersion (akin to general uncertainty) and\nthe shape its asymmetry, or skewness (akin to downside and upside risks). The\nempirical application is centred on US GDP growth and the real-time data come\nfrom Fred-MD. The results show that there is more to real-time data than their\nlevels or means: their dispersion and asymmetry provide valuable information\nfor nowcasting economic activity. Scale and shape common factors (i) yield more\nreliable measures of uncertainty and (ii) improve precision when macroeconomic\nuncertainty is at its peak.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.02601v4"
    },
    {
        "title": "A Multivariate Realized GARCH Model",
        "authors": [
            "Ilya Archakov",
            "Peter Reinhard Hansen",
            "Asger Lunde"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose a novel class of multivariate GARCH models that utilize realized\nmeasures of volatilities and correlations. The central component is an\nunconstrained vector parametrization of the conditional correlation matrix that\nfacilitates factor models for correlations. This offers an elegant solution to\nthe primary challenge that plagues multivariate GARCH models in\nhigh-dimensional settings. As an illustration, we consider block correlation\nstructures that naturally simplify to linear factor models for the conditional\ncorrelations. We apply the model to returns of nine assets and inspect\nin-sample and out-of-sample model performance in comparison with several\npopular benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.02708v2"
    },
    {
        "title": "Binary Response Models for Heterogeneous Panel Data with Interactive\n  Fixed Effects",
        "authors": [
            "Jiti Gao",
            "Fei Liu",
            "Bin Peng",
            "Yayi Yan"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper, we investigate binary response models for heterogeneous panel\ndata with interactive fixed effects by allowing both the cross-sectional\ndimension and the temporal dimension to diverge. From a practical point of\nview, the proposed framework can be applied to predict the probability of\ncorporate failure, conduct credit rating analysis, etc. Theoretically and\nmethodologically, we establish a link between a maximum likelihood estimation\nand a least squares approach, provide a simple information criterion to detect\nthe number of factors, and achieve the asymptotic distributions accordingly. In\naddition, we conduct intensive simulations to examine the theoretical findings.\nIn the empirical study, we focus on the sign prediction of stock returns, and\nthen use the results of sign forecast to conduct portfolio analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.03182v2"
    },
    {
        "title": "Asymptotic Normality for Multivariate Random Forest Estimators",
        "authors": [
            "Kevin Li"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Regression trees and random forests are popular and effective non-parametric\nestimators in practical applications. A recent paper by Athey and Wager shows\nthat the random forest estimate at any point is asymptotically Gaussian; in\nthis paper, we extend this result to the multivariate case and show that the\nvector of estimates at multiple points is jointly normal. Specifically, the\ncovariance matrix of the limiting normal distribution is diagonal, so that the\nestimates at any two points are independent in sufficiently deep trees.\nMoreover, the off-diagonal term is bounded by quantities capturing how likely\ntwo points belong to the same partition of the resulting tree. Our results\nrelies on certain a certain stability property when constructing splits, and we\ngive examples of splitting rules for which this assumption is and is not\nsatisfied. We test our proposed covariance bound and the associated coverage\nrates of confidence intervals in numerical simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.03486v3"
    },
    {
        "title": "Who Should Get Vaccinated? Individualized Allocation of Vaccines Over\n  SIR Network",
        "authors": [
            "Toru Kitagawa",
            "Guanyi Wang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  How to allocate vaccines over heterogeneous individuals is one of the\nimportant policy decisions in pandemic times. This paper develops a procedure\nto estimate an individualized vaccine allocation policy under limited supply,\nexploiting social network data containing individual demographic\ncharacteristics and health status. We model spillover effects of the vaccines\nbased on a Heterogeneous-Interacted-SIR network model and estimate an\nindividualized vaccine allocation policy by maximizing an estimated social\nwelfare (public health) criterion incorporating the spillovers. While this\noptimization problem is generally an NP-hard integer optimization problem, we\nshow that the SIR structure leads to a submodular objective function, and\nprovide a computationally attractive greedy algorithm for approximating a\nsolution that has theoretical performance guarantee. Moreover, we characterise\na finite sample welfare regret bound and examine how its uniform convergence\nrate depends on the complexity and riskiness of social network. In the\nsimulation, we illustrate the importance of considering spillovers by comparing\nour method with targeting without network information.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.04055v4"
    },
    {
        "title": "Welfare Analysis via Marginal Treatment Effects",
        "authors": [
            "Yuya Sasaki",
            "Takuya Ura"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Consider a causal structure with endogeneity (i.e., unobserved\nconfoundedness) in empirical data, where an instrumental variable is available.\nIn this setting, we show that the mean social welfare function can be\nidentified and represented via the marginal treatment effect (MTE, Bjorklund\nand Moffitt, 1987) as the operator kernel. This representation result can be\napplied to a variety of statistical decision rules for treatment choice,\nincluding plug-in rules, Bayes rules, and empirical welfare maximization (EWM)\nrules as in Hirano and Porter (2020, Section 2.3). Focusing on the application\nto the EWM framework of Kitagawa and Tetenov (2018), we provide convergence\nrates of the worst case average welfare loss (regret) in the spirit of Manski\n(2004).\n",
        "pdf_link": "http://arxiv.org/pdf/2012.07624v1"
    },
    {
        "title": "Identification of inferential parameters in the covariate-normalized\n  linear conditional logit model",
        "authors": [
            "Philip Erickson"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The conditional logit model is a standard workhorse approach to estimating\ncustomers' product feature preferences using choice data. Using these models at\nscale, however, can result in numerical imprecision and optimization failure\ndue to a combination of large-valued covariates and the softmax probability\nfunction. Standard machine learning approaches alleviate these concerns by\napplying a normalization scheme to the matrix of covariates, scaling all values\nto sit within some interval (such as the unit simplex). While this type of\nnormalization is innocuous when using models for prediction, it has the side\neffect of perturbing the estimated coefficients, which are necessary for\nresearchers interested in inference. This paper shows that, for two common\nclasses of normalizers, designated scaling and centered scaling, the\ndata-generating non-scaled model parameters can be analytically recovered along\nwith their asymptotic distributions. The paper also shows the numerical\nperformance of the analytical results using an example of a scaling normalizer.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08022v1"
    },
    {
        "title": "Real-time Inflation Forecasting Using Non-linear Dimension Reduction\n  Techniques",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber",
            "Karin Klieber"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper, we assess whether using non-linear dimension reduction\ntechniques pays off for forecasting inflation in real-time. Several recent\nmethods from the machine learning literature are adopted to map a large\ndimensional dataset into a lower dimensional set of latent factors. We model\nthe relationship between inflation and the latent factors using constant and\ntime-varying parameter (TVP) regressions with shrinkage priors. Our models are\nthen used to forecast monthly US inflation in real-time. The results suggest\nthat sophisticated dimension reduction methods yield inflation forecasts that\nare highly competitive to linear approaches based on principal components.\nAmong the techniques considered, the Autoencoder and squared principal\ncomponents yield factors that have high predictive power for one-month- and\none-quarter-ahead inflation. Zooming into model performance over time reveals\nthat controlling for non-linear relations in the data is of particular\nimportance during recessionary episodes of the business cycle or the current\nCOVID-19 pandemic.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08155v3"
    },
    {
        "title": "United States FDA drug approvals are persistent and polycyclic: Insights\n  into economic cycles, innovation dynamics, and national policy",
        "authors": [
            "Iraj Daizadeh"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  It is challenging to elucidate the effects of changes in external influences\n(such as economic or policy) on the rate of US drug approvals. Here, a novel\napproach, termed the Chronological Hurst Exponent (CHE), is proposed, which\nhypothesizes that changes in the long-range memory latent within the dynamics\nof time series data may be temporally associated with changes in such\ninfluences. Using the monthly number the FDA Center for Drug Evaluation and\nResearch (CDER) approvals from 1939 to 2019 as the data source, it is\ndemonstrated that the CHE has a distinct S-shaped structure demarcated by an\n8-year (1939-1947) Stagnation Period, a 27-year (1947-1974) Emergent\n(time-varying Period, and a 45-year (1974-2019) Saturation Period. Further,\ndominant periodicities (resolved via wavelet analyses) are identified during\nthe most recent 45-year CHE Saturation Period at 17, 8 and 4 years; thus, US\ndrug approvals have been following a Juglar-Kuznet mid-term cycle with\nKitchin-like bursts. As discussed, this work suggests that (1) changes in\nextrinsic factors (e.g., of economic and/or policy origin ) during the Emergent\nPeriod may have led to persistent growth in US drug approvals enjoyed since\n1974, (2) the CHE may be a valued method to explore influences on time series\ndata, and (3) innovation-related economic cycles exist (as viewed via the proxy\nmetric of US drug approvals).\n",
        "pdf_link": "http://arxiv.org/pdf/2012.09627v3"
    },
    {
        "title": "Two-way Fixed Effects and Differences-in-Differences Estimators with\n  Several Treatments",
        "authors": [
            "Clément de Chaisemartin",
            "Xavier D'Haultfœuille"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study two-way-fixed-effects regressions (TWFE) with several treatment\nvariables. Under a parallel trends assumption, we show that the coefficient on\neach treatment identifies a weighted sum of that treatment's effect, with\npossibly negative weights, plus a weighted sum of the effects of the other\ntreatments. Thus, those estimators are not robust to heterogeneous effects and\nmay be contaminated by other treatments' effects. We further show that omitting\na treatment from the regression can actually reduce the estimator's bias,\nunlike what would happen under constant treatment effects. We propose an\nalternative difference-in-differences estimator, robust to heterogeneous\neffects and immune to the contamination problem. In the application we\nconsider, the TWFE regression identifies a highly non-convex combination of\neffects, with large contamination weights, and one of its coefficients\nsignificantly differs from our heterogeneity-robust estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.10077v8"
    },
    {
        "title": "Trademark filings and patent application count time series are\n  structurally near-identical and cointegrated: Implications for studies in\n  innovation",
        "authors": [
            "Iraj Daizadeh"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Through time series analysis, this paper empirically explores, confirms and\nextends the trademark/patent inter-relationship as proposed in the normative\nintellectual-property (IP)-oriented Innovation Agenda view of the science and\ntechnology (S&T) firm. Beyond simple correlation, it is shown that\ntrademark-filing (Trademarks) and patent-application counts (Patents) have\nsimilar (if not, identical) structural attributes (including similar\ndistribution characteristics and seasonal variation, cross-wavelet\nsynchronicity/coherency (short-term cross-periodicity) and structural breaks)\nand are cointegrated (integration order of 1) over a period of approximately 40\nyears (given the monthly observations). The existence of cointegration strongly\nsuggests a \"long-run\" equilibrium between the two indices; that is, there is\n(are) exogenous force(s) restraining the two indices from diverging from one\nanother. Structural breakpoints in the chrono-dynamics of the indices supports\nthe existence of potentially similar exogeneous forces(s), as the break dates\nare simultaneous/near-simultaneous (Trademarks: 1987, 1993, 1999, 2005, 2011;\nPatents: 1988, 1994, 2000, and 2011). A discussion of potential triggers\n(affecting both time series) causing these breaks, and the concept of\nequilibrium in the context of these proxy measures are presented. The\ncointegration order and structural co-movements resemble other macro-economic\nvariables, stoking the opportunity of using econometrics approaches to further\nanalyze these data. As a corollary, this work further supports the inclusion of\ntrademark analysis in innovation studies. Lastly, the data and corresponding\nanalysis tools (R program) are presented as Supplementary Materials for\nreproducibility and convenience to conduct future work for interested readers.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.10400v1"
    },
    {
        "title": "Policy Transforms and Learning Optimal Policies",
        "authors": [
            "Thomas M. Russell"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study the problem of choosing optimal policy rules in uncertain\nenvironments using models that may be incomplete and/or partially identified.\nWe consider a policymaker who wishes to choose a policy to maximize a\nparticular counterfactual quantity called a policy transform. We characterize\nlearnability of a set of policy options by the existence of a decision rule\nthat closely approximates the maximin optimal value of the policy transform\nwith high probability. Sufficient conditions are provided for the existence of\nsuch a rule. However, learnability of an optimal policy is an ex-ante notion\n(i.e. before observing a sample), and so ex-post (i.e. after observing a\nsample) theoretical guarantees for certain policy rules are also provided. Our\nentire approach is applicable when the distribution of unobservables is not\nparametrically specified, although we discuss how semiparametric restrictions\ncan be used. Finally, we show possible applications of the procedure to a\nsimultaneous discrete choice example and a program evaluation example.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.11046v1"
    },
    {
        "title": "Binary Classification Tests, Imperfect Standards, and Ambiguous\n  Information",
        "authors": [
            "Gabriel Ziegler"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  New binary classification tests are often evaluated relative to a\npre-established test. For example, rapid Antigen tests for the detection of\nSARS-CoV-2 are assessed relative to more established PCR tests. In this paper,\nI argue that the new test can be described as producing ambiguous information\nwhen the pre-established is imperfect. This allows for a phenomenon called\ndilation -- an extreme form of non-informativeness. As an example, I present\nhypothetical test data satisfying the WHO's minimum quality requirement for\nrapid Antigen tests which leads to dilation. The ambiguity in the information\narises from a missing data problem due to imperfection of the established test:\nthe joint distribution of true infection and test results is not observed.\nUsing results from Copula theory, I construct the (usually non-singleton) set\nof all these possible joint distributions, which allows me to assess the new\ntest's informativeness. This analysis leads to a simple sufficient condition to\nmake sure that a new test is not a dilation. I illustrate my approach with\napplications to data from three COVID-19 related tests. Two rapid Antigen tests\nsatisfy my sufficient condition easily and are therefore informative. However,\nless accurate procedures, like chest CT scans, may exhibit dilation.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.11215v3"
    },
    {
        "title": "A Nearly Similar Powerful Test for Mediation",
        "authors": [
            "Kees Jan van Garderen",
            "Noud van Giersbergen"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper derives a new powerful test for mediation that is easy to use.\nTesting for mediation is empirically very important in psychology, sociology,\nmedicine, economics and business, generating over 100,000 citations to a single\nkey paper. The no-mediation hypothesis $H_{0}:\\theta_{1}\\theta _{2}=0$ also\nposes a theoretically interesting statistical problem since it defines a\nmanifold that is non-regular in the origin where rejection probabilities of\nstandard tests are extremely low. We prove that a similar test for mediation\nonly exists if the size is the reciprocal of an integer. It is unique, but has\nobjectionable properties. We propose a new test that is nearly similar with\npower close to the envelope without these abject properties and is easy to use\nin practice. Construction uses the general varying $g$-method that we propose.\nWe illustrate the results in an educational setting with gender role beliefs\nand in a trade union sentiment application.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.11342v2"
    },
    {
        "title": "Discordant Relaxations of Misspecified Models",
        "authors": [
            "Lixiong Li",
            "Désiré Kédagni",
            "Ismaël Mourifié"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In many set-identified models, it is difficult to obtain a tractable\ncharacterization of the identified set. Therefore, researchers often rely on\nnon-sharp identification conditions, and empirical results are often based on\nan outer set of the identified set. This practice is often viewed as\nconservative yet valid because an outer set is always a superset of the\nidentified set. However, this paper shows that when the model is refuted by the\ndata, two sets of non-sharp identification conditions derived from the same\nmodel could lead to disjoint outer sets and conflicting empirical results. We\nprovide a sufficient condition for the existence of such discordancy, which\ncovers models characterized by conditional moment inequalities and the Artstein\n(1983) inequalities. We also derive sufficient conditions for the non-existence\nof discordant submodels, therefore providing a class of models for which\nconstructing outer sets cannot lead to misleading interpretations. In the case\nof discordancy, we follow Masten and Poirier (2021) by developing a method to\nsalvage misspecified models, but unlike them, we focus on discrete relaxations.\nWe consider all minimum relaxations of a refuted model that restores\ndata-consistency. We find that the union of the identified sets of these\nminimum relaxations is robust to detectable misspecifications and has an\nintuitive empirical interpretation.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.11679v5"
    },
    {
        "title": "Quantile regression with generated dependent variable and covariates",
        "authors": [
            "Jayeeta Bhattacharya"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study linear quantile regression models when regressors and/or dependent\nvariable are not directly observed but estimated in an initial first step and\nused in the second step quantile regression for estimating the quantile\nparameters. This general class of generated quantile regression (GQR) covers\nvarious statistical applications, for instance, estimation of endogenous\nquantile regression models and triangular structural equation models, and some\nnew relevant applications are discussed. We study the asymptotic distribution\nof the two-step estimator, which is challenging because of the presence of\ngenerated covariates and/or dependent variable in the non-smooth quantile\nregression estimator. We employ techniques from empirical process theory to\nfind uniform Bahadur expansion for the two step estimator, which is used to\nestablish the asymptotic results. We illustrate the performance of the GQR\nestimator through simulations and an empirical application based on auctions.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.13614v1"
    },
    {
        "title": "Analysis of Randomized Experiments with Network Interference and\n  Noncompliance",
        "authors": [
            "Bora Kim"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Randomized experiments have become a standard tool in economics. In analyzing\nrandomized experiments, the traditional approach has been based on the Stable\nUnit Treatment Value (SUTVA: \\cite{rubin}) assumption which dictates that there\nis no interference between individuals. However, the SUTVA assumption fails to\nhold in many applications due to social interaction, general equilibrium,\nand/or externality effects. While much progress has been made in relaxing the\nSUTVA assumption, most of this literature has only considered a setting with\nperfect compliance to treatment assignment. In practice, however, noncompliance\noccurs frequently where the actual treatment receipt is different from the\nassignment to the treatment. In this paper, we study causal effects in\nrandomized experiments with network interference and noncompliance. Spillovers\nare allowed to occur at both treatment choice stage and outcome realization\nstage. In particular, we explicitly model treatment choices of agents as a\nbinary game of incomplete information where resulting equilibrium treatment\nchoice probabilities affect outcomes of interest. Outcomes are further\ncharacterized by a random coefficient model to allow for general unobserved\nheterogeneity in the causal effects. After defining our causal parameters of\ninterest, we propose a simple control function estimator and derive its\nasymptotic properties under large-network asymptotics. We apply our methods to\nthe randomized subsidy program of \\cite{dupas} where we find evidence of\nspillover effects on both short-run and long-run adoption of\ninsecticide-treated bed nets. Finally, we illustrate the usefulness of our\nmethods by analyzing the impact of counterfactual subsidy policies.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.13710v1"
    },
    {
        "title": "Time-Transformed Test for the Explosive Bubbles under Non-stationary\n  Volatility",
        "authors": [
            "Eiji Kurozumi",
            "Anton Skrobotov",
            "Alexey Tsarev"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper is devoted to testing for the explosive bubble under time-varying\nnon-stationary volatility. Because the limiting distribution of the seminal\nPhillips et al. (2011) test depends on the variance function and usually\nrequires a bootstrap implementation under heteroskedasticity, we construct the\ntest based on a deformation of the time domain. The proposed test is\nasymptotically pivotal under the null hypothesis and its limiting distribution\ncoincides with that of the standard test under homoskedasticity, so that the\ntest does not require computationally extensive methods for inference.\nAppealing finite sample properties are demonstrated through Monte-Carlo\nsimulations. An empirical application demonstrates that the upsurge behavior of\ncryptocurrency time series in the middle of the sample is partially explained\nby the volatility change.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.13937v2"
    },
    {
        "title": "The impact of Climate on Economic and Financial Cycles: A\n  Markov-switching Panel Approach",
        "authors": [
            "Monica Billio",
            "Roberto Casarin",
            "Enrica De Cian",
            "Malcolm Mistry",
            "Anthony Osuntuyi"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper examines the impact of climate shocks on 13 European economies\nanalysing jointly business and financial cycles, in different phases and\ndisentangling the effects for different sector channels. A Bayesian Panel\nMarkov-switching framework is proposed to jointly estimate the impact of\nextreme weather events on the economies as well as the interaction between\nbusiness and financial cycles. Results from the empirical analysis suggest that\nextreme weather events impact asymmetrically across the different phases of the\neconomy and heterogeneously across the EU countries. Moreover, we highlight how\nthe manufacturing output, a component of the industrial production index,\nconstitutes the main channel through which climate shocks impact the EU\neconomies.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.14693v1"
    },
    {
        "title": "A Pairwise Strategic Network Formation Model with Group Heterogeneity:\n  With an Application to International Travel",
        "authors": [
            "Tadao Hoshino"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this study, we consider a pairwise network formation model in which each\ndyad of agents strategically determines the link status between them. Our model\nallows the agents to have unobserved group heterogeneity in the propensity of\nlink formation. For the model estimation, we propose a three-step maximum\nlikelihood (ML) method. First, we obtain consistent estimates for the\nheterogeneity parameters at individual level using the ML estimator. Second, we\nestimate the latent group structure using the binary segmentation algorithm\nbased on the results obtained from the first step. Finally, based on the\nestimated group membership, we re-execute the ML estimation. Under certain\nregularity conditions, we show that the proposed estimator is asymptotically\nunbiased and distributed as normal at the parametric rate. As an empirical\nillustration, we focus on the network data of international visa-free travels.\nThe results indicate the presence of significant strategic complementarity and\na certain level of degree heterogeneity in the network formation behavior.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.14886v2"
    },
    {
        "title": "A first-stage representation for instrumental variables quantile\n  regression",
        "authors": [
            "Javier Alejo",
            "Antonio F. Galvao",
            "Gabriel Montes-Rojas"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper develops a first-stage linear regression representation for the\ninstrumental variables (IV) quantile regression (QR) model. The quantile\nfirst-stage is analogous to the least squares case, i.e., a linear projection\nof the endogenous variables on the instruments and other exogenous covariates,\nwith the difference that the QR case is a weighted projection. The weights are\ngiven by the conditional density function of the innovation term in the QR\nstructural model, conditional on the endogeneous and exogenous covariates, and\nthe instruments as well, at a given quantile. We also show that the required\nJacobian identification conditions for IVQR models are embedded in the quantile\nfirst-stage. We then suggest inference procedures to evaluate the adequacy of\ninstruments by evaluating their statistical significance using the first-stage\nresult. The test is developed in an over-identification context, since\nconsistent estimation of the weights for implementation of the first-stage\nrequires at least one valid instrument to be available. Monte Carlo experiments\nprovide numerical evidence that the proposed tests work as expected in terms of\nempirical size and power in finite samples. An empirical application\nillustrates that checking for the statistical significance of the instruments\nat different quantiles is important. The proposed procedures may be specially\nuseful in QR since the instruments may be relevant at some quantiles but not at\nothers.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.01212v4"
    },
    {
        "title": "Teams: Heterogeneity, Sorting, and Complementarity",
        "authors": [
            "Stephane Bonhomme"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  How much do individuals contribute to team output? I propose an econometric\nframework to quantify individual contributions when only the output of their\nteams is observed. The identification strategy relies on following individuals\nwho work in different teams over time. I consider two production technologies.\nFor a production function that is additive in worker inputs, I propose a\nregression estimator and show how to obtain unbiased estimates of variance\ncomponents that measure the contributions of heterogeneity and sorting. To\nestimate nonlinear models with complementarity, I propose a mixture approach\nunder the assumption that individual types are discrete, and rely on a\nmean-field variational approximation for estimation. To illustrate the methods,\nI estimate the impact of economists on their research output, and the\ncontributions of inventors to the quality of their patents.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.01802v1"
    },
    {
        "title": "Discretizing Unobserved Heterogeneity",
        "authors": [
            "Stéphane Bonhomme Thibaut Lamadon Elena Manresa"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We study discrete panel data methods where unobserved heterogeneity is\nrevealed in a first step, in environments where population heterogeneity is not\ndiscrete. We focus on two-step grouped fixed-effects (GFE) estimators, where\nindividuals are first classified into groups using kmeans clustering, and the\nmodel is then estimated allowing for group-specific heterogeneity. Our\nframework relies on two key properties: heterogeneity is a function - possibly\nnonlinear and time-varying - of a low-dimensional continuous latent type, and\ninformative moments are available for classification. We illustrate the method\nin a model of wages and labor market participation, and in a probit model with\ntime-varying heterogeneity. We derive asymptotic expansions of two-step GFE\nestimators as the number of groups grows with the two dimensions of the panel.\nWe propose a data-driven rule for the number of groups, and discuss bias\nreduction and inference.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.02124v1"
    },
    {
        "title": "The Econometrics and Some Properties of Separable Matching Models",
        "authors": [
            "Alfred Galichon",
            "Bernard Salanié"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We present a class of one-to-one matching models with perfectly transferable\nutility. We discuss identification and inference in these separable models, and\nwe show how their comparative statics are readily analyzed.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.02564v1"
    },
    {
        "title": "Hypothetical bias in stated choice experiments: Part I. Integrative\n  synthesis of empirical evidence and conceptualisation of external validity",
        "authors": [
            "Milad Haghani",
            "Michiel C. J. Bliemer",
            "John M. Rose",
            "Harmen Oppewal",
            "Emily Lancsar"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The notion of hypothetical bias (HB) constitutes, arguably, the most\nfundamental issue in relation to the use of hypothetical survey methods.\nWhether or to what extent choices of survey participants and subsequent\ninferred estimates translate to real-world settings continues to be debated.\nWhile HB has been extensively studied in the broader context of contingent\nvaluation, it is much less understood in relation to choice experiments (CE).\nThis paper reviews the empirical evidence for HB in CE in various fields of\napplied economics and presents an integrative framework for how HB relates to\nexternal validity. Results suggest mixed evidence on the prevalence, extent and\ndirection of HB as well as considerable context and measurement dependency.\nWhile HB is found to be an undeniable issue when conducting CEs, the empirical\nevidence on HB does not render CEs unable to represent real-world preferences.\nWhile health-related choice experiments often find negligible degrees of HB,\nexperiments in consumer behaviour and transport domains suggest that\nsignificant degrees of HB are ubiquitous. Assessments of bias in environmental\nvaluation studies provide mixed evidence. Also, across these disciplines many\nstudies display HB in their total willingness to pay estimates and opt-in rates\nbut not in their hypothetical marginal rates of substitution (subject to scale\ncorrection). Further, recent findings in psychology and brain imaging studies\nsuggest neurocognitive mechanisms underlying HB that may explain some of the\ndiscrepancies and unexpected findings in the mainstream CE literature. The\nreview also observes how the variety of operational definitions of HB prohibits\nconsistent measurement of HB in CE. The paper further identifies major sources\nof HB and possible moderating factors. Finally, it explains how HB represents\none component of the wider concept of external validity.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.02940v1"
    },
    {
        "title": "Hypothetical bias in stated choice experiments: Part II. Macro-scale\n  analysis of literature and effectiveness of bias mitigation methods",
        "authors": [
            "Milad Haghani",
            "Michiel C. J. Bliemer",
            "John M. Rose",
            "Harmen Oppewal",
            "Emily Lancsar"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper reviews methods of hypothetical bias (HB) mitigation in choice\nexperiments (CEs). It presents a bibliometric analysis and summary of empirical\nevidence of their effectiveness. The paper follows the review of empirical\nevidence on the existence of HB presented in Part I of this study. While the\nnumber of CE studies has rapidly increased since 2010, the critical issue of HB\nhas been studied in only a small fraction of CE studies. The present review\nincludes both ex-ante and ex-post bias mitigation methods. Ex-ante bias\nmitigation methods include cheap talk, real talk, consequentiality scripts,\nsolemn oath scripts, opt-out reminders, budget reminders, honesty priming,\ninduced truth telling, indirect questioning, time to think and pivot designs.\nEx-post methods include follow-up certainty calibration scales, respondent\nperceived consequentiality scales, and revealed-preference-assisted estimation.\nIt is observed that the use of mitigation methods markedly varies across\ndifferent sectors of applied economics. The existing empirical evidence points\nto their overall effectives in reducing HB, although there is some variation.\nThe paper further discusses how each mitigation method can counter a certain\nsubset of HB sources. Considering the prevalence of HB in CEs and the\neffectiveness of bias mitigation methods, it is recommended that implementation\nof at least one bias mitigation method (or a suitable combination where\npossible) becomes standard practice in conducting CEs. Mitigation method(s)\nsuited to the particular application should be implemented to ensure that\ninferences and subsequent policy decisions are as much as possible free of HB.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.02945v1"
    },
    {
        "title": "Identification of Matching Complementarities: A Geometric Viewpoint",
        "authors": [
            "Alfred Galichon"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We provide a geometric formulation of the problem of identification of the\nmatching surplus function and we show how the estimation problem can be solved\nby the introduction of a generalized entropy function over the set of\nmatchings.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.03875v1"
    },
    {
        "title": "Inference under Covariate-Adaptive Randomization with Imperfect\n  Compliance",
        "authors": [
            "Federico A. Bugni",
            "Mengsi Gao"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper studies inference in a randomized controlled trial (RCT) with\ncovariate-adaptive randomization (CAR) and imperfect compliance of a binary\ntreatment. In this context, we study inference on the LATE. As in Bugni et al.\n(2018,2019), CAR refers to randomization schemes that first stratify according\nto baseline covariates and then assign treatment status so as to achieve\n\"balance\" within each stratum. In contrast to these papers, however, we allow\nparticipants of the RCT to endogenously decide to comply or not with the\nassigned treatment status.\n  We study the properties of an estimator of the LATE derived from a \"fully\nsaturated\" IV linear regression, i.e., a linear regression of the outcome on\nall indicators for all strata and their interaction with the treatment\ndecision, with the latter instrumented with the treatment assignment. We show\nthat the proposed LATE estimator is asymptotically normal, and we characterize\nits asymptotic variance in terms of primitives of the problem. We provide\nconsistent estimators of the standard errors and asymptotically exact\nhypothesis tests. In the special case when the target proportion of units\nassigned to each treatment does not vary across strata, we can also consider\ntwo other estimators of the LATE, including the one based on the \"strata fixed\neffects\" IV linear regression, i.e., a linear regression of the outcome on\nindicators for all strata and the treatment decision, with the latter\ninstrumented with the treatment assignment.\n  Our characterization of the asymptotic variance of the LATE estimators allows\nus to understand the influence of the parameters of the RCT. We use this to\npropose strategies to minimize their asymptotic variance in a hypothetical RCT\nbased on data from a pilot study. We illustrate the practical relevance of\nthese results using a simulation study and an empirical application based on\nDupas et al. (2018).\n",
        "pdf_link": "http://arxiv.org/pdf/2102.03937v3"
    },
    {
        "title": "A note on global identification in structural vector autoregressions",
        "authors": [
            "Emanuele Bacchiocchi",
            "Toru Kitagawa"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In a landmark contribution to the structural vector autoregression (SVARs)\nliterature, Rubio-Ramirez, Waggoner, and Zha (2010, `Structural Vector\nAutoregressions: Theory of Identification and Algorithms for Inference,' Review\nof Economic Studies) shows a necessary and sufficient condition for equality\nrestrictions to globally identify the structural parameters of a SVAR. The\nsimplest form of the necessary and sufficient condition shown in Theorem 7 of\nRubio-Ramirez et al (2010) checks the number of zero restrictions and the ranks\nof particular matrices without requiring knowledge of the true value of the\nstructural or reduced-form parameters. However, this note shows by\ncounterexample that this condition is not sufficient for global identification.\nAnalytical investigation of the counterexample clarifies why their sufficiency\nclaim breaks down. The problem with the rank condition is that it allows for\nthe possibility that restrictions are redundant, in the sense that one or more\nrestrictions may be implied by other restrictions, in which case the implied\nrestriction contains no identifying information. We derive a modified necessary\nand sufficient condition for SVAR global identification and clarify how it can\nbe assessed in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.04048v2"
    },
    {
        "title": "A test of non-identifying restrictions and confidence regions for\n  partially identified parameters",
        "authors": [
            "Alfred Galichon",
            "Marc Henry"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose an easily implementable test of the validity of a set of\ntheoretical restrictions on the relationship between economic variables, which\ndo not necessarily identify the data generating process. The restrictions can\nbe derived from any model of interactions, allowing censoring and multiple\nequilibria. When the restrictions are parameterized, the test can be inverted\nto yield confidence regions for partially identified parameters, thereby\ncomplementing other proposals, primarily Chernozhukov et al. [Chernozhukov, V.,\nHong, H., Tamer, E., 2007. Estimation and confidence regions for parameter sets\nin econometric models. Econometrica 75, 1243-1285].\n",
        "pdf_link": "http://arxiv.org/pdf/2102.04151v1"
    },
    {
        "title": "Optimal transportation and the falsifiability of incompletely specified\n  economic models",
        "authors": [
            "Ivar Ekeland",
            "Alfred Galichon",
            "Marc Henry"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  A general framework is given to analyze the falsifiability of economic models\nbased on a sample of their observable components. It is shown that, when the\nrestrictions implied by the economic theory are insufficient to identify the\nunknown quantities of the structure, the duality of optimal transportation with\nzero-one cost function delivers interpretable and operational formulations of\nthe hypothesis of specification correctness from which tests can be constructed\nto falsify the model.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.04162v2"
    },
    {
        "title": "Assessing Sensitivity of Machine Learning Predictions.A Novel Toolbox\n  with an Application to Financial Literacy",
        "authors": [
            "Falco J. Bargagli Stoffi",
            "Kenneth De Beckker",
            "Joana E. Maldonado",
            "Kristof De Witte"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Despite their popularity, machine learning predictions are sensitive to\npotential unobserved predictors. This paper proposes a general algorithm that\nassesses how the omission of an unobserved variable with high explanatory power\ncould affect the predictions of the model. Moreover, the algorithm extends the\nusage of machine learning from pointwise predictions to inference and\nsensitivity analysis. In the application, we show how the framework can be\napplied to data with inherent uncertainty, such as students' scores in a\nstandardized assessment on financial literacy. First, using Bayesian Additive\nRegression Trees (BART), we predict students' financial literacy scores (FLS)\nfor a subgroup of students with missing FLS. Then, we assess the sensitivity of\npredictions by comparing the predictions and performance of models with and\nwithout a highly explanatory synthetic predictor. We find no significant\ndifference in the predictions and performances of the augmented (i.e., the\nmodel with the synthetic predictor) and original model. This evidence sheds a\nlight on the stability of the predictive model used in the application. The\nproposed methodology can be used, above and beyond our motivating empirical\nexample, in a wide range of machine learning applications in social and health\nsciences.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.04382v1"
    },
    {
        "title": "Dilation bootstrap",
        "authors": [
            "Alfred Galichon",
            "Marc Henry"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose a methodology for constructing confidence regions with partially\nidentified models of general form. The region is obtained by inverting a test\nof internal consistency of the econometric structure. We develop a dilation\nbootstrap methodology to deal with sampling uncertainty without reference to\nthe hypothesized economic structure. It requires bootstrapping the quantile\nprocess for univariate data and a novel generalization of the latter to higher\ndimensions. Once the dilation is chosen to control the confidence level, the\nunknown true distribution of the observed data can be replaced by the known\nempirical distribution and confidence regions can then be obtained as in\nGalichon and Henry (2011) and Beresteanu, Molchanov and Molinari (2011).\n",
        "pdf_link": "http://arxiv.org/pdf/2102.04457v1"
    },
    {
        "title": "Extreme dependence for multivariate data",
        "authors": [
            "Damien Bosc",
            "Alfred Galichon"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This article proposes a generalized notion of extreme multivariate dependence\nbetween two random vectors which relies on the extremality of the\ncross-covariance matrix between these two vectors. Using a partial ordering on\nthe cross-covariance matrices, we also generalize the notion of positive upper\ndependence. We then proposes a means to quantify the strength of the dependence\nbetween two given multivariate series and to increase this strength while\npreserving the marginal distributions. This allows for the design of\nstress-tests of the dependence between two sets of financial variables, that\ncan be useful in portfolio management or derivatives pricing.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.04461v1"
    },
    {
        "title": "Interactive Network Visualization of Opioid Crisis Related Data- Policy,\n  Pharmaceutical, Training, and More",
        "authors": [
            "Olga Scrivner",
            "Elizabeth McAvoy",
            "Thuy Nguyen",
            "Tenzin Choeden",
            "Kosali Simon",
            "Katy Börner"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Responding to the U.S. opioid crisis requires a holistic approach supported\nby evidence from linking and analyzing multiple data sources. This paper\ndiscusses how 20 available resources can be combined to answer pressing public\nhealth questions related to the crisis. It presents a network view based on\nU.S. geographical units and other standard concepts, crosswalked to communicate\nthe coverage and interlinkage of these resources. These opioid-related datasets\ncan be grouped by four themes: (1) drug prescriptions, (2) opioid related\nharms, (3) opioid treatment workforce, jobs, and training, and (4) drug policy.\nAn interactive network visualization was created and is freely available\nonline; it lets users explore key metadata, relevant scholarly works, and data\ninterlinkages in support of informed decision making through data analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.05596v1"
    },
    {
        "title": "Duality in dynamic discrete-choice models",
        "authors": [
            "Khai Xiang Chiong",
            "Alfred Galichon",
            "Matt Shum"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Using results from convex analysis, we investigate a novel approach to\nidentification and estimation of discrete choice models which we call the Mass\nTransport Approach (MTA). We show that the conditional choice probabilities and\nthe choice-specific payoffs in these models are related in the sense of\nconjugate duality, and that the identification problem is a mass transport\nproblem. Based on this, we propose a new two-step estimator for these models;\ninterestingly, the first step of our estimator involves solving a linear\nprogram which is identical to the classic assignment (two-sided matching) game\nof Shapley and Shubik (1971). The application of convex-analytic tools to\ndynamic discrete choice models, and the connection with two-sided matching\nmodels, is new in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.06076v2"
    },
    {
        "title": "Inference on two component mixtures under tail restrictions",
        "authors": [
            "Marc Henry",
            "Koen Jochmans",
            "Bernard Salanié"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Many econometric models can be analyzed as finite mixtures. We focus on\ntwo-component mixtures and we show that they are nonparametrically point\nidentified by a combination of an exclusion restriction and tail restrictions.\nOur identification analysis suggests simple closed-form estimators of the\ncomponent distributions and mixing proportions, as well as a specification\ntest. We derive their asymptotic properties using results on tail empirical\nprocesses and we present a simulation study that documents their finite-sample\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.06232v1"
    },
    {
        "title": "Identification and Inference Under Narrative Restrictions",
        "authors": [
            "Raffaella Giacomini",
            "Toru Kitagawa",
            "Matthew Read"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We consider structural vector autoregressions subject to 'narrative\nrestrictions', which are inequality restrictions on functions of the structural\nshocks in specific periods. These restrictions raise novel problems related to\nidentification and inference, and there is currently no frequentist procedure\nfor conducting inference in these models. We propose a solution that is valid\nfrom both Bayesian and frequentist perspectives by: 1) formalizing the\nidentification problem under narrative restrictions; 2) correcting a feature of\nthe existing (single-prior) Bayesian approach that can distort inference; 3)\nproposing a robust (multiple-prior) Bayesian approach that is useful for\nassessing and eliminating the posterior sensitivity that arises in these models\ndue to the likelihood having flat regions; and 4) showing that the robust\nBayesian approach has asymptotic frequentist validity. We illustrate our\nmethods by estimating the effects of US monetary policy under a variety of\nnarrative restrictions.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.06456v1"
    },
    {
        "title": "A Distance Covariance-based Estimator",
        "authors": [
            "Emmanuel Selorm Tsyawo",
            "Abdul-Nasah Soale"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper introduces an estimator that considerably weakens the conventional\nrelevance condition of instrumental variable (IV) methods, allowing for\ninstruments that are weakly correlated, uncorrelated, or even mean-independent\nbut not independent of endogenous covariates. Under the relevance condition,\nthe estimator achieves consistent estimation and reliable inference without\nrequiring instrument excludability, and it remains robust even when the first\nmoment of the disturbance term does not exist. In contrast to conventional IV\nmethods, it maximises the set of feasible instruments in any empirical setting.\nUnder a weak conditional median independence condition on pairwise differences\nin disturbances and mild regularity assumptions, identification holds, and the\nestimator is consistent and asymptotically normal.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.07008v2"
    },
    {
        "title": "Entropy methods for identifying hedonic models",
        "authors": [
            "Arnaud Dupuy",
            "Alfred Galichon",
            "Marc Henry"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper contributes to the literature on hedonic models in two ways.\nFirst, it makes use of Queyranne's reformulation of a hedonic model in the\ndiscrete case as a network flow problem in order to provide a proof of\nexistence and integrality of a hedonic equilibrium and efficient computation of\nhedonic prices. Second, elaborating on entropic methods developed in Galichon\nand Salani\\'{e} (2014), this paper proposes a new identification strategy for\nhedonic models in a single market. This methodology allows one to introduce\nheterogeneities in both consumers' and producers' attributes and to recover\nproducers' profits and consumers' utilities based on the observation of\nproduction and consumption patterns and the set of hedonic prices.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.07491v1"
    },
    {
        "title": "Constructing valid instrumental variables in generalized linear causal\n  models from directed acyclic graphs",
        "authors": [
            "Øyvind Hoveid"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Unlike other techniques of causality inference, the use of valid instrumental\nvariables can deal with unobserved sources of both variable errors, variable\nomissions, and sampling bias, and still arrive at consistent estimates of\naverage treatment effects. The only problem is to find the valid instruments.\nUsing the definition of Pearl (2009) of valid instrumental variables, a formal\ncondition for validity can be stated for variables in generalized linear causal\nmodels. The condition can be applied in two different ways: As a tool for\nconstructing valid instruments, or as a foundation for testing whether an\ninstrument is valid. When perfectly valid instruments are not found, the\nsquared bias of the IV-estimator induced by an imperfectly valid instrument --\nestimated with bootstrapping -- can be added to its empirical variance in a\nmean-square-error-like reliability measure.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.08056v1"
    },
    {
        "title": "A Unified Framework for Specification Tests of Continuous Treatment\n  Effect Models",
        "authors": [
            "Wei Huang",
            "Oliver Linton",
            "Zheng Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose a general framework for the specification testing of continuous\ntreatment effect models. We assume a general residual function, which includes\nthe average and quantile treatment effect models as special cases. The null\nmodels are identified under the unconfoundedness condition and contain a\nnonparametric weighting function. We propose a test statistic for the null\nmodel in which the weighting function is estimated by solving an expanding set\nof moment equations. We establish the asymptotic distributions of our test\nstatistic under the null hypothesis and under fixed and local alternatives. The\nproposed test statistic is shown to be more efficient than that constructed\nfrom the true weighting function and can detect local alternatives deviated\nfrom the null models at the rate of $O(N^{-1/2})$. A simulation method is\nprovided to approximate the null distribution of the test statistic.\nMonte-Carlo simulations show that our test exhibits a satisfactory\nfinite-sample performance, and an application shows its practical value.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.08063v2"
    },
    {
        "title": "On-Demand Transit User Preference Analysis using Hybrid Choice Models",
        "authors": [
            "Nael Alsaleh",
            "Bilal Farooq",
            "Yixue Zhang",
            "Steven Farber"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In light of the increasing interest to transform the fixed-route public\ntransit (FRT) services into on-demand transit (ODT) services, there exists a\nstrong need for a comprehensive evaluation of the effects of this shift on the\nusers. Such an analysis can help the municipalities and service providers to\ndesign and operate more convenient, attractive, and sustainable transit\nsolutions. To understand the user preferences, we developed three hybrid choice\nmodels: integrated choice and latent variable (ICLV), latent class (LC), and\nlatent class integrated choice and latent variable (LC-ICLV) models. We used\nthese models to analyze the public transit user's preferences in Belleville,\nOntario, Canada. Hybrid choice models were estimated using a rich dataset that\ncombined the actual level of service attributes obtained from Belleville's ODT\nservice and self-reported usage behaviour obtained from a revealed preference\nsurvey of the ODT users. The latent class models divided the users into two\ngroups with different travel behaviour and preferences. The results showed that\nthe captive user's preference for ODT service was significantly affected by the\nnumber of unassigned trips, in-vehicle time, and main travel mode before the\nODT service started. On the other hand, the non-captive user's service\npreference was significantly affected by the Time Sensitivity and the Online\nService Satisfaction latent variables, as well as the performance of the ODT\nservice and trip purpose. This study attaches importance to improving the\nreliability and performance of the ODT service and outlines directions for\nreducing operational costs by updating the required fleet size and assigning\nmore vehicles for work-related trips.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.08256v2"
    },
    {
        "title": "Testing for Nonlinear Cointegration under Heteroskedasticity",
        "authors": [
            "Christoph Hanck",
            "Till Massing"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This article discusses Shin (1994, Econometric Theory)-type tests for\nnonlinear cointegration in the presence of variance breaks. We build on\ncointegration test approaches under heteroskedasticity (Cavaliere and Taylor,\n2006, Journal of Time Series Analysis) and nonlinearity, serial correlation,\nand endogeneity (Choi and Saikkonen, 2010, Econometric Theory) to propose a\nbootstrap test and prove its consistency. A Monte Carlo study shows the\napproach to have satisfactory finite-sample properties in a variety of\nscenarios. We provide an empirical application to the environmental Kuznets\ncurves (EKC), finding that the cointegration test provides little evidence for\nthe EKC hypothesis. Additionally, we examine a nonlinear relation between the\nUS money demand and the interest rate, finding that our test does not reject\nthe null of a smooth transition cointegrating relation\n",
        "pdf_link": "http://arxiv.org/pdf/2102.08809v5"
    },
    {
        "title": "On the implementation of Approximate Randomization Tests in Linear\n  Models with a Small Number of Clusters",
        "authors": [
            "Yong Cai",
            "Ivan A. Canay",
            "Deborah Kim",
            "Azeem M. Shaikh"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper provides a user's guide to the general theory of approximate\nrandomization tests developed in Canay, Romano, and Shaikh (2017) when\nspecialized to linear regressions with clustered data. An important feature of\nthe methodology is that it applies to settings in which the number of clusters\nis small -- even as small as five. We provide a step-by-step algorithmic\ndescription of how to implement the test and construct confidence intervals for\nthe parameter of interest. In doing so, we additionally present three novel\nresults concerning the methodology: we show that the method admits an\nequivalent implementation based on weighted scores; we show the test and\nconfidence intervals are invariant to whether the test statistic is studentized\nor not; and we prove convexity of the confidence intervals for scalar\nparameters. We also articulate the main requirements underlying the test,\nemphasizing in particular common pitfalls that researchers may encounter.\nFinally, we illustrate the use of the methodology with two applications that\nfurther illuminate these points. The companion {\\tt R} and {\\tt Stata} packages\nfacilitate the implementation of the methodology and the replication of the\nempirical exercises.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.09058v4"
    },
    {
        "title": "Deep Structural Estimation: With an Application to Option Pricing",
        "authors": [
            "Hui Chen",
            "Antoine Didisheim",
            "Simon Scheidegger"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose a novel structural estimation framework in which we train a\nsurrogate of an economic model with deep neural networks. Our methodology\nalleviates the curse of dimensionality and speeds up the evaluation and\nparameter estimation by orders of magnitudes, which significantly enhances\none's ability to conduct analyses that require frequent parameter\nre-estimation. As an empirical application, we compare two popular option\npricing models (the Heston and the Bates model with double-exponential jumps)\nagainst a non-parametric random forest model. We document that: a) the Bates\nmodel produces better out-of-sample pricing on average, but both structural\nmodels fail to outperform random forest for large areas of the volatility\nsurface; b) random forest is more competitive at short horizons (e.g., 1-day),\nfor short-dated options (with less than 7 days to maturity), and on days with\npoor liquidity; c) both structural models outperform random forest in\nout-of-sample delta hedging; d) the Heston model's relative performance has\ndeteriorated significantly after the 2008 financial crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.09209v1"
    },
    {
        "title": "Spatial Correlation Robust Inference",
        "authors": [
            "Ulrich K. Müller",
            "Mark W. Watson"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose a method for constructing confidence intervals that account for\nmany forms of spatial correlation. The interval has the familiar `estimator\nplus and minus a standard error times a critical value' form, but we propose\nnew methods for constructing the standard error and the critical value. The\nstandard error is constructed using population principal components from a\ngiven `worst-case' spatial covariance model. The critical value is chosen to\nensure coverage in a benchmark parametric model for the spatial correlations.\nThe method is shown to control coverage in large samples whenever the spatial\ncorrelation is weak, i.e., with average pairwise correlations that vanish as\nthe sample size gets large. We also provide results on correct coverage in a\nrestricted but nonparametric class of strong spatial correlations, as well as\non the efficiency of the method. In a design calibrated to match economic\nactivity in U.S. states the method outperforms previous suggestions for\nspatially robust inference about the population mean.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.09353v1"
    },
    {
        "title": "Monitoring the pandemic: A fractional filter for the COVID-19 contact\n  rate",
        "authors": [
            "Tobias Hartl"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper aims to provide reliable estimates for the COVID-19 contact rate\nof a Susceptible-Infected-Recovered (SIR) model. From observable data on\nconfirmed, recovered, and deceased cases, a noisy measurement for the contact\nrate can be constructed. To filter out measurement errors and seasonality, a\nnovel unobserved components (UC) model is set up. It specifies the log contact\nrate as a latent, fractionally integrated process of unknown integration order.\nThe fractional specification reflects key characteristics of aggregate social\nbehavior such as strong persistence and gradual adjustments to new information.\nA computationally simple modification of the Kalman filter is introduced and is\ntermed the fractional filter. It allows to estimate UC models with richer\nlong-run dynamics, and provides a closed-form expression for the prediction\nerror of UC models. Based on the latter, a conditional-sum-of-squares (CSS)\nestimator for the model parameters is set up that is shown to be consistent and\nasymptotically normally distributed. The resulting contact rate estimates for\nseveral countries are well in line with the chronology of the pandemic, and\nallow to identify different contact regimes generated by policy interventions.\nAs the fractional filter is shown to provide precise contact rate estimates at\nthe end of the sample, it bears great potential for monitoring the pandemic in\nreal time.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10067v1"
    },
    {
        "title": "A Novel Multi-Period and Multilateral Price Index",
        "authors": [
            "Consuelo Rubina Nava",
            "Maria Grazia Zoia"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  A novel approach to price indices, leading to an innovative solution in both\na multi-period or a multilateral framework, is presented. The index turns out\nto be the generalized least squares solution of a regression model linking\nvalues and quantities of the commodities. The index reference basket, which is\nthe union of the intersections of the baskets of all country/period taken in\npair, has a coverage broader than extant indices. The properties of the index\nare investigated and updating formulas established. Applications to both real\nand simulated data provide evidence of the better index performance in\ncomparison with extant alternatives.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10528v1"
    },
    {
        "title": "Cointegrated Solutions of Unit-Root VARs: An Extended Representation\n  Theorem",
        "authors": [
            "Mario Faliva",
            "Maria Grazia Zoia"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper establishes an extended representation theorem for unit-root VARs.\nA specific algebraic technique is devised to recover stationarity from the\nsolution of the model in the form of a cointegrating transformation. Closed\nforms of the results of interest are derived for integrated processes up to the\n4-th order. An extension to higher-order processes turns out to be within the\nreach on an induction argument.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10626v1"
    },
    {
        "title": "Non-stationary GARCH modelling for fitting higher order moments of\n  financial series within moving time windows",
        "authors": [
            "Luke De Clerk",
            "Sergey Savel'ev"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Here, we have analysed a GARCH(1,1) model with the aim to fit higher order\nmoments for different companies' stock prices. When we assume a gaussian\nconditional distribution, we fail to capture any empirical data when fitting\nthe first three even moments of financial time series. We show instead that a\ndouble gaussian conditional probability distribution better captures the higher\norder moments of the data. To demonstrate this point, we construct regions\n(phase diagrams), in the fourth and sixth order standardised moment space,\nwhere a GARCH(1,1) model can be used to fit these moments and compare them with\nthe corresponding moments from empirical data for different sectors of the\neconomy. We found that the ability of the GARCH model with a double gaussian\nconditional distribution to fit higher order moments is dictated by the time\nwindow our data spans. We can only fit data collected within specific time\nwindow lengths and only with certain parameters of the conditional double\ngaussian distribution. In order to incorporate the non-stationarity of\nfinancial series, we assume that the parameters of the GARCH model have time\ndependence.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.11627v4"
    },
    {
        "title": "Set Identification in Models with Multiple Equilibria",
        "authors": [
            "Alfred Galichon",
            "Marc Henry"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose a computationally feasible way of deriving the identified features\nof models with multiple equilibria in pure or mixed strategies. It is shown\nthat in the case of Shapley regular normal form games, the identified set is\ncharacterized by the inclusion of the true data distribution within the core of\na Choquet capacity, which is interpreted as the generalized likelihood of the\nmodel. In turn, this inclusion is characterized by a finite set of inequalities\nand efficient and easily implementable combinatorial methods are described to\ncheck them. In all normal form games, the identified set is characterized in\nterms of the value of a submodular or convex optimization program. Efficient\nalgorithms are then given and compared to check inclusion of a parameter in\nthis identified set. The latter are illustrated with family bargaining games\nand oligopoly entry games.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12249v1"
    },
    {
        "title": "Inference in Incomplete Models",
        "authors": [
            "Alfred Galichon",
            "Marc Henry"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We provide a test for the specification of a structural model without\nidentifying assumptions. We show the equivalence of several natural\nformulations of correct specification, which we take as our null hypothesis.\nFrom a natural empirical version of the latter, we derive a Kolmogorov-Smirnov\nstatistic for Choquet capacity functionals, which we use to construct our test.\nWe derive the limiting distribution of our test statistic under the null, and\nshow that our test is consistent against certain classes of alternatives. When\nthe model is given in parametric form, the test can be inverted to yield\nconfidence regions for the identified parameter set. The approach can be\napplied to the estimation of models with sample selection, censored observables\nand to games with multiple equilibria.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12257v1"
    },
    {
        "title": "Quasi-maximum likelihood estimation of break point in high-dimensional\n  factor models",
        "authors": [
            "Jiangtao Duan",
            "Jushan Bai",
            "Xu Han"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper estimates the break point for large-dimensional factor models with\na single structural break in factor loadings at a common unknown date. First,\nwe propose a quasi-maximum likelihood (QML) estimator of the change point based\non the second moments of factors, which are estimated by principal component\nanalysis. We show that the QML estimator performs consistently when the\ncovariance matrix of the pre- or post-break factor loading, or both, is\nsingular. When the loading matrix undergoes a rotational type of change while\nthe number of factors remains constant over time, the QML estimator incurs a\nstochastically bounded estimation error. In this case, we establish an\nasymptotic distribution of the QML estimator. The simulation results validate\nthe feasibility of this estimator when used in finite samples. In addition, we\ndemonstrate empirical applications of the proposed method by applying it to\nestimate the break points in a U.S. macroeconomic dataset and a stock return\ndataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12666v3"
    },
    {
        "title": "A Control Function Approach to Estimate Panel Data Binary Response Model",
        "authors": [
            "Amaresh K Tiwari"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose a new control function (CF) method to estimate a binary response\nmodel in a triangular system with multiple unobserved heterogeneities The CFs\nare the expected values of the heterogeneity terms in the reduced form\nequations conditional on the histories of the endogenous and the exogenous\nvariables. The method requires weaker restrictions compared to CF methods with\nsimilar imposed structures. If the support of endogenous regressors is large,\naverage partial effects are point-identified even when instruments are\ndiscrete. Bounds are provided when the support assumption is violated. An\napplication and Monte Carlo experiments compare several alternative methods\nwith ours.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12927v2"
    },
    {
        "title": "Algorithmic subsampling under multiway clustering",
        "authors": [
            "Harold D. Chiang",
            "Jiatong Li",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper proposes a novel method of algorithmic subsampling (data\nsketching) for multiway cluster dependent data. We establish a new uniform weak\nlaw of large numbers and a new central limit theorem for the multiway\nalgorithmic subsample means. Consequently, we discover an additional advantage\nof the algorithmic subsampling that it allows for robustness against potential\ndegeneracy, and even non-Gaussian degeneracy, of the asymptotic distribution\nunder multiway clustering. Simulation studies support this novel result, and\ndemonstrate that inference with the algorithmic subsampling entails more\naccuracy than that without the algorithmic subsampling. Applying these basic\nasymptotic theories, we derive the consistency and the asymptotic normality for\nthe multiway algorithmic subsampling generalized method of moments estimator\nand for the multiway algorithmic subsampling M-estimator. We illustrate an\napplication to scanner data.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.00557v4"
    },
    {
        "title": "Structural models for policy-making: Coping with parametric uncertainty",
        "authors": [
            "Philipp Eisenhauer",
            "Janoś Gabler",
            "Lena Janys",
            "Christopher Walsh"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The ex-ante evaluation of policies using structural econometric models is\nbased on estimated parameters as a stand-in for the true parameters. This\npractice ignores uncertainty in the counterfactual policy predictions of the\nmodel. We develop a generic approach that deals with parametric uncertainty\nusing uncertainty sets and frames model-informed policy-making as a decision\nproblem under uncertainty. The seminal human capital investment model by Keane\nand Wolpin (1997) provides a well-known, influential, and empirically-grounded\ntest case. We document considerable uncertainty in the models's policy\npredictions and highlight the resulting policy recommendations obtained from\nusing different formal rules of decision-making under uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01115v4"
    },
    {
        "title": "The Kernel Trick for Nonlinear Factor Modeling",
        "authors": [
            "Varlam Kutateladze"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Factor modeling is a powerful statistical technique that permits to capture\nthe common dynamics in a large panel of data with a few latent variables, or\nfactors, thus alleviating the curse of dimensionality. Despite its popularity\nand widespread use for various applications ranging from genomics to finance,\nthis methodology has predominantly remained linear. This study estimates\nfactors nonlinearly through the kernel method, which allows flexible\nnonlinearities while still avoiding the curse of dimensionality. We focus on\nfactor-augmented forecasting of a single time series in a high-dimensional\nsetting, known as diffusion index forecasting in macroeconomics literature. Our\nmain contribution is twofold. First, we show that the proposed estimator is\nconsistent and it nests linear PCA estimator as well as some nonlinear\nestimators introduced in the literature as specific examples. Second, our\nempirical application to a classical macroeconomic dataset demonstrates that\nthis approach can offer substantial advantages over mainstream methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01266v1"
    },
    {
        "title": "Standing on the Shoulders of Machine Learning: Can We Improve Hypothesis\n  Testing?",
        "authors": [
            "Gary Cornwall",
            "Jeff Chen",
            "Beau Sauley"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In this paper we have updated the hypothesis testing framework by drawing\nupon modern computational power and classification models from machine\nlearning. We show that a simple classification algorithm such as a boosted\ndecision stump can be used to fully recover the full size-power trade-off for\nany single test statistic. This recovery implies an equivalence, under certain\nconditions, between the basic building block of modern machine learning and\nhypothesis testing. Second, we show that more complex algorithms such as the\nrandom forest and gradient boosted machine can serve as mapping functions in\nplace of the traditional null distribution. This allows for multiple test\nstatistics and other information to be evaluated simultaneously and thus form a\npseudo-composite hypothesis test. Moreover, we show how practitioners can make\nexplicit the relative costs of Type I and Type II errors to contextualize the\ntest into a specific decision framework. To illustrate this approach we revisit\nthe case of testing for unit roots, a difficult problem in time series\neconometrics for which existing tests are known to exhibit low power. Using a\nsimulation framework common to the literature we show that this approach can\nimprove upon overall accuracy of the traditional unit root test(s) by seventeen\npercentage points, and the sensitivity by thirty six percentage points.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01368v1"
    },
    {
        "title": "Some Finite Sample Properties of the Sign Test",
        "authors": [
            "Yong Cai"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper contains two finite-sample results concerning the sign test.\nFirst, we show that the sign-test is unbiased with independent, non-identically\ndistributed data for both one-sided and two-sided hypotheses. The proof for the\ntwo-sided case is based on a novel argument that relates the derivatives of the\npower function to a regular bipartite graph. Unbiasedness then follows from the\nexistence of perfect matchings on such graphs. Second, we provide a simple\ntheoretical counterexample to show that the sign test over-rejects when the\ndata exhibits correlation. Our results can be useful for understanding the\nproperties of approximate randomization tests in settings with few clusters.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01412v2"
    },
    {
        "title": "Modeling Macroeconomic Variations After COVID-19",
        "authors": [
            "Serena Ng"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The coronavirus is a global event of historical proportions and just a few\nmonths changed the time series properties of the data in ways that make many\npre-covid forecasting models inadequate. It also creates a new problem for\nestimation of economic factors and dynamic causal effects because the\nvariations around the outbreak can be interpreted as outliers, as shifts to the\ndistribution of existing shocks, or as addition of new shocks. I take the\nlatter view and use covid indicators as controls to 'de-covid' the data prior\nto estimation. I find that economic uncertainty remains high at the end of 2020\neven though real economic activity has recovered and covid uncertainty has\nreceded. Dynamic responses of variables to shocks in a VAR similar in magnitude\nand shape to the ones identified before 2020 can be recovered by directly or\nindirectly modeling covid and treating it as exogenous. These responses to\neconomic shocks are distinctly different from those to a covid shock which are\nmuch larger but shorter lived. Disentangling the two types of shocks can be\nimportant in macroeconomic modeling post-covid.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.02732v4"
    },
    {
        "title": "Factor-Based Imputation of Missing Values and Covariances in Panel Data\n  of Large Dimensions",
        "authors": [
            "Ercument Cahan",
            "Jushan Bai",
            "Serena Ng"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Economists are blessed with a wealth of data for analysis, but more often\nthan not, values in some entries of the data matrix are missing. Various\nmethods have been proposed to handle missing observations in a few variables.\nWe exploit the factor structure in panel data of large dimensions. Our\n\\textsc{tall-project} algorithm first estimates the factors from a\n\\textsc{tall} block in which data for all rows are observed, and projections of\nvariable specific length are then used to estimate the factor loadings. A\nmissing value is imputed as the estimated common component which we show is\nconsistent and asymptotically normal without further iteration. Implications\nfor using imputed data in factor augmented regressions are then discussed.\n  To compensate for the downward bias in covariance matrices created by an\nomitted noise when the data point is not observed, we overlay the imputed data\nwith re-sampled idiosyncratic residuals many times and use the average of the\ncovariances to estimate the parameters of interest. Simulations show that the\nprocedures have desirable finite sample properties.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.03045v3"
    },
    {
        "title": "Root-n-consistent Conditional ML estimation of dynamic panel logit\n  models with fixed effects",
        "authors": [
            "Hugo Kruiniger"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In this paper we first propose a root-n-consistent Conditional Maximum\nLikelihood (CML) estimator for all the common parameters in the panel logit\nAR(p) model with strictly exogenous covariates and fixed effects. Our CML\nestimator (CMLE) converges in probability faster and is more easily computed\nthan the kernel-weighted CMLE of Honor\\'e and Kyriazidou (2000). Next, we\npropose a root-n-consistent CMLE for the coefficients of the exogenous\ncovariates only. We also discuss new CMLEs for the panel logit AR(p) model\nwithout covariates. Finally, we propose CMLEs for multinomial dynamic panel\nlogit models with and without covariates. All CMLEs are asymptotically normally\ndistributed.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.04973v5"
    },
    {
        "title": "More Robust Estimators for Instrumental-Variable Panel Designs, With An\n  Application to the Effect of Imports from China on US Employment",
        "authors": [
            "Clément de Chaisemartin",
            "Ziteng Lei"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We show that first-difference two-stages-least-squares regressions identify\nnon-convex combinations of location-and-period-specific treatment effects.\nThus, those regressions could be biased if effects are heterogeneous. We\npropose an alternative instrumental-variable correlated-random-coefficient\n(IV-CRC) estimator, that is more robust to heterogeneous effects. We revisit\nAutor et al. (2013), who use a first-difference two-stages-least-squares\nregression to estimate the effect of imports from China on US manufacturing\nemployment. Their regression estimates a highly non-convex combination of\neffects. Our more robust IV-CRC estimator is small and insignificant. Though\nits confidence interval is wide, it significantly differs from the\nfirst-difference two-stages-least-squares estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.06437v10"
    },
    {
        "title": "Estimating the causal effect of an intervention in a time series\n  setting: the C-ARIMA approach",
        "authors": [
            "Fiammetta Menchetti",
            "Fabrizio Cipollini",
            "Fabrizia Mealli"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The Rubin Causal Model (RCM) is a framework that allows to define the causal\neffect of an intervention as a contrast of potential outcomes. In recent years,\nseveral methods have been developed under the RCM to estimate causal effects in\ntime series settings. None of these makes use of ARIMA models, which are\ninstead very common in the econometrics literature. In this paper, we propose a\nnovel approach, C-ARIMA, to define and estimate the causal effect of an\nintervention in a time series setting under the RCM. We first formalize the\nassumptions enabling the definition, the estimation and the attribution of the\neffect to the intervention; we then check the validity of the proposed method\nwith an extensive simulation study, comparing its performance against a\nstandard intervention analysis approach. In the empirical application, we use\nC-ARIMA to assess the causal effect of a permanent price reduction on\nsupermarket sales. The CausalArima R package provides an implementation of our\nproposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.06740v3"
    },
    {
        "title": "Feasible IV Regression without Excluded Instruments",
        "authors": [
            "Emmanuel Selorm Tsyawo"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The relevance condition of Integrated Conditional Moment (ICM) estimators is\nsignificantly weaker than the conventional IV's in at least two respects: (1)\nconsistent estimation without excluded instruments is possible, provided\nendogenous covariates are non-linearly mean-dependent on exogenous covariates,\nand (2) endogenous covariates may be uncorrelated with but mean-dependent on\ninstruments. These remarkable properties notwithstanding, multiplicative-kernel\nICM estimators suffer diminished identification strength, large bias, and\nsevere size distortions even for a moderately sized instrument vector. This\npaper proposes a computationally fast linear ICM estimator that better\npreserves identification strength in the presence of multiple instruments and a\ntest of the ICM relevance condition. Monte Carlo simulations demonstrate a\nconsiderably better size control in the presence of multiple instruments and a\nfavourably competitive performance in general. An empirical example illustrates\nthe practical usefulness of the estimator, where estimates remain plausible\nwhen no excluded instrument is used.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.09621v4"
    },
    {
        "title": "A Powerful Subvector Anderson Rubin Test in Linear Instrumental\n  Variables Regression with Conditional Heteroskedasticity",
        "authors": [
            "Patrik Guggenberger",
            "Frank Kleibergen",
            "Sophocles Mavroeidis"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We introduce a new test for a two-sided hypothesis involving a subset of the\nstructural parameter vector in the linear instrumental variables (IVs) model.\nGuggenberger et al. (2019), GKM19 from now on, introduce a subvector\nAnderson-Rubin (AR) test with data-dependent critical values that has\nasymptotic size equal to nominal size for a parameter space that allows for\narbitrary strength or weakness of the IVs and has uniformly nonsmaller power\nthan the projected AR test studied in Guggenberger et al. (2012). However,\nGKM19 imposes the restrictive assumption of conditional homoskedasticity. The\nmain contribution here is to robustify the procedure in GKM19 to arbitrary\nforms of conditional heteroskedasticity. We first adapt the method in GKM19 to\na setup where a certain covariance matrix has an approximate Kronecker product\n(AKP) structure which nests conditional homoskedasticity. The new test equals\nthis adaption when the data is consistent with AKP structure as decided by a\nmodel selection procedure. Otherwise the test equals the AR/AR test in Andrews\n(2017) that is fully robust to conditional heteroskedasticity but less powerful\nthan the adapted method. We show theoretically that the new test has asymptotic\nsize bounded by the nominal size and document improved power relative to the\nAR/AR test in a wide array of Monte Carlo simulations when the covariance\nmatrix is not too far from AKP.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.11371v4"
    },
    {
        "title": "What Do We Get from Two-Way Fixed Effects Regressions? Implications from\n  Numerical Equivalence",
        "authors": [
            "Shoya Ishimaru"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper develops general numerical and causal interpretations of the\ntwo-way fixed effects (TWFE) estimator in any multiperiod panel. At the sample\nlevel, the TWFE coefficient is a weighted average of first difference\nregression coefficients using all possible between-period gaps. This\ndecomposition improves transparency by revealing the sources of variation that\nthe TWFE coefficient captures. At the population level, a causal interpretation\nof the TWFE coefficient requires a common trends assumption for any\nbetween-period gap, conditional on changes, not levels, of time-varying\ncovariates. I propose a simple modification to the TWFE approach that naturally\nrelaxes these requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.12374v6"
    },
    {
        "title": "An investigation of higher order moments of empirical financial data and\n  the implications to risk",
        "authors": [
            "Luke De Clerk",
            "Sergey Savel'ev"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Here, we analyse the behaviour of the higher order standardised moments of\nfinancial time series when we truncate a large data set into smaller and\nsmaller subsets, referred to below as time windows. We look at the effect of\nthe economic environment on the behaviour of higher order moments in these time\nwindows. We observe two different scaling relations of higher order moments\nwhen the data sub sets' length decreases; one for longer time windows and\nanother for the shorter time windows. These scaling relations drastically\nchange when the time window encompasses a financial crisis. We also observe a\nqualitative change of higher order standardised moments compared to the\ngaussian values in response to a shrinking time window. We extend this analysis\nto incorporate the effects these scaling relations have upon risk. We decompose\nthe return series within these time windows and carry out a Value-at-Risk\ncalculation. In doing so, we observe the manifestation of the scaling relations\nthrough the change in the Value-at-Risk level. Moreover, we model the observed\nscaling laws by analysing the hierarchy of rare events on higher order moments.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.13199v3"
    },
    {
        "title": "A perturbed utility route choice model",
        "authors": [
            "Mogens Fosgerau",
            "Mads Paulsen",
            "Thomas Kjær Rasmussen"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose a route choice model in which traveler behavior is represented as\na utility maximizing assignment of flow across an entire network under a flow\nconservation constraint}. Substitution between routes depends on how much they\noverlap. {\\tr The model is estimated considering the full set of route\nalternatives, and no choice set generation is required. Nevertheless,\nestimation requires only linear regression and is very fast. Predictions from\nthe model can be computed using convex optimization, and computation is\nstraightforward even for large networks. We estimate and validate the model\nusing a large dataset comprising 1,337,096 GPS traces of trips in the Greater\nCopenhagen road network.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.13784v3"
    },
    {
        "title": "Addressing spatial dependence in technical efficiency estimation: A\n  Spatial DEA frontier approach",
        "authors": [
            "Julian Ramajo",
            "Miguel A. Marquez",
            "Geoffrey J. D. Hewings"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper introduces a new specification for the nonparametric\nproduction-frontier based on Data Envelopment Analysis (DEA) when dealing with\ndecision-making units whose economic performances are correlated with those of\nthe neighbors (spatial dependence). To illustrate the bias reduction that the\nSpDEA provides with respect to standard DEA methods, an analysis of the\nregional production frontiers for the NUTS-2 European regions during the period\n2000-2014 was carried out. The estimated SpDEA scores show a bimodal\ndistribution do not detected by the standard DEA estimates. The results confirm\nthe crucial role of space, offering important new insights on both the causes\nof regional disparities in labour productivity and the observed polarization of\nthe European distribution of per capita income.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.14063v1"
    },
    {
        "title": "Empirical Welfare Maximization with Constraints",
        "authors": [
            "Liyang Sun"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Empirical Welfare Maximization (EWM) is a framework that can be used to\nselect welfare program eligibility policies based on data. This paper extends\nEWM by allowing for uncertainty in estimating the budget needed to implement\nthe selected policy, in addition to its welfare. Due to the additional\nestimation error, I show there exist no rules that achieve the highest welfare\npossible while satisfying a budget constraint uniformly over a wide range of\nDGPs. This differs from the setting without a budget constraint where\nuniformity is achievable. I propose an alternative trade-off rule and\nillustrate it with Medicaid expansion, a setting with imperfect take-up and\nvarying program costs.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.15298v2"
    },
    {
        "title": "On a Standard Method for Measuring the Natural Rate of Interest",
        "authors": [
            "Daniel Buncic"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  I show that Holston, Laubach and Williams' (2017) implementation of Median\nUnbiased Estimation (MUE) cannot recover the signal-to-noise ratio of interest\nfrom their Stage 2 model. Moreover, their implementation of the structural\nbreak regressions which are used as an auxiliary model in MUE deviates from\nStock and Watson's (1998) formulation. This leads to spuriously large estimates\nof the signal-to-noise parameter $\\lambda _{z}$ and thereby an excessive\ndownward trend in other factor $z_{t}$ and the natural rate. I provide a\ncorrection to the Stage 2 model specification and the implementation of the\nstructural break regressions in MUE. This correction is quantitatively\nimportant. It results in substantially smaller point estimates of $\\lambda\n_{z}$ which affects the severity of the downward trend in other factor $z_{t}$.\nFor the US, the estimate of $\\lambda _{z}$ shrinks from $0.040$ to $0.013$ and\nis statistically highly insignificant. For the Euro Area, the UK and Canada,\nthe MUE point estimates of $\\lambda _{z}$ are \\emph{exactly} zero. Natural rate\nestimates from HLW's model using the correct Stage 2 MUE implementation are up\nto 100 basis points larger than originally computed.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.16452v2"
    },
    {
        "title": "Feature Selection for Personalized Policy Analysis",
        "authors": [
            "Maria Nareklishvili",
            "Nicholas Polson",
            "Vadim Sokolov"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this paper, we propose Forest-PLS, a feature selection method for\nanalyzing policy effect heterogeneity in a more flexible and comprehensive\nmanner than is typically available with conventional methods. In particular,\nour method is able to capture policy effect heterogeneity both within and\nacross subgroups of the population defined by observable characteristics. To\nachieve this, we employ partial least squares to identify target components of\nthe population and causal forests to estimate personalized policy effects\nacross these components. We show that the method is consistent and leads to\nasymptotically normally distributed policy effects. To demonstrate the efficacy\nof our approach, we apply it to the data from the Pennsylvania Reemployment\nBonus Experiments, which were conducted in 1988-1989. The analysis reveals that\nfinancial incentives can motivate some young non-white individuals to enter the\nlabor market. However, these incentives may also provide a temporary financial\ncushion for others, dissuading them from actively seeking employment. Our\nfindings highlight the need for targeted, personalized measures for young\nnon-white male participants.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.00251v3"
    },
    {
        "title": "Inference for Large Panel Data with Many Covariates",
        "authors": [
            "Markus Pelger",
            "Jiacheng Zou"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper proposes a novel testing procedure for selecting a sparse set of\ncovariates that explains a large dimensional panel. Our selection method\nprovides correct false detection control while having higher power than\nexisting approaches. We develop the inferential theory for large panels with\nmany covariates by combining post-selection inference with a novel multiple\ntesting adjustment. Our data-driven hypotheses are conditional on the sparse\ncovariate selection. We control for family-wise error rates for covariate\ndiscovery for large cross-sections. As an easy-to-use and practically relevant\nprocedure, we propose Panel-PoSI, which combines the data-driven adjustment for\npanel multiple testing with valid post-selection p-values of a generalized\nLASSO, that allows us to incorporate priors. In an empirical study, we select a\nsmall number of asset pricing factors that explain a large cross-section of\ninvestment strategies. Our method dominates the benchmarks out-of-sample due to\nits better size and power.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.00292v6"
    },
    {
        "title": "Time-Varying Coefficient DAR Model and Stability Measures for Stablecoin\n  Prices: An Application to Tether",
        "authors": [
            "Antoine Djobenou",
            "Emre Inan",
            "Joann Jasiak"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper examines the dynamics of Tether, the stablecoin with the largest\nmarket capitalization. We show that the distributional and dynamic properties\nof Tether/USD rates have been evolving from 2017 to 2021. We use local analysis\nmethods to detect and describe the local patterns, such as short-lived trends,\ntime-varying volatility and persistence. To accommodate these patterns, we\nconsider a time varying parameter Double Autoregressive tvDAR(1) model under\nthe assumption of local stationarity of Tether/USD rates. We estimate the tvDAR\nmodel non-parametrically and test hypotheses on the functional parameters. In\nthe application to Tether, the model provides a good fit and reliable\nout-of-sample forecasts at short horizons, while being robust to time-varying\npersistence and volatility. In addition, the model yields a simple plug-in\nmeasure of stability for Tether and other stablecoins for assessing and\ncomparing their stability.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.00509v1"
    },
    {
        "title": "Quantile Autoregression-based Non-causality Testing",
        "authors": [
            "Weifeng Jin"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Non-causal processes have been drawing attention recently in Macroeconomics\nand Finance for their ability to display nonlinear behaviors such as asymmetric\ndynamics, clustering volatility, and local explosiveness. In this paper, we\ninvestigate the statistical properties of empirical conditional quantiles of\nnon-causal processes. Specifically, we show that the quantile autoregression\n(QAR) estimates for non-causal processes do not remain constant across\ndifferent quantiles in contrast to their causal counterparts. Furthermore, we\ndemonstrate that non-causal autoregressive processes admit nonlinear\nrepresentations for conditional quantiles given past observations. Exploiting\nthese properties, we propose three novel testing strategies of non-causality\nfor non-Gaussian processes within the QAR framework. The tests are constructed\neither by verifying the constancy of the slope coefficients or by applying a\nmisspecification test of the linear QAR model over different quantiles of the\nprocess. Some numerical experiments are included to examine the finite sample\nperformance of the testing strategies, where we compare different specification\ntests for dynamic quantiles with the Kolmogorov-Smirnov constancy test. The new\nmethodology is applied to some time series from financial markets to\ninvestigate the presence of speculative bubbles. The extension of the approach\nbased on the specification tests to AR processes driven by innovations with\nheteroskedasticity is studied through simulations. The performance of QAR\nestimates of non-causal processes at extreme quantiles is also explored.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.02937v1"
    },
    {
        "title": "Asymptotic Theory for Two-Way Clustering",
        "authors": [
            "Luther Yap"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proves a new central limit theorem for a sample that exhibits\ntwo-way dependence and heterogeneity across clusters. Statistical inference for\nsituations with both two-way dependence and cluster heterogeneity has thus far\nbeen an open issue. The existing theory for two-way clustering inference\nrequires identical distributions across clusters (implied by the so-called\nseparate exchangeability assumption). Yet no such homogeneity requirement is\nneeded in the existing theory for one-way clustering. The new result therefore\ntheoretically justifies the view that two-way clustering is a more robust\nversion of one-way clustering, consistent with applied practice. In an\napplication to linear regression, I show that a standard plug-in variance\nestimator is valid for inference.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.03805v3"
    },
    {
        "title": "Uniform Inference in Linear Error-in-Variables Models:\n  Divide-and-Conquer",
        "authors": [
            "Tom Boot",
            "Artūras Juodis"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  It is customary to estimate error-in-variables models using higher-order\nmoments of observables. This moments-based estimator is consistent only when\nthe coefficient of the latent regressor is assumed to be non-zero. We develop a\nnew estimator based on the divide-and-conquer principle that is consistent for\nany value of the coefficient of the latent regressor. In an application on the\nrelation between investment, (mismeasured) Tobin's $q$ and cash flow, we find\ntime periods in which the effect of Tobin's $q$ is not statistically different\nfrom zero. The implausibly large higher-order moment estimates in these periods\ndisappear when using the proposed estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.04439v1"
    },
    {
        "title": "Testing for the appropriate level of clustering in linear regression\n  models",
        "authors": [
            "James G. MacKinnon",
            "Morten Ørregaard Nielsen",
            "Matthew D. Webb"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The overwhelming majority of empirical research that uses cluster-robust\ninference assumes that the clustering structure is known, even though there are\noften several possible ways in which a dataset could be clustered. We propose\ntwo tests for the correct level of clustering in regression models. One test\nfocuses on inference about a single coefficient, and the other on inference\nabout two or more coefficients. We provide both asymptotic and wild bootstrap\nimplementations. The proposed tests work for a null hypothesis of either no\nclustering or ``fine'' clustering against alternatives of ``coarser''\nclustering. We also propose a sequential testing procedure to determine the\nappropriate level of clustering. Simulations suggest that the bootstrap tests\nperform very well under the null hypothesis and can have excellent power. An\nempirical example suggests that using the tests leads to sensible inferences.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.04522v2"
    },
    {
        "title": "Fast and Reliable Jackknife and Bootstrap Methods for Cluster-Robust\n  Inference",
        "authors": [
            "James G. MacKinnon",
            "Morten Ørregaard Nielsen",
            "Matthew D. Webb"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We provide computationally attractive methods to obtain jackknife-based\ncluster-robust variance matrix estimators (CRVEs) for linear regression models\nestimated by least squares. We also propose several new variants of the wild\ncluster bootstrap, which involve these CRVEs, jackknife-based bootstrap\ndata-generating processes, or both. Extensive simulation experiments suggest\nthat the new methods can provide much more reliable inferences than existing\nones in cases where the latter are not trustworthy, such as when the number of\nclusters is small and/or cluster sizes vary substantially. Three empirical\nexamples illustrate the new methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.04527v2"
    },
    {
        "title": "Inference on quantile processes with a finite number of clusters",
        "authors": [
            "Andreas Hagemann"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  I introduce a generic method for inference on entire quantile and regression\nquantile processes in the presence of a finite number of large and arbitrarily\nheterogeneous clusters. The method asymptotically controls size by generating\nstatistics that exhibit enough distributional symmetry such that randomization\ntests can be applied. The randomization test does not require ex-ante matching\nof clusters, is free of user-chosen parameters, and performs well at\nconventional significance levels with as few as five clusters. The method tests\nstandard (non-sharp) hypotheses and can even be asymptotically similar in\nempirically relevant situations. The main focus of the paper is inference on\nquantile treatment effects but the method applies more broadly. Numerical and\nempirical examples are provided.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.04687v2"
    },
    {
        "title": "Interacting Treatments with Endogenous Takeup",
        "authors": [
            "Mate Kormos",
            "Robert P. Lieli",
            "Martin Huber"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We study causal inference in randomized experiments (or quasi-experiments)\nfollowing a $2\\times 2$ factorial design. There are two treatments, denoted $A$\nand $B$, and units are randomly assigned to one of four categories: treatment\n$A$ alone, treatment $B$ alone, joint treatment, or none. Allowing for\nendogenous non-compliance with the two binary instruments representing the\nintended assignment, as well as unrestricted interference across the two\ntreatments, we derive the causal interpretation of various instrumental\nvariable estimands under more general compliance conditions than in the\nliterature. In general, if treatment takeup is driven by both instruments for\nsome units, it becomes difficult to separate treatment interaction from\ntreatment effect heterogeneity. We provide auxiliary conditions and various\nbounding strategies that may help zero in on causally interesting parameters.\nAs an empirical illustration, we apply our results to a program randomly\noffering two different treatments, namely tutoring and financial incentives, to\nfirst year college students, in order to assess the treatments' effects on\nacademic performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.04876v2"
    },
    {
        "title": "Robust M-Estimation for Additive Single-Index Cointegrating Time Series\n  Models",
        "authors": [
            "Chaohua Dong",
            "Jiti Gao",
            "Yundong Tu",
            "Bin Peng"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Robust M-estimation uses loss functions, such as least absolute deviation\n(LAD), quantile loss and Huber's loss, to construct its objective function, in\norder to for example eschew the impact of outliers, whereas the difficulty in\nanalysing the resultant estimators rests on the nonsmoothness of these losses.\nGeneralized functions have advantages over ordinary functions in several\naspects, especially generalized functions possess derivatives of any order.\nGeneralized functions incorporate local integrable functions, the so-called\nregular generalized functions, while the so-called singular generalized\nfunctions (e.g. Dirac delta function) can be obtained as the limits of a\nsequence of sufficient smooth functions, so-called regular sequence in\ngeneralized function context. This makes it possible to use these singular\ngeneralized functions through approximation. Nevertheless, a significant\ncontribution of this paper is to establish the convergence rate of regular\nsequence to nonsmooth loss that answers a call from the relevant literature.\nFor parameter estimation where objective function may be nonsmooth, this paper\nfirst shows as a general paradigm that how generalized function approach can be\nused to tackle the nonsmooth loss functions in Section two using a very simple\nmodel. This approach is of general interest and applicability. We further use\nthe approach in robust M-estimation for additive single-index cointegrating\ntime series models; the asymptotic theory is established for the proposed\nestimators. We evaluate the finite-sample performance of the proposed\nestimation method and theory by both simulated data and an empirical analysis\nof predictive regression of stock returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.06631v1"
    },
    {
        "title": "Resolving the Conflict on Conduct Parameter Estimation in Homogeneous\n  Goods Markets between Bresnahan (1982) and Perloff and Shen (2012)",
        "authors": [
            "Yuri Matsumura",
            "Suguru Otani"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We revisit conduct parameter estimation in homogeneous goods markets to\nresolve the conflict between Bresnahan (1982) and Perloff and Shen (2012)\nregarding the identification and the estimation of conduct parameters. We point\nout that Perloff and Shen's (2012) proof is incorrect and its simulation\nsetting is invalid. Our simulation shows that estimation becomes accurate when\ndemand shifters are properly added in supply estimation and sample sizes are\nincreased, supporting Bresnahan (1982).\n",
        "pdf_link": "http://arxiv.org/pdf/2301.06665v5"
    },
    {
        "title": "Testing Firm Conduct",
        "authors": [
            "Marco Duarte",
            "Lorenzo Magnolfi",
            "Mikkel Sølvsten",
            "Christopher Sullivan"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Evaluating policy in imperfectly competitive markets requires understanding\nfirm behavior. While researchers test conduct via model selection and\nassessment, we present advantages of Rivers and Vuong (2002) (RV) model\nselection under misspecification. However, degeneracy of RV invalidates\ninference. With a novel definition of weak instruments for testing, we connect\ndegeneracy to instrument strength, derive weak instrument properties of RV, and\nprovide a diagnostic for weak instruments by extending the framework of Stock\nand Yogo (2005) to model selection. We test vertical conduct (Villas-Boas,\n2007) using common instrument sets. Some are weak, providing no power. Strong\ninstruments support manufacturers setting retail prices.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.06720v2"
    },
    {
        "title": "Unconditional Quantile Partial Effects via Conditional Quantile\n  Regression",
        "authors": [
            "Javier Alejo",
            "Antonio F. Galvao",
            "Julian Martinez-Iriarte",
            "Gabriel Montes-Rojas"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper develops a semi-parametric procedure for estimation of\nunconditional quantile partial effects using quantile regression coefficients.\nThe estimator is based on an identification result showing that, for continuous\ncovariates, unconditional quantile effects are a weighted average of\nconditional ones at particular quantile levels that depend on the covariates.\nWe propose a two-step estimator for the unconditional effects where in the\nfirst step one estimates a structural quantile regression model, and in the\nsecond step a nonparametric regression is applied to the first step\ncoefficients. We establish the asymptotic properties of the estimator, say\nconsistency and asymptotic normality. Monte Carlo simulations show numerical\nevidence that the estimator has very good finite sample performance and is\nrobust to the selection of bandwidth and kernel. To illustrate the proposed\nmethod, we study the canonical application of the Engel's curve, i.e. food\nexpenditures as a share of income.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.07241v4"
    },
    {
        "title": "Optimal Transport for Counterfactual Estimation: A Method for Causal\n  Inference",
        "authors": [
            "Arthur Charpentier",
            "Emmanuel Flachaire",
            "Ewen Gallic"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Many problems ask a question that can be formulated as a causal question:\n\"what would have happened if...?\" For example, \"would the person have had\nsurgery if he or she had been Black?\" To address this kind of questions,\ncalculating an average treatment effect (ATE) is often uninformative, because\none would like to know how much impact a variable (such as skin color) has on a\nspecific individual, characterized by certain covariates. Trying to calculate a\nconditional ATE (CATE) seems more appropriate. In causal inference, the\npropensity score approach assumes that the treatment is influenced by x, a\ncollection of covariates. Here, we will have the dual view: doing an\nintervention, or changing the treatment (even just hypothetically, in a thought\nexperiment, for example by asking what would have happened if a person had been\nBlack) can have an impact on the values of x. We will see here that optimal\ntransport allows us to change certain characteristics that are influenced by\nthe variable we are trying to quantify the effect of. We propose here a mutatis\nmutandis version of the CATE, which will be done simply in dimension one by\nsaying that the CATE must be computed relative to a level of probability,\nassociated to the proportion of x (a single covariate) in the control\npopulation, and by looking for the equivalent quantile in the test population.\nIn higher dimension, it will be necessary to go through transport, and an\napplication will be proposed on the impact of some variables on the probability\nof having an unnatural birth (the fact that the mother smokes, or that the\nmother is Black).\n",
        "pdf_link": "http://arxiv.org/pdf/2301.07755v1"
    },
    {
        "title": "Revisiting Panel Data Discrete Choice Models with Lagged Dependent\n  Variables",
        "authors": [
            "Christopher R. Dobronyi",
            "Fu Ouyang",
            "Thomas Tao Yang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper revisits the identification and estimation of a class of\nsemiparametric (distribution-free) panel data binary choice models with lagged\ndependent variables, exogenous covariates, and entity fixed effects. We provide\na novel identification strategy, using an \"identification at infinity\"\nargument. In contrast with the celebrated Honore and Kyriazidou (2000), our\nmethod permits time trends of any form and does not suffer from the \"curse of\ndimensionality\". We propose an easily implementable conditional maximum score\nestimator. The asymptotic properties of the proposed estimator are fully\ncharacterized. A small-scale Monte Carlo study demonstrates that our approach\nperforms satisfactorily in finite samples. We illustrate the usefulness of our\nmethod by presenting an empirical application to enrollment in private hospital\ninsurance using the Household, Income and Labour Dynamics in Australia (HILDA)\nSurvey data.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.09379v5"
    },
    {
        "title": "Automatic Locally Robust Estimation with Generated Regressors",
        "authors": [
            "Juan Carlos Escanciano",
            "Telmo Pérez-Izquierdo"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Many economic and causal parameters of interest depend on generated\nregressors. Examples include structural parameters in models with endogenous\nvariables estimated by control functions and in models with sample selection,\ntreatment effect estimation with propensity score matching, and marginal\ntreatment effects. Inference with generated regressors is complicated by the\nvery complex expression for influence functions and asymptotic variances. To\naddress this problem, we propose Automatic Locally Robust/debiased GMM\nestimators in a general setting with generated regressors. Importantly, we\nallow for the generated regressors to be generated from machine learners, such\nas Random Forest, Neural Nets, Boosting, and many others. We use our results to\nconstruct novel Doubly Robust and Locally Robust estimators for the\nCounterfactual Average Structural Function and Average Partial Effects in\nmodels with endogeneity and sample selection, respectively. We provide\nsufficient conditions for the asymptotic normality of our debiased GMM\nestimators and investigate their finite sample performance through Monte Carlo\nsimulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.10643v2"
    },
    {
        "title": "Simple Difference-in-Differences Estimation in Fixed-T Panels",
        "authors": [
            "Nicholas Brown",
            "Kyle Butts",
            "Joakim Westerlund"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The present paper proposes a new treatment effects estimator that is valid\nwhen the number of time periods is small, and the parallel trends condition\nholds conditional on covariates and unobserved heterogeneity in the form of\ninteractive fixed effects. The estimator also allow the control variables to be\naffected by treatment and it enables estimation of the resulting indirect\neffect on the outcome variable. The asymptotic properties of the estimator are\nestablished and their accuracy in small samples is investigated using Monte\nCarlo simulations. The empirical usefulness of the estimator is illustrated\nusing as an example the effect of increased trade competition on firm markups\nin China.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.11358v2"
    },
    {
        "title": "Synthetic Difference In Differences Estimation",
        "authors": [
            "Damian Clarke",
            "Daniel Pailañir",
            "Susan Athey",
            "Guido Imbens"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this paper, we describe a computational implementation of the Synthetic\ndifference-in-differences (SDID) estimator of Arkhangelsky et al. (2021) for\nStata. Synthetic difference-in-differences can be used in a wide class of\ncircumstances where treatment effects on some particular policy or event are\ndesired, and repeated observations on treated and untreated units are available\nover time. We lay out the theory underlying SDID, both when there is a single\ntreatment adoption date and when adoption is staggered over time, and discuss\nestimation and inference in each of these cases. We introduce the sdid command\nwhich implements these methods in Stata, and provide a number of examples of\nuse, discussing estimation, inference, and visualization of results.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.11859v3"
    },
    {
        "title": "A Note on the Estimation of Job Amenities and Labor Productivity",
        "authors": [
            "Arnaud Dupuy",
            "Alfred Galichon"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper introduces a maximum likelihood estimator of the value of job\namenities and labor productivity in a single matching market based on the\nobservation of equilibrium matches and wages. The estimation procedure\nsimultaneously fits both the matching patterns and the wage curve. While our\nestimator is suited for a wide range of assignment problems, we provide an\napplication to the estimation of the Value of a Statistical Life using\ncompensating wage differentials for the risk of fatal injury on the job. Using\nUS data for 2017, we estimate the Value of Statistical Life at \\$ 6.3 million\n(\\$2017).\n",
        "pdf_link": "http://arxiv.org/pdf/2301.12542v1"
    },
    {
        "title": "Nonlinearities in Macroeconomic Tail Risk through the Lens of Big Data\n  Quantile Regressions",
        "authors": [
            "Jan Prüser",
            "Florian Huber"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Modeling and predicting extreme movements in GDP is notoriously difficult and\nthe selection of appropriate covariates and/or possible forms of nonlinearities\nare key in obtaining precise forecasts. In this paper, our focus is on using\nlarge datasets in quantile regression models to forecast the conditional\ndistribution of US GDP growth. To capture possible non-linearities, we include\nseveral nonlinear specifications. The resulting models will be huge dimensional\nand we thus rely on a set of shrinkage priors. Since Markov Chain Monte Carlo\nestimation becomes slow in these dimensions, we rely on fast variational Bayes\napproximations to the posterior distribution of the coefficients and the latent\nstates. We find that our proposed set of models produces precise forecasts.\nThese gains are especially pronounced in the tails. Using Gaussian processes to\napproximate the nonlinear component of the model further improves the good\nperformance, in particular in the right tail.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.13604v2"
    },
    {
        "title": "Approximate Functional Differencing",
        "authors": [
            "Geert Dhaene",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Inference on common parameters in panel data models with individual-specific\nfixed effects is a classic example of Neyman and Scott's (1948) incidental\nparameter problem (IPP). One solution to this IPP is functional differencing\n(Bonhomme 2012), which works when the number of time periods T is fixed (and\nmay be small), but this solution is not applicable to all panel data models of\ninterest. Another solution, which applies to a larger class of models, is\n\"large-T\" bias correction (pioneered by Hahn and Kuersteiner 2002 and Hahn and\nNewey 2004), but this is only guaranteed to work well when T is sufficiently\nlarge. This paper provides a unified approach that connects those two seemingly\ndisparate solutions to the IPP. In doing so, we provide an approximate version\nof functional differencing, that is, an approximate solution to the IPP that is\napplicable to a large class of panel data models even when T is relatively\nsmall.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.13736v2"
    },
    {
        "title": "On Using The Two-Way Cluster-Robust Standard Errors",
        "authors": [
            "Harold D Chiang",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Thousands of papers have reported two-way cluster-robust (TWCR) standard\nerrors. However, the recent econometrics literature points out the potential\nnon-gaussianity of two-way cluster sample means, and thus invalidity of the\ninference based on the TWCR standard errors. Fortunately, simulation studies\nnonetheless show that the gaussianity is rather common than exceptional. This\npaper provides theoretical support for this encouraging observation.\nSpecifically, we derive a novel central limit theorem for two-way clustered\ntriangular arrays that justifies the use of the TWCR under very mild and\ninterpretable conditions. We, therefore, hope that this paper will provide a\ntheoretical justification for the legitimacy of most, if not all, of the\nthousands of those empirical papers that have used the TWCR standard errors. We\nprovide a guide in practice as to when a researcher can employ the TWCR\nstandard errors.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.13775v1"
    },
    {
        "title": "Machine Learning for Dynamic Discrete Choice",
        "authors": [
            "Vira Semenova"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Dynamic discrete choice models often discretize the state vector and restrict\nits dimension in order to achieve valid inference. I propose a novel two-stage\nestimator for the set-identified structural parameter that incorporates a\nhigh-dimensional state space into the dynamic model of imperfect competition.\nIn the first stage, I estimate the state variable's law of motion and the\nequilibrium policy function using machine learning tools. In the second stage,\nI plug the first-stage estimates into a moment inequality and solve for the\nstructural parameter. The moment function is presented as the sum of two\ncomponents, where the first one expresses the equilibrium assumption and the\nsecond one is a bias correction term that makes the sum insensitive (i.e.,\northogonal) to first-stage bias. The proposed estimator uniformly converges at\nthe root-N rate and I use it to construct confidence regions. The results\ndeveloped here can be used to incorporate high-dimensional state space into\nclassic dynamic discrete choice models, for example, those considered in Rust\n(1987), Bajari et al. (2007), and Scott (2013).\n",
        "pdf_link": "http://arxiv.org/pdf/1808.02569v2"
    },
    {
        "title": "A Unified Framework for Efficient Estimation of General Treatment Models",
        "authors": [
            "Chunrong Ai",
            "Oliver Linton",
            "Kaiji Motegi",
            "Zheng Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper presents a weighted optimization framework that unifies the\nbinary,multi-valued, continuous, as well as mixture of discrete and continuous\ntreatment, under the unconfounded treatment assignment. With a general loss\nfunction, the framework includes the average, quantile and asymmetric least\nsquares causal effect of treatment as special cases. For this general\nframework, we first derive the semiparametric efficiency bound for the causal\neffect of treatment, extending the existing bound results to a wider class of\nmodels. We then propose a generalized optimization estimation for the causal\neffect with weights estimated by solving an expanding set of equations. Under\nsome sufficient conditions, we establish consistency and asymptotic normality\nof the proposed estimator of the causal effect and show that the estimator\nattains our semiparametric efficiency bound, thereby extending the existing\nliterature on efficient estimation of causal effect to a wider class of\napplications. Finally, we discuss etimation of some causal effect functionals\nsuch as the treatment effect curve and the average outcome. To evaluate the\nfinite sample performance of the proposed procedure, we conduct a small scale\nsimulation study and find that the proposed estimation has practical value. To\nillustrate the applicability of the procedure, we revisit the literature on\ncampaign advertise and campaign contributions. Unlike the existing procedures\nwhich produce mixed results, we find no evidence of campaign advertise on\ncampaign contribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.04936v2"
    },
    {
        "title": "Can GDP measurement be further improved? Data revision and\n  reconciliation",
        "authors": [
            "Jan P. A. M. Jacobs",
            "Samad Sarferaz",
            "Jan-Egbert Sturm",
            "Simon van Norden"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Recent years have seen many attempts to combine expenditure-side estimates of\nU.S. real output (GDE) growth with income-side estimates (GDI) to improve\nestimates of real GDP growth. We show how to incorporate information from\nmultiple releases of noisy data to provide more precise estimates while\navoiding some of the identifying assumptions required in earlier work. This\nrelies on a new insight: using multiple data releases allows us to distinguish\nnews and noise measurement errors in situations where a single vintage does\nnot.\n  Our new measure, GDP++, fits the data better than GDP+, the GDP growth\nmeasure of Aruoba et al. (2016) published by the Federal Reserve Bank of\nPhiladephia. Historical decompositions show that GDE releases are more\ninformative than GDI, while the use of multiple data releases is particularly\nimportant in the quarters leading up to the Great Recession.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.04970v1"
    },
    {
        "title": "When Do Households Invest in Solar Photovoltaics? An Application of\n  Prospect Theory",
        "authors": [
            "Martin Klein",
            "Marc Deissenroth"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  While investments in renewable energy sources (RES) are incentivized around\nthe world, the policy tools that do so are still poorly understood, leading to\ncostly misadjustments in many cases. As a case study, the deployment dynamics\nof residential solar photovoltaics (PV) invoked by the German feed-in tariff\nlegislation are investigated. Here we report a model showing that the question\nof when people invest in residential PV systems is found to be not only\ndetermined by profitability, but also by profitability's change compared to the\nstatus quo. This finding is interpreted in the light of loss aversion, a\nconcept developed in Kahneman and Tversky's Prospect Theory. The model is able\nto reproduce most of the dynamics of the uptake with only a few financial and\nbehavioral assumptions\n",
        "pdf_link": "http://arxiv.org/pdf/1808.05572v1"
    },
    {
        "title": "Estimation in a Generalization of Bivariate Probit Models with Dummy\n  Endogenous Regressors",
        "authors": [
            "Sukjin Han",
            "Sungwon Lee"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The purpose of this paper is to provide guidelines for empirical researchers\nwho use a class of bivariate threshold crossing models with dummy endogenous\nvariables. A common practice employed by the researchers is the specification\nof the joint distribution of the unobservables as a bivariate normal\ndistribution, which results in a bivariate probit model. To address the problem\nof misspecification in this practice, we propose an easy-to-implement\nsemiparametric estimation framework with parametric copula and nonparametric\nmarginal distributions. We establish asymptotic theory, including root-n\nnormality, for the sieve maximum likelihood estimators that can be used to\nconduct inference on the individual structural parameters and the average\ntreatment effect (ATE). In order to show the practical relevance of the\nproposed framework, we conduct a sensitivity analysis via extensive Monte Carlo\nsimulation exercises. The results suggest that the estimates of the parameters,\nespecially the ATE, are sensitive to parametric specification, while\nsemiparametric estimation exhibits robustness to underlying data generating\nprocesses. We then provide an empirical illustration where we estimate the\neffect of health insurance on doctor visits. In this paper, we also show that\nthe absence of excluded instruments may result in identification failure, in\ncontrast to what some practitioners believe.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.05792v2"
    },
    {
        "title": "Quantifying the Computational Advantage of Forward Orthogonal Deviations",
        "authors": [
            "Robert F. Phillips"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Under suitable conditions, one-step generalized method of moments (GMM) based\non the first-difference (FD) transformation is numerically equal to one-step\nGMM based on the forward orthogonal deviations (FOD) transformation. However,\nwhen the number of time periods ($T$) is not small, the FOD transformation\nrequires less computational work. This paper shows that the computational\ncomplexity of the FD and FOD transformations increases with the number of\nindividuals ($N$) linearly, but the computational complexity of the FOD\ntransformation increases with $T$ at the rate $T^{4}$ increases, while the\ncomputational complexity of the FD transformation increases at the rate $T^{6}$\nincreases. Simulations illustrate that calculations exploiting the FOD\ntransformation are performed orders of magnitude faster than those using the FD\ntransformation. The results in the paper indicate that, when one-step GMM based\non the FD and FOD transformations are the same, Monte Carlo experiments can be\nconducted much faster if the FOD version of the estimator is used.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.05995v1"
    },
    {
        "title": "Tests for price indices in a dynamic item universe",
        "authors": [
            "Li-Chun Zhang",
            "Ingvild Johansen",
            "Ragnhild Nygaard"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  There is generally a need to deal with quality change and new goods in the\nconsumer price index due to the underlying dynamic item universe. Traditionally\naxiomatic tests are defined for a fixed universe. We propose five tests\nexplicitly formulated for a dynamic item universe, and motivate them both from\nthe perspectives of a cost-of-goods index and a cost-of-living index. None of\nthe indices satisfies all the tests at the same time, which are currently\navailable for making use of scanner data that comprises the whole item\nuniverse. The set of tests provides a rigorous diagnostic for whether an index\nis completely appropriate in a dynamic item universe, as well as pointing\ntowards the directions of possible remedies. We thus outline a large index\nfamily that potentially can satisfy all the tests.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.08995v2"
    },
    {
        "title": "A Residual Bootstrap for Conditional Value-at-Risk",
        "authors": [
            "Eric Beutner",
            "Alexander Heinemann",
            "Stephan Smeekes"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  A fixed-design residual bootstrap method is proposed for the two-step\nestimator of Francq and Zako\\\"ian (2015) associated with the conditional\nValue-at-Risk. The bootstrap's consistency is proven for a general class of\nvolatility models and intervals are constructed for the conditional\nValue-at-Risk. A simulation study reveals that the equal-tailed percentile\nbootstrap interval tends to fall short of its nominal value. In contrast, the\nreversed-tails bootstrap interval yields accurate coverage. We also compare the\ntheoretically analyzed fixed-design bootstrap with the recursive-design\nbootstrap. It turns out that the fixed-design bootstrap performs equally well\nin terms of average coverage, yet leads on average to shorter intervals in\nsmaller samples. An empirical application illustrates the interval estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.09125v4"
    },
    {
        "title": "Inference based on Kotlarski's Identity",
        "authors": [
            "Kengo Kato",
            "Yuya Sasaki",
            "Takuya Ura"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Kotlarski's identity has been widely used in applied economic research.\nHowever, how to conduct inference based on this popular identification approach\nhas been an open question for two decades. This paper addresses this open\nproblem by constructing a novel confidence band for the density function of a\nlatent variable in repeated measurement error model. The confidence band builds\non our finding that we can rewrite Kotlarski's identity as a system of linear\nmoment restrictions. The confidence band controls the asymptotic size uniformly\nover a class of data generating processes, and it is consistent against all\nfixed alternatives. Simulation studies support our theoretical results.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.09375v3"
    },
    {
        "title": "Efficient Difference-in-Differences Estimation with High-Dimensional\n  Common Trend Confounding",
        "authors": [
            "Michael Zimmert"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This study considers various semiparametric difference-in-differences models\nunder different assumptions on the relation between the treatment group\nidentifier, time and covariates for cross-sectional and panel data. The\nvariance lower bound is shown to be sensitive to the model assumptions imposed\nimplying a robustness-efficiency trade-off. The obtained efficient influence\nfunctions lead to estimators that are rate double robust and have desirable\nasymptotic properties under weak first stage convergence conditions. This\nenables to use sophisticated machine-learning algorithms that can cope with\nsettings where common trend confounding is high-dimensional. The usefulness of\nthe proposed estimators is assessed in an empirical example. It is shown that\nthe efficiency-robustness trade-offs and the choice of first stage predictors\ncan lead to divergent empirical results in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.01643v5"
    },
    {
        "title": "Bootstrap Methods in Econometrics",
        "authors": [
            "Joel L. Horowitz"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The bootstrap is a method for estimating the distribution of an estimator or\ntest statistic by re-sampling the data or a model estimated from the data.\nUnder conditions that hold in a wide variety of econometric applications, the\nbootstrap provides approximations to distributions of statistics, coverage\nprobabilities of confidence intervals, and rejection probabilities of\nhypothesis tests that are more accurate than the approximations of first-order\nasymptotic distribution theory. The reductions in the differences between true\nand nominal coverage or rejection probabilities can be very large. In addition,\nthe bootstrap provides a way to carry out inference in certain settings where\nobtaining analytic distributional approximations is difficult or impossible.\nThis article explains the usefulness and limitations of the bootstrap in\ncontexts of interest in econometrics. The presentation is informal and\nexpository. It provides an intuitive understanding of how the bootstrap works.\nMathematical details are available in references that are cited.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.04016v1"
    },
    {
        "title": "Bayesian shrinkage in mixture of experts models: Identifying robust\n  determinants of class membership",
        "authors": [
            "Gregor Zens"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  A method for implicit variable selection in mixture of experts frameworks is\nproposed. We introduce a prior structure where information is taken from a set\nof independent covariates. Robust class membership predictors are identified\nusing a normal gamma prior. The resulting model setup is used in a finite\nmixture of Bernoulli distributions to find homogenous clusters of women in\nMozambique based on their information sources on HIV. Fully Bayesian inference\nis carried out via the implementation of a Gibbs sampler.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.04853v2"
    },
    {
        "title": "On the Choice of Instruments in Mixed Frequency Specification Tests",
        "authors": [
            "Yun Liu",
            "Yeonwoo Rho"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Time averaging has been the traditional approach to handle mixed sampling\nfrequencies. However, it ignores information possibly embedded in high\nfrequency. Mixed data sampling (MIDAS) regression models provide a concise way\nto utilize the additional information in high-frequency variables. In this\npaper, we propose a specification test to choose between time averaging and\nMIDAS models, based on a Durbin-Wu-Hausman test. In particular, a set of\ninstrumental variables is proposed and theoretically validated when the\nfrequency ratio is large. As a result, our method tends to be more powerful\nthan existing methods, as reconfirmed through the simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.05503v1"
    },
    {
        "title": "Estimating grouped data models with a binary dependent variable and\n  fixed effects: What are the issues",
        "authors": [
            "Nathaniel Beck"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This article deals with asimple issue: if we have grouped data with a binary\ndependent variable and want to include fixed effects (group specific\nintercepts) in the specification, is Ordinary Least Squares (OLS) in any way\nsuperior to a (conditional) logit form? In particular, what are the\nconsequences of using OLS instead of a fixed effects logit model with respect\nto the latter dropping all units which show no variability in the dependent\nvariable while the former allows for estimation using all units. First, we show\nthat the discussion of fthe incidental parameters problem is based on an\nassumption about the kinds of data being studied; for what appears to be the\ncommon use of fixed effect models in political science the incidental\nparameters issue is illusory. Turning to linear models, we see that OLS yields\na linear combination of the estimates for the units with and without variation\nin the dependent variable, and so the coefficient estimates must be carefully\ninterpreted. The article then compares two methods of estimating logit models\nwith fixed effects, and shows that the Chamberlain conditional logit is as good\nas or better than a logit analysis which simply includes group specific\nintercepts (even though the conditional logit technique was designed to deal\nwith the incidental parameters problem!). Related to this, the article\ndiscusses the estimation of marginal effects using both OLS and logit. While it\nappears that a form of logit with fixed effects can be used to estimate\nmarginal effects, this method can be improved by starting with conditional\nlogit and then using the those parameter estimates to constrain the logit with\nfixed effects model. This method produces estimates of sample average marginal\neffects that are at least as good as OLS, and much better when group size is\nsmall or the number of groups is large. .\n",
        "pdf_link": "http://arxiv.org/pdf/1809.06505v1"
    },
    {
        "title": "Partial Mean Processes with Generated Regressors: Continuous Treatment\n  Effects and Nonseparable Models",
        "authors": [
            "Ying-Ying Lee"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Partial mean with generated regressors arises in several econometric\nproblems, such as the distribution of potential outcomes with continuous\ntreatments and the quantile structural function in a nonseparable triangular\nmodel. This paper proposes a nonparametric estimator for the partial mean\nprocess, where the second step consists of a kernel regression on regressors\nthat are estimated in the first step. The main contribution is a uniform\nexpansion that characterizes in detail how the estimation error associated with\nthe generated regressor affects the limiting distribution of the marginal\nintegration estimator. The general results are illustrated with two examples:\nthe generalized propensity score for a continuous treatment (Hirano and Imbens,\n2004) and control variables in triangular models (Newey, Powell, and Vella,\n1999; Imbens and Newey, 2009). An empirical application to the Job Corps\nprogram evaluation demonstrates the usefulness of the method.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.00157v1"
    },
    {
        "title": "Treatment Effect Estimation with Noisy Conditioning Variables",
        "authors": [
            "Kenichi Nagasawa"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  I develop a new identification strategy for treatment effects when noisy\nmeasurements of unobserved confounding factors are available. I use proxy\nvariables to construct a random variable conditional on which treatment\nvariables become exogenous. The key idea is that, under appropriate conditions,\nthere exists a one-to-one mapping between the distribution of unobserved\nconfounding factors and the distribution of proxies. To ensure sufficient\nvariation in the constructed control variable, I use an additional variable,\ntermed excluded variable, which satisfies certain exclusion restrictions and\nrelevance conditions. I establish asymptotic distributional results for\nsemiparametric and flexible parametric estimators of causal parameters. I\nillustrate empirical relevance and usefulness of my results by estimating\ncausal effects of attending selective college on earnings.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.00667v4"
    },
    {
        "title": "Randomization Tests for Equality in Dependence Structure",
        "authors": [
            "Juwon Seo"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We develop a new statistical procedure to test whether the dependence\nstructure is identical between two groups. Rather than relying on a single\nindex such as Pearson's correlation coefficient or Kendall's Tau, we consider\nthe entire dependence structure by investigating the dependence functions\n(copulas). The critical values are obtained by a modified randomization\nprocedure designed to exploit asymptotic group invariance conditions.\nImplementation of the test is intuitive and simple, and does not require any\nspecification of a tuning parameter or weight function. At the same time, the\ntest exhibits excellent finite sample performance, with the null rejection\nrates almost equal to the nominal level even when the sample size is extremely\nsmall. Two empirical applications concerning the dependence between income and\nconsumption, and the Brexit effect on European financial market integration are\nprovided.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.02105v1"
    },
    {
        "title": "Nonparametric Analysis of Finite Mixtures",
        "authors": [
            "Yuichi Kitamura",
            "Louise Laage"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Finite mixture models are useful in applied econometrics. They can be used to\nmodel unobserved heterogeneity, which plays major roles in labor economics,\nindustrial organization and other fields. Mixtures are also convenient in\ndealing with contaminated sampling models and models with multiple equilibria.\nThis paper shows that finite mixture models are nonparametrically identified\nunder weak assumptions that are plausible in economic applications. The key is\nto utilize the identification power implied by information in covariates\nvariation. First, three identification approaches are presented, under distinct\nand non-nested sets of sufficient conditions. Observable features of data\ninform us which of the three approaches is valid. These results apply to\ngeneral nonparametric switching regressions, as well as to structural\neconometric models, such as auction models with unobserved heterogeneity.\nSecond, some extensions of the identification results are developed. In\nparticular, a mixture regression where the mixing weights depend on the value\nof the regressors in a fully unrestricted manner is shown to be\nnonparametrically identifiable. This means a finite mixture model with\nfunction-valued unobserved heterogeneity can be identified in a cross-section\nsetting, without restricting the dependence pattern between the regressor and\nthe unobserved heterogeneity. In this aspect it is akin to fixed effects panel\ndata models which permit unrestricted correlation between unobserved\nheterogeneity and covariates. Third, the paper shows that fully nonparametric\nestimation of the entire mixture model is possible, by forming a sample\nanalogue of one of the new identification strategies. The estimator is shown to\npossess a desirable polynomial rate of convergence as in a standard\nnonparametric estimation problem, despite nonregular features of the model.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.02727v1"
    },
    {
        "title": "Nonparametric maximum likelihood methods for binary response models with\n  random coefficients",
        "authors": [
            "Jiaying Gu",
            "Roger Koenker"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Single index linear models for binary response with random coefficients have\nbeen extensively employed in many econometric settings under various parametric\nspecifications of the distribution of the random coefficients. Nonparametric\nmaximum likelihood estimation (NPMLE) as proposed by Cosslett (1983) and\nIchimura and Thompson (1998), in contrast, has received less attention in\napplied work due primarily to computational difficulties. We propose a new\napproach to computation of NPMLEs for binary response models that significantly\nincrease their computational tractability thereby facilitating greater\nflexibility in applications. Our approach, which relies on recent developments\ninvolving the geometry of hyperplane arrangements, is contrasted with the\nrecently proposed deconvolution method of Gautier and Kitamura (2013). An\napplication to modal choice for the journey to work in the Washington DC area\nillustrates the methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03329v3"
    },
    {
        "title": "Estimation of a Structural Break Point in Linear Regression Models",
        "authors": [
            "Yaein Baek"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This study proposes a point estimator of the break location for a one-time\nstructural break in linear regression models. If the break magnitude is small,\nthe least-squares estimator of the break date has two modes at the ends of the\nfinite sample period, regardless of the true break location. To solve this\nproblem, I suggest an alternative estimator based on a modification of the\nleast-squares objective function. The modified objective function incorporates\nestimation uncertainty that varies across potential break dates. The new break\npoint estimator is consistent and has a unimodal finite sample distribution\nunder small break magnitudes. A limit distribution is provided under an in-fill\nasymptotic framework. Monte Carlo simulation results suggest that the new\nestimator outperforms the least-squares estimator. I apply the method to\nestimate the break date in U.S. real GDP growth and U.S. and UK stock return\nprediction models.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03720v3"
    },
    {
        "title": "Bootstrapping Structural Change Tests",
        "authors": [
            "Otilia Boldea",
            "Adriana Cornea-Madeira",
            "Alastair R. Hall"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper analyses the use of bootstrap methods to test for parameter change\nin linear models estimated via Two Stage Least Squares (2SLS). Two types of\ntest are considered: one where the null hypothesis is of no change and the\nalternative hypothesis involves discrete change at k unknown break-points in\nthe sample; and a second test where the null hypothesis is that there is\ndiscrete parameter change at l break-points in the sample against an\nalternative in which the parameters change at l + 1 break-points. In both\ncases, we consider inferences based on a sup-Wald-type statistic using either\nthe wild recursive bootstrap or the wild fixed bootstrap. We establish the\nasymptotic validity of these bootstrap tests under a set of general conditions\nthat allow the errors to exhibit conditional and/or unconditional\nheteroskedasticity, and report results from a simulation study that indicate\nthe tests yield reliable inferences in the sample sizes often encountered in\nmacroeconomics. The analysis covers the cases where the first-stage estimation\nof 2SLS involves a model whose parameters are either constant or themselves\nsubject to discrete parameter change. If the errors exhibit unconditional\nheteroskedasticity and/or the reduced form is unstable then the bootstrap\nmethods are particularly attractive because the limiting distributions of the\ntest statistics are not pivotal.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.04125v1"
    },
    {
        "title": "Identification and estimation of multinomial choice models with latent\n  special covariates",
        "authors": [
            "Nail Kashaev"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Identification of multinomial choice models is often established by using\nspecial covariates that have full support. This paper shows how these\nidentification results can be extended to a large class of multinomial choice\nmodels when all covariates are bounded. I also provide a new\n$\\sqrt{n}$-consistent asymptotically normal estimator of the finite-dimensional\nparameters of the model.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.05555v3"
    },
    {
        "title": "Estimation of High-Dimensional Seemingly Unrelated Regression Models",
        "authors": [
            "Lidan Tan",
            "Khai X. Chiong",
            "Hyungsik Roger Moon"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper, we investigate seemingly unrelated regression (SUR) models\nthat allow the number of equations (N) to be large, and to be comparable to the\nnumber of the observations in each equation (T). It is well known in the\nliterature that the conventional SUR estimator, for example, the generalized\nleast squares (GLS) estimator of Zellner (1962) does not perform well. As the\nmain contribution of the paper, we propose a new feasible GLS estimator called\nthe feasible graphical lasso (FGLasso) estimator. For a feasible implementation\nof the GLS estimator, we use the graphical lasso estimation of the precision\nmatrix (the inverse of the covariance matrix of the equation system errors)\nassuming that the underlying unknown precision matrix is sparse. We derive\nasymptotic theories of the new estimator and investigate its finite sample\nproperties via Monte-Carlo simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.05567v1"
    },
    {
        "title": "Bayesian Inference for Structural Vector Autoregressions Identified by\n  Markov-Switching Heteroskedasticity",
        "authors": [
            "Helmut Lütkepohl",
            "Tomasz Woźniak"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this study, Bayesian inference is developed for structural vector\nautoregressive models in which the structural parameters are identified via\nMarkov-switching heteroskedasticity. In such a model, restrictions that are\njust-identifying in the homoskedastic case, become over-identifying and can be\ntested. A set of parametric restrictions is derived under which the structural\nmatrix is globally or partially identified and a Savage-Dickey density ratio is\nused to assess the validity of the identification conditions. The latter is\nfacilitated by analytical derivations that make the computations fast and\nnumerical standard errors small. As an empirical example, monetary models are\ncompared using heteroskedasticity as an additional device for identification.\nThe empirical results support models with money in the interest rate reaction\nfunction.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.08167v1"
    },
    {
        "title": "Model instability in predictive exchange rate regressions",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper we aim to improve existing empirical exchange rate models by\naccounting for uncertainty with respect to the underlying structural\nrepresentation. Within a flexible Bayesian non-linear time series framework,\nour modeling approach assumes that different regimes are characterized by\ncommonly used structural exchange rate models, with their evolution being\ndriven by a Markov process. We assume a time-varying transition probability\nmatrix with transition probabilities depending on a measure of the monetary\npolicy stance of the central bank at the home and foreign country. We apply\nthis model to a set of eight exchange rates against the US dollar. In a\nforecasting exercise, we show that model evidence varies over time and a model\napproach that takes this empirical evidence seriously yields improvements in\naccuracy of density forecasts for most currency pairs considered.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.08818v2"
    },
    {
        "title": "Generalized Dynamic Factor Models and Volatilities: Consistency, rates,\n  and prediction intervals",
        "authors": [
            "Matteo Barigozzi",
            "Marc Hallin"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Volatilities, in high-dimensional panels of economic time series with a\ndynamic factor structure on the levels or returns, typically also admit a\ndynamic factor decomposition. We consider a two-stage dynamic factor model\nmethod recovering the common and idiosyncratic components of both levels and\nlog-volatilities. Specifically, in a first estimation step, we extract the\ncommon and idiosyncratic shocks for the levels, from which a log-volatility\nproxy is computed. In a second step, we estimate a dynamic factor model, which\nis equivalent to a multiplicative factor structure for volatilities, for the\nlog-volatility panel. By exploiting this two-stage factor approach, we build\none-step-ahead conditional prediction intervals for large $n \\times T$ panels\nof returns. Those intervals are based on empirical quantiles, not on\nconditional variances; they can be either equal- or unequal- tailed. We provide\nuniform consistency and consistency rates results for the proposed estimators\nas both $n$ and $T$ tend to infinity. We study the finite-sample properties of\nour estimators by means of Monte Carlo simulations. Finally, we apply our\nmethodology to a panel of asset returns belonging to the S&P100 index in order\nto compute one-step-ahead conditional prediction intervals for the period\n2006-2013. A comparison with the componentwise GARCH benchmark (which does not\ntake advantage of cross-sectional information) demonstrates the superiority of\nour approach, which is genuinely multivariate (and high-dimensional),\nnonparametric, and model-free.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.10045v2"
    },
    {
        "title": "LM-BIC Model Selection in Semiparametric Models",
        "authors": [
            "Ivan Korolev"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper studies model selection in semiparametric econometric models. It\ndevelops a consistent series-based model selection procedure based on a\nBayesian Information Criterion (BIC) type criterion to select between several\nclasses of models. The procedure selects a model by minimizing the\nsemiparametric Lagrange Multiplier (LM) type test statistic from Korolev (2018)\nbut additionally rewards simpler models. The paper also develops consistent\nupward testing (UT) and downward testing (DT) procedures based on the\nsemiparametric LM type specification test. The proposed semiparametric LM-BIC\nand UT procedures demonstrate good performance in simulations. To illustrate\nthe use of these semiparametric model selection procedures, I apply them to the\nparametric and semiparametric gasoline demand specifications from Yatchew and\nNo (2001). The LM-BIC procedure selects the semiparametric specification that\nis nonparametric in age but parametric in all other variables, which is in line\nwith the conclusions in Yatchew and No (2001). The results of the UT and DT\nprocedures heavily depend on the choice of tuning parameters and assumptions\nabout the model errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.10676v1"
    },
    {
        "title": "A Residual Bootstrap for Conditional Expected Shortfall",
        "authors": [
            "Alexander Heinemann",
            "Sean Telg"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper studies a fixed-design residual bootstrap method for the two-step\nestimator of Francq and Zako\\\"ian (2015) associated with the conditional\nExpected Shortfall. For a general class of volatility models the bootstrap is\nshown to be asymptotically valid under the conditions imposed by Beutner et al.\n(2018). A simulation study is conducted revealing that the average coverage\nrates are satisfactory for most settings considered. There is no clear evidence\nto have a preference for any of the three proposed bootstrap intervals. This\ncontrasts results in Beutner et al. (2018) for the VaR, for which the\nreversed-tails interval has a superior performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.11557v1"
    },
    {
        "title": "Nonparametric Instrumental Variables Estimation Under Misspecification",
        "authors": [
            "Ben Deaner"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Nonparametric Instrumental Variables (NPIV) analysis is based on a\nconditional moment restriction. We show that if this moment condition is even\nslightly misspecified, say because instruments are not quite valid, then NPIV\nestimates can be subject to substantial asymptotic error and the identified set\nunder a relaxed moment condition may be large. Imposing strong a priori\nsmoothness restrictions mitigates the problem but induces bias if the\nrestrictions are too strong. In order to manage this trade-off we develop a\nmethods for empirical sensitivity analysis and apply them to the consumer\ndemand data previously analyzed in Blundell (2007) and Horowitz (2011).\n",
        "pdf_link": "http://arxiv.org/pdf/1901.01241v7"
    },
    {
        "title": "Shrinkage for Categorical Regressors",
        "authors": [
            "Phillip Heiler",
            "Jana Mareckova"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper introduces a flexible regularization approach that reduces point\nestimation risk of group means stemming from e.g. categorical regressors,\n(quasi-)experimental data or panel data models. The loss function is penalized\nby adding weighted squared l2-norm differences between group location\nparameters and informative first-stage estimates. Under quadratic loss, the\npenalized estimation problem has a simple interpretable closed-form solution\nthat nests methods established in the literature on ridge regression,\ndiscretized support smoothing kernels and model averaging methods. We derive\nrisk-optimal penalty parameters and propose a plug-in approach for estimation.\nThe large sample properties are analyzed in an asymptotic local to zero\nframework by introducing a class of sequences for close and distant systems of\nlocations that is sufficient for describing a large range of data generating\nprocesses. We provide the asymptotic distributions of the shrinkage estimators\nunder different penalization schemes. The proposed plug-in estimator uniformly\ndominates the ordinary least squares in terms of asymptotic risk if the number\nof groups is larger than three. Monte Carlo simulations reveal robust\nimprovements over standard methods in finite samples. Real data examples of\nestimating time trends in a panel and a difference-in-differences study\nillustrate potential applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.01898v1"
    },
    {
        "title": "lassopack: Model selection and prediction with regularized regression in\n  Stata",
        "authors": [
            "Achim Ahrens",
            "Christian B. Hansen",
            "Mark E. Schaffer"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This article introduces lassopack, a suite of programs for regularized\nregression in Stata. lassopack implements lasso, square-root lasso, elastic\nnet, ridge regression, adaptive lasso and post-estimation OLS. The methods are\nsuitable for the high-dimensional setting where the number of predictors $p$\nmay be large and possibly greater than the number of observations, $n$. We\noffer three different approaches for selecting the penalization (`tuning')\nparameters: information criteria (implemented in lasso2), $K$-fold\ncross-validation and $h$-step ahead rolling cross-validation for cross-section,\npanel and time-series data (cvlasso), and theory-driven (`rigorous')\npenalization for the lasso and square-root lasso for cross-section and panel\ndata (rlasso). We discuss the theoretical framework and practical\nconsiderations for each approach. We also present Monte Carlo results to\ncompare the performance of the penalization approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.05397v1"
    },
    {
        "title": "Relaxing the Exclusion Restriction in Shift-Share Instrumental Variable\n  Estimation",
        "authors": [
            "Nicolas Apfel"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Many economic studies use shift-share instruments to estimate causal effects.\nOften, all shares need to fulfil an exclusion restriction, making the\nidentifying assumption strict. This paper proposes to use methods that relax\nthe exclusion restriction by selecting invalid shares. I apply the methods in\ntwo empirical examples: the effect of immigration on wages and of Chinese\nimport exposure on employment. In the first application, the coefficient\nbecomes lower and often changes sign, but this is reconcilable with arguments\nmade in the literature. In the second application, the findings are mostly\nrobust to the use of the new methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.00222v4"
    },
    {
        "title": "Simulation smoothing for nowcasting with large mixed-frequency VARs",
        "authors": [
            "Sebastian Ankargren",
            "Paulina Jonéus"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  There is currently an increasing interest in large vector autoregressive\n(VAR) models. VARs are popular tools for macroeconomic forecasting and use of\nlarger models has been demonstrated to often improve the forecasting ability\ncompared to more traditional small-scale models. Mixed-frequency VARs deal with\ndata sampled at different frequencies while remaining within the realms of\nVARs. Estimation of mixed-frequency VARs makes use of simulation smoothing, but\nusing the standard procedure these models quickly become prohibitive in\nnowcasting situations as the size of the model grows. We propose two algorithms\nthat alleviate the computational efficiency of the simulation smoothing\nalgorithm. Our preferred choice is an adaptive algorithm, which augments the\nstate vector as necessary to sample also monthly variables that are missing at\nthe end of the sample. For large VARs, we find considerable improvements in\nspeed using our adaptive algorithm. The algorithm therefore provides a crucial\nbuilding block for bringing the mixed-frequency VARs to the high-dimensional\nregime.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.01075v1"
    },
    {
        "title": "Heterogeneous Choice Sets and Preferences",
        "authors": [
            "Levon Barseghyan",
            "Maura Coughlin",
            "Francesca Molinari",
            "Joshua C. Teitelbaum"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a robust method of discrete choice analysis when agents' choice\nsets are unobserved. Our core model assumes nothing about agents' choice sets\napart from their minimum size. Importantly, it leaves unrestricted the\ndependence, conditional on observables, between choice sets and preferences. We\nfirst characterize the sharp identification region of the model's parameters by\na finite set of conditional moment inequalities. We then apply our theoretical\nfindings to learn about households' risk preferences and choice sets from data\non their deductible choices in auto collision insurance. We find that the data\ncan be explained by expected utility theory with low levels of risk aversion\nand heterogeneous non-singleton choice sets, and that more than three in four\nhouseholds require limited choice sets to explain their deductible choices. We\nalso provide simulation evidence on the computational tractability of our\nmethod in applications with larger feasible sets or higher-dimensional\nunobserved heterogeneity.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.02337v2"
    },
    {
        "title": "Random Forest Estimation of the Ordered Choice Model",
        "authors": [
            "Michael Lechner",
            "Gabriel Okasa"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In this paper we develop a new machine learning estimator for ordered choice\nmodels based on the random forest. The proposed Ordered Forest flexibly\nestimates the conditional choice probabilities while taking the ordering\ninformation explicitly into account. In addition to common machine learning\nestimators, it enables the estimation of marginal effects as well as conducting\ninference and thus provides the same output as classical econometric\nestimators. An extensive simulation study reveals a good predictive\nperformance, particularly in settings with non-linearities and\nnear-multicollinearity. An empirical application contrasts the estimation of\nmarginal effects and their standard errors with an ordered logit model. A\nsoftware implementation of the Ordered Forest is provided both in R and Python\nin the package orf available on CRAN and PyPI, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.02436v3"
    },
    {
        "title": "On the residues vectors of a rational class of complex functions.\n  Application to autoregressive processes",
        "authors": [
            "Guillermo Daniel Scheidereiter",
            "Omar Roberto Faure"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Complex functions have multiple uses in various fields of study, so analyze\ntheir characteristics it is of extensive interest to other sciences. This work\nbegins with a particular class of rational functions of a complex variable;\nover this is deduced two elementals properties concerning the residues and is\nproposed one results which establishes one lower bound for the p-norm of the\nresidues vector. Applications to the autoregressive processes are presented and\nthe exemplifications are indicated in historical data of electric generation\nand econometric series.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.05949v1"
    },
    {
        "title": "Testing for Quantile Sample Selection",
        "authors": [
            "Valentina Corradi",
            "Daniel Gutknecht"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper provides tests for detecting sample selection in nonparametric\nconditional quantile functions. The first test is an omitted predictor test\nwith the propensity score as the omitted variable. As with any omnibus test, in\nthe case of rejection we cannot distinguish between rejection due to genuine\nselection or to misspecification. Thus, we suggest a second test to provide\nsupporting evidence whether the cause for rejection at the first stage was\nsolely due to selection or not. Using only individuals with propensity score\nclose to one, this second test relies on an `identification at infinity'\nargument, but accommodates cases of irregular identification. Importantly,\nneither of the two tests requires parametric assumptions on the selection\nequation nor a continuous exclusion restriction. Data-driven bandwidth\nprocedures are proposed, and Monte Carlo evidence suggests a good finite sample\nperformance in particular of the first test. Finally, we also derive an\nextension of the first test to nonparametric conditional mean functions, and\napply our procedure to test for selection in log hourly wages using UK Family\nExpenditure Survey data as \\citet{AB2017}.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.07412v5"
    },
    {
        "title": "Testing for Unobserved Heterogeneity via k-means Clustering",
        "authors": [
            "Andrew J. Patton",
            "Brian M. Weller"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Clustering methods such as k-means have found widespread use in a variety of\napplications. This paper proposes a formal testing procedure to determine\nwhether a null hypothesis of a single cluster, indicating homogeneity of the\ndata, can be rejected in favor of multiple clusters. The test is simple to\nimplement, valid under relatively mild conditions (including non-normality, and\nheterogeneity of the data in aspects beyond those in the clustering analysis),\nand applicable in a range of contexts (including clustering when the time\nseries dimension is small, or clustering on parameters other than the mean). We\nverify that the test has good size control in finite samples, and we illustrate\nthe test in applications to clustering vehicle manufacturers and U.S. mutual\nfunds.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.07582v1"
    },
    {
        "title": "X-model: further development and possible modifications",
        "authors": [
            "Sergei Kulakov"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Despite its critical importance, the famous X-model elaborated by Ziel and\nSteinert (2016) has neither bin been widely studied nor further developed. And\nyet, the possibilities to improve the model are as numerous as the fields it\ncan be applied to. The present paper takes advantage of a technique proposed by\nCoulon et al. (2014) to enhance the X-model. Instead of using the wholesale\nsupply and demand curves as inputs for the model, we rely on the transformed\nversions of these curves with a perfectly inelastic demand. As a result,\ncomputational requirements of our X-model reduce and its forecasting power\nincreases substantially. Moreover, our X-model becomes more robust towards\noutliers present in the initial auction curves data.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.09206v1"
    },
    {
        "title": "Rebuttal of \"On Nonparametric Identification of Treatment Effects in\n  Duration Models\"",
        "authors": [
            "Jaap H. Abbring",
            "Gerard J. van den Berg"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In their IZA Discussion Paper 10247, Johansson and Lee claim that the main\nresult (Proposition 3) in Abbring and Van den Berg (2003b) does not hold. We\nshow that their claim is incorrect. At a certain point within their line of\nreasoning, they make a rather basic error while transforming one random\nvariable into another random variable, and this leads them to draw incorrect\nconclusions. As a result, their paper can be discarded.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.09886v1"
    },
    {
        "title": "Testing for time-varying properties under misspecified conditional mean\n  and variance",
        "authors": [
            "Daiki Maki",
            "Yasushi Ota"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This study examines statistical performance of tests for time-varying\nproperties under misspecified conditional mean and variance. When we test for\ntime-varying properties of the conditional mean in the case in which data have\nno time-varying mean but have time-varying variance, asymptotic tests have size\ndistortions. This is improved by the use of a bootstrap method. Similarly, when\nwe test for time-varying properties of the conditional variance in the case in\nwhich data have time-varying mean but no time-varying variance, asymptotic\ntests have large size distortions. This is not improved even by the use of\nbootstrap methods. We show that tests for time-varying properties of the\nconditional mean by the bootstrap are robust regardless of the time-varying\nvariance model, whereas tests for time-varying properties of the conditional\nvariance do not perform well in the presence of misspecified time-varying mean.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12107v2"
    },
    {
        "title": "Robust tests for ARCH in the presence of the misspecified conditional\n  mean: A comparison of nonparametric approches",
        "authors": [
            "Daiki Maki",
            "Yasushi Ota"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This study compares statistical properties of ARCH tests that are robust to\nthe presence of the misspecified conditional mean. The approaches employed in\nthis study are based on two nonparametric regressions for the conditional mean.\nFirst is the ARCH test using Nadayara-Watson kernel regression. Second is the\nARCH test using the polynomial approximation regression. The two approaches do\nnot require specification of the conditional mean and can adapt to various\nnonlinear models, which are unknown a priori. Accordingly, they are robust to\nmisspecified conditional mean models. Simulation results show that ARCH tests\nbased on the polynomial approximation regression approach have better\nstatistical properties than ARCH tests using Nadayara-Watson kernel regression\napproach for various nonlinear models.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12752v2"
    },
    {
        "title": "A Comparison of First-Difference and Forward Orthogonal Deviations GMM",
        "authors": [
            "Robert F. Phillips"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper provides a necessary and sufficient instruments condition assuring\ntwo-step generalized method of moments (GMM) based on the forward orthogonal\ndeviations transformation is numerically equivalent to two-step GMM based on\nthe first-difference transformation. The condition also tells us when system\nGMM, based on differencing, can be computed using forward orthogonal\ndeviations. Additionally, it tells us when forward orthogonal deviations and\ndifferencing do not lead to the same GMM estimator. When estimators based on\nthese two transformations differ, Monte Carlo simulations indicate that\nestimators based on forward orthogonal deviations have better finite sample\nproperties than estimators based on differencing.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12880v1"
    },
    {
        "title": "An econometric analysis of the Italian cultural supply",
        "authors": [
            "Consuelo Nava",
            "Maria Grazia Zoia"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Price indexes in time and space is a most relevant topic in statistical\nanalysis from both the methodological and the application side. In this paper a\nprice index providing a novel and effective solution to price indexes over\nseveral periods and among several countries, that is in both a multi-period and\na multilateral framework, is devised. The reference basket of the devised index\nis the union of the intersections of the baskets of all periods/countries in\npairs. As such, it provides a broader coverage than usual indexes. Index\nclosed-form expressions and updating formulas are provided and properties\ninvestigated. Last, applications with real and simulated data provide evidence\nof the performance of the index at stake.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.00073v3"
    },
    {
        "title": "Informational Content of Factor Structures in Simultaneous Binary\n  Response Models",
        "authors": [
            "Shakeeb Khan",
            "Arnaud Maurel",
            "Yichong Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We study the informational content of factor structures in discrete\ntriangular systems. Factor structures have been employed in a variety of\nsettings in cross sectional and panel data models, and in this paper we\nformally quantify their identifying power in a bivariate system often employed\nin the treatment effects literature. Our main findings are that imposing a\nfactor structure yields point identification of parameters of interest, such as\nthe coefficient associated with the endogenous regressor in the outcome\nequation, under weaker assumptions than usually required in these models. In\nparticular, we show that a \"non-standard\" exclusion restriction that requires\nan explanatory variable in the outcome equation to be excluded from the\ntreatment equation is no longer necessary for identification, even in cases\nwhere all of the regressors from the outcome equation are discrete. We also\nestablish identification of the coefficient of the endogenous regressor in\nmodels with more general factor structures, in situations where one has access\nto at least two continuous measurements of the common factor.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.01318v3"
    },
    {
        "title": "Identification and Estimation of SVARMA models with Independent and\n  Non-Gaussian Inputs",
        "authors": [
            "Bernd Funovits"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper analyzes identifiability properties of structural vector\nautoregressive moving average (SVARMA) models driven by independent and\nnon-Gaussian shocks. It is well known, that SVARMA models driven by Gaussian\nerrors are not identified without imposing further identifying restrictions on\nthe parameters. Even in reduced form and assuming stability and invertibility,\nvector autoregressive moving average models are in general not identified\nwithout requiring certain parameter matrices to be non-singular. Independence\nand non-Gaussianity of the shocks is used to show that they are identified up\nto permutations and scalings. In this way, typically imposed identifying\nrestrictions are made testable. Furthermore, we introduce a maximum-likelihood\nestimator of the non-Gaussian SVARMA model which is consistent and\nasymptotically normally distributed.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.04087v1"
    },
    {
        "title": "Identifiability of Structural Singular Vector Autoregressive Models",
        "authors": [
            "Bernd Funovits",
            "Alexander Braumann"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We generalize well-known results on structural identifiability of vector\nautoregressive models (VAR) to the case where the innovation covariance matrix\nhas reduced rank. Structural singular VAR models appear, for example, as\nsolutions of rational expectation models where the number of shocks is usually\nsmaller than the number of endogenous variables, and as an essential building\nblock in dynamic factor models. We show that order conditions for\nidentifiability are misleading in the singular case and provide a rank\ncondition for identifiability of the noise parameters. Since the Yule-Walker\nequations may have multiple solutions, we analyze the effect of restrictions on\nthe system parameters on over- and underidentification in detail and provide\neasily verifiable conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.04096v2"
    },
    {
        "title": "Averaging estimation for instrumental variables quantile regression",
        "authors": [
            "Xin Liu"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper proposes averaging estimation methods to improve the finite-sample\nefficiency of the instrumental variables quantile regression (IVQR) estimation.\nFirst, I apply Cheng, Liao, Shi's (2019) averaging GMM framework to the IVQR\nmodel. I propose using the usual quantile regression moments for averaging to\ntake advantage of cases when endogeneity is not too strong. I also propose\nusing two-stage least squares slope moments to take advantage of cases when\nheterogeneity is not too strong. The empirical optimal weight formula of Cheng\net al. (2019) helps optimize the bias-variance tradeoff, ensuring uniformly\nbetter (asymptotic) risk of the averaging estimator over the standard IVQR\nestimator under certain conditions. My implementation involves many\ncomputational considerations and builds on recent developments in the quantile\nliterature. Second, I propose a bootstrap method that directly averages among\nIVQR, quantile regression, and two-stage least squares estimators. More\nspecifically, I find the optimal weights in the bootstrap world and then apply\nthe bootstrap-optimal weights to the original sample. The bootstrap method is\nsimpler to compute and generally performs better in simulations, but it lacks\nthe formal uniform dominance results of Cheng et al. (2019). Simulation results\ndemonstrate that in the multiple-regressors/instruments case, both the GMM\naveraging and bootstrap estimators have uniformly smaller risk than the IVQR\nestimator across data-generating processes (DGPs) with all kinds of\ncombinations of different endogeneity levels and heterogeneity levels. In DGPs\nwith a single endogenous regressor and instrument, where averaging estimation\nis known to have least opportunity for improvement, the proposed averaging\nestimators outperform the IVQR estimator in some cases but not others.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.04245v1"
    },
    {
        "title": "Matrix Completion, Counterfactuals, and Factor Analysis of Missing Data",
        "authors": [
            "Jushan Bai",
            "Serena Ng"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper proposes an imputation procedure that uses the factors estimated\nfrom a tall block along with the re-rotated loadings estimated from a wide\nblock to impute missing values in a panel of data. Assuming that a strong\nfactor structure holds for the full panel of data and its sub-blocks, it is\nshown that the common component can be consistently estimated at four different\nrates of convergence without requiring regularization or iteration. An\nasymptotic analysis of the estimation error is obtained. An application of our\nanalysis is estimation of counterfactuals when potential outcomes have a factor\nstructure. We study the estimation of average and individual treatment effects\non the treated and establish a normal distribution theory that can be useful\nfor hypothesis testing.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.06677v5"
    },
    {
        "title": "Standard Errors for Panel Data Models with Unknown Clusters",
        "authors": [
            "Jushan Bai",
            "Sung Hoon Choi",
            "Yuan Liao"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper develops a new standard-error estimator for linear panel data\nmodels. The proposed estimator is robust to heteroskedasticity, serial\ncorrelation, and cross-sectional correlation of unknown forms. The serial\ncorrelation is controlled by the Newey-West method. To control for\ncross-sectional correlations, we propose to use the thresholding method,\nwithout assuming the clusters to be known. We establish the consistency of the\nproposed estimator. Monte Carlo simulations show the method works well. An\nempirical application is considered.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.07406v2"
    },
    {
        "title": "Econometric Models of Network Formation",
        "authors": [
            "Aureo de Paula"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This article provides a selective review on the recent literature on\neconometric models of network formation. The survey starts with a brief\nexposition on basic concepts and tools for the statistical description of\nnetworks. I then offer a review of dyadic models, focussing on statistical\nmodels on pairs of nodes and describe several developments of interest to the\neconometrics literature. The article also presents a discussion of non-dyadic\nmodels where link formation might be influenced by the presence or absence of\nadditional links, which themselves are subject to similar influences. This is\nrelated to the statistical literature on conditionally specified models and the\neconometrics of game theoretical models. I close with a (non-exhaustive)\ndiscussion of potential areas for further development.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.07781v2"
    },
    {
        "title": "Forecasting under Long Memory and Nonstationarity",
        "authors": [
            "Uwe Hassler",
            "Marc-Oliver Pohle"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Long memory in the sense of slowly decaying autocorrelations is a stylized\nfact in many time series from economics and finance. The fractionally\nintegrated process is the workhorse model for the analysis of these time\nseries. Nevertheless, there is mixed evidence in the literature concerning its\nusefulness for forecasting and how forecasting based on it should be\nimplemented.\n  Employing pseudo-out-of-sample forecasting on inflation and realized\nvolatility time series and simulations we show that methods based on fractional\nintegration clearly are superior to alternative methods not accounting for long\nmemory, including autoregressions and exponential smoothing. Our proposal of\nchoosing a fixed fractional integration parameter of $d=0.5$ a priori yields\nthe best results overall, capturing long memory behavior, but overcoming the\ndeficiencies of methods using an estimated parameter.\n  Regarding the implementation of forecasting methods based on fractional\nintegration, we use simulations to compare local and global semiparametric and\nparametric estimators of the long memory parameter from the Whittle family and\nprovide asymptotic theory backed up by simulations to compare different mean\nestimators. Both of these analyses lead to new results, which are also of\ninterest outside the realm of forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08202v1"
    },
    {
        "title": "Large Dimensional Latent Factor Modeling with Missing Observations and\n  Applications to Causal Inference",
        "authors": [
            "Ruoxuan Xiong",
            "Markus Pelger"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper develops the inferential theory for latent factor models estimated\nfrom large dimensional panel data with missing observations. We propose an\neasy-to-use all-purpose estimator for a latent factor model by applying\nprincipal component analysis to an adjusted covariance matrix estimated from\npartially observed panel data. We derive the asymptotic distribution for the\nestimated factors, loadings and the imputed values under an approximate factor\nmodel and general missing patterns. The key application is to estimate\ncounterfactual outcomes in causal inference from panel data. The unobserved\ncontrol group is modeled as missing values, which are inferred from the latent\nfactor model. The inferential theory for the imputed values allows us to test\nfor individual treatment effects at any time under general adoption patterns\nwhere the units can be affected by unobserved factors.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08273v6"
    },
    {
        "title": "Feasible Generalized Least Squares for Panel Data with Cross-sectional\n  and Serial Correlations",
        "authors": [
            "Jushan Bai",
            "Sung Hoon Choi",
            "Yuan Liao"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper considers generalized least squares (GLS) estimation for linear\npanel data models. By estimating the large error covariance matrix\nconsistently, the proposed feasible GLS (FGLS) estimator is more efficient than\nthe ordinary least squares (OLS) in the presence of heteroskedasticity, serial,\nand cross-sectional correlations. To take into account the serial correlations,\nwe employ the banding method. To take into account the cross-sectional\ncorrelations, we suggest to use the thresholding method. We establish the\nlimiting distribution of the proposed estimator. A Monte Carlo study is\nconsidered. The proposed method is applied to an empirical application.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.09004v3"
    },
    {
        "title": "Quasi Maximum Likelihood Estimation of Non-Stationary Large Approximate\n  Dynamic Factor Models",
        "authors": [
            "Matteo Barigozzi",
            "Matteo Luciani"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper considers estimation of large dynamic factor models with common\nand idiosyncratic trends by means of the Expectation Maximization algorithm,\nimplemented jointly with the Kalman smoother. We show that, as the\ncross-sectional dimension $n$ and the sample size $T$ diverge to infinity, the\ncommon component for a given unit estimated at a given point in time is\n$\\min(\\sqrt n,\\sqrt T)$-consistent. The case of local levels and/or local\nlinear trends trends is also considered. By means of a MonteCarlo simulation\nexercise, we compare our approach with estimators based on principal component\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.09841v1"
    },
    {
        "title": "Nonparametric identification of an interdependent value model with buyer\n  covariates from first-price auction bids",
        "authors": [
            "Nathalie Gimenes",
            "Emmanuel Guerre"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper introduces a version of the interdependent value model of Milgrom\nand Weber (1982), where the signals are given by an index gathering signal\nshifters observed by the econometrician and private ones specific to each\nbidders. The model primitives are shown to be nonparametrically identified from\nfirst-price auction bids under a testable mild rank condition. Identification\nholds for all possible signal values. This allows to consider a wide range of\ncounterfactuals where this is important, as expected revenue in second-price\nauction. An estimation procedure is briefly discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.10646v1"
    },
    {
        "title": "Estimating a Large Covariance Matrix in Time-varying Factor Models",
        "authors": [
            "Jaeheon Jung"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper deals with the time-varying high dimensional covariance matrix\nestimation. We propose two covariance matrix estimators corresponding with a\ntime-varying approximate factor model and a time-varying approximate\ncharacteristic-based factor model, respectively. The models allow the factor\nloadings, factor covariance matrix, and error covariance matrix to change\nsmoothly over time. We study the rate of convergence of each estimator. Our\nsimulation and empirical study indicate that time-varying covariance matrix\nestimators generally perform better than time-invariant covariance matrix\nestimators. Also, if characteristics are available that genuinely explain true\nloadings, the characteristics can be used to estimate loadings more precisely\nin finite samples; their helpfulness increases when loadings rapidly change.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.11965v1"
    },
    {
        "title": "Identification of Random Coefficient Latent Utility Models",
        "authors": [
            "Roy Allen",
            "John Rehbeck"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper provides nonparametric identification results for random\ncoefficient distributions in perturbed utility models. We cover discrete and\ncontinuous choice models. We establish identification using variation in mean\nquantities, and the results apply when an analyst observes aggregate demands\nbut not whether goods are chosen together. We require exclusion restrictions\nand independence between random slope coefficients and random intercepts. We do\nnot require regressors to have large supports or parametric assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.00276v1"
    },
    {
        "title": "Equal Predictive Ability Tests Based on Panel Data with Applications to\n  OECD and IMF Forecasts",
        "authors": [
            "Oguzhan Akgun",
            "Alain Pirotte",
            "Giovanni Urga",
            "Zhenlin Yang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose two types of equal predictive ability (EPA) tests with panels to\ncompare the predictions made by two forecasters. The first type, namely\n$S$-statistics, focuses on the overall EPA hypothesis which states that the EPA\nholds on average over all panel units and over time. The second, called\n$C$-statistics, focuses on the clustered EPA hypothesis where the EPA holds\njointly for a fixed number of clusters of panel units. The asymptotic\nproperties of the proposed tests are evaluated under weak and strong\ncross-sectional dependence. An extensive Monte Carlo simulation shows that the\nproposed tests have very good finite sample properties even with little\ninformation about the cross-sectional dependence in the data. The proposed\nframework is applied to compare the economic growth forecasts of the OECD and\nthe IMF, and to evaluate the performance of the consumer price inflation\nforecasts of the IMF.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.02803v3"
    },
    {
        "title": "Double Machine Learning based Program Evaluation under Unconfoundedness",
        "authors": [
            "Michael C. Knaus"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper reviews, applies and extends recently proposed methods based on\nDouble Machine Learning (DML) with a focus on program evaluation under\nunconfoundedness. DML based methods leverage flexible prediction models to\nadjust for confounding variables in the estimation of (i) standard average\neffects, (ii) different forms of heterogeneous effects, and (iii) optimal\ntreatment assignment rules. An evaluation of multiple programs of the Swiss\nActive Labour Market Policy illustrates how DML based methods enable a\ncomprehensive program evaluation. Motivated by extreme individualised treatment\neffect estimates of the DR-learner, we propose the normalised DR-learner\n(NDR-learner) to address this issue. The NDR-learner acknowledges that\nindividualised effect estimates can be stabilised by an individualised\nnormalisation of inverse probability weights.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.03191v5"
    },
    {
        "title": "Unit Root Testing with Slowly Varying Trends",
        "authors": [
            "Sven Otto"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  A unit root test is proposed for time series with a general nonlinear\ndeterministic trend component. It is shown that asymptotically the pooled OLS\nestimator of overlapping blocks filters out any trend component that satisfies\nsome Lipschitz condition. Under both fixed-$b$ and small-$b$ block asymptotics,\nthe limiting distribution of the t-statistic for the unit root hypothesis is\nderived. Nuisance parameter corrections provide heteroskedasticity-robust\ntests, and serial correlation is accounted for by pre-whitening. A Monte Carlo\nstudy that considers slowly varying trends yields both good size and improved\npower results for the proposed tests when compared to conventional unit root\ntests.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.04066v3"
    },
    {
        "title": "Identification and Estimation of Weakly Separable Models Without\n  Monotonicity",
        "authors": [
            "Songnian Chen",
            "Shakeeb Khan",
            "Xun Tang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study the identification and estimation of treatment effect parameters in\nweakly separable models. In their seminal work, Vytlacil and Yildiz (2007)\nshowed how to identify and estimate the average treatment effect of a dummy\nendogenous variable when the outcome is weakly separable in a single index.\nTheir identification result builds on a monotonicity condition with respect to\nthis single index. In comparison, we consider similar weakly separable models\nwith multiple indices, and relax the monotonicity condition for identification.\nUnlike Vytlacil and Yildiz (2007), we exploit the full information in the\ndistribution of the outcome variable, instead of just its mean. Indeed, when\nthe outcome distribution function is more informative than the mean, our method\nis applicable to more general settings than theirs; in particular we do not\nrely on their monotonicity assumption and at the same time we also allow for\nmultiple indices. To illustrate the advantage of our approach, we provide\nexamples of models where our approach can identify parameters of interest\nwhereas existing methods would fail. These examples include models with\nmultiple unobserved disturbance terms such as the Roy model and multinomial\nchoice models with dummy endogenous variables, as well as potential outcome\nmodels with endogenous random coefficients. Our method is easy to implement and\ncan be applied to a wide class of models. We establish standard asymptotic\nproperties such as consistency and asymptotic normality.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.04337v2"
    },
    {
        "title": "Causal Spillover Effects Using Instrumental Variables",
        "authors": [
            "Gonzalo Vazquez-Bare"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  I set up a potential outcomes framework to analyze spillover effects using\ninstrumental variables. I characterize the population compliance types in a\nsetting in which spillovers can occur on both treatment take-up and outcomes,\nand provide conditions for identification of the marginal distribution of\ncompliance types. I show that intention-to-treat (ITT) parameters aggregate\nmultiple direct and spillover effects for different compliance types, and hence\ndo not have a clear link to causally interpretable parameters. Moreover,\nrescaling ITT parameters by first-stage estimands generally recovers a weighted\ncombination of average effects where the sum of weights is larger than one. I\nthen analyze identification of causal direct and spillover effects under\none-sided noncompliance, and show that causal effects can be estimated by 2SLS\nin this case. I illustrate the proposed methods using data from an experiment\non social interactions and voting behavior. I also introduce an alternative\nassumption, independence of peers' types, that identifies parameters of\ninterest under two-sided noncompliance by restricting the amount of\nheterogeneity in average potential outcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.06023v5"
    },
    {
        "title": "A Correlated Random Coefficient Panel Model with Time-Varying\n  Endogeneity",
        "authors": [
            "Louise Laage"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper studies a class of linear panel models with random coefficients.\nWe do not restrict the joint distribution of the time-invariant unobserved\nheterogeneity and the covariates. We investigate identification of the average\npartial effect (APE) when fixed-effect techniques cannot be used to control for\nthe correlation between the regressors and the time-varying disturbances.\nRelying on control variables, we develop a constructive two-step identification\nargument. The first step identifies nonparametrically the conditional\nexpectation of the disturbances given the regressors and the control variables,\nand the second step uses ``between-group'' variations, correcting for\nendogeneity, to identify the APE. We propose a natural semiparametric estimator\nof the APE, show its $\\sqrt{n}$ asymptotic normality and compute its asymptotic\nvariance. The estimator is computationally easy to implement, and Monte Carlo\nsimulations show favorable finite sample properties. Control variables arise in\nvarious economic and econometric models, and we propose applications of our\nargument in several models. As an empirical illustration, we estimate the\naverage elasticity of intertemporal substitution in a labor supply model with\nrandom coefficients.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.09367v2"
    },
    {
        "title": "Rationalizing Rational Expectations: Characterization and Tests",
        "authors": [
            "Xavier D'Haultfoeuille",
            "Christophe Gaillac",
            "Arnaud Maurel"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper, we build a new test of rational expectations based on the\nmarginal distributions of realizations and subjective beliefs. This test is\nwidely applicable, including in the common situation where realizations and\nbeliefs are observed in two different datasets that cannot be matched. We show\nthat whether one can rationalize rational expectations is equivalent to the\ndistribution of realizations being a mean-preserving spread of the distribution\nof beliefs. The null hypothesis can then be rewritten as a system of many\nmoment inequality and equality constraints, for which tests have been recently\ndeveloped in the literature. The test is robust to measurement errors under\nsome restrictions and can be extended to account for aggregate shocks. Finally,\nwe apply our methodology to test for rational expectations about future\nearnings. While individuals tend to be right on average about their future\nearnings, our test strongly rejects rational expectations.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.11537v3"
    },
    {
        "title": "Targeting predictors in random forest regression",
        "authors": [
            "Daniel Borup",
            "Bent Jesper Christensen",
            "Nicolaj Nørgaard Mühlbach",
            "Mikkel Slot Nielsen"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Random forest regression (RF) is an extremely popular tool for the analysis\nof high-dimensional data. Nonetheless, its benefits may be lessened in sparse\nsettings due to weak predictors, and a pre-estimation dimension reduction\n(targeting) step is required. We show that proper targeting controls the\nprobability of placing splits along strong predictors, thus providing an\nimportant complement to RF's feature sampling. This is supported by simulations\nusing representative finite samples. Moreover, we quantify the immediate gain\nfrom targeting in terms of increased strength of individual trees.\nMacroeconomic and financial applications show that the bias-variance trade-off\nimplied by targeting, due to increased correlation among trees in the forest,\nis balanced at a medium degree of targeting, selecting the best 10--30\\% of\ncommonly applied predictors. Improvements in predictive accuracy of targeted RF\nrelative to ordinary RF are considerable, up to 12-13\\%, occurring both in\nrecessions and expansions, particularly at long horizons.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.01411v4"
    },
    {
        "title": "Double Debiased Machine Learning Nonparametric Inference with Continuous\n  Treatments",
        "authors": [
            "Kyle Colangelo",
            "Ying-Ying Lee"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose a doubly robust inference method for causal effects of continuous\ntreatment variables, under unconfoundedness and with nonparametric or\nhigh-dimensional nuisance functions. Our double debiased machine learning (DML)\nestimators for the average dose-response function (or the average structural\nfunction) and the partial effects are asymptotically normal with non-parametric\nconvergence rates. The first-step estimators for the nuisance conditional\nexpectation function and the conditional density can be nonparametric or ML\nmethods. Utilizing a kernel-based doubly robust moment function and\ncross-fitting, we give high-level conditions under which the nuisance function\nestimators do not affect the first-order large sample distribution of the DML\nestimators. We provide sufficient low-level conditions for kernel, series, and\ndeep neural networks. We justify the use of kernel to localize the continuous\ntreatment at a given value by the Gateaux derivative. We implement various ML\nmethods in Monte Carlo simulations and an empirical application on a job\ntraining program evaluation\n",
        "pdf_link": "http://arxiv.org/pdf/2004.03036v8"
    },
    {
        "title": "Inference in Unbalanced Panel Data Models with Interactive Fixed Effects",
        "authors": [
            "Daniel Czarnowske",
            "Amrei Stammann"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this article, we study the limiting behavior of Bai (2009)'s interactive\nfixed effects estimator in the presence of randomly missing data. In extensive\nsimulation experiments, we show that the inferential theory derived by Bai\n(2009) and Moon and Weidner (2017) approximates the behavior of the estimator\nfairly well. However, we find that the fraction and pattern of randomly missing\ndata affect the performance of the estimator. Additionally, we use the\ninteractive fixed effects estimator to reassess the baseline analysis of\nAcemoglu et al. (2019). Allowing for a more general form of unobserved\nheterogeneity as the authors, we confirm significant effects of democratization\non growth.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.03414v1"
    },
    {
        "title": "Bias optimal vol-of-vol estimation: the role of window overlapping",
        "authors": [
            "Giacomo Toscano",
            "Maria Cristina Recchioni"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We derive a feasible criterion for the bias-optimal selection of the tuning\nparameters involved in estimating the integrated volatility of the spot\nvolatility via the simple realized estimator by Barndorff-Nielsen and Veraart\n(2009). Our analytic results are obtained assuming that the spot volatility is\na continuous mean-reverting process and that consecutive local windows for\nestimating the spot volatility are allowed to overlap in a finite sample\nsetting. Moreover, our analytic results support some optimal selections of\ntuning parameters prescribed in the literature, based on numerical evidence.\nInterestingly, it emerges that window-overlapping is crucial for optimizing the\nfinite-sample bias of volatility-of-volatility estimates.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.04013v2"
    },
    {
        "title": "On the Factors Influencing the Choices of Weekly Telecommuting\n  Frequencies of Post-secondary Students in Toronto",
        "authors": [
            "Khandker Nurul Habib",
            "Ph. D.",
            " PEng"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The paper presents an empirical investigation of telecommuting frequency\nchoices by post-secondary students in Toronto. It uses a dataset collected\nthrough a large-scale travel survey conducted on post-secondary students of\nfour major universities in Toronto and it employs multiple alternative\neconometric modelling techniques for the empirical investigation. Results\ncontribute on two fronts. Firstly, it presents empirical investigations of\nfactors affecting telecommuting frequency choices of post-secondary students\nthat are rare in literature. Secondly, it identifies better a performing\neconometric modelling technique for modelling telecommuting frequency choices.\nEmpirical investigation clearly reveals that telecommuting for school related\nactivities is prevalent among post-secondary students in Toronto. Around 80\npercent of 0.18 million of the post-secondary students of the region, who make\nroughly 36,000 trips per day, also telecommute at least once a week.\nConsidering that large numbers of students need to spend a long time travelling\nfrom home to campus with around 33 percent spending more than two hours a day\non travelling, telecommuting has potential to enhance their quality of life.\nEmpirical investigations reveal that car ownership and living farther from the\ncampus have similar positive effects on the choice of higher frequency of\ntelecommuting. Students who use a bicycle for regular travel are least likely\nto telecommute, compared to those using transit or a private car.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.04683v1"
    },
    {
        "title": "Wild Bootstrap Inference for Penalized Quantile Regression for\n  Longitudinal Data",
        "authors": [
            "Carlos Lamarche",
            "Thomas Parker"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The existing theory of penalized quantile regression for longitudinal data\nhas focused primarily on point estimation. In this work, we investigate\nstatistical inference. We propose a wild residual bootstrap procedure and show\nthat it is asymptotically valid for approximating the distribution of the\npenalized estimator. The model puts no restrictions on individual effects, and\nthe estimator achieves consistency by letting the shrinkage decay in importance\nasymptotically. The new method is easy to implement and simulation studies show\nthat it has accurate small sample behavior in comparison with existing\nprocedures. Finally, we illustrate the new approach using U.S. Census data to\nestimate a model that includes more than eighty thousand parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.05127v3"
    },
    {
        "title": "The direct and spillover effects of a nationwide socio-emotional\n  learning program for disruptive students",
        "authors": [
            "Clément de Chaisemartin",
            "Nicolás Navarrete H."
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Social and emotional learning (SEL) programs teach disruptive students to\nimprove their classroom behavior. Small-scale programs in high-income countries\nhave been shown to improve treated students' behavior and academic outcomes.\nUsing a randomized experiment, we show that a nationwide SEL program in Chile\nhas no effect on eligible students. We find evidence that very disruptive\nstudents may hamper the program's effectiveness. ADHD, a disorder correlated\nwith disruptiveness, is much more prevalent in Chile than in high-income\ncountries, so very disruptive students may be more present in Chile than in the\ncontexts where SEL programs have been shown to work.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.08126v1"
    },
    {
        "title": "Loss aversion and the welfare ranking of policy interventions",
        "authors": [
            "Sergio Firpo",
            "Antonio F. Galvao",
            "Martyna Kobus",
            "Thomas Parker",
            "Pedro Rosa-Dias"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper develops theoretical criteria and econometric methods to rank\npolicy interventions in terms of welfare when individuals are loss-averse. Our\nnew criterion for \"loss aversion-sensitive dominance\" defines a weak partial\nordering of the distributions of policy-induced gains and losses. It applies to\nthe class of welfare functions which model individual preferences with\nnon-decreasing and loss-averse attitudes towards changes in outcomes. We also\ndevelop new statistical methods to test loss aversion-sensitive dominance in\npractice, using nonparametric plug-in estimates; these allow inference to be\nconducted through a special resampling procedure. Since point-identification of\nthe distribution of policy-induced gains and losses may require strong\nassumptions, we extend our comparison criteria, test statistics, and resampling\nprocedures to the partially-identified case. We illustrate our methods with a\nsimple empirical application to the welfare comparison of alternative income\nsupport programs in the US.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.08468v4"
    },
    {
        "title": "Estimating High-Dimensional Discrete Choice Model of Differentiated\n  Products with Random Coefficients",
        "authors": [
            "Masayuki Sawada",
            "Kohei Kawaguchi"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose an estimation procedure for discrete choice models of\ndifferentiated products with possibly high-dimensional product attributes. In\nour model, high-dimensional attributes can be determinants of both mean and\nvariance of the indirect utility of a product. The key restriction in our model\nis that the high-dimensional attributes affect the variance of indirect\nutilities only through finitely many indices. In a framework of the\nrandom-coefficients logit model, we show a bound on the error rate of a\n$l_1$-regularized minimum distance estimator and prove the asymptotic linearity\nof the de-biased estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.08791v1"
    },
    {
        "title": "Awareness of crash risk improves Kelly strategies in simulated financial\n  time series",
        "authors": [
            "Jan-Christian Gerlach",
            "Jerome Kreuser",
            "Didier Sornette"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We simulate a simplified version of the price process including bubbles and\ncrashes proposed in Kreuser and Sornette (2018). The price process is defined\nas a geometric random walk combined with jumps modelled by separate, discrete\ndistributions associated with positive (and negative) bubbles. The key\ningredient of the model is to assume that the sizes of the jumps are\nproportional to the bubble size. Thus, the jumps tend to efficiently bring back\nexcess bubble prices close to a normal or fundamental value (efficient\ncrashes). This is different from existing processes studied that assume jumps\nthat are independent of the mispricing. The present model is simplified\ncompared to Kreuser and Sornette (2018) in that we ignore the possibility of a\nchange of the probability of a crash as the price accelerates above the normal\nprice. We study the behaviour of investment strategies that maximize the\nexpected log of wealth (Kelly criterion) for the risky asset and a risk-free\nasset. We show that the method behaves similarly to Kelly on Geometric Brownian\nMotion in that it outperforms other methods in the long-term and it beats\nclassical Kelly. As a primary source of outperformance, we determine knowledge\nabout the presence of crashes, but interestingly find that knowledge of only\nthe size, and not the time of occurrence, already provides a significant and\nrobust edge. We then perform an error analysis to show that the method is\nrobust with respect to variations in the parameters. The method is most\nsensitive to errors in the expected return.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.09368v1"
    },
    {
        "title": "Does Subjective Well-being Contribute to Our Understanding of Mexican\n  Well-being?",
        "authors": [
            "Jeremy Heald",
            "Erick Treviño Aguilar"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The article reviews the history of well-being to gauge how subjective\nquestion surveys can improve our understanding of well-being in Mexico. The\nresearch uses data at the level of the 32 federal entities or States, taking\nadvantage of the heterogeneity in development indicator readings between and\nwithin geographical areas, the product of socioeconomic inequality. The data\ncome principally from two innovative subjective questionnaires, BIARE and\nENVIPE, which intersect in their fully representative state-wide applications\nin 2014, but also from conventional objective indicator sources such as the HDI\nand conventional surveys. This study uses two approaches, a descriptive\nanalysis of a state-by-state landscape of indicators, both subjective and\nobjective, in an initial search for stand-out well-being patterns, and an\neconometric study of a large selection of mainly subjective indicators inspired\nby theory and the findings of previous Mexican research. Descriptive analysis\nconfirms that subjective well-being correlates strongly with and complements\nobjective data, providing interesting directions for analysis. The econometrics\nliterature indicates that happiness increases with income and satisfying of\nmaterial needs as theory suggests, but also that Mexicans are relatively happy\nconsidering their mediocre incomes and high levels of insecurity, the last of\nwhich, by categorizing according to satisfaction with life, can be shown to\nimpact poorer people disproportionately. The article suggests that well-being\nis a complex, multidimensional construct which can be revealed by using\nexploratory multi-regression and partial correlations models which juxtapose\nsubjective and objective indicators.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11420v1"
    },
    {
        "title": "Microeconometrics with Partial Identification",
        "authors": [
            "Francesca Molinari"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This chapter reviews the microeconometrics literature on partial\nidentification, focusing on the developments of the last thirty years. The\ntopics presented illustrate that the available data combined with credible\nmaintained assumptions may yield much information about a parameter of\ninterest, even if they do not reveal it exactly. Special attention is devoted\nto discussing the challenges associated with, and some of the solutions put\nforward to, (1) obtain a tractable characterization of the values for the\nparameters of interest which are observationally equivalent, given the\navailable data and maintained assumptions; (2) estimate this set of values; (3)\nconduct test of hypotheses and make confidence statements. The chapter reviews\nadvances in partial identification analysis both as applied to learning\n(functionals of) probability distributions that are well-defined in the absence\nof models, as well as to learning parameters that are well-defined only in the\ncontext of particular models. A simple organizing principle is highlighted: the\nsource of the identification problem can often be traced to a collection of\nrandom variables that are consistent with the available data and maintained\nassumptions. This collection may be part of the observed data or be a model\nimplication. In either case, it can be formalized as a random set. Random set\ntheory is then used as a mathematical framework to unify a number of special\nresults and produce a general methodology to carry out partial identification\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11751v1"
    },
    {
        "title": "Sensitivity to Calibrated Parameters",
        "authors": [
            "Thomas H. Jørgensen"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  A common approach to estimation of economic models is to calibrate a sub-set\nof model parameters and keep them fixed when estimating the remaining\nparameters. Calibrated parameters likely affect conclusions based on the model\nbut estimation time often makes a systematic investigation of the sensitivity\nto calibrated parameters infeasible. I propose a simple and computationally\nlow-cost measure of the sensitivity of parameters and other objects of interest\nto the calibrated parameters. In the main empirical application, I revisit the\nanalysis of life-cycle savings motives in Gourinchas and Parker (2002) and show\nthat some estimates are sensitive to calibrations.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12100v2"
    },
    {
        "title": "Inference with Many Weak Instruments",
        "authors": [
            "Anna Mikusheva",
            "Liyang Sun"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We develop a concept of weak identification in linear IV models in which the\nnumber of instruments can grow at the same rate or slower than the sample size.\nWe propose a jackknifed version of the classical weak identification-robust\nAnderson-Rubin (AR) test statistic. Large-sample inference based on the\njackknifed AR is valid under heteroscedasticity and weak identification. The\nfeasible version of this statistic uses a novel variance estimator. The test\nhas uniformly correct size and good power properties. We also develop a\npre-test for weak identification that is related to the size property of a Wald\ntest based on the Jackknife Instrumental Variable Estimator (JIVE). This new\npre-test is valid under heteroscedasticity and with many instruments.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12445v3"
    },
    {
        "title": "State Dependence and Unobserved Heterogeneity in the Extensive Margin of\n  Trade",
        "authors": [
            "Julian Hinz",
            "Amrei Stammann",
            "Joschka Wanner"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study the role and drivers of persistence in the extensive margin of\nbilateral trade. Motivated by a stylized heterogeneous firms model of\ninternational trade with market entry costs, we consider dynamic three-way\nfixed effects binary choice models and study the corresponding incidental\nparameter problem. The standard maximum likelihood estimator is consistent\nunder asymptotics where all panel dimensions grow at a constant rate, but it\nhas an asymptotic bias in its limiting distribution, invalidating inference\neven in situations where the bias appears to be small. Thus, we propose two\ndifferent bias-corrected estimators. Monte Carlo simulations confirm their\ndesirable statistical properties. We apply these estimators in a reassessment\nof the most commonly studied determinants of the extensive margin of trade.\nBoth true state dependence and unobserved heterogeneity contribute considerably\nto trade persistence and taking this persistence into account matters\nsignificantly in identifying the effects of trade policies on the extensive\nmargin.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12655v2"
    },
    {
        "title": "Measuring wage inequality under right censoring",
        "authors": [
            "João Nicolau",
            "Pedro Raposo",
            "Paulo M. M. Rodrigues"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper we investigate potential changes which may have occurred over\nthe last two decades in the probability mass of the right tail of the wage\ndistribution, through the analysis of the corresponding tail index. In\nspecific, a conditional tail index estimator is introduced which explicitly\nallows for right tail censoring (top-coding), which is a feature of the widely\nused current population survey (CPS), as well as of other surveys. Ignoring the\ntop-coding may lead to inconsistent estimates of the tail index and to under or\nover statements of inequality and of its evolution over time. Thus, having a\ntail index estimator that explicitly accounts for this sample characteristic is\nof importance to better understand and compute the tail index dynamics in the\ncensored right tail of the wage distribution. The contribution of this paper is\nthreefold: i) we introduce a conditional tail index estimator that explicitly\nhandles the top-coding problem, and evaluate its finite sample performance and\ncompare it with competing methods; ii) we highlight that the factor values used\nto adjust the top-coded wage have changed over time and depend on the\ncharacteristics of individuals, occupations and industries, and propose\nsuitable values; and iii) we provide an in-depth empirical analysis of the\ndynamics of the US wage distribution's right tail using the public-use CPS\ndatabase from 1992 to 2017.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12856v1"
    },
    {
        "title": "The Interaction Between Credit Constraints and Uncertainty Shocks",
        "authors": [
            "Pratiti Chatterjee",
            "David Gunawan",
            "Robert Kohn"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Can uncertainty about credit availability trigger a slowdown in real\nactivity? This question is answered by using a novel method to identify shocks\nto uncertainty in access to credit. Time-variation in uncertainty about credit\navailability is estimated using particle Markov Chain Monte Carlo. We extract\nshocks to time-varying credit uncertainty and decompose it into two parts: the\nfirst captures the \"pure\" effect of a shock to the second moment; the second\ncaptures total effects of uncertainty including effects on the first moment.\nUsing state-dependent local projections, we find that the \"pure\" effect by\nitself generates a sharp slowdown in real activity and the effects are largely\ncountercyclical. We feed the estimated shocks into a flexible price real\nbusiness cycle model with a collateral constraint and show that when the\ncollateral constraint binds, an uncertainty shock about credit access is\nrecessionary leading to a simultaneous decline in consumption, investment, and\noutput.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.14719v1"
    },
    {
        "title": "When are Google data useful to nowcast GDP? An approach via\n  pre-selection and shrinkage",
        "authors": [
            "Laurent Ferrara",
            "Anna Simoni"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Alternative data sets are widely used for macroeconomic nowcasting together\nwith machine learning--based tools. The latter are often applied without a\ncomplete picture of their theoretical nowcasting properties. Against this\nbackground, this paper proposes a theoretically grounded nowcasting methodology\nthat allows researchers to incorporate alternative Google Search Data (GSD)\namong the predictors and that combines targeted preselection, Ridge\nregularization, and Generalized Cross Validation. Breaking with most existing\nliterature, which focuses on asymptotic in-sample theoretical properties, we\nestablish the theoretical out-of-sample properties of our methodology and\nsupport them by Monte-Carlo simulations. We apply our methodology to GSD to\nnowcast GDP growth rate of several countries during various economic periods.\nOur empirical findings support the idea that GSD tend to increase nowcasting\naccuracy, even after controlling for official variables, but that the gain\ndiffers between periods of recessions and of macroeconomic stability.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.00273v3"
    },
    {
        "title": "Forecasting with Bayesian Grouped Random Effects in Panel Data",
        "authors": [
            "Boyuan Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper, we estimate and leverage latent constant group structure to\ngenerate the point, set, and density forecasts for short dynamic panel data. We\nimplement a nonparametric Bayesian approach to simultaneously identify\ncoefficients and group membership in the random effects which are heterogeneous\nacross groups but fixed within a group. This method allows us to flexibly\nincorporate subjective prior knowledge on the group structure that potentially\nimproves the predictive accuracy. In Monte Carlo experiments, we demonstrate\nthat our Bayesian grouped random effects (BGRE) estimators produce accurate\nestimates and score predictive gains over standard panel data estimators. With\na data-driven group structure, the BGRE estimators exhibit comparable accuracy\nof clustering with the Kmeans algorithm and outperform a two-step Bayesian\ngrouped estimator whose group structure relies on Kmeans. In the empirical\nanalysis, we apply our method to forecast the investment rate across a broad\nrange of firms and illustrate that the estimated latent group structure\nimproves forecasts relative to standard panel data estimators.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.02435v8"
    },
    {
        "title": "Spectral Targeting Estimation of $λ$-GARCH models",
        "authors": [
            "Simon Hetland"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper presents a novel estimator of orthogonal GARCH models, which\ncombines (eigenvalue and -vector) targeting estimation with stepwise\n(univariate) estimation. We denote this the spectral targeting estimator. This\ntwo-step estimator is consistent under finite second order moments, while\nasymptotic normality holds under finite fourth order moments. The estimator is\nespecially well suited for modelling larger portfolios: we compare the\nempirical performance of the spectral targeting estimator to that of the quasi\nmaximum likelihood estimator for five portfolios of 25 assets. The spectral\ntargeting estimator dominates in terms of computational complexity, being up to\n57 times faster in estimation, while both estimators produce similar\nout-of-sample forecasts, indicating that the spectral targeting estimator is\nwell suited for high-dimensional empirical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.02588v1"
    },
    {
        "title": "Teacher-to-classroom assignment and student achievement",
        "authors": [
            "Bryan S. Graham",
            "Geert Ridder",
            "Petra Thiemann",
            "Gema Zamarro"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study the effects of counterfactual teacher-to-classroom assignments on\naverage student achievement in elementary and middle schools in the US. We use\nthe Measures of Effective Teaching (MET) experiment to semiparametrically\nidentify the average reallocation effects (AREs) of such assignments. Our\nfindings suggest that changes in within-district teacher assignments could have\nappreciable effects on student achievement. Unlike policies which require\nhiring additional teachers (e.g., class-size reduction measures), or those\naimed at changing the stock of teachers (e.g., VAM-guided teacher tenure\npolicies), alternative teacher-to-classroom assignments are resource neutral;\nthey raise student achievement through a more efficient deployment of existing\nteachers.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.02653v2"
    },
    {
        "title": "Optimal Decision Rules for Weak GMM",
        "authors": [
            "Isaiah Andrews",
            "Anna Mikusheva"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper studies optimal decision rules, including estimators and tests,\nfor weakly identified GMM models. We derive the limit experiment for weakly\nidentified GMM, and propose a theoretically-motivated class of priors which\ngive rise to quasi-Bayes decision rules as a limiting case. Together with\nresults in the previous literature, this establishes desirable properties for\nthe quasi-Bayes approach regardless of model identification status, and we\nrecommend quasi-Bayes for settings where identification is a concern. We\nfurther propose weighted average power-optimal identification-robust\nfrequentist tests and confidence sets, and prove a Bernstein-von Mises-type\nresult for the quasi-Bayes posterior under weak identification.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.04050v7"
    },
    {
        "title": "Talents from Abroad. Foreign Managers and Productivity in the United\n  Kingdom",
        "authors": [
            "Dimitrios Exadaktylos",
            "Massimo Riccaboni",
            "Armando Rungi"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper, we test the contribution of foreign management on firms'\ncompetitiveness. We use a novel dataset on the careers of 165,084 managers\nemployed by 13,106 companies in the United Kingdom in the period 2009-2017. We\nfind that domestic manufacturing firms become, on average, between 7% and 12%\nmore productive after hiring the first foreign managers, whereas foreign-owned\nfirms register no significant improvement. In particular, we test that previous\nindustry-specific experience is the primary driver of productivity gains in\ndomestic firms (15.6%), in a way that allows the latter to catch up with\nforeign-owned firms. Managers from the European Union are highly valuable, as\nthey represent about half of the recruits in our data. Our identification\nstrategy combines matching techniques, difference-in-difference, and\npre-recruitment trends to challenge reverse causality. Results are robust to\nplacebo tests and to different estimators of Total Factor Productivity.\nEventually, we argue that upcoming limits to the mobility of foreign talents\nafter the Brexit event can hamper the allocation of productive managerial\nresources.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.04055v1"
    },
    {
        "title": "Difference-in-Differences Estimators of Intertemporal Treatment Effects",
        "authors": [
            "Clément de Chaisemartin",
            "Xavier D'Haultfœuille"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study treatment-effect estimation using panel data. The treatment may be\nnon-binary, non-absorbing, and the outcome may be affected by treatment lags.\nWe make a parallel-trends assumption, and propose event-study estimators of the\neffect of being exposed to a weakly higher treatment dose for $\\ell$ periods.\nWe also propose normalized estimators, that estimate a weighted average of the\neffects of the current treatment and its lags. We also analyze commonly-used\ntwo-way-fixed-effects regressions. Unlike our estimators, they can be biased in\nthe presence of heterogeneous treatment effects. A local-projection version of\nthose regressions is biased even with homogeneous effects.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.04267v13"
    },
    {
        "title": "Efficient Covariate Balancing for the Local Average Treatment Effect",
        "authors": [
            "Phillip Heiler"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper develops an empirical balancing approach for the estimation of\ntreatment effects under two-sided noncompliance using a binary conditionally\nindependent instrumental variable. The method weighs both treatment and outcome\ninformation with inverse probabilities to produce exact finite sample balance\nacross instrument level groups. It is free of functional form assumptions on\nthe outcome or the treatment selection step. By tailoring the loss function for\nthe instrument propensity scores, the resulting treatment effect estimates\nexhibit both low bias and a reduced variance in finite samples compared to\nconventional inverse probability weighting methods. The estimator is\nautomatically weight normalized and has similar bias properties compared to\nconventional two-stage least squares estimation under constant causal effects\nfor the compliers. We provide conditions for asymptotic normality and\nsemiparametric efficiency and demonstrate how to utilize additional information\nabout the treatment selection step for bias reduction in finite samples. The\nmethod can be easily combined with regularization or other statistical learning\napproaches to deal with a high-dimensional number of observed confounding\nvariables. Monte Carlo simulations suggest that the theoretical advantages\ntranslate well to finite samples. The method is illustrated in an empirical\nexample.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.04346v1"
    },
    {
        "title": "A Semiparametric Network Formation Model with Unobserved Linear\n  Heterogeneity",
        "authors": [
            "Luis E. Candelaria"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper analyzes a semiparametric model of network formation in the\npresence of unobserved agent-specific heterogeneity. The objective is to\nidentify and estimate the preference parameters associated with homophily on\nobserved attributes when the distributions of the unobserved factors are not\nparametrically specified. This paper offers two main contributions to the\nliterature on network formation. First, it establishes a new point\nidentification result for the vector of parameters that relies on the existence\nof a special repressor. The identification proof is constructive and\ncharacterizes a closed-form for the parameter of interest. Second, it\nintroduces a simple two-step semiparametric estimator for the vector of\nparameters with a first-step kernel estimator. The estimator is computationally\ntractable and can be applied to both dense and sparse networks. Moreover, I\nshow that the estimator is consistent and has a limiting normal distribution as\nthe number of individuals in the network increases. Monte Carlo experiments\ndemonstrate that the estimator performs well in finite samples and in networks\nwith different levels of sparsity.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.05403v2"
    },
    {
        "title": "Persistence in Financial Connectedness and Systemic Risk",
        "authors": [
            "Jozef Barunik",
            "Michael Ellington"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper characterises dynamic linkages arising from shocks with\nheterogeneous degrees of persistence. Using frequency domain techniques, we\nintroduce measures that identify smoothly varying links of a transitory and\npersistent nature. Our approach allows us to test for statistical differences\nin such dynamic links. We document substantial differences in transitory and\npersistent linkages among US financial industry volatilities, argue that they\ntrack heterogeneously persistent sources of systemic risk, and thus may serve\nas a useful tool for market participants.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.07842v4"
    },
    {
        "title": "Global Representation of the Conditional LATE Model: A Separability\n  Result",
        "authors": [
            "Yu-Chang Chen",
            "Haitian Xie"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper studies the latent index representation of the conditional LATE\nmodel, making explicit the role of covariates in treatment selection. We find\nthat if the directions of the monotonicity condition are the same across all\nvalues of the conditioning covariate, which is often assumed in the literature,\nthen the treatment choice equation has to satisfy a separability condition\nbetween the instrument and the covariate. This global representation result\nestablishes testable restrictions imposed on the way covariates enter the\ntreatment choice equation. We later extend the representation theorem to\nincorporate multiple ordered levels of treatment.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.08106v3"
    },
    {
        "title": "Government spending and multi-category treatment effects:The modified\n  conditional independence assumption",
        "authors": [
            "Koiti Yano"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  I devise a novel approach to evaluate the effectiveness of fiscal policy in\nthe short run with multi-category treatment effects and inverse probability\nweighting based on the potential outcome framework. This study's main\ncontribution to the literature is the proposed modified conditional\nindependence assumption to improve the evaluation of fiscal policy. Using this\napproach, I analyze the effects of government spending on the US economy from\n1992 to 2019. The empirical study indicates that large fiscal contraction\ngenerates a negative effect on the economic growth rate, and small and large\nfiscal expansions realize a positive effect. However, these effects are not\nsignificant in the traditional multiple regression approach. I conclude that\nthis new approach significantly improves the evaluation of fiscal policy.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.08396v3"
    },
    {
        "title": "Permutation-based tests for discontinuities in event studies",
        "authors": [
            "Federico A. Bugni",
            "Jia Li",
            "Qiyuan Li"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose using a permutation test to detect discontinuities in an\nunderlying economic model at a known cutoff point. Relative to the existing\nliterature, we show that this test is well suited for event studies based on\ntime-series data. The test statistic measures the distance between the\nempirical distribution functions of observed data in two local subsamples on\nthe two sides of the cutoff. Critical values are computed via a standard\npermutation algorithm. Under a high-level condition that the observed data can\nbe coupled by a collection of conditionally independent variables, we establish\nthe asymptotic validity of the permutation test, allowing the sizes of the\nlocal subsamples to be either be fixed or grow to infinity. In the latter case,\nwe also establish that the permutation test is consistent. We demonstrate that\nour high-level condition can be verified in a broad range of problems in the\ninfill asymptotic time-series setting, which justifies using the permutation\ntest to detect jumps in economic variables such as volatility, trading\nactivity, and liquidity. These potential applications are illustrated in an\nempirical case study for selected FOMC announcements during the ongoing\nCOVID-19 pandemic.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.09837v4"
    },
    {
        "title": "The Mode Treatment Effect",
        "authors": [
            "Neng-Chieh Chang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Mean, median, and mode are three essential measures of the centrality of\nprobability distributions. In program evaluation, the average treatment effect\n(mean) and the quantile treatment effect (median) have been intensively studied\nin the past decades. The mode treatment effect, however, has long been\nneglected in program evaluation. This paper fills the gap by discussing both\nthe estimation and inference of the mode treatment effect. I propose both\ntraditional kernel and machine learning methods to estimate the mode treatment\neffect. I also derive the asymptotic properties of the proposed estimators and\nfind that both estimators follow the asymptotic normality but with the rate of\nconvergence slower than the regular rate $\\sqrt{N}$, which is different from\nthe rates of the classical average and quantile treatment effect estimators.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11606v1"
    },
    {
        "title": "Scalable Bayesian estimation in the multinomial probit model",
        "authors": [
            "Ruben Loaiza-Maya",
            "Didier Nibbering"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The multinomial probit model is a popular tool for analyzing choice behaviour\nas it allows for correlation between choice alternatives. Because current model\nspecifications employ a full covariance matrix of the latent utilities for the\nchoice alternatives, they are not scalable to a large number of choice\nalternatives. This paper proposes a factor structure on the covariance matrix,\nwhich makes the model scalable to large choice sets. The main challenge in\nestimating this structure is that the model parameters require identifying\nrestrictions. We identify the parameters by a trace-restriction on the\ncovariance matrix, which is imposed through a reparametrization of the factor\nstructure. We specify interpretable prior distributions on the model parameters\nand develop an MCMC sampler for parameter estimation. The proposed approach\nsignificantly improves performance in large choice sets relative to existing\nmultinomial probit specifications. Applications to purchase data show the\neconomic importance of including a large number of choice alternatives in\nconsumer choice analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.13247v2"
    },
    {
        "title": "Unconditional Quantile Regression with High Dimensional Data",
        "authors": [
            "Yuya Sasaki",
            "Takuya Ura",
            "Yichong Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper considers estimation and inference for heterogeneous\ncounterfactual effects with high-dimensional data. We propose a novel robust\nscore for debiased estimation of the unconditional quantile regression (Firpo,\nFortin, and Lemieux, 2009) as a measure of heterogeneous counterfactual\nmarginal effects. We propose a multiplier bootstrap inference and develop\nasymptotic theories to guarantee the size control in large sample. Simulation\nstudies support our theories. Applying the proposed method to Job Corps survey\ndata, we find that a policy which counterfactually extends the duration of\nexposures to the Job Corps training program will be effective especially for\nthe targeted subpopulations of lower potential wage earners.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.13659v4"
    },
    {
        "title": "Local Projection Inference is Simpler and More Robust Than You Think",
        "authors": [
            "José Luis Montiel Olea",
            "Mikkel Plagborg-Møller"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Applied macroeconomists often compute confidence intervals for impulse\nresponses using local projections, i.e., direct linear regressions of future\noutcomes on current covariates. This paper proves that local projection\ninference robustly handles two issues that commonly arise in applications:\nhighly persistent data and the estimation of impulse responses at long\nhorizons. We consider local projections that control for lags of the variables\nin the regression. We show that lag-augmented local projections with normal\ncritical values are asymptotically valid uniformly over (i) both stationary and\nnon-stationary data, and also over (ii) a wide range of response horizons.\nMoreover, lag augmentation obviates the need to correct standard errors for\nserial correlation in the regression residuals. Hence, local projection\ninference is arguably both simpler than previously thought and more robust than\nstandard autoregressive inference, whose validity is known to depend\nsensitively on the persistence of the data and on the length of the horizon.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.13888v3"
    },
    {
        "title": "Robust and Efficient Estimation of Potential Outcome Means under Random\n  Assignment",
        "authors": [
            "Akanksha Negi",
            "Jeffrey M. Wooldridge"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study efficiency improvements in randomized experiments for estimating a\nvector of potential outcome means using regression adjustment (RA) when there\nare more than two treatment levels. We show that linear RA which estimates\nseparate slopes for each assignment level is never worse, asymptotically, than\nusing the subsample averages. We also show that separate RA improves over\npooled RA except in the obvious case where slope parameters in the linear\nprojections are identical across the different assignment levels. We further\ncharacterize the class of nonlinear RA methods that preserve consistency of the\npotential outcome means despite arbitrary misspecification of the conditional\nmean functions. Finally, we apply these regression adjustment techniques to\nefficiently estimate the lower bound mean willingness to pay for an oil spill\nprevention program in California.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.01800v2"
    },
    {
        "title": "Testing homogeneity in dynamic discrete games in finite samples",
        "authors": [
            "Federico A. Bugni",
            "Jackson Bunting",
            "Takuya Ura"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The literature on dynamic discrete games often assumes that the conditional\nchoice probabilities and the state transition probabilities are homogeneous\nacross markets and over time. We refer to this as the \"homogeneity assumption\"\nin dynamic discrete games. This assumption enables empirical studies to\nestimate the game's structural parameters by pooling data from multiple markets\nand from many time periods. In this paper, we propose a hypothesis test to\nevaluate whether the homogeneity assumption holds in the data. Our hypothesis\ntest is the result of an approximate randomization test, implemented via a\nMarkov chain Monte Carlo (MCMC) algorithm. We show that our hypothesis test\nbecomes valid as the (user-defined) number of MCMC draws diverges, for any\nfixed number of markets, time periods, and players. We apply our test to the\nempirical study of the U.S.\\ Portland cement industry in Ryan (2012).\n",
        "pdf_link": "http://arxiv.org/pdf/2010.02297v3"
    },
    {
        "title": "Comment on Gouriéroux, Monfort, Renne (2019): Identification and\n  Estimation in Non-Fundamental Structural VARMA Models",
        "authors": [
            "Bernd Funovits"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This comment points out a serious flaw in the article \"Gouri\\'eroux, Monfort,\nRenne (2019): Identification and Estimation in Non-Fundamental Structural VARMA\nModels\" with regard to mirroring complex-valued roots with Blaschke polynomial\nmatrices. Moreover, the (non-) feasibility of the proposed method (if the\nhandling of Blaschke transformation were not prohibitive) for cross-sectional\ndimensions greater than two and vector moving average (VMA) polynomial matrices\nof degree greater than one is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.02711v1"
    },
    {
        "title": "Interpreting Unconditional Quantile Regression with Conditional\n  Independence",
        "authors": [
            "David M. Kaplan"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This note provides additional interpretation for the counterfactual outcome\ndistribution and corresponding unconditional quantile \"effects\" defined and\nestimated by Firpo, Fortin, and Lemieux (2009) and Chernozhukov,\nFern\\'andez-Val, and Melly (2013). With conditional independence of the policy\nvariable of interest, these methods estimate the policy effect for certain\ntypes of policies, but not others. In particular, they estimate the effect of a\npolicy change that itself satisfies conditional independence.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.03606v2"
    },
    {
        "title": "Consistent Specification Test of the Quantile Autoregression",
        "authors": [
            "Anthoulla Phella"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper proposes a test for the joint hypothesis of correct dynamic\nspecification and no omitted latent factors for the Quantile Autoregression. If\nthe composite null is rejected we proceed to disentangle the cause of\nrejection, i.e., dynamic misspecification or an omitted variable. We establish\nthe asymptotic distribution of the test statistics under fairly weak conditions\nand show that factor estimation error is negligible. A Monte Carlo study shows\nthat the suggested tests have good finite sample properties. Finally, we\nundertake an empirical illustration of modelling GDP growth and CPI inflation\nin the United Kingdom, where we find evidence that factor augmented models are\ncorrectly specified in contrast with their non-augmented counterparts when it\ncomes to GDP growth, while also exploring the asymmetric behaviour of the\ngrowth and inflation distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.03898v2"
    },
    {
        "title": "Identification of multi-valued treatment effects with unobserved\n  heterogeneity",
        "authors": [
            "Koki Fusejima"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper, we establish sufficient conditions for identifying treatment\neffects on continuous outcomes in endogenous and multi-valued discrete\ntreatment settings with unobserved heterogeneity. We employ the monotonicity\nassumption for multi-valued discrete treatments and instruments, and our\nidentification condition has a clear economic interpretation. In addition, we\nidentify the local treatment effects in multi-valued treatment settings and\nderive closed-form expressions of the identified treatment effects. We provide\nexamples to illustrate the usefulness of our result.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.04385v5"
    },
    {
        "title": "Asymptotic Properties of the Maximum Likelihood Estimator in\n  Regime-Switching Models with Time-Varying Transition Probabilities",
        "authors": [
            "Chaojun Li",
            "Yan Liu"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We prove the asymptotic properties of the maximum likelihood estimator (MLE)\nin time-varying transition probability (TVTP) regime-switching models. This\nclass of models extends the constant regime transition probability in\nMarkov-switching models to a time-varying probability by including information\nfrom observations. An important feature in this proof is the mixing rate of the\nregime process conditional on the observations, which is time varying owing to\nthe time-varying transition probabilities. Consistency and asymptotic normality\nfollow from the almost deterministic geometrically decaying bound of the mixing\nrate. The assumptions are verified in regime-switching autoregressive models\nwith widely-applied TVTP specifications. A simulation study examines the\nfinite-sample distributions of the MLE and compares the estimates of the\nasymptotic variance constructed from the Hessian matrix and the outer product\nof the score. The simulation results favour the latter. As an empirical\nexample, we compare three leading economic indicators in terms of describing\nU.S. industrial production.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.04930v3"
    },
    {
        "title": "Heteroscedasticity test of high-frequency data with jumps and\n  microstructure noise",
        "authors": [
            "Qiang Liu",
            "Zhi Liu",
            "Chuanhai Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper, we are interested in testing if the volatility process is\nconstant or not during a given time span by using high-frequency data with the\npresence of jumps and microstructure noise. Based on estimators of integrated\nvolatility and spot volatility, we propose a nonparametric way to depict the\ndiscrepancy between local variation and global variation. We show that our\nproposed test estimator converges to a standard normal distribution if the\nvolatility is constant, otherwise it diverges to infinity. Simulation studies\nverify the theoretical results and show a good finite sample performance of the\ntest procedure. We also apply our test procedure to do the heteroscedasticity\ntest for some real high-frequency financial data. We observe that in almost\nhalf of the days tested, the assumption of constant volatility within a day is\nviolated. And this is due to that the stock prices during opening and closing\nperiods are highly volatile and account for a relative large proportion of\nintraday variation.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.07659v1"
    },
    {
        "title": "Measures of Model Risk in Continuous-time Finance Models",
        "authors": [
            "Emese Lazar",
            "Shuyuan Qi",
            "Radu Tunaru"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Measuring model risk is required by regulators on financial and insurance\nmarkets. We separate model risk into parameter estimation risk and model\nspecification risk, and we propose expected shortfall type model risk measures\napplied to Levy jump models and affine jump-diffusion models. We investigate\nthe impact of parameter estimation risk and model specification risk on the\nmodels' ability to capture the joint dynamics of stock and option prices. We\nestimate the parameters using Markov chain Monte Carlo techniques, under the\nrisk-neutral probability measure and the real-world probability measure\njointly. We find strong evidence supporting modeling of price jumps.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08113v2"
    },
    {
        "title": "Synchronization analysis between exchange rates on the basis of\n  purchasing power parity using the Hilbert transform",
        "authors": [
            "Makoto Muto",
            "Yoshitaka Saiki"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Synchronization is a phenomenon in which a pair of fluctuations adjust their\nrhythms when interacting with each other. We measure the degree of\nsynchronization between the U.S. dollar (USD) and euro exchange rates and\nbetween the USD and Japanese yen exchange rates on the basis of purchasing\npower parity (PPP) over time. We employ a method of synchronization analysis\nusing the Hilbert transform, which is common in the field of nonlinear science.\nWe find that the degree of synchronization is high most of the time, suggesting\nthe establishment of PPP. The degree of synchronization does not remain high\nacross periods with economic events with asymmetric effects, such as the U.S.\nreal estate bubble.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08825v2"
    },
    {
        "title": "A Decomposition Approach to Counterfactual Analysis in Game-Theoretic\n  Models",
        "authors": [
            "Nathan Canen",
            "Kyungchul Song"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Decomposition methods are often used for producing counterfactual predictions\nin non-strategic settings. When the outcome of interest arises from a\ngame-theoretic setting where agents are better off by deviating from their\nstrategies after a new policy, such predictions, despite their practical\nsimplicity, are hard to justify. We present conditions in Bayesian games under\nwhich the decomposition-based predictions coincide with the equilibrium-based\nones. In many games, such coincidence follows from an invariance condition for\nequilibrium selection rules. To illustrate our message, we revisit an empirical\nanalysis in Ciliberto and Tamer (2009) on firms' entry decisions in the airline\nindustry.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08868v7"
    },
    {
        "title": "L2-Relaxation: With Applications to Forecast Combination and Portfolio\n  Analysis",
        "authors": [
            "Zhentao Shi",
            "Liangjun Su",
            "Tian Xie"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper tackles forecast combination with many forecasts or minimum\nvariance portfolio selection with many assets. A novel convex problem called\nL2-relaxation is proposed. In contrast to standard formulations, L2-relaxation\nminimizes the squared Euclidean norm of the weight vector subject to a set of\nrelaxed linear inequality constraints. The magnitude of relaxation, controlled\nby a tuning parameter, balances the bias and variance. When the\nvariance-covariance (VC) matrix of the individual forecast errors or financial\nassets exhibits latent group structures -- a block equicorrelation matrix plus\na VC for idiosyncratic noises, the solution to L2-relaxation delivers roughly\nequal within-group weights. Optimality of the new method is established under\nthe asymptotic framework when the number of the cross-sectional units $N$\npotentially grows much faster than the time dimension $T$. Excellent finite\nsample performance of our method is demonstrated in Monte Carlo simulations.\nIts wide applicability is highlighted in three real data examples concerning\nempirical applications of microeconomics, macroeconomics, and finance.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.09477v2"
    },
    {
        "title": "Time-varying Forecast Combination for High-Dimensional Data",
        "authors": [
            "Bin Chen",
            "Kenwin Maung"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper, we propose a new nonparametric estimator of time-varying\nforecast combination weights. When the number of individual forecasts is small,\nwe study the asymptotic properties of the local linear estimator. When the\nnumber of candidate forecasts exceeds or diverges with the sample size, we\nconsider penalized local linear estimation with the group SCAD penalty. We show\nthat the estimator exhibits the oracle property and correctly selects relevant\nforecasts with probability approaching one. Simulations indicate that the\nproposed estimators outperform existing combination schemes when structural\nchanges exist. Two empirical studies on inflation forecasting and equity\npremium prediction highlight the merits of our approach relative to other\npopular methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.10435v1"
    },
    {
        "title": "A Simple, Short, but Never-Empty Confidence Interval for Partially\n  Identified Parameters",
        "authors": [
            "Jörg Stoye"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper revisits the simple, but empirically salient, problem of inference\non a real-valued parameter that is partially identified through upper and lower\nbounds with asymptotically normal estimators. A simple confidence interval is\nproposed and is shown to have the following properties:\n  - It is never empty or awkwardly short, including when the sample analog of\nthe identified set is empty.\n  - It is valid for a well-defined pseudotrue parameter whether or not the\nmodel is well-specified.\n  - It involves no tuning parameters and minimal computation.\n  Computing the interval requires concentrating out one scalar nuisance\nparameter. In most cases, the practical result will be simple: To achieve 95%\ncoverage, report the union of a simple 90% (!) confidence interval for the\nidentified set and a standard 95% confidence interval for the pseudotrue\nparameter.\n  For uncorrelated estimators -- notably if bounds are estimated from distinct\nsubsamples -- and conventional coverage levels, validity of this simple\nprocedure can be shown analytically. The case obtains in the motivating\nempirical application (de Quidt, Haushofer, and Roth, 2018), in which\nimprovement over existing inference methods is demonstrated. More generally,\nsimulations suggest that the novel confidence interval has excellent length and\nsize control. This is partly because, in anticipation of never being empty, the\ninterval can be made shorter than conventional ones in relevant regions of\nsample space.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.10484v3"
    },
    {
        "title": "A Test for Kronecker Product Structure Covariance Matrix",
        "authors": [
            "Patrik Guggenberger",
            "Frank Kleibergen",
            "Sophocles Mavroeidis"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose a test for a covariance matrix to have Kronecker Product Structure\n(KPS). KPS implies a reduced rank restriction on a certain transformation of\nthe covariance matrix and the new procedure is an adaptation of the Kleibergen\nand Paap (2006) reduced rank test. To derive the limiting distribution of the\nWald type test statistic proves challenging partly because of the singularity\nof the covariance matrix estimator that appears in the weighting matrix. We\nshow that the test statistic has a chi square limiting null distribution with\ndegrees of freedom equal to the number of restrictions tested. Local asymptotic\npower results are derived. Monte Carlo simulations reveal good size and power\nproperties of the test. Re-examining fifteen highly cited papers conducting\ninstrumental variable regressions, we find that KPS is not rejected in 56 out\nof 118 specifications at the 5% nominal size.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.10961v4"
    },
    {
        "title": "Approximation-Robust Inference in Dynamic Discrete Choice",
        "authors": [
            "Ben Deaner"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Estimation and inference in dynamic discrete choice models often relies on\napproximation to lower the computational burden of dynamic programming.\nUnfortunately, the use of approximation can impart substantial bias in\nestimation and results in invalid confidence sets. We present a method for set\nestimation and inference that explicitly accounts for the use of approximation\nand is thus valid regardless of the approximation error. We show how one can\naccount for the error from approximation at low computational cost. Our\nmethodology allows researchers to assess the estimation error due to the use of\napproximation and thus more effectively manage the trade-off between bias and\ncomputational expedience. We provide simulation evidence to demonstrate the\npracticality of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.11482v1"
    },
    {
        "title": "Forecasting With Factor-Augmented Quantile Autoregressions: A Model\n  Averaging Approach",
        "authors": [
            "Anthoulla Phella"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper considers forecasts of the growth and inflation distributions of\nthe United Kingdom with factor-augmented quantile autoregressions under a model\naveraging framework. We investigate model combinations across models using\nweights that minimise the Akaike Information Criterion (AIC), the Bayesian\nInformation Criterion (BIC), the Quantile Regression Information Criterion\n(QRIC) as well as the leave-one-out cross validation criterion. The unobserved\nfactors are estimated by principal components of a large panel with N\npredictors over T periods under a recursive estimation scheme. We apply the\naforementioned methods to the UK GDP growth and CPI inflation rate. We find\nthat, on average, for GDP growth, in terms of coverage and final prediction\nerror, the equal weights or the weights obtained by the AIC and BIC perform\nequally well but are outperformed by the QRIC and the Jackknife approach on the\nmajority of the quantiles of interest. In contrast, the naive QAR(1) model of\ninflation outperforms all model averaging methodologies.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.12263v1"
    },
    {
        "title": "Low-Rank Approximations of Nonseparable Panel Models",
        "authors": [
            "Iván Fernández-Val",
            "Hugo Freeman",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We provide estimation methods for nonseparable panel models based on low-rank\nfactor structure approximations. The factor structures are estimated by\nmatrix-completion methods to deal with the computational challenges of\nprincipal component analysis in the presence of missing data. We show that the\nresulting estimators are consistent in large panels, but suffer from\napproximation and shrinkage biases. We correct these biases using matching and\ndifference-in-differences approaches. Numerical examples and an empirical\napplication to the effect of election day registration on voter turnout in the\nU.S. illustrate the properties and usefulness of our methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.12439v2"
    },
    {
        "title": "Consumer Theory with Non-Parametric Taste Uncertainty and Individual\n  Heterogeneity",
        "authors": [
            "Christopher Dobronyi",
            "Christian Gouriéroux"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We introduce two models of non-parametric random utility for demand systems:\nthe stochastic absolute risk aversion (SARA) model, and the stochastic\nsafety-first (SSF) model. In each model, individual-level heterogeneity is\ncharacterized by a distribution $\\pi\\in\\Pi$ of taste parameters, and\nheterogeneity across consumers is introduced using a distribution $F$ over the\ndistributions in $\\Pi$. Demand is non-separable and heterogeneity is\ninfinite-dimensional. Both models admit corner solutions. We consider two\nframeworks for estimation: a Bayesian framework in which $F$ is known, and a\nhyperparametric (or empirical Bayesian) framework in which $F$ is a member of a\nknown parametric family. Our methods are illustrated by an application to a\nlarge U.S. panel of scanner data on alcohol consumption.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.13937v4"
    },
    {
        "title": "E-Commerce Delivery Demand Modeling Framework for An Agent-Based\n  Simulation Platform",
        "authors": [
            "Takanori Sakai",
            "Yusuke Hara",
            "Ravi Seshadri",
            "André Alho",
            "Md Sami Hasnine",
            "Peiyu Jing",
            "ZhiYuan Chua",
            "Moshe Ben-Akiva"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The e-commerce delivery demand has grown rapidly in the past two decades and\nsuch trend has accelerated tremendously due to the ongoing coronavirus\npandemic. Given the situation, the need for predicting e-commerce delivery\ndemand and evaluating relevant logistics solutions is increasing. However, the\nexisting simulation models for e-commerce delivery demand are still limited and\ndo not consider the delivery options and their attributes that shoppers face on\ne-commerce order placements. We propose a novel modeling framework which\njointly predicts the average total value of e-commerce purchase, the purchase\namount per transaction, and delivery option choices. The proposed framework can\nsimulate the changes in e-commerce delivery demand attributable to the changes\nin delivery options. We assume the model parameters based on various sources of\nrelevant information and conduct a demonstrative sensitivity analysis.\nFurthermore, we have applied the model to the simulation for the\nAuto-Innovative Prototype city. While the calibration of the model using\nreal-world survey data is required, the result of the analysis highlights the\napplicability of the proposed framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.14375v1"
    },
    {
        "title": "Modeling European regional FDI flows using a Bayesian spatial Poisson\n  interaction model",
        "authors": [
            "Tamás Krisztin",
            "Philipp Piribauer"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper presents an empirical study of spatial origin and destination\neffects of European regional FDI dyads. Recent regional studies primarily focus\non locational determinants, but ignore bilateral origin- and intervening\nfactors, as well as associated spatial dependence. This paper fills this gap by\nusing observations on interregional FDI flows within a spatially augmented\nPoisson interaction model. We explicitly distinguish FDI activities between\nthree different stages of the value chain. Our results provide important\ninsights on drivers of regional FDI activities, both from origin and\ndestination perspectives. We moreover show that spatial dependence plays a key\nrole in both dimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.14856v1"
    },
    {
        "title": "Identification and Estimation of Unconditional Policy Effects of an\n  Endogenous Binary Treatment: An Unconditional MTE Approach",
        "authors": [
            "Julian Martinez-Iriarte",
            "Yixiao Sun"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper studies the identification and estimation of policy effects when\ntreatment status is binary and endogenous. We introduce a new class of marginal\ntreatment effects (MTEs) based on the influence function of the functional\nunderlying the policy target. We show that an unconditional policy effect can\nbe represented as a weighted average of the newly defined MTEs over the\nindividuals who are indifferent about their treatment status. We provide\nconditions for point identification of the unconditional policy effects. When a\nquantile is the functional of interest, we introduce the UNconditional\nInstrumental Quantile Estimator (UNIQUE) and establish its consistency and\nasymptotic distribution. In the empirical application, we estimate the effect\nof changing college enrollment status, induced by higher tuition subsidy, on\nthe quantiles of the wage distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.15864v6"
    },
    {
        "title": "Machine Learning for Experimental Design: Methods for Improved Blocking",
        "authors": [
            "Brian Quistorff",
            "Gentry Johnson"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Restricting randomization in the design of experiments (e.g., using\nblocking/stratification, pair-wise matching, or rerandomization) can improve\nthe treatment-control balance on important covariates and therefore improve the\nestimation of the treatment effect, particularly for small- and medium-sized\nexperiments. Existing guidance on how to identify these variables and implement\nthe restrictions is incomplete and conflicting. We identify that differences\nare mainly due to the fact that what is important in the pre-treatment data may\nnot translate to the post-treatment data. We highlight settings where there is\nsufficient data to provide clear guidance and outline improved methods to\nmostly automate the process using modern machine learning (ML) techniques. We\nshow in simulations using real-world data, that these methods reduce both the\nmean squared error of the estimate (14%-34%) and the size of the standard error\n(6%-16%).\n",
        "pdf_link": "http://arxiv.org/pdf/2010.15966v1"
    },
    {
        "title": "Nonparametric Identification of Production Function, Total Factor\n  Productivity, and Markup from Revenue Data",
        "authors": [
            "Hiroyuki Kasahara",
            "Yoichi Sugita"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Commonly used methods of production function and markup estimation assume\nthat a firm's output quantity can be observed as data, but typical datasets\ncontain only revenue, not output quantity. We examine the nonparametric\nidentification of production function and markup from revenue data when a firm\nfaces a general nonparametri demand function under imperfect competition. Under\nstandard assumptions, we provide the constructive nonparametric identification\nof various firm-level objects: gross production function, total factor\nproductivity, price markups over marginal costs, output prices, output\nquantities, a demand system, and a representative consumer's utility function.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.00143v1"
    },
    {
        "title": "Instrumental Variable Identification of Dynamic Variance Decompositions",
        "authors": [
            "Mikkel Plagborg-Møller",
            "Christian K. Wolf"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Macroeconomists increasingly use external sources of exogenous variation for\ncausal inference. However, unless such external instruments (proxies) capture\nthe underlying shock without measurement error, existing methods are silent on\nthe importance of that shock for macroeconomic fluctuations. We show that, in a\ngeneral moving average model with external instruments, variance decompositions\nfor the instrumented shock are interval-identified, with informative bounds.\nVarious additional restrictions guarantee point identification of both variance\nand historical decompositions. Unlike SVAR analysis, our methods do not require\ninvertibility. Applied to U.S. data, they give a tight upper bound on the\nimportance of monetary shocks for inflation dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.01380v2"
    },
    {
        "title": "Learning from Forecast Errors: A New Approach to Forecast Combinations",
        "authors": [
            "Tae-Hwy Lee",
            "Ekaterina Seregina"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Forecasters often use common information and hence make common mistakes. We\npropose a new approach, Factor Graphical Model (FGM), to forecast combinations\nthat separates idiosyncratic forecast errors from the common errors. FGM\nexploits the factor structure of forecast errors and the sparsity of the\nprecision matrix of the idiosyncratic errors. We prove the consistency of\nforecast combination weights and mean squared forecast error estimated using\nFGM, supporting the results with extensive simulations. Empirical applications\nto forecasting macroeconomic series shows that forecast combination using FGM\noutperforms combined forecasts using equal weights and graphical models without\nincorporating factor structure of forecast errors.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.02077v2"
    },
    {
        "title": "Robust Forecasting",
        "authors": [
            "Timothy Christensen",
            "Hyungsik Roger Moon",
            "Frank Schorfheide"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We use a decision-theoretic framework to study the problem of forecasting\ndiscrete outcomes when the forecaster is unable to discriminate among a set of\nplausible forecast distributions because of partial identification or concerns\nabout model misspecification or structural breaks. We derive \"robust\" forecasts\nwhich minimize maximum risk or regret over the set of forecast distributions.\nWe show that for a large class of models including semiparametric panel data\nmodels for dynamic discrete choice, the robust forecasts depend in a natural\nway on a small number of convex optimization problems which can be simplified\nusing duality methods. Finally, we derive \"efficient robust\" forecasts to deal\nwith the problem of first having to estimate the set of forecast distributions\nand develop a suitable asymptotic efficiency theory. Forecasts obtained by\nreplacing nuisance parameters that characterize the set of forecast\ndistributions with efficient first-stage estimators can be strictly dominated\nby our efficient robust forecasts.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.03153v4"
    },
    {
        "title": "Optimal Policy Learning: From Theory to Practice",
        "authors": [
            "Giovanni Cerulli"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Following in the footsteps of the literature on empirical welfare\nmaximization, this paper wants to contribute by stressing the policymaker\nperspective via a practical illustration of an optimal policy assignment\nproblem. More specifically, by focusing on the class of threshold-based\npolicies, we first set up the theoretical underpinnings of the policymaker\nselection problem, to then offer a practical solution to this problem via an\nempirical illustration using the popular LaLonde (1986) training program\ndataset. The paper proposes an implementation protocol for the optimal solution\nthat is straightforward to apply and easy to program with standard statistical\nsoftware.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.04993v1"
    },
    {
        "title": "Identifying the effect of a mis-classified, binary, endogenous regressor",
        "authors": [
            "Francis J. DiTraglia",
            "Camilo Garcia-Jimeno"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper studies identification of the effect of a mis-classified, binary,\nendogenous regressor when a discrete-valued instrumental variable is available.\nWe begin by showing that the only existing point identification result for this\nmodel is incorrect. We go on to derive the sharp identified set under mean\nindependence assumptions for the instrument and measurement error. The\nresulting bounds are novel and informative, but fail to point identify the\neffect of interest. This motivates us to consider alternative and slightly\nstronger assumptions: we show that adding second and third moment independence\nassumptions suffices to identify the model.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.07272v1"
    },
    {
        "title": "A Framework for Eliciting, Incorporating, and Disciplining\n  Identification Beliefs in Linear Models",
        "authors": [
            "Francis J. DiTraglia",
            "Camilo Garcia-Jimeno"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  To estimate causal effects from observational data, an applied researcher\nmust impose beliefs. The instrumental variables exclusion restriction, for\nexample, represents the belief that the instrument has no direct effect on the\noutcome of interest. Yet beliefs about instrument validity do not exist in\nisolation. Applied researchers often discuss the likely direction of selection\nand the potential for measurement error in their articles but lack formal tools\nfor incorporating this information into their analyses. Failing to use all\nrelevant information not only leaves money on the table; it runs the risk of\nleading to a contradiction in which one holds mutually incompatible beliefs\nabout the problem at hand. To address these issues, we first characterize the\njoint restrictions relating instrument invalidity, treatment endogeneity, and\nnon-differential measurement error in a workhorse linear model, showing how\nbeliefs over these three dimensions are mutually constrained by each other and\nthe data. Using this information, we propose a Bayesian framework to help\nresearchers elicit their beliefs, incorporate them into estimation, and ensure\ntheir mutual coherence. We conclude by illustrating our framework in a number\nof examples drawn from the empirical microeconomics literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.07276v1"
    },
    {
        "title": "A Semi-Parametric Bayesian Generalized Least Squares Estimator",
        "authors": [
            "Ruochen Wu",
            "Melvyn Weeks"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper we propose a semi-parametric Bayesian Generalized Least Squares\nestimator. In a generic setting where each error is a vector, the parametric\nGeneralized Least Square estimator maintains the assumption that each error\nvector has the same distributional parameters. In reality, however, errors are\nlikely to be heterogeneous regarding their distributions. To cope with such\nheterogeneity, a Dirichlet process prior is introduced for the distributional\nparameters of the errors, leading to the error distribution being a mixture of\na variable number of normal distributions. Our method let the number of normal\ncomponents be data driven. Semi-parametric Bayesian estimators for two specific\ncases are then presented: the Seemingly Unrelated Regression for equation\nsystems and the Random Effects Model for panel data. We design a series of\nsimulation experiments to explore the performance of our estimators. The\nresults demonstrate that our estimators obtain smaller posterior standard\ndeviations and mean squared errors than the Bayesian estimators using a\nparametric mixture of normal distributions or a normal distribution. We then\napply our semi-parametric Bayesian estimators for equation systems and panel\ndata models to empirical data.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.10252v2"
    },
    {
        "title": "Doubly weighted M-estimation for nonrandom assignment and missing\n  outcomes",
        "authors": [
            "Akanksha Negi"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper proposes a new class of M-estimators that double weight for the\ntwin problems of nonrandom treatment assignment and missing outcomes, both of\nwhich are common issues in the treatment effects literature. The proposed class\nis characterized by a `robustness' property, which makes it resilient to\nparametric misspecification in either a conditional model of interest (for\nexample, mean or quantile function) or the two weighting functions. As leading\napplications, the paper discusses estimation of two specific causal parameters;\naverage and quantile treatment effects (ATE, QTEs), which can be expressed as\nfunctions of the doubly weighted estimator, under misspecification of the\nframework's parametric components. With respect to the ATE, this paper shows\nthat the proposed estimator is doubly robust even in the presence of missing\noutcomes. Finally, to demonstrate the estimator's viability in empirical\nsettings, it is applied to Calonico and Smith (2017)'s reconstructed sample\nfrom the National Supported Work training program.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.11485v1"
    },
    {
        "title": "The Law of Large Numbers for Large Stable Matchings",
        "authors": [
            "Jacob Schwartz",
            "Kyungchul Song"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In many empirical studies of a large two-sided matching market (such as in a\ncollege admissions problem), the researcher performs statistical inference\nunder the assumption that they observe a random sample from a large matching\nmarket. In this paper, we consider a setting in which the researcher observes\neither all or a nontrivial fraction of outcomes from a stable matching. We\nestablish a concentration inequality for empirical matching probabilities\nassuming strong correlation among the colleges' preferences while allowing\nstudents' preferences to be fully heterogeneous. Our concentration inequality\nyields laws of large numbers for the empirical matching probabilities and other\nstatistics commonly used in empirical analyses of a large matching market. To\nillustrate the usefulness of our concentration inequality, we prove consistency\nfor estimators of conditional matching probabilities and measures of positive\nassortative matching.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.00399v8"
    },
    {
        "title": "Shoiuld Humans Lie to Machines: The Incentive Compatibility of Lasso and\n  General Weighted Lasso",
        "authors": [
            "Mehmet Caner",
            "Kfir Eliaz"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We consider situations where a user feeds her attributes to a machine\nlearning method that tries to predict her best option based on a random sample\nof other users. The predictor is incentive-compatible if the user has no\nincentive to misreport her covariates. Focusing on the popular Lasso estimation\ntechnique, we borrow tools from high-dimensional statistics to characterize\nsufficient conditions that ensure that Lasso is incentive compatible in large\nsamples. We extend our results to the Conservative Lasso estimator and provide\nnew moment bounds for this generalized weighted version of Lasso. Our results\nshow that incentive compatibility is achieved if the tuning parameter is kept\nabove some threshold. We present simulations that illustrate how this can be\ndone in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.01144v2"
    },
    {
        "title": "Partial Identification in Nonseparable Binary Response Models with\n  Endogenous Regressors",
        "authors": [
            "Jiaying Gu",
            "Thomas M. Russell"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper considers (partial) identification of a variety of counterfactual\nparameters in binary response models with possibly endogenous regressors. Our\nframework allows for nonseparable index functions with multi-dimensional latent\nvariables, and does not require parametric distributional assumptions. We\nleverage results on hyperplane arrangements and cell enumeration from the\nliterature on computational geometry in order to provide a tractable means of\ncomputing the identified set. We demonstrate how various functional form,\nindependence, and monotonicity assumptions can be imposed as constraints in our\noptimization procedure to tighten the identified set. Finally, we apply our\nmethod to study the effects of health insurance on the decision to seek medical\ntreatment.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.01254v5"
    },
    {
        "title": "Bootstrapping Non-Stationary Stochastic Volatility",
        "authors": [
            "H. Peter Boswijk",
            "Giuseppe Cavaliere",
            "Anders Rahbek",
            "Iliyan Georgiev"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In this paper we investigate how the bootstrap can be applied to time series\nregressions when the volatility of the innovations is random and\nnon-stationary. The volatility of many economic and financial time series\ndisplays persistent changes and possible non-stationarity. However, the theory\nof the bootstrap for such models has focused on deterministic changes of the\nunconditional variance and little is known about the performance and the\nvalidity of the bootstrap when the volatility is driven by a non-stationary\nstochastic process. This includes near-integrated volatility processes as well\nas near-integrated GARCH processes. This paper develops conditions for\nbootstrap validity in time series regressions with non-stationary, stochastic\nvolatility. We show that in such cases the distribution of bootstrap statistics\n(conditional on the data) is random in the limit. Consequently, the\nconventional approaches to proving bootstrap validity, involving weak\nconvergence in probability of the bootstrap statistic, fail to deliver the\nrequired results. Instead, we use the concept of `weak convergence in\ndistribution' to develop and establish novel conditions for validity of the\nwild bootstrap, conditional on the volatility process. We apply our results to\nseveral testing problems in the presence of non-stationary stochastic\nvolatility, including testing in a location model, testing for structural\nchange and testing for an autoregressive unit root. Sufficient conditions for\nbootstrap validity include the absence of statistical leverage effects, i.e.,\ncorrelation between the error process and its future conditional variance. The\nresults are illustrated using Monte Carlo simulations, which indicate that the\nwild bootstrap leads to size control even in small samples.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03562v1"
    },
    {
        "title": "Dynamic Ordering Learning in Multivariate Forecasting",
        "authors": [
            "Bruno P. C. Levy",
            "Hedibert F. Lopes"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In many fields where the main goal is to produce sequential forecasts for\ndecision making problems, the good understanding of the contemporaneous\nrelations among different series is crucial for the estimation of the\ncovariance matrix. In recent years, the modified Cholesky decomposition\nappeared as a popular approach to covariance matrix estimation. However, its\nmain drawback relies on the imposition of the series ordering structure. In\nthis work, we propose a highly flexible and fast method to deal with the\nproblem of ordering uncertainty in a dynamic fashion with the use of Dynamic\nOrder Probabilities. We apply the proposed method in two different forecasting\ncontexts. The first is a dynamic portfolio allocation problem, where the\ninvestor is able to learn the contemporaneous relationships among different\ncurrencies improving final decisions and economic performance. The second is a\nmacroeconomic application, where the econometrician can adapt sequentially to\nnew economic environments, switching the contemporaneous relations among\nmacroeconomic variables over time.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.04164v3"
    },
    {
        "title": "Empirical Decomposition of the IV-OLS Gap with Heterogeneous and\n  Nonlinear Effects",
        "authors": [
            "Shoya Ishimaru"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This study proposes an econometric framework to interpret and empirically\ndecompose the difference between IV and OLS estimates given by a linear\nregression model when the true causal effects of the treatment are nonlinear in\ntreatment levels and heterogeneous across covariates. I show that the IV-OLS\ncoefficient gap consists of three estimable components: the difference in\nweights on the covariates, the difference in weights on the treatment levels,\nand the difference in identified marginal effects that arises from endogeneity\nbias. Applications of this framework to return-to-schooling estimates\ndemonstrate the empirical relevance of this distinction in properly\ninterpreting the IV-OLS gap.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.04346v5"
    },
    {
        "title": "Full-Information Estimation of Heterogeneous Agent Models Using Macro\n  and Micro Data",
        "authors": [
            "Laura Liu",
            "Mikkel Plagborg-Møller"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We develop a generally applicable full-information inference method for\nheterogeneous agent models, combining aggregate time series data and repeated\ncross sections of micro data. To handle unobserved aggregate state variables\nthat affect cross-sectional distributions, we compute a numerically unbiased\nestimate of the model-implied likelihood function. Employing the likelihood\nestimate in a Markov Chain Monte Carlo algorithm, we obtain fully efficient and\nvalid Bayesian inference. Evaluation of the micro part of the likelihood lends\nitself naturally to parallel computing. Numerical illustrations in models with\nheterogeneous households or firms demonstrate that the proposed\nfull-information method substantially sharpens inference relative to using only\nmacro data, and for some parameters micro data is essential for identification.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.04771v2"
    },
    {
        "title": "GDP Forecasting using Payments Transaction Data",
        "authors": [
            "Arunav Das"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  UK GDP data is published with a lag time of more than a month and it is often\nadjusted for prior periods. This paper contemplates breaking away from the\nhistoric GDP measure to a more dynamic method using Bank Account, Cheque and\nCredit Card payment transactions as possible predictors for faster and real\ntime measure of GDP value. Historic timeseries data available from various\npublic domain for various payment types, values, volume and nominal UK GDP was\nused for this analysis. Low Value Payments was selected for simple Ordinary\nLeast Square Simple Linear Regression with mixed results around explanatory\npower of the model and reliability measured through residuals distribution and\nvariance. Future research could potentially expand this work using datasets\nsplit by period of economic shocks to further test the OLS method or explore\none of General Least Square method or an autoregression on GDP timeseries\nitself.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.06478v1"
    },
    {
        "title": "Decomposition of Bilateral Trade Flows Using a Three-Dimensional Panel\n  Data Model",
        "authors": [
            "Yufeng Mao",
            "Bin Peng",
            "Mervyn Silvapulle",
            "Param Silvapulle",
            "Yanrong Yang"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This study decomposes the bilateral trade flows using a three-dimensional\npanel data model. Under the scenario that all three dimensions diverge to\ninfinity, we propose an estimation approach to identify the number of global\nshocks and country-specific shocks sequentially, and establish the asymptotic\ntheories accordingly. From the practical point of view, being able to separate\nthe pervasive and nonpervasive shocks in a multi-dimensional panel data is\ncrucial for a range of applications, such as, international financial linkages,\nmigration flows, etc. In the numerical studies, we first conduct intensive\nsimulations to examine the theoretical findings, and then use the proposed\napproach to investigate the international trade flows from two major trading\ngroups (APEC and EU) over 1982-2019, and quantify the network of bilateral\ntrade.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.06805v1"
    },
    {
        "title": "Robustness of the international oil trade network under targeted attacks\n  to economies",
        "authors": [
            "N. Wei",
            "W. -J. Xie",
            "W. -X. Zhou"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In the international oil trade network (iOTN), trade shocks triggered by\nextreme events may spread over the entire network along the trade links of the\ncentral economies and even lead to the collapse of the whole system. In this\nstudy, we focus on the concept of \"too central to fail\" and use traditional\ncentrality indicators as strategic indicators for simulating attacks on\neconomic nodes, and simulates various situations in which the structure and\nfunction of the global oil trade network are lost when the economies suffer\nextreme trade shocks. The simulation results show that the global oil trade\nsystem has become more vulnerable in recent years. The regional aggregation of\noil trade is an essential source of iOTN's vulnerability. Maintaining global\noil trade stability and security requires a focus on economies with greater\ninfluence within the network module of the iOTN. International organizations\nsuch as OPEC and OECD established more trade links around the world, but their\ninfluence on the iOTN is declining. We improve the framework of oil security\nand trade risk assessment based on the topological index of iOTN, and provide a\nreference for finding methods to maintain network robustness and trade\nstability.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.10679v2"
    },
    {
        "title": "The sooner the better: lives saved by the lockdown during the COVID-19\n  outbreak. The case of Italy",
        "authors": [
            "Roy Cerqueti",
            "Raffaella Coppier",
            "Alessandro Girardi",
            "Marco Ventura"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper estimates the effects of non-pharmaceutical interventions -\nmainly, the lockdown - on the COVID-19 mortality rate for the case of Italy,\nthe first Western country to impose a national shelter-in-place order. We use a\nnew estimator, the Augmented Synthetic Control Method (ASCM), that overcomes\nsome limits of the standard Synthetic Control Method (SCM). The results are\ntwofold. From a methodological point of view, the ASCM outperforms the SCM in\nthat the latter cannot select a valid donor set, assigning all the weights to\nonly one country (Spain) while placing zero weights to all the remaining. From\nan empirical point of view, we find strong evidence of the effectiveness of\nnon-pharmaceutical interventions in avoiding losses of human lives in Italy:\nconservative estimates indicate that for each human life actually lost, in the\nabsence of lockdown there would have been on average other 1.15, the policy\nsaved in total 20,400 human lives.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.11901v1"
    },
    {
        "title": "A Bayesian approach for estimation of weight matrices in spatial\n  autoregressive models",
        "authors": [
            "Tamás Krisztin",
            "Philipp Piribauer"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We develop a Bayesian approach to estimate weight matrices in spatial\nautoregressive (or spatial lag) models. Datasets in regional economic\nliterature are typically characterized by a limited number of time periods T\nrelative to spatial units N. When the spatial weight matrix is subject to\nestimation severe problems of over-parametrization are likely. To make\nestimation feasible, our approach focusses on spatial weight matrices which are\nbinary prior to row-standardization. We discuss the use of hierarchical priors\nwhich impose sparsity in the spatial weight matrix. Monte Carlo simulations\nshow that these priors perform very well where the number of unknown parameters\nis large relative to the observations. The virtues of our approach are\ndemonstrated using global data from the early phase of the COVID-19 pandemic.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.11938v2"
    },
    {
        "title": "The Bootstrap for Network Dependent Processes",
        "authors": [
            "Denis Kojevnikov"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper focuses on the bootstrap for network dependent processes under the\nconditional $\\psi$-weak dependence. Such processes are distinct from other\nforms of random fields studied in the statistics and econometrics literature so\nthat the existing bootstrap methods cannot be applied directly. We propose a\nblock-based approach and a modification of the dependent wild bootstrap for\nconstructing confidence sets for the mean of a network dependent process. In\naddition, we establish the consistency of these methods for the smooth function\nmodel and provide the bootstrap alternatives to the network\nheteroskedasticity-autocorrelation consistent (HAC) variance estimator. We find\nthat the modified dependent wild bootstrap and the corresponding variance\nestimator are consistent under weaker conditions relative to the block-based\nmethod, which makes the former approach preferable for practical\nimplementation.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.12312v1"
    },
    {
        "title": "Local Projections vs. VARs: Lessons From Thousands of DGPs",
        "authors": [
            "Dake Li",
            "Mikkel Plagborg-Møller",
            "Christian K. Wolf"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We conduct a simulation study of Local Projection (LP) and Vector\nAutoregression (VAR) estimators of structural impulse responses across\nthousands of data generating processes, designed to mimic the properties of the\nuniverse of U.S. macroeconomic data. Our analysis considers various\nidentification schemes and several variants of LP and VAR estimators, employing\nbias correction, shrinkage, or model averaging. A clear bias-variance trade-off\nemerges: LP estimators have lower bias than VAR estimators, but they also have\nsubstantially higher variance at intermediate and long horizons. Bias-corrected\nLP is the preferred method if and only if the researcher overwhelmingly\nprioritizes bias. For researchers who also care about precision, VAR methods\nare the most attractive -- Bayesian VARs at short and long horizons, and\nleast-squares VARs at intermediate and long horizons.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.00655v4"
    },
    {
        "title": "Revisiting the empirical fundamental relationship of traffic flow for\n  highways using a causal econometric approach",
        "authors": [
            " Anupriya",
            "Daniel J. Graham",
            "Daniel Hörcher",
            "Prateek Bansal"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The fundamental relationship of traffic flow is empirically estimated by\nfitting a regression curve to a cloud of observations of traffic variables.\nSuch estimates, however, may suffer from the confounding/endogeneity bias due\nto omitted variables such as driving behaviour and weather. To this end, this\npaper adopts a causal approach to obtain an unbiased estimate of the\nfundamental flow-density relationship using traffic detector data. In\nparticular, we apply a Bayesian non-parametric spline-based regression approach\nwith instrumental variables to adjust for the aforementioned confounding bias.\nThe proposed approach is benchmarked against standard curve-fitting methods in\nestimating the flow-density relationship for three highway bottlenecks in the\nUnited States. Our empirical results suggest that the saturated (or\nhypercongested) regime of the estimated flow-density relationship using\ncorrelational curve fitting methods may be severely biased, which in turn leads\nto biased estimates of important traffic control inputs such as capacity and\ncapacity-drop. We emphasise that our causal approach is based on the physical\nlaws of vehicle movement in a traffic stream as opposed to a demand-supply\nframework adopted in the economics literature. By doing so, we also aim to\nconciliate the engineering and economics approaches to this empirical problem.\nOur results, thus, have important implications both for traffic engineers and\ntransport economists.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.02399v1"
    },
    {
        "title": "Bootstrap Inference for Hawkes and General Point Processes",
        "authors": [
            "Giuseppe Cavaliere",
            "Ye Lu",
            "Anders Rahbek",
            "Jacob Stærk-Østergaard"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Inference and testing in general point process models such as the Hawkes\nmodel is predominantly based on asymptotic approximations for likelihood-based\nestimators and tests. As an alternative, and to improve finite sample\nperformance, this paper considers bootstrap-based inference for interval\nestimation and testing. Specifically, for a wide class of point process models\nwe consider a novel bootstrap scheme labeled 'fixed intensity bootstrap' (FIB),\nwhere the conditional intensity is kept fixed across bootstrap repetitions. The\nFIB, which is very simple to implement and fast in practice, extends previous\nideas from the bootstrap literature on time series in discrete time, where the\nso-called 'fixed design' and 'fixed volatility' bootstrap schemes have shown to\nbe particularly useful and effective. We compare the FIB with the classic\nrecursive bootstrap, which is here labeled 'recursive intensity bootstrap'\n(RIB). In RIB algorithms, the intensity is stochastic in the bootstrap world\nand implementation of the bootstrap is more involved, due to its sequential\nstructure. For both bootstrap schemes, we provide new bootstrap (asymptotic)\ntheory which allows to assess bootstrap validity, and propose a\n'non-parametric' approach based on resampling time-changed transformations of\nthe original waiting times. We also establish the link between the proposed\nbootstraps for point process models and the related autoregressive conditional\nduration (ACD) models. Lastly, we show effectiveness of the different bootstrap\nschemes in finite samples through a set of detailed Monte Carlo experiments,\nand provide applications to both financial data and social media data to\nillustrate the proposed methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.03122v2"
    },
    {
        "title": "Min(d)ing the President: A text analytic approach to measuring tax news",
        "authors": [
            "Lenard Lieb",
            "Adam Jassem",
            "Rui Jorge Almeida",
            "Nalan Baştürk",
            "Stephan Smeekes"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Economic agents react to signals about future tax policy changes.\nConsequently, estimating their macroeconomic effects requires identification of\nsuch signals. We propose a novel text analytic approach for transforming\ntextual information into an economically meaningful time series. Using this\nmethod, we create a tax news measure from all publicly available post-war\ncommunications of U.S. presidents. Our measure predicts the direction and size\nof future tax changes and contains signals not present in previously considered\n(narrative) measures of tax changes. We investigate the effects of tax news and\nfind that, for long anticipation horizons, pre-implementation effects lead\ninitially to contractions in output.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.03261v3"
    },
    {
        "title": "CATE meets ML -- The Conditional Average Treatment Effect and Machine\n  Learning",
        "authors": [
            "Daniel Jacob"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  For treatment effects - one of the core issues in modern econometric analysis\n- prediction and estimation are two sides of the same coin. As it turns out,\nmachine learning methods are the tool for generalized prediction models.\nCombined with econometric theory, they allow us to estimate not only the\naverage but a personalized treatment effect - the conditional average treatment\neffect (CATE). In this tutorial, we give an overview of novel methods, explain\nthem in detail, and apply them via Quantlets in real data applications. We\nstudy the effect that microcredit availability has on the amount of money\nborrowed and if 401(k) pension plan eligibility has an impact on net financial\nassets, as two empirical examples. The presented toolbox of methods contains\nmeta-learners, like the Doubly-Robust, R-, T- and X-learner, and methods that\nare specially designed to estimate the CATE like the causal BART and the\ngeneralized random forest. In both, the microcredit and 401(k) example, we find\na positive treatment effect for all observations but conflicting evidence of\ntreatment effect heterogeneity. An additional simulation study, where the true\ntreatment effect is known, allows us to compare the different methods and to\nobserve patterns and similarities.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.09935v2"
    },
    {
        "title": "Investigating farming efficiency through a two stage analytical\n  approach: Application to the agricultural sector in Northern Oman",
        "authors": [
            "Amar Oukil",
            "Slim Zekri"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In this paper, we develop a two-stage analytical framework to investigate\nfarming efficiency. In the first stage, data envelopment analysis is employed\nto estimate the efficiency of the farms and conduct slack and scale economies\nanalyses. In the second stage, we propose a stochastic model to identify\npotential sources of inefficiency. The latter model integrates within a unified\nstructure all variables, including inputs, outputs and contextual factors. As\nan application ground, we use a sample of 60 farms from the Batinah coastal\nregion, an agricultural area representing more than 53 per cent of the total\ncropped area of Oman. The findings of the study lay emphasis on the\ninter-dependence of groundwater salinity, irrigation technology and operational\nefficiency of a farm, with as a key recommendation the necessity for more\nregulated water consumption and a readjustment of governmental subsidiary\npolicies.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.10943v1"
    },
    {
        "title": "Weak Instrumental Variables: Limitations of Traditional 2SLS and\n  Exploring Alternative Instrumental Variable Estimators",
        "authors": [
            "Aiwei Huang",
            "Madhurima Chandra",
            "Laura Malkhasyan"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Instrumental variables estimation has gained considerable traction in recent\ndecades as a tool for causal inference, particularly amongst empirical\nresearchers. This paper makes three contributions. First, we provide a detailed\ntheoretical discussion on the properties of the standard two-stage least\nsquares estimator in the presence of weak instruments and introduce and derive\ntwo alternative estimators. Second, we conduct Monte-Carlo simulations to\ncompare the finite-sample behavior of the different estimators, particularly in\nthe weak-instruments case. Third, we apply the estimators to a real-world\ncontext; we employ the different estimators to calculate returns to schooling.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.12370v1"
    },
    {
        "title": "Nonparametric Difference-in-Differences in Repeated Cross-Sections with\n  Continuous Treatments",
        "authors": [
            "Xavier D'Haultfoeuille",
            "Stefan Hoderlein",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper studies the identification of causal effects of a continuous\ntreatment using a new difference-in-difference strategy. Our approach allows\nfor endogeneity of the treatment, and employs repeated cross-sections. It\nrequires an exogenous change over time which affects the treatment in a\nheterogeneous way, stationarity of the distribution of unobservables and a rank\ninvariance condition on the time trend. On the other hand, we do not impose any\nfunctional form restrictions or an additive time trend, and we are invariant to\nthe scaling of the dependent variable. Under our conditions, the time trend can\nbe identified using a control group, as in the binary difference-in-differences\nliterature. In our scenario, however, this control group is defined by the\ndata. We then identify average and quantile treatment effect parameters. We\ndevelop corresponding nonparametric estimators and study their asymptotic\nproperties. Finally, we apply our results to the effect of disposable income on\nconsumption.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14458v2"
    },
    {
        "title": "Local Average and Marginal Treatment Effects with a Misclassified\n  Treatment",
        "authors": [
            "Santiago Acerenza",
            "Kyunghoon Ban",
            "Désiré Kédagni"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper studies identification of the local average and marginal treatment\neffects (LATE and MTE) with a misclassified binary treatment variable. We\nderive bounds on the (generalized) LATE and exploit its relationship with the\nMTE to further bound the MTE. Indeed, under some standard assumptions, the MTE\nis a limit of the ratio of the variation in the conditional expectation of the\nobserved outcome given the instrument to the variation in the true propensity\nscore, which is partially identified. We characterize the identified set for\nthe propensity score, and then for the MTE. We show that our LATE bounds are\ntighter than the existing bounds and that the sign of the MTE is locally\nidentified under some mild regularity conditions. We use our MTE bounds to\nderive bounds on other commonly used parameters in the literature and\nillustrate the practical relevance of our derived bounds through numerical and\nempirical results.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.00358v8"
    },
    {
        "title": "A Modified Randomization Test for the Level of Clustering",
        "authors": [
            "Yong Cai"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Suppose a researcher observes individuals within a county within a state.\nGiven concerns about correlation across individuals, it is common to group\nobservations into clusters and conduct inference treating observations across\nclusters as roughly independent. However, a researcher that has chosen to\ncluster at the county level may be unsure of their decision, given knowledge\nthat observations are independent across states. This paper proposes a modified\nrandomization test as a robustness check for the chosen level of clustering in\na linear regression setting. Existing tests require either the number of states\nor number of counties to be large. Our method is designed for settings with few\nstates and few counties. While the method is conservative, it has competitive\npower in settings that may be relevant to empirical work.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.01008v2"
    },
    {
        "title": "Difference-in-Differences Estimation with Spatial Spillovers",
        "authors": [
            "Kyle Butts"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Empirical work often uses treatment assigned following geographic boundaries.\nWhen the effects of treatment cross over borders, classical\ndifference-in-differences estimation produces biased estimates for the average\ntreatment effect. In this paper, I introduce a potential outcomes framework to\nmodel spillover effects and decompose the estimate's bias in two parts: (1) the\ncontrol group no longer identifies the counterfactual trend because their\noutcomes are affected by treatment and (2) changes in treated units' outcomes\nreflect the effect of their own treatment status and the effect from the\ntreatment status of 'close' units. I propose conditions for non-parametric\nidentification that can remove both sources of bias and semi-parametrically\nestimate the spillover effects themselves including in settings with staggered\ntreatment timing. To highlight the importance of spillover effects, I revisit\nanalyses of three place-based interventions.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.03737v3"
    },
    {
        "title": "Robust Inference on Income Inequality: $t$-Statistic Based Approaches",
        "authors": [
            "Rustam Ibragimov",
            "Paul Kattuman",
            "Anton Skrobotov"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Empirical analyses on income and wealth inequality and those in other fields\nin economics and finance often face the difficulty that the data is\nheterogeneous, heavy-tailed or correlated in some unknown fashion. The paper\nfocuses on applications of the recently developed \\textit{t}-statistic based\nrobust inference approaches in the analysis of inequality measures and their\ncomparisons under the above problems. Following the approaches, in particular,\na robust large sample test on equality of two parameters of interest (e.g., a\ntest of equality of inequality measures in two regions or countries considered)\nis conducted as follows: The data in the two samples dealt with is partitioned\ninto fixed numbers $q_1, q_2\\ge 2$ (e.g., $q_1=q_2=2, 4, 8$) of groups, the\nparameters (inequality measures dealt with) are estimated for each group, and\ninference is based on a standard two-sample $t-$test with the resulting $q_1,\nq_2$ group estimators. Robust $t-$statistic approaches result in valid\ninference under general conditions that group estimators of parameters (e.g.,\ninequality measures) considered are asymptotically independent, unbiased and\nGaussian of possibly different variances, or weakly converge, at an arbitrary\nrate, to independent scale mixtures of normal random variables. These\nconditions are typically satisfied in empirical applications even under\npronounced heavy-tailedness and heterogeneity and possible dependence in\nobservations. The methods dealt with in the paper complement and compare\nfavorably with other inference approaches available in the literature. The use\nof robust inference approaches is illustrated by an empirical analysis of\nincome inequality measures and their comparisons across different regions in\nRussia.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.05335v2"
    },
    {
        "title": "Policy Evaluation during a Pandemic",
        "authors": [
            "Brantly Callaway",
            "Tong Li"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  National and local governments have implemented a large number of policies in\nresponse to the Covid-19 pandemic. Evaluating the effects of these policies,\nboth on the number of Covid-19 cases as well as on other economic outcomes is a\nkey ingredient for policymakers to be able to determine which policies are most\neffective as well as the relative costs and benefits of particular policies. In\nthis paper, we consider the relative merits of common identification strategies\nthat exploit variation in the timing of policies across different locations by\nchecking whether the identification strategies are compatible with leading\nepidemic models in the epidemiology literature. We argue that unconfoundedness\ntype approaches, that condition on the pre-treatment \"state\" of the pandemic,\nare likely to be more useful for evaluating policies than\ndifference-in-differences type approaches due to the highly nonlinear spread of\ncases during a pandemic. For difference-in-differences, we further show that a\nversion of this problem continues to exist even when one is interested in\nunderstanding the effect of a policy on other economic outcomes when those\noutcomes also depend on the number of Covid-19 cases. We propose alternative\napproaches that are able to circumvent these issues. We apply our proposed\napproach to study the effect of state level shelter-in-place orders early in\nthe pandemic.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.06927v2"
    },
    {
        "title": "Double robust inference for continuous updating GMM",
        "authors": [
            "Frank Kleibergen",
            "Zhaoguo Zhan"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose the double robust Lagrange multiplier (DRLM) statistic for testing\nhypotheses specified on the pseudo-true value of the structural parameters in\nthe generalized method of moments. The pseudo-true value is defined as the\nminimizer of the population continuous updating objective function and equals\nthe true value of the structural parameter in the absence of\nmisspecification.\\nocite{hhy96} The (bounding) chi-squared limiting\ndistribution of the DRLM statistic is robust to both misspecification and weak\nidentification of the structural parameters, hence its name. To emphasize its\nimportance for applied work, we use the DRLM test to analyze the return on\neducation, which is often perceived to be weakly identified, using data from\nCard (1995) where misspecification occurs in case of treatment heterogeneity;\nand to analyze the risk premia associated with risk factors proposed in Adrian\net al. (2014) and He et al. (2017), where both misspecification and weak\nidentification need to be addressed.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.08345v1"
    },
    {
        "title": "Identification robust inference for moments based analysis of linear\n  dynamic panel data models",
        "authors": [
            "Maurice J. G. Bun",
            "Frank Kleibergen"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We use identification robust tests to show that difference, level and\nnon-linear moment conditions, as proposed by Arellano and Bond (1991), Arellano\nand Bover (1995), Blundell and Bond (1998) and Ahn and Schmidt (1995) for the\nlinear dynamic panel data model, do not separately identify the autoregressive\nparameter when its true value is close to one and the variance of the initial\nobservations is large. We prove that combinations of these moment conditions,\nhowever, do so when there are more than three time series observations. This\nidentification then solely results from a set of, so-called, robust moment\nconditions. These robust moments are spanned by the combined difference, level\nand non-linear moment conditions and only depend on differenced data. We show\nthat, when only the robust moments contain identifying information on the\nautoregressive parameter, the discriminatory power of the Kleibergen (2005) LM\ntest using the combined moments is identical to the largest rejection\nfrequencies that can be obtained from solely using the robust moments. This\nshows that the KLM test implicitly uses the robust moments when only they\ncontain information on the autoregressive parameter.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.08346v1"
    },
    {
        "title": "Incorporating Social Welfare in Program-Evaluation and Treatment Choice",
        "authors": [
            "Debopam Bhattacharya",
            "Tatiana Komarova"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The econometric literature on treatment-effects typically takes functionals\nof outcome-distributions as `social welfare' and ignores program-impacts on\nunobserved utilities. We show how to incorporate aggregate utility within\neconometric program-evaluation and optimal treatment-targeting for a\nheterogenous population. In the practically important setting of\ndiscrete-choice, under unrestricted preference-heterogeneity and\nincome-effects, the indirect-utility distribution becomes a closed-form\nfunctional of average demand. This enables nonparametric cost-benefit analysis\nof policy-interventions and their optimal targeting based on planners'\nredistributional preferences. For ordered/continuous choice,\nutility-distributions can be bounded. Our methods are illustrated with Indian\nsurvey-data on private-tuition, where income-paths of usage-maximizing\nsubsidies differ significantly from welfare-maximizing ones.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.08689v2"
    },
    {
        "title": "Two Sample Unconditional Quantile Effect",
        "authors": [
            "Atsushi Inoue",
            "Tong Li",
            "Qi Xu"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper proposes a new framework to evaluate unconditional quantile\neffects (UQE) in a data combination model. The UQE measures the effect of a\nmarginal counterfactual change in the unconditional distribution of a covariate\non quantiles of the unconditional distribution of a target outcome. Under rank\nsimilarity and conditional independence assumptions, we provide a set of\nidentification results for UQEs when the target covariate is continuously\ndistributed and when it is discrete, respectively. Based on these\nidentification results, we propose semiparametric estimators and establish\ntheir large sample properties under primitive conditions. Applying our method\nto a variant of Mincer's earnings function, we study the counterfactual\nquantile effect of actual work experience on income.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.09445v1"
    },
    {
        "title": "Identification and Estimation of a Partially Linear Regression Model\n  using Network Data: Inference and an Application to Network Peer Effects",
        "authors": [
            "Eric Auerbach"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper provides additional results relevant to the setting, model, and\nestimators of Auerbach (2019a). Section 1 contains results about the large\nsample properties of the estimators from Section 2 of Auerbach (2019a). Section\n2 considers some extensions to the model. Section 3 provides an application to\nestimating network peer effects. Section 4 shows the results from some\nsimulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.10002v1"
    },
    {
        "title": "Identification and Estimation of Partial Effects in Nonlinear\n  Semiparametric Panel Models",
        "authors": [
            "Laura Liu",
            "Alexandre Poirier",
            "Ji-Liang Shiu"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Average partial effects (APEs) are often not point identified in panel models\nwith unrestricted unobserved individual heterogeneity, such as a binary\nresponse panel model with fixed effects and logistic errors as a special case.\nThis lack of point identification occurs despite the identification of these\nmodels' common coefficients. We provide a unified framework to establish the\npoint identification of various partial effects in a wide class of nonlinear\nsemiparametric models under an index sufficiency assumption on the unobserved\nheterogeneity, even when the error distribution is unspecified and\nnon-stationary. This assumption does not impose parametric restrictions on the\nunobserved heterogeneity and idiosyncratic errors. We also present partial\nidentification results when the support condition fails. We then propose\nthree-step semiparametric estimators for APEs, average structural functions,\nand average marginal effects, and show their consistency and asymptotic\nnormality. Finally, we illustrate our approach in a study of determinants of\nmarried women's labor supply.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.12891v6"
    },
    {
        "title": "Asset volatility forecasting:The optimal decay parameter in the EWMA\n  model",
        "authors": [
            "Axel A. Araneda"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The exponentially weighted moving average (EMWA) could be labeled as a\ncompetitive volatility estimator, where its main strength relies on computation\nsimplicity, especially in a multi-asset scenario, due to dependency only on the\ndecay parameter, $\\lambda$. But, what is the best election for $\\lambda$ in the\nEMWA volatility model? Through a large time-series data set of historical\nreturns of the top US large-cap companies; we test empirically the forecasting\nperformance of the EWMA approach, under different time horizons and varying the\ndecay parameter. Using a rolling window scheme, the out-of-sample performance\nof the variance-covariance matrix is computed following two approaches. First,\nif we look for a fixed decay parameter for the full sample, the results are in\nagreement with the RiskMetrics suggestion for 1-month forecasting. In addition,\nwe provide the full-sample optimal decay parameter for the weekly and bi-weekly\nforecasting horizon cases, confirming two facts: i) the optimal value is as a\nfunction of the forecasting horizon, and ii) for lower forecasting horizons the\nshort-term memory gains importance. In a second way, we also evaluate the\nforecasting performance of EWMA, but this time using the optimal time-varying\ndecay parameter which minimizes the in-sample variance-covariance estimator,\narriving at better accuracy than the use of a fixed-full-sample optimal\nparameter.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.14382v1"
    },
    {
        "title": "Crime and Mismeasured Punishment: Marginal Treatment Effect with\n  Misclassification",
        "authors": [
            "Vitor Possebom"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  I partially identify the marginal treatment effect (MTE) when the treatment\nis misclassified. I explore two restrictions, allowing for dependence between\nthe instrument and the misclassification decision. If the signs of the\nderivatives of the propensity scores are equal, I identify the MTE sign. If\nthose derivatives are similar, I bound the MTE. To illustrate, I analyze the\nimpact of alternative sentences (fines and community service v. no punishment)\non recidivism in Brazil, where Appeals processes generate misclassification.\nThe estimated misclassification bias may be as large as 10% of the largest\npossible MTE, and the bounds contain the correctly estimated MTE.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.00536v7"
    },
    {
        "title": "Linear Rescaling to Accurately Interpret Logarithms",
        "authors": [
            "Nick Huntington-Klein"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The standard approximation of a natural logarithm in statistical analysis\ninterprets a linear change of \\(p\\) in \\(\\ln(X)\\) as a \\((1+p)\\) proportional\nchange in \\(X\\), which is only accurate for small values of \\(p\\). I suggest\nbase-\\((1+p)\\) logarithms, where \\(p\\) is chosen ahead of time. A one-unit\nchange in \\(\\log_{1+p}(X)\\) is exactly equivalent to a \\((1+p)\\) proportional\nchange in \\(X\\). This avoids an approximation applied too broadly, makes exact\ninterpretation easier and less error-prone, improves approximation quality when\napproximations are used, makes the change of interest a one-log-unit change\nlike other regression variables, and reduces error from the use of\n\\(\\log(1+X)\\).\n",
        "pdf_link": "http://arxiv.org/pdf/2106.03070v3"
    },
    {
        "title": "Panel Data with Unknown Clusters",
        "authors": [
            "Yong Cai"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Clustered standard errors and approximate randomization tests are popular\ninference methods that allow for dependence within observations. However, they\nrequire researchers to know the cluster structure ex ante. We propose a\nprocedure to help researchers discover clusters in panel data. Our method is\nbased on thresholding an estimated long-run variance-covariance matrix and\nrequires the panel to be large in the time dimension, but imposes no lower\nbound on the number of units. We show that our procedure recovers the true\nclusters with high probability with no assumptions on the cluster structure.\nThe estimated clusters are independently of interest, but they can also be used\nin the approximate randomization tests or with conventional cluster-robust\ncovariance estimators. The resulting procedures control size and have good\npower.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.05503v4"
    },
    {
        "title": "Sensitivity of LATE Estimates to Violations of the Monotonicity\n  Assumption",
        "authors": [
            "Claudia Noack"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In this paper, we develop a method to assess the sensitivity of local average\ntreatment effect estimates to potential violations of the monotonicity\nassumption of Imbens and Angrist (1994). We parameterize the degree to which\nmonotonicity is violated using two sensitivity parameters: the first one\ndetermines the share of defiers in the population, and the second one measures\ndifferences in the distributions of outcomes between compliers and defiers. For\neach pair of values of these sensitivity parameters, we derive sharp bounds on\nthe outcome distributions of compliers in the first-order stochastic dominance\nsense. We identify the robust region that is the set of all values of\nsensitivity parameters for which a given empirical conclusion, e.g. that the\nlocal average treatment effect is positive, is valid. Researchers can assess\nthe credibility of their conclusion by evaluating whether all the plausible\nsensitivity parameters lie in the robust region. We obtain confidence sets for\nthe robust region through a bootstrap procedure and illustrate the sensitivity\nanalysis in an empirical application. We also extend this framework to analyze\ntreatment effects of the entire population.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.06421v1"
    },
    {
        "title": "Dynamic Asymmetric Causality Tests with an Application",
        "authors": [
            "Abdulnasser Hatemi-J"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Testing for causation, defined as the preceding impact of the past values of\none variable on the current value of another one when all other pertinent\ninformation is accounted for, is increasingly utilized in empirical research of\nthe time-series data in different scientific disciplines. A relatively recent\nextension of this approach has been allowing for potential asymmetric impacts\nsince it is harmonious with the way reality operates in many cases according to\nHatemi-J (2012). The current paper maintains that it is also important to\naccount for the potential change in the parameters when asymmetric causation\ntests are conducted, as there exists a number of reasons for changing the\npotential causal connection between variables across time. The current paper\nextends therefore the static asymmetric causality tests by making them dynamic\nvia the usage of subsamples. An application is also provided consistent with\nmeasurable definitions of economic or financial bad as well as good news and\ntheir potential interaction across time.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.07612v2"
    },
    {
        "title": "Comparisons of Australian Mental Health Distributions",
        "authors": [
            "David Gunawan",
            "William Griffiths",
            "Duangkamon Chotikapanich"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Bayesian nonparametric estimates of Australian mental health distributions\nare obtained to assess how the mental health status of the population has\nchanged over time and to compare the mental health status of female/male and\nindigenous/non-indigenous population subgroups. First- and second-order\nstochastic dominance are used to compare distributions, with results presented\nin terms of the posterior probability of dominance and the posterior\nprobability of no dominance. Our results suggest mental health has deteriorated\nin recent years, that males mental health status is better than that of\nfemales, and non-indigenous health status is better than that of the indigenous\npopulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.08047v1"
    },
    {
        "title": "Set coverage and robust policy",
        "authors": [
            "Marc Henry",
            "Alexei Onatski"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  When conducting inference on partially identified parameters, confidence\nregions may cover the whole identified set with a prescribed probability, to\nwhich we will refer as set coverage, or they may cover each of its point with a\nprescribed probability, to which we will refer as point coverage. Since set\ncoverage implies point coverage, confidence regions satisfying point coverage\nare generally preferred on the grounds that they may be more informative. The\nobject of this note is to describe a decision problem in which, contrary to\nreceived wisdom, point coverage is clearly undesirable.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.09784v1"
    },
    {
        "title": "Semiparametric inference for partially linear regressions with Box-Cox\n  transformation",
        "authors": [
            "Daniel Becker",
            "Alois Kneip",
            "Valentin Patilea"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In this paper, a semiparametric partially linear model in the spirit of\nRobinson (1988) with Box- Cox transformed dependent variable is studied.\nTransformation regression models are widely used in applied econometrics to\navoid misspecification. In addition, a partially linear semiparametric model is\nan intermediate strategy that tries to balance advantages and disadvantages of\na fully parametric model and nonparametric models. A combination of\ntransformation and partially linear semiparametric model is, thus, a natural\nstrategy. The model parameters are estimated by a semiparametric extension of\nthe so called smooth minimum distance (SmoothMD) approach proposed by Lavergne\nand Patilea (2013). SmoothMD is suitable for models defined by conditional\nmoment conditions and allows the variance of the error terms to depend on the\ncovariates. In addition, here we allow for infinite-dimension nuisance\nparameters. The asymptotic behavior of the new SmoothMD estimator is studied\nunder general conditions and new inference methods are proposed. A simulation\nexperiment illustrates the performance of the methods for finite samples.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.10723v1"
    },
    {
        "title": "A Neural Frequency-Severity Model and Its Application to Insurance\n  Claims",
        "authors": [
            "Dong-Young Lim"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper proposes a flexible and analytically tractable class of frequency\nand severity models for predicting insurance claims. The proposed model is able\nto capture nonlinear relationships in explanatory variables by characterizing\nthe logarithmic mean functions of frequency and severity distributions as\nneural networks. Moreover, a potential dependence between the claim frequency\nand severity can be incorporated. In particular, the paper provides analytic\nformulas for mean and variance of the total claim cost, making our model ideal\nfor many applications such as pricing insurance contracts and the pure premium.\nA simulation study demonstrates that our method successfully recovers nonlinear\nfeatures of explanatory variables as well as the dependency between frequency\nand severity. Then, this paper uses a French auto insurance claim dataset to\nillustrate that the proposed model is superior to the existing methods in\nfitting and predicting the claim frequency, severity, and the total claim loss.\nNumerical results indicate that the proposed model helps in maintaining the\ncompetitiveness of an insurer by accurately predicting insurance claims and\navoiding adverse selection.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.10770v2"
    },
    {
        "title": "On the Use of Two-Way Fixed Effects Models for Policy Evaluation During\n  Pandemics",
        "authors": [
            "Germain Gauthier"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In the context of the Covid-19 pandemic, multiple studies rely on two-way\nfixed effects (FE) models to assess the impact of mitigation policies on health\noutcomes. Building on the SIRD model of disease transmission, I show that FE\nmodels tend to be misspecified for three reasons. First, despite misleading\ncommon trends in the pre-treatment period, the parallel trends assumption\ngenerally does not hold. Second, heterogeneity in infection rates and infected\npopulations across regions cannot be accounted for by region-specific fixed\neffects, nor by conditioning on observable time-varying confounders. Third,\nepidemiological theory predicts heterogeneous treatment effects across regions\nand over time. Via simulations, I find that the bias resulting from model\nmisspecification can be substantial, in magnitude and sometimes in sign.\nOverall, my results caution against the use of FE models for mitigation policy\nevaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.10949v1"
    },
    {
        "title": "Nonparametric inference on counterfactuals in first-price auctions",
        "authors": [
            "Pasha Andreyanov",
            "Grigory Franguridi"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In a classical model of the first-price sealed-bid auction with independent\nprivate values, we develop nonparametric estimators for several policy-relevant\ntargets, such as the bidder's surplus and auctioneer's revenue under\ncounterfactual reserve prices. Motivated by the linearity of these targets in\nthe quantile function of bidders' values, we propose an estimator of the latter\nand derive its Bahadur-Kiefer expansion. This makes it possible to construct\nuniform confidence bands and test complex hypotheses about the auction design.\nUsing the data on U.S. Forest Service timber auctions, we test whether setting\nzero reserve prices in these auctions was revenue maximizing.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.13856v3"
    },
    {
        "title": "Difference-in-Differences with a Continuous Treatment",
        "authors": [
            "Brantly Callaway",
            "Andrew Goodman-Bacon",
            "Pedro H. C. Sant'Anna"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper analyzes difference-in-differences designs with a continuous\ntreatment. We show that treatment effect on the treated-type parameters can be\nidentified under a generalized parallel trends assumption that is similar to\nthe binary treatment setup. However, interpreting differences in these\nparameters across different values of the treatment can be particularly\nchallenging due to treatment effect heterogeneity. We discuss alternative,\ntypically stronger, assumptions that alleviate these challenges. We also\nprovide a variety of treatment effect decomposition results, highlighting that\nparameters associated with popular linear two-way fixed-effect (TWFE)\nspecifications can be hard to interpret, \\emph{even} when there are only two\ntime periods. We introduce alternative estimation procedures that do not suffer\nfrom these TWFE drawbacks, and show in an application that they can lead to\ndifferent conclusions.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.02637v4"
    },
    {
        "title": "Dynamic Ordered Panel Logit Models",
        "authors": [
            "Bo E. Honoré",
            "Chris Muris",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper studies a dynamic ordered logit model for panel data with fixed\neffects. The main contribution of the paper is to construct a set of valid\nmoment conditions that are free of the fixed effects. The moment functions can\nbe computed using four or more periods of data, and the paper presents\nsufficient conditions for the moment conditions to identify the common\nparameters of the model, namely the regression coefficients, the autoregressive\nparameters, and the threshold parameters. The availability of moment conditions\nsuggests that these common parameters can be estimated using the generalized\nmethod of moments, and the paper documents the performance of this estimator\nusing Monte Carlo simulations and an empirical illustration to self-reported\nhealth status using the British Household Panel Survey.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.03253v4"
    },
    {
        "title": "Testability of Reverse Causality Without Exogenous Variation",
        "authors": [
            "Christoph Breunig",
            "Patrick Burauel"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper shows that testability of reverse causality is possible even in\nthe absence of exogenous variation, such as in the form of instrumental\nvariables. Instead of relying on exogenous variation, we achieve testability by\nimposing relatively weak model restrictions and exploiting that a dependence of\nresidual and purported cause is informative about the causal direction. Our\nmain assumption is that the true functional relationship is nonlinear and that\nerror terms are additively separable. We extend previous results by\nincorporating control variables and allowing heteroskedastic errors. We build\non reproducing kernel Hilbert space (RKHS) embeddings of probability\ndistributions to test conditional independence and demonstrate the efficacy in\ndetecting the causal direction in both Monte Carlo simulations and an\napplication to German survey data.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.05936v2"
    },
    {
        "title": "Identification of Average Marginal Effects in Fixed Effects Dynamic\n  Discrete Choice Models",
        "authors": [
            "Victor Aguirregabiria",
            "Jesus M. Carro"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In nonlinear panel data models, fixed effects methods are often criticized\nbecause they cannot identify average marginal effects (AMEs) in short panels.\nThe common argument is that identifying AMEs requires knowledge of the\ndistribution of unobserved heterogeneity, but this distribution is not\nidentified in a fixed effects model with a short panel. In this paper, we\nderive identification results that contradict this argument. In a panel data\ndynamic logit model, and for $T$ as small as three, we prove the point\nidentification of different AMEs, including causal effects of changes in the\nlagged dependent variable or the last choice's duration. Our proofs are\nconstructive and provide simple closed-form expressions for the AMEs in terms\nof probabilities of choice histories. We illustrate our results using Monte\nCarlo experiments and with an empirical application of a dynamic structural\nmodel of consumer brand choice with state dependence.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.06141v2"
    },
    {
        "title": "Distributional Effects with Two-Sided Measurement Error: An Application\n  to Intergenerational Income Mobility",
        "authors": [
            "Brantly Callaway",
            "Tong Li",
            "Irina Murtazashvili"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper considers identification and estimation of distributional effect\nparameters that depend on the joint distribution of an outcome and another\nvariable of interest (\"treatment\") in a setting with \"two-sided\" measurement\nerror -- that is, where both variables are possibly measured with error.\nExamples of these parameters in the context of intergenerational income\nmobility include transition matrices, rank-rank correlations, and the poverty\nrate of children as a function of their parents' income, among others. Building\non recent work on quantile regression (QR) with measurement error in the\noutcome (particularly, Hausman, Liu, Luo, and Palmer (2021)), we show that,\ngiven (i) two linear QR models separately for the outcome and treatment\nconditional on other observed covariates and (ii) assumptions about the\nmeasurement error for each variable, one can recover the joint distribution of\nthe outcome and the treatment. Besides these conditions, our approach does not\nrequire an instrument, repeated measurements, or distributional assumptions\nabout the measurement error. Using recent data from the 1997 National\nLongitudinal Study of Youth, we find that accounting for measurement error\nnotably reduces several estimates of intergenerational mobility parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.09235v3"
    },
    {
        "title": "Recent Developments in Inference: Practicalities for Applied Economics",
        "authors": [
            "Jeffrey D. Michler",
            "Anna Josephson"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We provide a review of recent developments in the calculation of standard\nerrors and test statistics for statistical inference. While much of the focus\nof the last two decades in economics has been on generating unbiased\ncoefficients, recent years has seen a variety of advancements in correcting for\nnon-standard standard errors. We synthesize these recent advances in addressing\nchallenges to conventional inference, like heteroskedasticity, clustering,\nserial correlation, and testing multiple hypotheses. We also discuss recent\nadvancements in numerical methods, such as the bootstrap, wild bootstrap, and\nrandomization inference. We make three specific recommendations. First, applied\neconomists need to clearly articulate the challenges to statistical inference\nthat are present in data as well as the source of those challenges. Second,\nmodern computing power and statistical software means that applied economists\nhave no excuse for not correctly calculating their standard errors and test\nstatistics. Third, because complicated sampling strategies and research designs\nmake it difficult to work out the correct formula for standard errors and test\nstatistics, we believe that in the applied economics profession it should\nbecome standard practice to rely on asymptotic refinements to the distribution\nof an estimator or test statistic via bootstrapping. Throughout, we reference\nbuilt-in and user-written Stata commands that allow one to quickly calculate\naccurate standard errors and relevant test statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.09736v1"
    },
    {
        "title": "Estimating high-dimensional Markov-switching VARs",
        "authors": [
            "Kenwin Maung"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Maximum likelihood estimation of large Markov-switching vector\nautoregressions (MS-VARs) can be challenging or infeasible due to parameter\nproliferation. To accommodate situations where dimensionality may be of\ncomparable order to or exceeds the sample size, we adopt a sparse framework and\npropose two penalized maximum likelihood estimators with either the Lasso or\nthe smoothly clipped absolute deviation (SCAD) penalty. We show that both\nestimators are estimation consistent, while the SCAD estimator also selects\nrelevant parameters with probability approaching one. A modified EM-algorithm\nis developed for the case of Gaussian errors and simulations show that the\nalgorithm exhibits desirable finite sample performance. In an application to\nshort-horizon return predictability in the US, we estimate a 15 variable\n2-state MS-VAR(1) and obtain the often reported counter-cyclicality in\npredictability. The variable selection property of our estimators helps to\nidentify predictors that contribute strongly to predictability during economic\ncontractions but are otherwise irrelevant in expansions. Furthermore,\nout-of-sample analyses indicate that large MS-VARs can significantly outperform\n\"hard-to-beat\" predictors like the historical average.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.12552v1"
    },
    {
        "title": "Partial Identification and Inference for Conditional Distributions of\n  Treatment Effects",
        "authors": [
            "Sungwon Lee"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper considers identification and inference for the distribution of\ntreatment effects conditional on observable covariates. Since the conditional\ndistribution of treatment effects is not point identified without strong\nassumptions, we obtain bounds on the conditional distribution of treatment\neffects by using the Makarov bounds. We also consider the case where the\ntreatment is endogenous and propose two stochastic dominance assumptions to\ntighten the bounds. We develop a nonparametric framework to estimate the bounds\nand establish the asymptotic theory that is uniformly valid over the support of\ntreatment effects. An empirical example illustrates the usefulness of the\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.00723v6"
    },
    {
        "title": "Nested Pseudo Likelihood Estimation of Continuous-Time Dynamic Discrete\n  Games",
        "authors": [
            "Jason R. Blevins",
            "Minhae Kim"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We introduce a sequential estimator for continuous time dynamic discrete\nchoice models (single-agent models and games) by adapting the nested pseudo\nlikelihood (NPL) estimator of Aguirregabiria and Mira (2002, 2007), developed\nfor discrete time models with discrete time data, to the continuous time case\nwith data sampled either discretely (i.e., uniformly-spaced snapshot data) or\ncontinuously. We establish conditions for consistency and asymptotic normality\nof the estimator, a local convergence condition, and, for single agent models,\na zero Jacobian property assuring local convergence. We carry out a series of\nMonte Carlo experiments using an entry-exit game with five heterogeneous firms\nto confirm the large-sample properties and demonstrate finite-sample bias\nreduction via iteration. In our simulations we show that the convergence issues\ndocumented for the NPL estimator in discrete time models are less likely to\naffect comparable continuous-time models. We also show that there can be large\nbias in economically-relevant parameters, such as the competitive effect and\nentry cost, from estimating a misspecified discrete time model when in fact the\ndata generating process is a continuous time model.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.02182v2"
    },
    {
        "title": "Weighted asymmetric least squares regression with fixed-effects",
        "authors": [
            "Amadou Barry",
            "Karim Oualkacha",
            "Arthur Charpentier"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The fixed-effects model estimates the regressor effects on the mean of the\nresponse, which is inadequate to summarize the variable relationships in the\npresence of heteroscedasticity. In this paper, we adapt the asymmetric least\nsquares (expectile) regression to the fixed-effects model and propose a new\nmodel: expectile regression with fixed-effects $(\\ERFE).$ The $\\ERFE$ model\napplies the within transformation strategy to concentrate out the incidental\nparameter and estimates the regressor effects on the expectiles of the response\ndistribution. The $\\ERFE$ model captures the data heteroscedasticity and\neliminates any bias resulting from the correlation between the regressors and\nthe omitted factors. We derive the asymptotic properties of the $\\ERFE$\nestimators and suggest robust estimators of its covariance matrix. Our\nsimulations show that the $\\ERFE$ estimator is unbiased and outperforms its\ncompetitors. Our real data analysis shows its ability to capture data\nheteroscedasticity (see our R package, \\url{github.com/AmBarry/erfe}).\n",
        "pdf_link": "http://arxiv.org/pdf/2108.04737v1"
    },
    {
        "title": "Inference in high-dimensional regression models without the exact or\n  $L^p$ sparsity",
        "authors": [
            "Jooyoung Cha",
            "Harold D. Chiang",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper proposes a new method of inference in high-dimensional regression\nmodels and high-dimensional IV regression models. Estimation is based on a\ncombined use of the orthogonal greedy algorithm, high-dimensional Akaike\ninformation criterion, and double/debiased machine learning. The method of\ninference for any low-dimensional subvector of high-dimensional parameters is\nbased on a root-$N$ asymptotic normality, which is shown to hold without\nrequiring the exact sparsity condition or the $L^p$ sparsity condition.\nSimulation studies demonstrate superior finite-sample performance of this\nproposed method over those based on the LASSO or the random forest, especially\nunder less sparse models. We illustrate an application to production analysis\nwith a panel of Chilean firms.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.09520v2"
    },
    {
        "title": "Feasible Weighted Projected Principal Component Analysis for Factor\n  Models with an Application to Bond Risk Premia",
        "authors": [
            "Sung Hoon Choi"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  I develop a feasible weighted projected principal component (FPPC) analysis\nfor factor models in which observable characteristics partially explain the\nlatent factors. This novel method provides more efficient and accurate\nestimators than existing methods. To increase estimation efficiency, I take\ninto account both cross-sectional dependence and heteroskedasticity by using a\nconsistent estimator of the inverse error covariance matrix as the weight\nmatrix. To improve accuracy, I employ a projection approach using\ncharacteristics because it removes noise components in high-dimensional factor\nanalysis. By using the FPPC method, estimators of the factors and loadings have\nfaster rates of convergence than those of the conventional factor analysis.\nMoreover, I propose an FPPC-based diffusion index forecasting model. The\nlimiting distribution of the parameter estimates and the rate of convergence\nfor forecast errors are obtained. Using U.S. bond market and macroeconomic\ndata, I demonstrate that the proposed model outperforms models based on\nconventional principal component estimators. I also show that the proposed\nmodel performs well among a large group of machine learning techniques in\nforecasting excess bond returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.10250v3"
    },
    {
        "title": "Double Machine Learning and Automated Confounder Selection -- A\n  Cautionary Tale",
        "authors": [
            "Paul Hünermund",
            "Beyers Louw",
            "Itamar Caspi"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Double machine learning (DML) has become an increasingly popular tool for\nautomated variable selection in high-dimensional settings. Even though the\nability to deal with a large number of potential covariates can render\nselection-on-observables assumptions more plausible, there is at the same time\na growing risk that endogenous variables are included, which would lead to the\nviolation of conditional independence. This paper demonstrates that DML is very\nsensitive to the inclusion of only a few \"bad controls\" in the covariate space.\nThe resulting bias varies with the nature of the theoretical causal model,\nwhich raises concerns about the feasibility of selecting control variables in a\ndata-driven way.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.11294v4"
    },
    {
        "title": "Revisiting Event Study Designs: Robust and Efficient Estimation",
        "authors": [
            "Kirill Borusyak",
            "Xavier Jaravel",
            "Jann Spiess"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We develop a framework for difference-in-differences designs with staggered\ntreatment adoption and heterogeneous causal effects. We show that conventional\nregression-based estimators fail to provide unbiased estimates of relevant\nestimands absent strong restrictions on treatment-effect homogeneity. We then\nderive the efficient estimator addressing this challenge, which takes an\nintuitive \"imputation\" form when treatment-effect heterogeneity is\nunrestricted. We characterize the asymptotic behavior of the estimator, propose\ntools for inference, and develop tests for identifying assumptions. Our method\napplies with time-varying controls, in triple-difference designs, and with\ncertain non-binary treatments. We show the practical relevance of our results\nin a simulation study and an application. Studying the consumption response to\ntax rebates in the United States, we find that the notional marginal propensity\nto consume is between 8 and 11 percent in the first quarter - about half as\nlarge as benchmark estimates used to calibrate macroeconomic models - and\npredominantly occurs in the first month after the rebate.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.12419v5"
    },
    {
        "title": "Wild Bootstrap for Instrumental Variables Regressions with Weak and Few\n  Clusters",
        "authors": [
            "Wenjie Wang",
            "Yichong Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We study the wild bootstrap inference for instrumental variable regressions\nin the framework of a small number of large clusters in which the number of\nclusters is viewed as fixed and the number of observations for each cluster\ndiverges to infinity. We first show that the wild bootstrap Wald test, with or\nwithout using the cluster-robust covariance estimator, controls size\nasymptotically up to a small error as long as the parameters of endogenous\nvariables are strongly identified in at least one of the clusters. Then, we\nestablish the required number of strong clusters for the test to have power\nagainst local alternatives. We further develop a wild bootstrap Anderson-Rubin\ntest for the full-vector inference and show that it controls size\nasymptotically up to a small error even under weak or partial identification in\nall clusters. We illustrate the good finite sample performance of the new\ninference methods using simulations and provide an empirical application to a\nwell-known dataset about US local labor markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.13707v5"
    },
    {
        "title": "How to Detect Network Dependence in Latent Factor Models? A\n  Bias-Corrected CD Test",
        "authors": [
            "M. Hashem Pesaran",
            "Yimeng Xie"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In a recent paper Juodis and Reese (2022) (JR) show that the application of\nthe CD test proposed by Pesaran (2004) to residuals from panels with latent\nfactors results in over-rejection. They propose a randomized test statistic to\ncorrect for over-rejection, and add a screening component to achieve power.\nThis paper considers the same problem but from a different perspective, and\nshows that the standard CD test remains valid if the latent factors are weak in\nthe sense the strength is less than half. In the case where latent factors are\nstrong, we propose a bias-corrected version, CD*, which is shown to be\nasymptotically standard normal under the null of error cross-sectional\nindependence and have power against network type alternatives. This result is\nshown to hold for pure latent factor models as well as for panel regression\nmodels with latent factors. The case where the errors are serially correlated\nis also considered. Small sample properties of the CD* test are investigated by\nMonte Carlo experiments and are shown to have the correct size for strong and\nweak factors as well as for Gaussian and non-Gaussian errors. In contrast, it\nis found that JR's test tends to over-reject in the case of panels with\nnon-Gaussian errors, and has low power against spatial network alternatives. In\nan empirical application, using the CD* test, it is shown that there remains\nspatial error dependence in a panel data model for real house price changes\nacross 377 Metropolitan Statistical Areas in the U.S., even after the effects\nof latent factors are filtered out.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.00408v4"
    },
    {
        "title": "Dynamic Games in Empirical Industrial Organization",
        "authors": [
            "Victor Aguirregabiria",
            "Allan Collard-Wexler",
            "Stephen P. Ryan"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This survey is organized around three main topics: models, econometrics, and\nempirical applications. Section 2 presents the theoretical framework,\nintroduces the concept of Markov Perfect Nash Equilibrium, discusses existence\nand multiplicity, and describes the representation of this equilibrium in terms\nof conditional choice probabilities. We also discuss extensions of the basic\nframework, including models in continuous time, the concepts of oblivious\nequilibrium and experience-based equilibrium, and dynamic games where firms\nhave non-equilibrium beliefs. In section 3, we first provide an overview of the\ntypes of data used in this literature, before turning to a discussion of\nidentification issues and results, and estimation methods. We review different\nmethods to deal with multiple equilibria and large state spaces. We also\ndescribe recent developments for estimating games in continuous time and\nincorporating serially correlated unobservables, and discuss the use of machine\nlearning methods to solving and estimating dynamic games. Section 4 discusses\nempirical applications of dynamic games in IO. We start describing the first\nempirical applications in this literature during the early 2000s. Then, we\nreview recent applications dealing with innovation, antitrust and mergers,\ndynamic pricing, regulation, product repositioning, advertising, uncertainty\nand investment, airline network competition, dynamic matching, and natural\nresources. We conclude with our view of the progress made in this literature\nand the remaining challenges.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.01725v2"
    },
    {
        "title": "A Framework for Using Value-Added in Regressions",
        "authors": [
            "Antoine Deeb"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  As increasingly popular metrics of worker and institutional quality,\nestimated value-added (VA) measures are now widely used as dependent or\nexplanatory variables in regressions. For example, VA is used as an explanatory\nvariable when examining the relationship between teacher VA and students'\nlong-run outcomes. Due to the multi-step nature of VA estimation, the standard\nerrors (SEs) researchers routinely use when including VA measures in OLS\nregressions are incorrect. In this paper, I show that the assumptions\nunderpinning VA models naturally lead to a generalized method of moments (GMM)\nframework. Using this insight, I construct correct SEs' for regressions that\nuse VA as an explanatory variable and for regressions where VA is the outcome.\nIn addition, I identify the causes of incorrect SEs when using OLS, discuss the\nneed to adjust SEs under different sets of assumptions, and propose a more\nefficient estimator for using VA as an explanatory variable. Finally, I\nillustrate my results using data from North Carolina, and show that correcting\nSEs results in an increase that is larger than the impact of clustering SEs.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.01741v3"
    },
    {
        "title": "The multilayer architecture of the global input-output network and its\n  properties",
        "authors": [
            "Rosanna Grassi",
            "Paolo Bartesaghi",
            "Gian Paolo Clemente",
            "Duc Thi Luu"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We analyze the multilayer architecture of the global input-output network\nusing sectoral trade data (WIOD, 2016 release). With a focus on the mesoscale\nstructure and related properties, our multilayer analysis takes into\nconsideration the splitting into industry-based layers in order to catch more\npeculiar relationships between countries that cannot be detected from the\nanalysis of the single-layer aggregated network. We can identify several large\ninternational communities in which some countries trade more intensively in\nsome specific layers. However, interestingly, our results show that these\nclusters can restructure and evolve over time. In general, not only their\ninternal composition changes, but the centrality rankings of the members inside\nare also reordered, industries from some countries diminishing their role and\nothers from other countries growing importance. These changes in the large\ninternational clusters may reflect the outcomes and the dynamics of\ncooperation, partner selection and competition among industries and among\ncountries in the global input-output network.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.02946v2"
    },
    {
        "title": "Some Impossibility Results for Inference With Cluster Dependence with\n  Large Clusters",
        "authors": [
            "Denis Kojevnikov",
            "Kyungchul Song"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper focuses on a setting with observations having a cluster dependence\nstructure and presents two main impossibility results. First, we show that when\nthere is only one large cluster, i.e., the researcher does not have any\nknowledge on the dependence structure of the observations, it is not possible\nto consistently discriminate the mean. When within-cluster observations satisfy\nthe uniform central limit theorem, we also show that a sufficient condition for\nconsistent $\\sqrt{n}$-discrimination of the mean is that we have at least two\nlarge clusters. This result shows some limitations for inference when we lack\ninformation on the dependence structure of observations. Our second result\nprovides a necessary and sufficient condition for the cluster structure that\nthe long run variance is consistently estimable. Our result implies that when\nthere is at least one large cluster, the long run variance is not consistently\nestimable.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.03971v4"
    },
    {
        "title": "Variable Selection for Causal Inference via Outcome-Adaptive Random\n  Forest",
        "authors": [
            "Daniel Jacob"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Estimating a causal effect from observational data can be biased if we do not\ncontrol for self-selection. This selection is based on confounding variables\nthat affect the treatment assignment and the outcome. Propensity score methods\naim to correct for confounding. However, not all covariates are confounders. We\npropose the outcome-adaptive random forest (OARF) that only includes desirable\nvariables for estimating the propensity score to decrease bias and variance.\nOur approach works in high-dimensional datasets and if the outcome and\npropensity score model are non-linear and potentially complicated. The OARF\nexcludes covariates that are not associated with the outcome, even in the\npresence of a large number of spurious variables. Simulation results suggest\nthat the OARF produces unbiased estimates, has a smaller variance and is\nsuperior in variable selection compared to other approaches. The results from\ntwo empirical examples, the effect of right heart catheterization on mortality\nand the effect of maternal smoking during pregnancy on birth weight, show\ncomparable treatment effects to previous findings but tighter confidence\nintervals and more plausible selected variables.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.04154v1"
    },
    {
        "title": "{did2s}: Two-Stage Difference-in-Differences",
        "authors": [
            "Kyle Butts",
            "John Gardner"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Recent work has highlighted the difficulties of estimating\ndifference-in-differences models when treatment timing occurs at different\ntimes for different units. This article introduces the R package did2s which\nimplements the estimator introduced in Gardner (2021). The article provides an\napproachable review of the underlying econometric theory and introduces the\nsyntax for the function did2s. Further, the package introduces a function,\nevent_study, that provides a common syntax for all the modern event-study\nestimators and plot_event_study to plot the results of each estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.05913v2"
    },
    {
        "title": "Bayesian hierarchical analysis of a multifaceted program against extreme\n  poverty",
        "authors": [
            "Louis Charlot"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The evaluation of a multifaceted program against extreme poverty in different\ndeveloping countries gave encouraging results, but with important heterogeneity\nbetween countries. This master thesis proposes to study this heterogeneity with\na Bayesian hierarchical analysis. The analysis we carry out with two different\nhierarchical models leads to a very low amount of pooling of information\nbetween countries, indicating that this observed heterogeneity should be\ninterpreted mostly as true heterogeneity, and not as sampling error. We analyze\nthe first order behavior of our hierarchical models, in order to understand\nwhat leads to this very low amount of pooling. We try to give to this work a\ndidactic approach, with an introduction of Bayesian analysis and an explanation\nof the different modeling and computational choices of our analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.06759v1"
    },
    {
        "title": "Geographic Difference-in-Discontinuities",
        "authors": [
            "Kyle Butts"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  A recent econometric literature has critiqued the use of regression\ndiscontinuities where administrative borders serves as the 'cutoff'.\nIdentification in this context is difficult since multiple treatments can\nchange at the cutoff and individuals can easily sort on either side of the\nborder. This note extends the difference-in-discontinuities framework discussed\nin Grembi et. al. (2016) to a geographic setting. The paper formalizes the\nidentifying assumptions in this context which will allow for the removal of\ntime-invariant sorting and compound-treatments similar to the\ndifference-in-differences methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.07406v2"
    },
    {
        "title": "Structural Estimation of Matching Markets with Transferable Utility",
        "authors": [
            "Alfred Galichon",
            "Bernard Salanié"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper provides an introduction to structural estimation methods for\nmatching markets with transferable utility.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.07932v1"
    },
    {
        "title": "Standard Errors for Calibrated Parameters",
        "authors": [
            "Matthew D. Cocci",
            "Mikkel Plagborg-Møller"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Calibration, the practice of choosing the parameters of a structural model to\nmatch certain empirical moments, can be viewed as minimum distance estimation.\nExisting standard error formulas for such estimators require a consistent\nestimate of the correlation structure of the empirical moments, which is often\nunavailable in practice. Instead, the variances of the individual empirical\nmoments are usually readily estimable. Using only these variances, we derive\nconservative standard errors and confidence intervals for the structural\nparameters that are valid even under the worst-case correlation structure. In\nthe over-identified case, we show that the moment weighting scheme that\nminimizes the worst-case estimator variance amounts to a moment selection\nproblem with a simple solution. Finally, we develop tests of over-identifying\nor parameter restrictions. We apply our methods empirically to a model of menu\ncost pricing for multi-product firms and to a heterogeneous agent New Keynesian\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.08109v3"
    },
    {
        "title": "Short and Simple Confidence Intervals when the Directions of Some\n  Effects are Known",
        "authors": [
            "Philipp Ketz",
            "Adam McCloskey"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We provide adaptive confidence intervals on a parameter of interest in the\npresence of nuisance parameters when some of the nuisance parameters have known\nsigns. The confidence intervals are adaptive in the sense that they tend to be\nshort at and near the points where the nuisance parameters are equal to zero.\nWe focus our results primarily on the practical problem of inference on a\ncoefficient of interest in the linear regression model when it is unclear\nwhether or not it is necessary to include a subset of control variables whose\npartial effects on the dependent variable have known directions (signs). Our\nconfidence intervals are trivial to compute and can provide significant length\nreductions relative to standard confidence intervals in cases for which the\ncontrol variables do not have large effects. At the same time, they entail\nminimal length increases at any parameter values. We prove that our confidence\nintervals are asymptotically valid uniformly over the parameter space and\nillustrate their length properties in an empirical application to a factorial\ndesign field experiment and a Monte Carlo study calibrated to the empirical\napplication.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.08222v1"
    },
    {
        "title": "Composite Likelihood for Stochastic Migration Model with Unobserved\n  Factor",
        "authors": [
            "Antoine Djogbenou",
            "Christian Gouriéroux",
            "Joann Jasiak",
            "Maygol Bandehali"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We introduce the conditional Maximum Composite Likelihood (MCL) estimation\nmethod for the stochastic factor ordered Probit model of credit rating\ntransitions of firms. This model is recommended for internal credit risk\nassessment procedures in banks and financial institutions under the Basel III\nregulations. Its exact likelihood function involves a high-dimensional\nintegral, which can be approximated numerically before maximization. However,\nthe estimated migration risk and required capital tend to be sensitive to the\nquality of this approximation, potentially leading to statistical regulatory\narbitrage. The proposed conditional MCL estimator circumvents this problem and\nmaximizes the composite log-likelihood of the factor ordered Probit model. We\npresent three conditional MCL estimators of different complexity and examine\ntheir consistency and asymptotic normality when n and T tend to infinity. The\nperformance of these estimators at finite T is examined and compared with a\ngranularity-based approach in a simulation study. The use of the MCL estimator\nis also illustrated in an empirical application.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.09043v2"
    },
    {
        "title": "Tests for Group-Specific Heterogeneity in High-Dimensional Factor Models",
        "authors": [
            "Antoine Djogbenou",
            "Razvan Sufana"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Standard high-dimensional factor models assume that the comovements in a\nlarge set of variables could be modeled using a small number of latent factors\nthat affect all variables. In many relevant applications in economics and\nfinance, heterogenous comovements specific to some known groups of variables\nnaturally arise, and reflect distinct cyclical movements within those groups.\nThis paper develops two new statistical tests that can be used to investigate\nwhether there is evidence supporting group-specific heterogeneity in the data.\nThe first test statistic is designed for the alternative hypothesis of\ngroup-specific heterogeneity appearing in at least one pair of groups; the\nsecond is for the alternative of group-specific heterogeneity appearing in all\npairs of groups. We show that the second moment of factor loadings changes\nacross groups when heterogeneity is present, and use this feature to establish\nthe theoretical validity of the tests. We also propose and prove the validity\nof a permutation approach for approximating the asymptotic distributions of the\ntwo test statistics. The simulations and the empirical financial application\nindicate that the proposed tests are useful for detecting group-specific\nheterogeneity.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.09049v2"
    },
    {
        "title": "Algorithms for Inference in SVARs Identified with Sign and Zero\n  Restrictions",
        "authors": [
            "Matthew Read"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  I develop algorithms to facilitate Bayesian inference in structural vector\nautoregressions that are set-identified with sign and zero restrictions by\nshowing that the system of restrictions is equivalent to a system of sign\nrestrictions in a lower-dimensional space. Consequently, algorithms applicable\nunder sign restrictions can be extended to allow for zero restrictions.\nSpecifically, I extend algorithms proposed in Amir-Ahmadi and Drautzburg (2021)\nto check whether the identified set is nonempty and to sample from the\nidentified set without rejection sampling. I compare the new algorithms to\nalternatives by applying them to variations of the model considered by Arias et\nal. (2019), who estimate the effects of US monetary policy using sign and zero\nrestrictions on the monetary policy reaction function. The new algorithms are\nparticularly useful when a rich set of sign restrictions substantially\ntruncates the identified set given the zero restrictions.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.10676v2"
    },
    {
        "title": "Linear Panel Regressions with Two-Way Unobserved Heterogeneity",
        "authors": [
            "Hugo Freeman",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We study linear panel regression models in which the unobserved error term is\nan unknown smooth function of two-way unobserved fixed effects. In standard\nadditive or interactive fixed effect models the individual specific and time\nspecific effects are assumed to enter with a known functional form (additive or\nmultiplicative). In this paper, we allow for this functional form to be more\ngeneral and unknown. We discuss two different estimation approaches that allow\nconsistent estimation of the regression parameters in this setting as the\nnumber of individuals and the number of time periods grow to infinity. The\nfirst approach uses the interactive fixed effect estimator in Bai (2009), which\nis still applicable here, as long as the number of factors in the estimation\ngrows asymptotically. The second approach first discretizes the two-way\nunobserved heterogeneity (similar to what Bonhomme, Lamadon and Manresa 2021\nare doing for one-way heterogeneity) and then estimates a simple linear fixed\neffect model with additive two-way grouped fixed effects. For both estimation\nmethods we obtain asymptotic convergence results, perform Monte Carlo\nsimulations, and employ the estimators in an empirical application to UK house\nprice data.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.11911v3"
    },
    {
        "title": "No-Regret Forecasting with Egalitarian Committees",
        "authors": [
            "Jiun-Hua Su"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The forecast combination puzzle is often found in literature: The\nequal-weight scheme tends to outperform sophisticated methods of combining\nindividual forecasts. Exploiting this finding, we propose a hedge egalitarian\ncommittees algorithm (HECA), which can be implemented via mixed integer\nquadratic programming. Specifically, egalitarian committees are formed by the\nridge regression with shrinkage toward equal weights; subsequently, the\nforecasts provided by these committees are averaged by the hedge algorithm. We\nestablish the no-regret property of HECA. Using data collected from the ECB\nSurvey of Professional Forecasters, we find the superiority of HECA relative to\nthe equal-weight scheme during the COVID-19 recession.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.13801v1"
    },
    {
        "title": "Testing the Presence of Implicit Hiring Quotas with Application to\n  German Universities",
        "authors": [
            "Lena Janys"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  It is widely accepted that women are underrepresented in academia in general\nand economics in particular. This paper introduces a test to detect an\nunder-researched form of hiring bias: implicit quotas. I derive a test under\nthe Null of random hiring that requires no information about individual hires\nunder some assumptions. I derive the asymptotic distribution of this test\nstatistic and, as an alternative, propose a parametric bootstrap procedure that\nsamples from the exact distribution. This test can be used to analyze a variety\nof other hiring settings. I analyze the distribution of female professors at\nGerman universities across 50 different disciplines. I show that the\ndistribution of women, given the average number of women in the respective\nfield, is highly unlikely to result from a random allocation of women across\ndepartments and more likely to stem from an implicit quota of one or two women\non the department level. I also show that a large part of the variation in the\nshare of women across STEM and non-STEM disciplines could be explained by a\ntwo-women quota on the department level. These findings have important\nimplications for the potential effectiveness of policies aimed at reducing\nunderrepresentation and providing evidence of how stakeholders perceive and\nevaluate diversity.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.14343v2"
    },
    {
        "title": "Nonparametric Bounds on Treatment Effects with Imperfect Instruments",
        "authors": [
            "Kyunghoon Ban",
            "Désiré Kédagni"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper extends the identification results in Nevo and Rosen (2012) to\nnonparametric models. We derive nonparametric bounds on the average treatment\neffect when an imperfect instrument is available. As in Nevo and Rosen (2012),\nwe assume that the correlation between the imperfect instrument and the\nunobserved latent variables has the same sign as the correlation between the\nendogenous variable and the latent variables. We show that the monotone\ntreatment selection and monotone instrumental variable restrictions, introduced\nby Manski and Pepper (2000, 2009), jointly imply this assumption. Moreover, we\nshow how the monotone treatment response assumption can help tighten the\nbounds. The identified set can be written in the form of intersection bounds,\nwhich is more conducive to inference. We illustrate our methodology using the\nNational Longitudinal Survey of Young Men data to estimate returns to\nschooling.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.14785v1"
    },
    {
        "title": "Identification and Estimation in a Time-Varying Endogenous Random\n  Coefficient Panel Data Model",
        "authors": [
            "Ming Li"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper proposes a correlated random coefficient linear panel data model,\nwhere regressors can be correlated with time-varying and individual-specific\nrandom coefficients through both a fixed effect and a time-varying random\nshock. I develop a new panel data-based identification method to identify the\naverage partial effect and the local average response function. The\nidentification strategy employs a sufficient statistic to control for the fixed\neffect and a conditional control variable for the random shock. Conditional on\nthese two controls, the residual variation in the regressors is driven solely\nby the exogenous instrumental variables, and thus can be exploited to identify\nthe parameters of interest. The constructive identification analysis leads to\nthree-step series estimators, for which I establish rates of convergence and\nasymptotic normality. To illustrate the method, I estimate a heterogeneous\nCobb-Douglas production function for manufacturing firms in China, finding\nsubstantial variations in output elasticities across firms.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.00982v2"
    },
    {
        "title": "Effect or Treatment Heterogeneity? Policy Evaluation with Aggregated and\n  Disaggregated Treatments",
        "authors": [
            "Phillip Heiler",
            "Michael C. Knaus"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Binary treatments are often ex-post aggregates of multiple treatments or can\nbe disaggregated into multiple treatment versions. Thus, effects can be\nheterogeneous due to either effect or treatment heterogeneity. We propose a\ndecomposition method that uncovers masked heterogeneity, avoids spurious\ndiscoveries, and evaluates treatment assignment quality. The estimation and\ninference procedure based on double/debiased machine learning allows for\nhigh-dimensional confounding, many treatments and extreme propensity scores.\nOur applications suggest that heterogeneous effects of smoking on birthweight\nare partially due to different smoking intensities and that gender gaps in Job\nCorps effectiveness are largely explained by differential selection into\nvocational training.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.01427v4"
    },
    {
        "title": "New insights into price drivers of crude oil futures markets: Evidence\n  from quantile ARDL approach",
        "authors": [
            "Hao-Lin Shao",
            "Ying-Hui Shao",
            "Yan-Hong Yang"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper investigates the cointegration between possible determinants of\ncrude oil futures prices during the COVID-19 pandemic period. We perform\ncomparative analysis of WTI and newly-launched Shanghai crude oil futures (SC)\nvia the Autoregressive Distributed Lag (ARDL) model and Quantile Autoregressive\nDistributed Lag (QARDL) model. The empirical results confirm that economic\npolicy uncertainty, stock markets, interest rates and coronavirus panic are\nimportant drivers of WTI futures prices. Our findings also suggest that the US\nand China's stock markets play vital roles in movements of SC futures prices.\nMeanwhile, CSI300 stock index has a significant positive short-run impact on SC\nfutures prices while S\\&P500 prices possess a positive nexus with SC futures\nprices both in long-run and short-run. Overall, these empirical evidences\nprovide practical implications for investors and policymakers.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.02693v1"
    },
    {
        "title": "Dyadic double/debiased machine learning for analyzing determinants of\n  free trade agreements",
        "authors": [
            "Harold D Chiang",
            "Yukun Ma",
            "Joel Rodrigue",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper presents novel methods and theories for estimation and inference\nabout parameters in econometric models using machine learning for nuisance\nparameters estimation when data are dyadic. We propose a dyadic cross fitting\nmethod to remove over-fitting biases under arbitrary dyadic dependence.\nTogether with the use of Neyman orthogonal scores, this novel cross fitting\nmethod enables root-$n$ consistent estimation and inference robustly against\ndyadic dependence. We illustrate an application of our general framework to\nhigh-dimensional network link formation models. With this method applied to\nempirical data of international economic networks, we reexamine determinants of\nfree trade agreements (FTA) viewed as links formed in the dyad composed of\nworld economies. We document that standard methods may lead to misleading\nconclusions for numerous classic determinants of FTA formation due to biased\npoint estimates or standard errors which are too small.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.04365v3"
    },
    {
        "title": "Estimating High Dimensional Monotone Index Models by Iterative Convex\n  Optimization1",
        "authors": [
            "Shakeeb Khan",
            "Xiaoying Lan",
            "Elie Tamer",
            "Qingsong Yao"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In this paper we propose new approaches to estimating large dimensional\nmonotone index models. This class of models has been popular in the applied and\ntheoretical econometrics literatures as it includes discrete choice,\nnonparametric transformation, and duration models. A main advantage of our\napproach is computational. For instance, rank estimation procedures such as\nthose proposed in Han (1987) and Cavanagh and Sherman (1998) that optimize a\nnonsmooth, non convex objective function are difficult to use with more than a\nfew regressors and so limits their use in with economic data sets. For such\nmonotone index models with increasing dimension, we propose to use a new class\nof estimators based on batched gradient descent (BGD) involving nonparametric\nmethods such as kernel estimation or sieve estimation, and study their\nasymptotic properties. The BGD algorithm uses an iterative procedure where the\nkey step exploits a strictly convex objective function, resulting in\ncomputational advantages. A contribution of our approach is that our model is\nlarge dimensional and semiparametric and so does not require the use of\nparametric distributional assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.04388v2"
    },
    {
        "title": "Nonparametric Tests of Conditional Independence for Time Series",
        "authors": [
            "Xiaojun Song",
            "Haoyu Wei"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose consistent nonparametric tests of conditional independence for\ntime series data. Our methods are motivated from the difference between joint\nconditional cumulative distribution function (CDF) and the product of\nconditional CDFs. The difference is transformed into a proper conditional\nmoment restriction (CMR), which forms the basis for our testing procedure. Our\ntest statistics are then constructed using the integrated moment restrictions\nthat are equivalent to the CMR. We establish the asymptotic behavior of the\ntest statistics under the null, the alternative, and the sequence of local\nalternatives converging to conditional independence at the parametric rate. Our\ntests are implemented with the assistance of a multiplier bootstrap. Monte\nCarlo simulations are conducted to evaluate the finite sample performance of\nthe proposed tests. We apply our tests to examine the predictability of equity\nrisk premium using variance risk premium for different horizons and find that\nthere exist various degrees of nonlinear predictability at mid-run and long-run\nhorizons.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.04847v1"
    },
    {
        "title": "Smooth Tests for Normality in ANOVA",
        "authors": [
            "Haoyu Wei",
            "Xiaojun Song"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The normality assumption for errors in the Analysis of Variance (ANOVA) is\ncommon when using ANOVA models. But there are few people to test this normality\nassumption before using ANOVA models, and the existent literature also rarely\nmentions this problem. In this article, we propose an easy-to-use method to\ntesting the normality assumption in ANOVA models by using smooth tests. The\ntest statistic we propose has asymptotic chi-square distribution and our tests\nare always consistent in various different types of ANOVA models. Discussion\nabout how to choose the dimension of the smooth model (the number of the basis\nfunctions) are also included in this article. Several simulation experiments\nshow the superiority of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.04849v1"
    },
    {
        "title": "Fixed $T$ Estimation of Linear Panel Data Models with Interactive Fixed\n  Effects",
        "authors": [
            "Ayden Higgins"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper studies the estimation of linear panel data models with\ninteractive fixed effects, where one dimension of the panel, typically time,\nmay be fixed. To this end, a novel transformation is introduced that reduces\nthe model to a lower dimension, and, in doing so, relieves the model of\nincidental parameters in the cross-section. The central result of this paper\ndemonstrates that transforming the model and then applying the principal\ncomponent (PC) estimator of \\cite{bai_panel_2009} delivers $\\sqrt{n}$\nconsistent estimates of regression slope coefficients with $T$ fixed. Moreover,\nthese estimates are shown to be asymptotically unbiased in the presence of\ncross-sectional dependence, serial dependence, and with the inclusion of\ndynamic regressors, in stark contrast to the usual case. The large $n$, large\n$T$ properties of this approach are also studied, where many of these results\ncarry over to the case in which $n$ is growing sufficiently fast relative to\n$T$. Transforming the model also proves to be useful beyond estimation, a point\nillustrated by showing that with $T$ fixed, the eigenvalue ratio test of\n\\cite{horenstein} provides a consistent test for the number of factors when\napplied to the transformed model.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.05579v1"
    },
    {
        "title": "Partial Identification of Marginal Treatment Effects with discrete\n  instruments and misreported treatment",
        "authors": [
            "Santiago Acerenza"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper provides partial identification results for the marginal treatment\neffect ($MTE$) when the binary treatment variable is potentially misreported\nand the instrumental variable is discrete. Identification results are derived\nunder different sets of nonparametric assumptions. The identification results\nare illustrated in identifying the marginal treatment effects of food stamps on\nhealth.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.06285v3"
    },
    {
        "title": "Machine Learning, Deep Learning, and Hedonic Methods for Real Estate\n  Price Prediction",
        "authors": [
            "Mahdieh Yazdani"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In recent years several complaints about racial discrimination in appraising\nhome values have been accumulating. For several decades, to estimate the sale\nprice of the residential properties, appraisers have been walking through the\nproperties, observing the property, collecting data, and making use of the\nhedonic pricing models. However, this method bears some costs and by nature is\nsubjective and biased. To minimize human involvement and the biases in the real\nestate appraisals and boost the accuracy of the real estate market price\nprediction models, in this research we design data-efficient learning machines\ncapable of learning and extracting the relation or patterns between the inputs\n(features for the house) and output (value of the houses). We compare the\nperformance of some machine learning and deep learning algorithms, specifically\nartificial neural networks, random forest, and k nearest neighbor approaches to\nthat of hedonic method on house price prediction in the city of Boulder,\nColorado. Even though this study has been done over the houses in the city of\nBoulder it can be generalized to the housing market in any cities. The results\nindicate non-linear association between the dwelling features and dwelling\nprices. In light of these findings, this study demonstrates that random forest\nand artificial neural networks algorithms can be better alternatives over the\nhedonic regression analysis for prediction of the house prices in the city of\nBoulder, Colorado.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.07151v1"
    },
    {
        "title": "Choice probabilities and correlations in closed-form route choice\n  models: specifications and drawbacks",
        "authors": [
            "Fiore Tinessa",
            "Vittorio Marzano",
            "Andrea Papola"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper investigates the performance, in terms of choice probabilities and\ncorrelations, of existing and new specifications of closed-form route choice\nmodels with flexible correlation patterns, namely the Link Nested Logit (LNL),\nthe Paired Combinatorial Logit (PCL) and the more recent Combination of Nested\nLogit (CoNL) models. Following a consolidated track in the literature, choice\nprobabilities and correlations of the Multinomial Probit (MNP) model by\n(Daganzo and Sheffi, 1977) are taken as target. Laboratory experiments on\nsmall/medium-size networks are illustrated, also leveraging a procedure for\npractical calculation of correlations of any GEV models, proposed by (Marzano\n2014). Results show that models with inherent limitations in the coverage of\nthe domain of feasible correlations yield unsatisfactory performance, whilst\nthe specifications of the CoNL proposed in the paper appear the best in fitting\nboth MNP correlations and probabilities. Performance of the models are\nappreciably ameliorated by introducing lower bounds to the nesting parameters.\nOverall, the paper provides guidance for the practical application of tested\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.07224v1"
    },
    {
        "title": "Revisiting identification concepts in Bayesian analysis",
        "authors": [
            "Jean-Pierre Florens",
            "Anna Simoni"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper studies the role played by identification in the Bayesian analysis\nof statistical and econometric models. First, for unidentified models we\ndemonstrate that there are situations where the introduction of a\nnon-degenerate prior distribution can make a parameter that is nonidentified in\nfrequentist theory identified in Bayesian theory. In other situations, it is\npreferable to work with the unidentified model and construct a Markov Chain\nMonte Carlo (MCMC) algorithms for it instead of introducing identifying\nassumptions. Second, for partially identified models we demonstrate how to\nconstruct the prior and posterior distributions for the identified set\nparameter and how to conduct Bayesian analysis. Finally, for models that\ncontain some parameters that are identified and others that are not we show\nthat marginalizing out the identified parameter from the likelihood with\nrespect to its conditional prior, given the nonidentified parameter, allows the\ndata to be informative about the nonidentified and partially identified\nparameter. The paper provides examples and simulations that illustrate how to\nimplement our techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.09954v1"
    },
    {
        "title": "Difference-in-Differences with Geocoded Microdata",
        "authors": [
            "Kyle Butts"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper formalizes a common approach for estimating effects of treatment\nat a specific location using geocoded microdata. This estimator compares units\nimmediately next to treatment (an inner-ring) to units just slightly further\naway (an outer-ring). I introduce intuitive assumptions needed to identify the\naverage treatment effect among the affected units and illustrates pitfalls that\noccur when these assumptions fail. Since one of these assumptions requires\nknowledge of exactly how far treatment effects are experienced, I propose a new\nmethod that relaxes this assumption and allows for nonparametric estimation\nusing partitioning-based least squares developed in Cattaneo et. al. (2019).\nSince treatment effects typically decay/change over distance, this estimator\nimproves analysis by estimating a treatment effect curve as a function of\ndistance from treatment. This is contrast to the traditional method which, at\nbest, identifies the average effect of treatment. To illustrate the advantages\nof this method, I show that Linden and Rockoff (2008) under estimate the\neffects of increased crime risk on home values closest to the treatment and\noverestimate how far the effects extend by selecting a treatment ring that is\ntoo wide.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.10192v1"
    },
    {
        "title": "Bi-integrative analysis of two-dimensional heterogeneous panel data\n  model",
        "authors": [
            "Wei Wang",
            "Xiaodong Yan",
            "Yanyan Ren",
            "Zhijie Xiao"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Heterogeneous panel data models that allow the coefficients to vary across\nindividuals and/or change over time have received increasingly more attention\nin statistics and econometrics. This paper proposes a two-dimensional\nheterogeneous panel regression model that incorporate a group structure of\nindividual heterogeneous effects with cohort formation for their\ntime-variations, which allows common coefficients between nonadjacent time\npoints. A bi-integrative procedure that detects the information regarding group\nand cohort patterns simultaneously via a doubly penalized least square with\nconcave fused penalties is introduced. We use an alternating direction method\nof multipliers (ADMM) algorithm that automatically bi-integrates the\ntwo-dimensional heterogeneous panel data model pertaining to a common one.\nConsistency and asymptotic normality for the proposed estimators are developed.\nWe show that the resulting estimators exhibit oracle properties, i.e., the\nproposed estimator is asymptotically equivalent to the oracle estimator\nobtained using the known group and cohort structures. Furthermore, the\nsimulation studies provide supportive evidence that the proposed method has\ngood finite sample performance. A real data empirical application has been\nprovided to highlight the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.10480v1"
    },
    {
        "title": "Slow Movers in Panel Data",
        "authors": [
            "Yuya Sasaki",
            "Takuya Ura"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Panel data often contain stayers (units with no within-variations) and slow\nmovers (units with little within-variations). In the presence of many slow\nmovers, conventional econometric methods can fail to work. We propose a novel\nmethod of robust inference for the average partial effects in correlated random\ncoefficient models robustly across various distributions of within-variations,\nincluding the cases with many stayers and/or many slow movers in a unified\nmanner. In addition to this robustness property, our proposed method entails\nsmaller biases and hence improves accuracy in inference compared to existing\nalternatives. Simulation studies demonstrate our theoretical claims about these\nproperties: the conventional 95% confidence interval covers the true parameter\nvalue with 37-93% frequencies, whereas our proposed one achieves 93-96%\ncoverage frequencies.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.12041v1"
    },
    {
        "title": "Regime-Switching Density Forecasts Using Economists' Scenarios",
        "authors": [
            "Graziano Moramarco"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose an approach for generating macroeconomic density forecasts that\nincorporate information on multiple scenarios defined by experts. We adopt a\nregime-switching framework in which sets of scenarios (\"views\") are used as\nBayesian priors on economic regimes. Predictive densities coming from different\nviews are then combined by optimizing objective functions of density\nforecasting. We illustrate the approach with an empirical application to\nquarterly real-time forecasts of U.S. GDP growth, in which we exploit the Fed's\nmacroeconomic scenarios used for bank stress tests. We show that the approach\nachieves good accuracy in terms of average predictive scores and good\ncalibration of forecast distributions. Moreover, it can be used to evaluate the\ncontribution of economists' scenarios to density forecast performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.13761v2"
    },
    {
        "title": "Forecasting with a Panel Tobit Model",
        "authors": [
            "Laura Liu",
            "Hyungsik Roger Moon",
            "Frank Schorfheide"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We use a dynamic panel Tobit model with heteroskedasticity to generate\nforecasts for a large cross-section of short time series of censored\nobservations. Our fully Bayesian approach allows us to flexibly estimate the\ncross-sectional distribution of heterogeneous coefficients and then implicitly\nuse this distribution as prior to construct Bayes forecasts for the individual\ntime series. In addition to density forecasts, we construct set forecasts that\nexplicitly target the average coverage probability for the cross-section. We\npresent a novel application in which we forecast bank-level loan charge-off\nrates for small banks.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.14117v2"
    },
    {
        "title": "Testing and Estimating Structural Breaks in Time Series and Panel Data\n  in Stata",
        "authors": [
            "Jan Ditzen",
            "Yiannis Karavias",
            "Joakim Westerlund"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Identifying structural change is a crucial step in analysis of time series\nand panel data. The longer the time span, the higher the likelihood that the\nmodel parameters have changed as a result of major disruptive events, such as\nthe 2007-2008 financial crisis and the 2020 COVID-19 outbreak. Detecting the\nexistence of breaks, and dating them is therefore necessary not only for\nestimation purposes but also for understanding drivers of change and their\neffect on relationships. This article introduces a new community contributed\ncommand called xtbreak, which provides researchers with a complete toolbox for\nanalysing multiple structural breaks in time series and panel data. xtbreak can\ndetect the existence of breaks, determine their number and location, and\nprovide break date confidence intervals. The new command is used to explore\nchanges in the relationship between COVID-19 cases and deaths in the US using\nboth country-level time series data and state-level panel data.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.14550v2"
    },
    {
        "title": "Productivity Convergence in Manufacturing: A Hierarchical Panel Data\n  Approach",
        "authors": [
            "Guohua Feng",
            "Jiti Gao",
            "Bin Peng"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Despite its paramount importance in the empirical growth literature,\nproductivity convergence analysis has three problems that have yet to be\nresolved: (1) little attempt has been made to explore the hierarchical\nstructure of industry-level datasets; (2) industry-level technology\nheterogeneity has largely been ignored; and (3) cross-sectional dependence has\nrarely been allowed for. This paper aims to address these three problems within\na hierarchical panel data framework. We propose an estimation procedure and\nthen derive the corresponding asymptotic theory. Finally, we apply the\nframework to a dataset of 23 manufacturing industries from a wide range of\ncountries over the period 1963-2018. Our results show that both the\nmanufacturing industry as a whole and individual manufacturing industries at\nthe ISIC two-digit level exhibit strong conditional convergence in labour\nproductivity, but not unconditional convergence. In addition, our results show\nthat both global and industry-specific shocks are important in explaining the\nconvergence behaviours of the manufacturing industries.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.00449v1"
    },
    {
        "title": "On Time-Varying VAR Models: Estimation, Testing and Impulse Response\n  Analysis",
        "authors": [
            "Yayi Yan",
            "Jiti Gao",
            "Bin Peng"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Vector autoregressive (VAR) models are widely used in practical studies,\ne.g., forecasting, modelling policy transmission mechanism, and measuring\nconnection of economic agents. To better capture the dynamics, this paper\nintroduces a new class of time-varying VAR models in which the coefficients and\ncovariance matrix of the error innovations are allowed to change smoothly over\ntime. Accordingly, we establish a set of theories, including the impulse\nresponses analyses subject to both of the short-run timing and the long-run\nrestrictions, an information criterion to select the optimal lag, and a\nWald-type test to determine the constant coefficients. Simulation studies are\nconducted to evaluate the theoretical findings. Finally, we demonstrate the\nempirical relevance and usefulness of the proposed methods through an\napplication to the transmission mechanism of U.S. monetary policy.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.00450v1"
    },
    {
        "title": "Financial-cycle ratios and medium-term predictions of GDP: Evidence from\n  the United States",
        "authors": [
            "Graziano Moramarco"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Using a large quarterly macroeconomic dataset for the period 1960-2017, we\ndocument the ability of specific financial ratios from the housing market and\nfirms' aggregate balance sheets to predict GDP over medium-term horizons in the\nUnited States. A cyclically adjusted house price-to-rent ratio and the\nliabilities-to-income ratio of the non-financial non-corporate business sector\nprovide the best in-sample and out-of-sample predictions of GDP growth over\nhorizons of one to five years, based on a wide variety of rankings. Small\nforecasting models that include these indicators outperform popular\nhigh-dimensional models and forecast combinations. The predictive power of the\ntwo ratios appears strong during both recessions and expansions, stable over\ntime, and consistent with well-established macro-finance theory.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.00822v3"
    },
    {
        "title": "Nonparametric Cointegrating Regression Functions with Endogeneity and\n  Semi-Long Memory",
        "authors": [
            "Sepideh Mosaferi",
            "Mark S. Kaiser"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This article develops nonparametric cointegrating regression models with\nendogeneity and semi-long memory. We assume semi-long memory is produced in the\nregressor process by tempering of random shock coefficients. The fundamental\nproperties of long memory processes are thus retained in the regressor process.\nNonparametric nonlinear cointegrating regressions with serially dependent\nerrors and endogenous regressors that are driven by long memory innovations\nhave been considered in Wang and Phillips (2016). That work also implemented a\nstatistical specification test for testing whether the regression function\nfollows a parametric form. The convergence rate of the proposed test is\nparameter dependent, and its limit theory involves the local time of fractional\nBrownian motion. The present paper modifies the test statistic proposed for the\nlong memory case by Wang and Phillips (2016) to be suitable for the semi-long\nmemory case. With this modification, the limit theory for the test involves the\nlocal time of standard Brownian motion. Through simulation studies, we\ninvestigate properties of nonparametric regression function estimation with\nsemi-long memory regressors as well as long memory regressors.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.00972v2"
    },
    {
        "title": "Funding liquidity, credit risk and unconventional monetary policy in the\n  Euro area: A GVAR approach",
        "authors": [
            "Graziano Moramarco"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper investigates the transmission of funding liquidity shocks, credit\nrisk shocks and unconventional monetary policy within the Euro area. To this\naim, we estimate a financial GVAR model for Germany, France, Italy and Spain on\nmonthly data over the period 2006-2017. The interactions between repo markets,\nsovereign bonds and banks' CDS spreads are analyzed, explicitly accounting for\nthe country-specific effects of the ECB's asset purchase programmes. Impulse\nresponse analysis signals marginally significant core-periphery heterogeneity,\nflight-to-quality effects and spillovers between liquidity conditions and\ncredit risk. Simulated reductions in ECB programmes tend to result in higher\ngovernment bond yields and bank CDS spreads, especially for Italy and Spain, as\nwell as in falling repo trade volumes and rising repo rates across the Euro\narea. However, only a few responses to shocks achieve statistical significance.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.01078v2"
    },
    {
        "title": "Multiple-index Nonstationary Time Series Models: Robust Estimation\n  Theory and Practice",
        "authors": [
            "Chaohua Dong",
            "Jiti Gao",
            "Bin Peng",
            "Yundong Tu"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper proposes a class of parametric multiple-index time series models\nthat involve linear combinations of time trends, stationary variables and unit\nroot processes as regressors. The inclusion of the three different types of\ntime series, along with the use of a multiple-index structure for these\nvariables to circumvent the curse of dimensionality, is due to both theoretical\nand practical considerations. The M-type estimators (including OLS, LAD,\nHuber's estimator, quantile and expectile estimators, etc.) for the index\nvectors are proposed, and their asymptotic properties are established, with the\naid of the generalized function approach to accommodate a wide class of loss\nfunctions that may not be necessarily differentiable at every point. The\nproposed multiple-index model is then applied to study the stock return\npredictability, which reveals strong nonlinear predictability under various\nloss measures. Monte Carlo simulations are also included to evaluate the\nfinite-sample performance of the proposed estimators.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.02023v1"
    },
    {
        "title": "Multiplicative Component GARCH Model of Intraday Volatility",
        "authors": [
            "Xiufeng Yan"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper proposes a multiplicative component intraday volatility model. The\nintraday conditional volatility is expressed as the product of intraday\nperiodic component, intraday stochastic volatility component and daily\nconditional volatility component. I extend the multiplicative component\nintraday volatility model of Engle (2012) and Andersen and Bollerslev (1998) by\nincorporating the durations between consecutive transactions. The model can be\napplied to both regularly and irregularly spaced returns. I also provide a\nnonparametric estimation technique of the intraday volatility periodicity. The\nempirical results suggest the model can successfully capture the\ninterdependency of intraday returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.02376v1"
    },
    {
        "title": "occ2vec: A principal approach to representing occupations using natural\n  language processing",
        "authors": [
            "Nicolaj Søndergaard Mühlbach"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose \\textbf{occ2vec}, a principal approach to representing\noccupations, which can be used in matching, predictive and causal modeling, and\nother economic areas. In particular, we use it to score occupations on any\ndefinable characteristic of interest, say the degree of \\textquote{greenness}.\nUsing more than 17,000 occupation-specific text descriptors, we transform each\noccupation into a high-dimensional vector using natural language processing.\nSimilar, we assign a vector to the target characteristic and estimate the\noccupational degree of this characteristic as the cosine similarity between the\nvectors. The main advantages of this approach are its universal applicability\nand verifiability contrary to existing ad-hoc approaches. We extensively\nvalidate our approach on several exercises and then use it to estimate the\noccupational degree of charisma and emotional intelligence (EQ). We find that\noccupations that score high on these tend to have higher educational\nrequirements. Turning to wages, highly charismatic occupations are either found\nin the lower or upper tail in the wage distribution. This is not found for EQ,\nwhere higher levels of EQ are generally correlated with higher wages.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.02528v2"
    },
    {
        "title": "Structural Breaks in Interactive Effects Panels and the Stock Market\n  Reaction to COVID-19",
        "authors": [
            "Yiannis Karavias",
            "Paresh Narayan",
            "Joakim Westerlund"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Dealing with structural breaks is an important step in most, if not all,\nempirical economic research. This is particularly true in panel data comprised\nof many cross-sectional units, such as individuals, firms or countries, which\nare all affected by major events. The COVID-19 pandemic has affected most\nsectors of the global economy, and there is by now plenty of evidence to\nsupport this. The impact on stock markets is, however, still unclear. The fact\nthat most markets seem to have partly recovered while the pandemic is still\nongoing suggests that the relationship between stock returns and COVID-19 has\nbeen subject to structural change. It is therefore important to know if a\nstructural break has occurred and, if it has, to infer the date of the break.\nIn the present paper we take this last observation as a source of motivation to\ndevelop a new break detection toolbox that is applicable to different sized\npanels, easy to implement and robust to general forms of unobserved\nheterogeneity. The toolbox, which is the first of its kind, includes a test for\nstructural change, a break date estimator, and a break date confidence\ninterval. Application to a panel covering 61 countries from January 3 to\nSeptember 25, 2020, leads to the detection of a structural break that is dated\nto the first week of April. The effect of COVID-19 is negative before the break\nand zero thereafter, implying that while markets did react, the reaction was\nshort-lived. A possible explanation for this is the quantitative easing\nprograms announced by central banks all over the world in the second half of\nMarch.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.03035v1"
    },
    {
        "title": "Bootstrap inference for panel data quantile regression",
        "authors": [
            "Antonio F. Galvao",
            "Thomas Parker",
            "Zhijie Xiao"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper develops bootstrap methods for practical statistical inference in\npanel data quantile regression models with fixed effects. We consider\nrandom-weighted bootstrap resampling and formally establish its validity for\nasymptotic inference. The bootstrap algorithm is simple to implement in\npractice by using a weighted quantile regression estimation for fixed effects\npanel data. We provide results under conditions that allow for temporal\ndependence of observations within individuals, thus encompassing a large class\nof possible empirical applications. Monte Carlo simulations provide numerical\nevidence the proposed bootstrap methods have correct finite sample properties.\nFinally, we provide an empirical illustration using the environmental Kuznets\ncurve.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.03626v1"
    },
    {
        "title": "Pair copula constructions of point-optimal sign-based tests for\n  predictive linear and nonlinear regressions",
        "authors": [
            "Kaveh Salehzadeh Nobari"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose pair copula constructed point-optimal sign tests in the context of\nlinear and nonlinear predictive regressions with endogenous, persistent\nregressors, and disturbances exhibiting serial (nonlinear) dependence. The\nproposed approach entails considering the entire dependence structure of the\nsigns to capture the serial dependence, and building feasible test statistics\nbased on pair copula constructions of the sign process. The tests are exact and\nvalid in the presence of heavy tailed and nonstandard errors, as well as\nheterogeneous and persistent volatility. Furthermore, they may be inverted to\nbuild confidence regions for the parameters of the regression function.\nFinally, we adopt an adaptive approach based on the split-sample technique to\nmaximize the power of the test by finding an appropriate alternative\nhypothesis. In a Monte Carlo study, we compare the performance of the proposed\n\"quasi\"-point-optimal sign tests based on pair copula constructions by\ncomparing its size and power to those of certain existing tests that are\nintended to be robust against heteroskedasticity. The simulation results\nmaintain the superiority of our procedures to existing popular tests.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.04919v1"
    },
    {
        "title": "Bounds for Treatment Effects in the Presence of Anticipatory Behavior",
        "authors": [
            "Aibo Gong"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In program evaluations, units can often anticipate the implementation of a\nnew policy before it occurs. Such anticipatory behavior can lead to units'\noutcomes becoming dependent on their future treatment assignments. In this\npaper, I employ a potential-outcomes framework to analyze the treatment effect\nwith anticipation. I start with a classical difference-in-differences model\nwith two time periods and provide identified sets with easy-to-implement\nestimation and inference strategies for causal parameters. Empirical\napplications and generalizations are provided. I illustrate my results by\nanalyzing the effect of an early retirement incentive program for teachers,\nwhich the target units were likely to anticipate, on student achievement. The\nempirical results show the result can be overestimated by up to 30\\% in the\nworst case and demonstrate the potential pitfalls of failing to consider\nanticipation in policy evaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.06573v2"
    },
    {
        "title": "When Can We Ignore Measurement Error in the Running Variable?",
        "authors": [
            "Yingying Dong",
            "Michal Kolesár"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In many applications of regression discontinuity designs, the running\nvariable used by the administrator to assign treatment is only observed with\nerror. We show that, provided the observed running variable (i) correctly\nclassifies the treatment assignment, and (ii) affects the conditional means of\nthe potential outcomes smoothly, ignoring the measurement error nonetheless\nyields an estimate with a causal interpretation: the average treatment effect\nfor units whose observed running variable equals to the cutoff. We show that,\npossibly after doughnut trimming, these assumptions accommodate a variety of\nsettings where support of the measurement error is not too wide. We propose to\nconduct inference using bias-aware methods, which remain valid even when\ndiscreteness or irregular support in the observed running variable may lead to\npartial identification. We illustrate the results for both sharp and fuzzy\ndesigns in an empirical application.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.07388v4"
    },
    {
        "title": "Dynamic Network Quantile Regression Model",
        "authors": [
            "Xiu Xu",
            "Weining Wang",
            "Yongcheol Shin",
            "Chaowen Zheng"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose a dynamic network quantile regression model to investigate the\nquantile connectedness using a predetermined network information. We extend the\nexisting network quantile autoregression model of Zhu et al. (2019b) by\nexplicitly allowing the contemporaneous network effects and controlling for the\ncommon factors across quantiles. To cope with the endogeneity issue due to\nsimultaneous network spillovers, we adopt the instrumental variable quantile\nregression (IVQR) estimation and derive the consistency and asymptotic\nnormality of the IVQR estimator using the near epoch dependence property of the\nnetwork process. Via Monte Carlo simulations, we confirm the satisfactory\nperformance of the IVQR estimator across different quantiles under the\ndifferent network structures. Finally, we demonstrate the usefulness of our\nproposed approach with an application to the dataset on the stocks traded in\nNYSE and NASDAQ in 2016.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.07633v1"
    },
    {
        "title": "Optimized Inference in Regression Kink Designs",
        "authors": [
            "Majed Dodin"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose a method to remedy finite sample coverage problems and improve\nupon the efficiency of commonly employed procedures for the construction of\nnonparametric confidence intervals in regression kink designs. The proposed\ninterval is centered at the half-length optimal, numerically obtained linear\nminimax estimator over distributions with Lipschitz constrained conditional\nmean function. Its construction ensures excellent finite sample coverage and\nlength properties which are demonstrated in a simulation study and an empirical\nillustration. Given the Lipschitz constant that governs how much curvature one\nplausibly allows for, the procedure is fully data driven, computationally\ninexpensive, incorporates shape constraints and is valid irrespective of the\ndistribution of the assignment variable.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.10713v1"
    },
    {
        "title": "Identifying Dynamic Discrete Choice Models with Hyperbolic Discounting",
        "authors": [
            "Taiga Tsubota"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We study identification of dynamic discrete choice models with hyperbolic\ndiscounting. We show that the standard discount factor, present bias factor,\nand instantaneous utility functions for the sophisticated agent are\npoint-identified from observed conditional choice probabilities and transition\nprobabilities in a finite horizon model. The main idea to achieve\nidentification is to exploit variation in the observed conditional choice\nprobabilities over time. We present the estimation method and demonstrate a\ngood performance of the estimator by simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.10721v4"
    },
    {
        "title": "Orthogonal Policy Learning Under Ambiguity",
        "authors": [
            "Riccardo D'Adamo"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper studies the problem of estimating individualized treatment rules\nwhen treatment effects are partially identified, as it is often the case with\nobservational data. By drawing connections between the treatment assignment\nproblem and classical decision theory, we characterize several notions of\noptimal treatment policies in the presence of partial identification. Our\nunified framework allows to incorporate user-defined constraints on the set of\nallowable policies, such as restrictions for transparency or interpretability,\nwhile also ensuring computational feasibility. We show how partial\nidentification leads to a new policy learning problem where the objective\nfunction is directionally -- but not fully -- differentiable with respect to\nthe nuisance first-stage. We then propose an estimation procedure that ensures\nNeyman-orthogonality with respect to the nuisance components and we provide\nstatistical guarantees that depend on the amount of concentration around the\npoints of non-differentiability in the data-generating-process. The proposed\nmethods are illustrated using data from the Job Partnership Training Act study.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.10904v3"
    },
    {
        "title": "Interactive Effects Panel Data Models with General Factors and\n  Regressors",
        "authors": [
            "Bin Peng",
            "Liangjun Su",
            "Joakim Westerlund",
            "Yanrong Yang"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper considers a model with general regressors and unobservable\nfactors. An estimator based on iterated principal components is proposed, which\nis shown to be not only asymptotically normal and oracle efficient, but under\ncertain conditions also free of the otherwise so common asymptotic incidental\nparameters bias. Interestingly, the conditions required to achieve unbiasedness\nbecome weaker the stronger the trends in the factors, and if the trending is\nstrong enough unbiasedness comes at no cost at all. In particular, the approach\ndoes not require any knowledge of how many factors there are, or whether they\nare deterministic or stochastic. The order of integration of the factors is\nalso treated as unknown, as is the order of integration of the regressors,\nwhich means that there is no need to pre-test for unit roots, or to decide on\nwhich deterministic terms to include in the model.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.11506v1"
    },
    {
        "title": "Maximum Likelihood Estimation of Differentiated Products Demand Systems",
        "authors": [
            "Greg Lewis",
            "Bora Ozaltun",
            "Georgios Zervas"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We discuss estimation of the differentiated products demand system of Berry\net al (1995) (BLP) by maximum likelihood estimation (MLE). We derive the\nmaximum likelihood estimator in the case where prices are endogenously\ngenerated by firms that set prices in Bertrand-Nash equilibrium. In Monte Carlo\nsimulations the MLE estimator outperforms the best-practice GMM estimator on\nboth bias and mean squared error when the model is correctly specified. This\nremains true under some forms of misspecification. In our simulations, the\ncoverage of the ML estimator is close to its nominal level, whereas the GMM\nestimator tends to under-cover. We conclude the paper by estimating BLP on the\ncar data used in the original Berry et al (1995) paper, obtaining similar\nestimates with considerably tighter standard errors.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.12397v1"
    },
    {
        "title": "Difference in Differences and Ratio in Ratios for Limited Dependent\n  Variables",
        "authors": [
            "Myoung-jae Lee",
            "Sanghyeok Lee"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Difference in differences (DD) is widely used to find policy/treatment\neffects with observational data, but applying DD to limited dependent variables\n(LDV's) Y has been problematic. This paper addresses how to apply DD and\nrelated approaches (such as \"ratio in ratios\" or \"ratio in odds ratios\") to\nbinary, count, fractional, multinomial or zero-censored Y under the unifying\nframework of `generalized linear models with link functions'. We evaluate DD\nand the related approaches with simulation and empirical studies, and recommend\n'Poisson Quasi-MLE' for non-negative (such as count or zero-censored) Y and\n(multinomial) logit MLE for binary, fractional or multinomial Y.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.12948v2"
    },
    {
        "title": "Yogurts Choose Consumers? Estimation of Random-Utility Models via\n  Two-Sided Matching",
        "authors": [
            "Odran Bonnet",
            "Alfred Galichon",
            "Yu-Wei Hsieh",
            "Keith O'Hara",
            "Matt Shum"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The problem of demand inversion - a crucial step in the estimation of random\nutility discrete-choice models - is equivalent to the determination of stable\noutcomes in two-sided matching models. This equivalence applies to random\nutility models that are not necessarily additive, smooth, nor even invertible.\nBased on this equivalence, algorithms for the determination of stable matchings\nprovide effective computational methods for estimating these models. For\nnon-invertible models, the identified set of utility vectors is a lattice, and\nthe matching algorithms recover sharp upper and lower bounds on the utilities.\nOur matching approach facilitates estimation of models that were previously\ndifficult to estimate, such as the pure characteristics model. An empirical\napplication to voting data from the 1999 European Parliament elections\nillustrates the good performance of our matching-based demand inversion\nalgorithms in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.13744v1"
    },
    {
        "title": "Robust Permutation Tests in Linear Instrumental Variables Regression",
        "authors": [
            "Purevdorj Tuvaandorj"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper develops permutation versions of identification-robust tests in\nlinear instrumental variables (IV) regression. Unlike the existing\nrandomization and rank-based tests in which independence between the\ninstruments and the error terms is assumed, the permutation Anderson- Rubin\n(AR), Lagrange Multiplier (LM) and Conditional Likelihood Ratio (CLR) tests are\nasymptotically similar and robust to conditional heteroskedasticity under\nstandard exclusion restriction i.e. the orthogonality between the instruments\nand the error terms. Moreover, when the instruments are independent of the\nstructural error term, the permutation AR tests are exact, hence robust to\nheavy tails. As such, these tests share the strengths of the rank-based tests\nand the wild bootstrap AR tests. Numerical illustrations corroborate the\ntheoretical results.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.13774v4"
    },
    {
        "title": "RIF Regression via Sensitivity Curves",
        "authors": [
            "Javier Alejo",
            "Gabriel Montes-Rojas",
            "Walter Sosa-Escudero"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper proposes an empirical method to implement the recentered influence\nfunction (RIF) regression of Firpo, Fortin and Lemieux (2009), a relevant\nmethod to study the effect of covariates on many statistics beyond the mean. In\nempirically relevant situations where the influence function is not available\nor difficult to compute, we suggest to use the \\emph{sensitivity curve} (Tukey,\n1977) as a feasible alternative. This may be computationally cumbersome when\nthe sample size is large. The relevance of the proposed strategy derives from\nthe fact that, under general conditions, the sensitivity curve converges in\nprobability to the influence function. In order to save computational time we\npropose to use a cubic splines non-parametric method for a random subsample and\nthen to interpolate to the rest of the cases where it was not computed. Monte\nCarlo simulations show good finite sample properties. We illustrate the\nproposed estimator with an application to the polarization index of Duclos,\nEsteban and Ray (2004).\n",
        "pdf_link": "http://arxiv.org/pdf/2112.01435v1"
    },
    {
        "title": "Simple Alternatives to the Common Correlated Effects Model",
        "authors": [
            "Nicholas L. Brown",
            "Peter Schmidt",
            "Jeffrey M. Wooldridge"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We study estimation of factor models in a fixed-T panel data setting and\nsignificantly relax the common correlated effects (CCE) assumptions pioneered\nby Pesaran (2006) and used in dozens of papers since. In the simplest case, we\nmodel the unobserved factors as functions of the cross-sectional averages of\nthe explanatory variables and show that this is implied by Pesaran's\nassumptions when the number of factors does not exceed the number of\nexplanatory variables. Our approach allows discrete explanatory variables and\nflexible functional forms in the covariates. Plus, it extends to a framework\nthat easily incorporates general functions of cross-sectional moments, in\naddition to heterogeneous intercepts and time trends. Our proposed estimators\ninclude Pesaran's pooled correlated common effects (CCEP) estimator as a\nspecial case. We also show that in the presence of heterogeneous slopes our\nestimator is consistent under assumptions much weaker than those previously\nused. We derive the fixed-T asymptotic normality of a general estimator and\nshow how to adjust for estimation of the population moments in the factor\nloading equation.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.01486v1"
    },
    {
        "title": "Patient-Centered Appraisal of Race-Free Clinical Risk Assessment",
        "authors": [
            "Charles F. Manski"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Until recently, there has been a consensus that clinicians should condition\npatient risk assessments on all observed patient covariates with predictive\npower. The broad idea is that knowing more about patients enables more accurate\npredictions of their health risks and, hence, better clinical decisions. This\nconsensus has recently unraveled with respect to a specific covariate, namely\nrace. There have been increasing calls for race-free risk assessment, arguing\nthat using race to predict patient outcomes contributes to racial disparities\nand inequities in health care. Writers calling for race-free risk assessment\nhave not studied how it would affect the quality of clinical decisions.\nConsidering the matter from the patient-centered perspective of medical\neconomics yields a disturbing conclusion: Race-free risk assessment would harm\npatients of all races.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.01639v2"
    },
    {
        "title": "Gaussian Process Vector Autoregressions and Macroeconomic Uncertainty",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber",
            "Massimiliano Marcellino",
            "Nico Petz"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We develop a non-parametric multivariate time series model that remains\nagnostic on the precise relationship between a (possibly) large set of\nmacroeconomic time series and their lagged values. The main building block of\nour model is a Gaussian process prior on the functional relationship that\ndetermines the conditional mean of the model, hence the name of Gaussian\nprocess vector autoregression (GP-VAR). A flexible stochastic volatility\nspecification is used to provide additional flexibility and control for\nheteroskedasticity. Markov chain Monte Carlo (MCMC) estimation is carried out\nthrough an efficient and scalable algorithm which can handle large models. The\nGP-VAR is illustrated by means of simulated data and in a forecasting exercise\nwith US data. Moreover, we use the GP-VAR to analyze the effects of\nmacroeconomic uncertainty, with a particular emphasis on time variation and\nasymmetries in the transmission mechanisms.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.01995v3"
    },
    {
        "title": "Visual Inference and Graphical Representation in Regression\n  Discontinuity Designs",
        "authors": [
            "Christina Korting",
            "Carl Lieberman",
            "Jordan Matsudaira",
            "Zhuan Pei",
            "Yi Shen"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Despite the widespread use of graphs in empirical research, little is known\nabout readers' ability to process the statistical information they are meant to\nconvey (\"visual inference\"). We study visual inference within the context of\nregression discontinuity (RD) designs by measuring how accurately readers\nidentify discontinuities in graphs produced from data generating processes\ncalibrated on 11 published papers from leading economics journals. First, we\nassess the effects of different graphical representation methods on visual\ninference using randomized experiments. We find that bin widths and fit lines\nhave the largest impacts on whether participants correctly perceive the\npresence or absence of a discontinuity. Our experimental results allow us to\nmake evidence-based recommendations to practitioners, and we suggest using\nsmall bins with no fit lines as a starting point to construct RD graphs.\nSecond, we compare visual inference on graphs constructed using our preferred\nmethod with widely used econometric inference procedures. We find that visual\ninference achieves similar or lower type I error (false positive) rates and\ncomplements econometric inference.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.03096v2"
    },
    {
        "title": "A decomposition method to evaluate the `paradox of progress' with\n  evidence for Argentina",
        "authors": [
            "Javier Alejo",
            "Leonardo Gasparini",
            "Gabriel Montes-Rojas",
            "Walter Sosa-Escudero"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  The `paradox of progress' is an empirical regularity that associates more\neducation with larger income inequality. Two driving and competing factors\nbehind this phenomenon are the convexity of the `Mincer equation' (that links\nwages and education) and the heterogeneity in its returns, as captured by\nquantile regressions. We propose a joint least-squares and quantile regression\nstatistical framework to derive a decomposition in order to evaluate the\nrelative contribution of each explanation. The estimators are based on the\n`functional derivative' approach. We apply the proposed decomposition strategy\nto the case of Argentina 1992 to 2015.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.03836v1"
    },
    {
        "title": "Two-Way Fixed Effects and Differences-in-Differences with Heterogeneous\n  Treatment Effects: A Survey",
        "authors": [
            "Clément de Chaisemartin",
            "Xavier D'Haultfœuille"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Linear regressions with period and group fixed effects are widely used to\nestimate policies' effects: 26 of the 100 most cited papers published by the\nAmerican Economic Review from 2015 to 2019 estimate such regressions. It has\nrecently been shown that those regressions may produce misleading estimates, if\nthe policy's effect is heterogeneous between groups or over time, as is often\nthe case. This survey reviews a fast-growing literature that documents this\nissue, and that proposes alternative estimators robust to heterogeneous\neffects. We use those alternative estimators to revisit Wolfers (2006).\n",
        "pdf_link": "http://arxiv.org/pdf/2112.04565v6"
    },
    {
        "title": "Efficient counterfactual estimation in semiparametric discrete choice\n  models: a note on Chiong, Hsieh, and Shum (2017)",
        "authors": [
            "Grigory Franguridi"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  I suggest an enhancement of the procedure of Chiong, Hsieh, and Shum (2017)\nfor calculating bounds on counterfactual demand in semiparametric discrete\nchoice models. Their algorithm relies on a system of inequalities indexed by\ncycles of a large number $M$ of observed markets and hence seems to require\ncomputationally infeasible enumeration of all such cycles. I show that such\nenumeration is unnecessary because solving the \"fully efficient\" inequality\nsystem exploiting cycles of all possible lengths $K=1,\\dots,M$ can be reduced\nto finding the length of the shortest path between every pair of vertices in a\ncomplete bidirected weighted graph on $M$ vertices. The latter problem can be\nsolved using the Floyd--Warshall algorithm with computational complexity\n$O\\left(M^3\\right)$, which takes only seconds to run even for thousands of\nmarkets. Monte Carlo simulations illustrate the efficiency gain from using\ncycles of all lengths, which turns out to be positive, but small.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.04637v1"
    },
    {
        "title": "Housing Price Prediction Model Selection Based on Lorenz and\n  Concentration Curves: Empirical Evidence from Tehran Housing Market",
        "authors": [
            "Mohammad Mirbagherijam"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This study contributes a house price prediction model selection in Tehran\nCity based on the area between Lorenz curve (LC) and concentration curve (CC)\nof the predicted price by using 206,556 observed transaction data over the\nperiod from March 21, 2018, to February 19, 2021. Several different methods\nsuch as generalized linear models (GLM) and recursive partitioning and\nregression trees (RPART), random forests (RF) regression models, and neural\nnetwork (NN) models were examined house price prediction. We used 90% of all\ndata samples which were chosen randomly to estimate the parameters of pricing\nmodels and 10% of remaining datasets to test the accuracy of prediction.\nResults showed that the area between the LC and CC curves (which are known as\nABC criterion) of real and predicted prices in the test data sample of the\nrandom forest regression model was less than by other models under study. The\ncomparison of the calculated ABC criteria leads us to conclude that the\nnonlinear regression models such as RF regression models give an accurate\nprediction of house prices in Tehran City.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.06192v1"
    },
    {
        "title": "Quantile Regression under Limited Dependent Variable",
        "authors": [
            "Javier Alejo",
            "Gabriel Montes-Rojas"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  A new Stata command, ldvqreg, is developed to estimate quantile regression\nmodels for the cases of censored (with lower and/or upper censoring) and binary\ndependent variables. The estimators are implemented using a smoothed version of\nthe quantile regression objective function. Simulation exercises show that it\ncorrectly estimates the parameters and it should be implemented instead of the\navailable quantile regression methods when censoring is present. An empirical\napplication to women's labor supply in Uruguay is considered.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.06822v1"
    },
    {
        "title": "Identifying Marginal Treatment Effects in the Presence of Sample\n  Selection",
        "authors": [
            "Otávio Bartalotti",
            "Désiré Kédagni",
            "Vitor Possebom"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This article presents identification results for the marginal treatment\neffect (MTE) when there is sample selection. We show that the MTE is partially\nidentified for individuals who are always observed regardless of treatment, and\nderive uniformly sharp bounds on this parameter under three increasingly\nrestrictive sets of assumptions. The first result imposes standard MTE\nassumptions with an unrestricted sample selection mechanism. The second set of\nconditions imposes monotonicity of the sample selection variable with respect\nto treatment, considerably shrinking the identified set. Finally, we\nincorporate a stochastic dominance assumption which tightens the lower bound\nfor the MTE. Our analysis extends to discrete instruments. The results rely on\na mixture reformulation of the problem where the mixture weights are\nidentified, extending Lee's (2009) trimming procedure to the MTE context. We\npropose estimators for the bounds derived and use data made available by Deb,\nMunking and Trivedi (2006) to empirically illustrate the usefulness of our\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.07014v1"
    },
    {
        "title": "Testing Instrument Validity with Covariates",
        "authors": [
            "Thomas Carr",
            "Toru Kitagawa"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We develop a novel test of the instrumental variable identifying assumptions\nfor heterogeneous treatment effect models with conditioning covariates. We\nassume semiparametric dependence between potential outcomes and conditioning\ncovariates. This allows us to obtain testable equality and inequality\nrestrictions among the subdensities of estimable partial residuals. We propose\njointly testing these restrictions. To improve power, we introduce\ndistillation, where a trimmed sample is used to test the inequality\nrestrictions. In Monte Carlo exercises we find gains in finite sample power\nfrom testing restrictions jointly and distillation. We apply our test procedure\nto three instruments and reject the null for one.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.08092v2"
    },
    {
        "title": "Uniform Convergence Results for the Local Linear Regression Estimation\n  of the Conditional Distribution",
        "authors": [
            "Haitian Xie"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper examines the local linear regression (LLR) estimate of the\nconditional distribution function $F(y|x)$. We derive three uniform convergence\nresults: the uniform bias expansion, the uniform convergence rate, and the\nuniform asymptotic linear representation. The uniformity in the above results\nis with respect to both $x$ and $y$ and therefore has not previously been\naddressed in the literature on local polynomial regression. Such uniform\nconvergence results are especially useful when the conditional distribution\nestimator is the first stage of a semiparametric estimator. We demonstrate the\nusefulness of these uniform results with two examples: the stochastic\nequicontinuity condition in $y$, and the estimation of the integrated\nconditional distribution function.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.08546v2"
    },
    {
        "title": "Lassoed Boosting and Linear Prediction in the Equities Market",
        "authors": [
            "Xiao Huang"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We consider a two-stage estimation method for linear regression. First, it\nuses the lasso in Tibshirani (1996) to screen variables and, second,\nre-estimates the coefficients using the least-squares boosting method in\nFriedman (2001) on every set of selected variables. Based on the large-scale\nsimulation experiment in Hastie et al. (2020), lassoed boosting performs as\nwell as the relaxed lasso in Meinshausen (2007) and, under certain scenarios,\ncan yield a sparser model. Applied to predicting equity returns, lassoed\nboosting gives the smallest mean-squared prediction error compared to several\nother methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.08934v4"
    },
    {
        "title": "Robustness, Heterogeneous Treatment Effects and Covariate Shifts",
        "authors": [
            "Pietro Emilio Spini"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper studies the robustness of estimated policy effects to changes in\nthe distribution of covariates. Robustness to covariate shifts is important,\nfor example, when evaluating the external validity of quasi-experimental\nresults, which are often used as a benchmark for evidence-based policy-making.\nI propose a novel scalar robustness metric. This metric measures the magnitude\nof the smallest covariate shift needed to invalidate a claim on the policy\neffect (for example, $ATE \\geq 0$) supported by the quasi-experimental\nevidence. My metric links the heterogeneity of policy effects and robustness in\na flexible, nonparametric way and does not require functional form assumptions.\nI cast the estimation of the robustness metric as a de-biased GMM problem. This\napproach guarantees a parametric convergence rate for the robustness metric\nwhile allowing for machine learning-based estimators of policy effect\nheterogeneity (for example, lasso, random forest, boosting, neural nets). I\napply my procedure to the Oregon Health Insurance experiment. I study the\nrobustness of policy effects estimates of health-care utilization and financial\nstrain outcomes, relative to a shift in the distribution of context-specific\ncovariates. Such covariates are likely to differ across US states, making\nquantification of robustness an important exercise for adoption of the\ninsurance policy in states other than Oregon. I find that the effect on\noutpatient visits is the most robust among the metrics of health-care\nutilization considered.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.09259v2"
    },
    {
        "title": "Heckman-Selection or Two-Part models for alcohol studies? Depends",
        "authors": [
            "Reka Sundaram-Stukel"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Aims: To re-introduce the Heckman model as a valid empirical technique in\nalcohol studies. Design: To estimate the determinants of problem drinking using\na Heckman and a two-part estimation model. Psychological and neuro-scientific\nstudies justify my underlying estimation assumptions and covariate exclusion\nrestrictions. Higher order tests checking for multicollinearity validate the\nuse of Heckman over the use of two-part estimation models. I discuss the\ngeneralizability of the two models in applied research. Settings and\nParticipants: Two pooled national population surveys from 2016 and 2017 were\nused: the Behavioral Risk Factor Surveillance Survey (BRFS), and the National\nSurvey of Drug Use and Health (NSDUH). Measurements: Participation in problem\ndrinking and meeting the criteria for problem drinking. Findings: Both U.S.\nnational surveys perform well with the Heckman model and pass all higher order\ntests. The Heckman model corrects for selection bias and reveals the direction\nof bias, where the two-part model does not. For example, the coefficients on\nage are upward biased and unemployment is downward biased in the two-part where\nthe Heckman model does not have a selection bias. Covariate exclusion\nrestrictions are sensitive to survey conditions and are contextually\ngeneralizable. Conclusions: The Heckman model can be used for alcohol (smoking\nstudies as well) if the underlying estimation specification passes higher order\ntests for multicollinearity and the exclusion restrictions are justified with\nintegrity for the data used. Its use is merit-worthy because it corrects for\nand reveals the direction and the magnitude of selection bias where the\ntwo-part does not.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.10542v2"
    },
    {
        "title": "Ranking and Selection from Pairwise Comparisons: Empirical Bayes Methods\n  for Citation Analysis",
        "authors": [
            "Jiaying Gu",
            "Roger Koenker"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We study the Stigler model of citation flows among journals adapting the\npairwise comparison model of Bradley and Terry to do ranking and selection of\njournal influence based on nonparametric empirical Bayes procedures.\nComparisons with several other rankings are made.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.11064v1"
    },
    {
        "title": "An Analysis of an Alternative Pythagorean Expected Win Percentage Model:\n  Applications Using Major League Baseball Team Quality Simulations",
        "authors": [
            "Justin Ehrlich",
            "Christopher Boudreaux",
            "James Boudreau",
            "Shane Sanders"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We ask if there are alternative contest models that minimize error or\ninformation loss from misspecification and outperform the Pythagorean model.\nThis article aims to use simulated data to select the optimal expected win\npercentage model among the choice of relevant alternatives. The choices include\nthe traditional Pythagorean model and the difference-form contest success\nfunction (CSF). Method. We simulate 1,000 iterations of the 2014 MLB season for\nthe purpose of estimating and analyzing alternative models of expected win\npercentage (team quality). We use the open-source, Strategic Baseball Simulator\nand develop an AutoHotKey script that programmatically executes the SBS\napplication, chooses the correct settings for the 2014 season, enters a unique\nID for the simulation data file, and iterates these steps 1,000 times. We\nestimate expected win percentage using the traditional Pythagorean model, as\nwell as the difference-form CSF model that is used in game theory and public\nchoice economics. Each model is estimated while accounting for fixed (team)\neffects. We find that the difference-form CSF model outperforms the traditional\nPythagorean model in terms of explanatory power and in terms of\nmisspecification-based information loss as estimated by the Akaike Information\nCriterion. Through parametric estimation, we further confirm that the simulator\nyields realistic statistical outcomes. The simulation methodology offers the\nadvantage of greatly improved sample size. As the season is held constant, our\nsimulation-based statistical inference also allows for estimation and model\ncomparison without the (time series) issue of non-stationarity. The results\nsuggest that improved win (productivity) estimation can be achieved through\nalternative CSF specifications.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.14846v1"
    },
    {
        "title": "Modeling and Forecasting Intraday Market Returns: a Machine Learning\n  Approach",
        "authors": [
            "Iuri H. Ferreira",
            "Marcelo C. Medeiros"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In this paper we examine the relation between market returns and volatility\nmeasures through machine learning methods in a high-frequency environment. We\nimplement a minute-by-minute rolling window intraday estimation method using\ntwo nonlinear models: Long-Short-Term Memory (LSTM) neural networks and Random\nForests (RF). Our estimations show that the CBOE Volatility Index (VIX) is the\nstrongest candidate predictor for intraday market returns in our analysis,\nspecially when implemented through the LSTM model. This model also improves\nsignificantly the performance of the lagged market return as predictive\nvariable. Finally, intraday RF estimation outputs indicate that there is no\nperformance improvement with this method, and it may even worsen the results in\nsome cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.15108v1"
    },
    {
        "title": "A Double Robust Approach for Non-Monotone Missingness in Multi-Stage\n  Data",
        "authors": [
            "Shenshen Yang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Multivariate missingness with a non-monotone missing pattern is complicated\nto deal with in empirical studies. The traditional Missing at Random (MAR)\nassumption is difficult to justify in such cases. Previous studies have\nstrengthened the MAR assumption, suggesting that the missing mechanism of any\nvariable is random when conditioned on a uniform set of fully observed\nvariables. However, empirical evidence indicates that this assumption may be\nviolated for variables collected at different stages. This paper proposes a new\nMAR-type assumption that fits non-monotone missing scenarios involving\nmulti-stage variables. Based on this assumption, we construct an Augmented\nInverse Probability Weighted GMM (AIPW-GMM) estimator. This estimator features\nan asymmetric format for the augmentation term, guarantees double robustness,\nand achieves the closed-form semiparametric efficiency bound. We apply this\nmethod to cases of missingness in both endogenous regressor and outcome, using\nthe Oregon Health Insurance Experiment as an example. We check the correlation\nbetween missing probabilities and partially observed variables to justify the\nassumption. Moreover, we find that excluding incomplete data results in a loss\nof efficiency and insignificant estimators. The proposed estimator reduces the\nstandard error by more than 50% for the estimated effects of the Oregon Health\nPlan on the elderly.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.01010v2"
    },
    {
        "title": "Unconditional Effects of General Policy Interventions",
        "authors": [
            "Julian Martinez-Iriarte",
            "Gabriel Montes-Rojas",
            "Yixiao Sun"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper studies the unconditional effects of a general policy\nintervention, which includes location-scale shifts and simultaneous shifts as\nspecial cases. The location-scale shift is intended to study a counterfactual\npolicy aimed at changing not only the mean or location of a covariate but also\nits dispersion or scale. The simultaneous shift refers to the situation where\nshifts in two or more covariates take place simultaneously. For example, a\nshift in one covariate is compensated at a certain rate by a shift in another\ncovariate. Not accounting for these possible scale or simultaneous shifts will\nresult in an incorrect assessment of the potential policy effects on an outcome\nvariable of interest. The unconditional policy parameters are estimated with\nsimple semiparametric estimators, for which asymptotic properties are studied.\nMonte Carlo simulations are implemented to study their finite sample\nperformances. The proposed approach is applied to a Mincer equation to study\nthe effects of changing years of education on wages and to study the effect of\nsmoking during pregnancy on birth weight.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.02292v3"
    },
    {
        "title": "A machine learning search for optimal GARCH parameters",
        "authors": [
            "Luke De Clerk",
            "Sergey Savl'ev"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Here, we use Machine Learning (ML) algorithms to update and improve the\nefficiencies of fitting GARCH model parameters to empirical data. We employ an\nArtificial Neural Network (ANN) to predict the parameters of these models. We\npresent a fitting algorithm for GARCH-normal(1,1) models to predict one of the\nmodel's parameters, $\\alpha_1$ and then use the analytical expressions for the\nfourth order standardised moment, $\\Gamma_4$ and the unconditional second order\nmoment, $\\sigma^2$ to fit the other two parameters; $\\beta_1$ and $\\alpha_0$,\nrespectively. The speed of fitting of the parameters and quick implementation\nof this approach allows for real time tracking of GARCH parameters. We further\nshow that different inputs to the ANN namely, higher order standardised moments\nand the autocovariance of time series can be used for fitting model parameters\nusing the ANN, but not always with the same level of accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.03286v1"
    },
    {
        "title": "Detecting Multiple Structural Breaks in Systems of Linear Regression\n  Equations with Integrated and Stationary Regressors",
        "authors": [
            "Karsten Schweikert"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this paper, we propose a two-step procedure based on the group LASSO\nestimator in combination with a backward elimination algorithm to detect\nmultiple structural breaks in linear regressions with multivariate responses.\nApplying the two-step estimator, we jointly detect the number and location of\nstructural breaks, and provide consistent estimates of the coefficients. Our\nframework is flexible enough to allow for a mix of integrated and stationary\nregressors, as well as deterministic terms. Using simulation experiments, we\nshow that the proposed two-step estimator performs competitively against the\nlikelihood-based approach (Qu and Perron, 2007; Li and Perron, 2017; Oka and\nPerron, 2018) in finite samples. However, the two-step estimator is\ncomputationally much more efficient. An economic application to the\nidentification of structural breaks in the term structure of interest rates\nillustrates this methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.05430v4"
    },
    {
        "title": "Nonparametric Identification of Random Coefficients in Endogenous and\n  Heterogeneous Aggregate Demand Models",
        "authors": [
            "Fabian Dunker",
            "Stefan Hoderlein",
            "Hiroaki Kaido"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper studies nonparametric identification in market level demand models\nfor differentiated products with heterogeneous consumers. We consider a general\nclass of models that allows for the individual specific coefficients to vary\ncontinuously across the population and give conditions under which the density\nof these coefficients, and hence also functionals such as welfare measures, is\nidentified. A key finding is that two leading models, the BLP-model (Berry,\nLevinsohn, and Pakes, 1995) and the pure characteristics model (Berry and\nPakes, 2007), require considerably different conditions on the support of the\nproduct characteristics.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.06140v1"
    },
    {
        "title": "An Entropy-Based Approach for Nonparametrically Testing Simple\n  Probability Distribution Hypotheses",
        "authors": [
            "Ron Mittelhammer",
            "George Judge",
            "Miguel Henry"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this paper, we introduce a flexible and widely applicable nonparametric\nentropy-based testing procedure that can be used to assess the validity of\nsimple hypotheses about a specific parametric population distribution. The\ntesting methodology relies on the characteristic function of the population\nprobability distribution being tested and is attractive in that, regardless of\nthe null hypothesis being tested, it provides a unified framework for\nconducting such tests. The testing procedure is also computationally tractable\nand relatively straightforward to implement. In contrast to some alternative\ntest statistics, the proposed entropy test is free from user-specified kernel\nand bandwidth choices, idiosyncratic and complex regularity conditions, and/or\nchoices of evaluation grids. Several simulation exercises were performed to\ndocument the empirical performance of our proposed test, including a regression\nexample that is illustrative of how, in some contexts, the approach can be\napplied to composite hypothesis-testing situations via data transformations.\nOverall, the testing procedure exhibits notable promise, exhibiting appreciable\nincreasing power as sample size increases for a number of alternative\ndistributions when contrasted with hypothesized null distributions. Possible\ngeneral extensions of the approach to composite hypothesis-testing contexts,\nand directions for future work are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.06647v1"
    },
    {
        "title": "Homophily in preferences or meetings? Identifying and estimating an\n  iterative network formation model",
        "authors": [
            "Luis Alvarez",
            "Cristine Pinto",
            "Vladimir Ponczek"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Is homophily in social and economic networks driven by a taste for\nhomogeneity (preferences) or by a higher probability of meeting individuals\nwith similar attributes (opportunity)? This paper studies identification and\nestimation of an iterative network game that distinguishes between these two\nmechanisms. Our approach enables us to assess the counterfactual effects of\nchanging the meeting protocol between agents. As an application, we study the\nrole of preferences and meetings in shaping classroom friendship networks in\nBrazil. In a network structure in which homophily due to preferences is\nstronger than homophily due to meeting opportunities, tracking students may\nimprove welfare. Still, the relative benefit of this policy diminishes over the\nschool year.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.06694v4"
    },
    {
        "title": "Difference-in-Differences Estimators for Treatments Continuously\n  Distributed at Every Period",
        "authors": [
            "Clément de Chaisemartin",
            "Xavier D'Haultfoeuille",
            "Félix Pasquier",
            "Doulo Sow",
            "Gonzalo Vazquez-Bare"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose difference-in-differences estimators in designs where the\ntreatment is continuously distributed at every period, as is often the case\nwhen one studies the effects of taxes, tariffs, or prices. We assume that\nbetween consecutive periods, the treatment of some units, the switchers,\nchanges, while the treatment of other units remains constant. We show that\nunder a placebo-testable parallel-trends assumption, averages of the slopes of\nswitchers' potential outcomes can be nonparametrically estimated. We generalize\nour estimators to the instrumental-variable case. We use our estimators to\nestimate the price-elasticity of gasoline consumption.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.06898v4"
    },
    {
        "title": "Close Enough? A Large-Scale Exploration of Non-Experimental Approaches\n  to Advertising Measurement",
        "authors": [
            "Brett R. Gordon",
            "Robert Moakler",
            "Florian Zettelmeyer"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Despite their popularity, randomized controlled trials (RCTs) are not always\navailable for the purposes of advertising measurement. Non-experimental data is\nthus required. However, Facebook and other ad platforms use complex and\nevolving processes to select ads for users. Therefore, successful\nnon-experimental approaches need to \"undo\" this selection. We analyze 663\nlarge-scale experiments at Facebook to investigate whether this is possible\nwith the data typically logged at large ad platforms. With access to over 5,000\nuser-level features, these data are richer than what most advertisers or their\nmeasurement partners can access. We investigate how accurately two\nnon-experimental methods -- double/debiased machine learning (DML) and\nstratified propensity score matching (SPSM) -- can recover the experimental\neffects. Although DML performs better than SPSM, neither method performs well,\neven using flexible deep learning models to implement the propensity and\noutcome models. The median RCT lifts are 29%, 18%, and 5% for the upper,\nmiddle, and lower funnel outcomes, respectively. Using DML (SPSM), the median\nlift by funnel is 83% (173%), 58% (176%), and 24% (64%), respectively,\nindicating significant relative measurement errors. We further characterize the\ncircumstances under which each method performs comparatively better. Overall,\ndespite having access to large-scale experiments and rich user-level data, we\nare unable to reliably estimate an ad campaign's causal effect.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.07055v2"
    },
    {
        "title": "The Time-Varying Multivariate Autoregressive Index Model",
        "authors": [
            "G. Cubadda",
            "S. Grassi",
            "B. Guardabascio"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Many economic variables feature changes in their conditional mean and\nvolatility, and Time Varying Vector Autoregressive Models are often used to\nhandle such complexity in the data. Unfortunately, when the number of series\ngrows, they present increasing estimation and interpretation problems. This\npaper tries to address this issue proposing a new Multivariate Autoregressive\nIndex model that features time varying means and volatility. Technically, we\ndevelop a new estimation methodology that mix switching algorithms with the\nforgetting factors strategy of Koop and Korobilis (2012). This substantially\nreduces the computational burden and allows to select or weight, in real time,\nthe number of common components and other features of the data using Dynamic\nModel Selection or Dynamic Model Averaging without further computational cost.\nUsing USA macroeconomic data, we provide a structural analysis and a\nforecasting exercise that demonstrates the feasibility and usefulness of this\nnew model.\n  Keywords: Large datasets, Multivariate Autoregressive Index models,\nStochastic volatility, Bayesian VARs.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.07069v1"
    },
    {
        "title": "Identification of Direct Socio-Geographical Price Discrimination: An\n  Empirical Study on iPhones",
        "authors": [
            "Davidson Cheng"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Price discrimination is a practice where firms utilize varying sensitivities\nto prices among consumers to increase profits. The welfare effects of price\ndiscrimination are not agreed on among economists, but identification of such\nactions may contribute to our standing of firms' pricing behaviors. In this\nletter, I use econometric tools to analyze whether Apple Inc, one of the\nlargest companies in the globe, is practicing price discrimination on the basis\nof socio-economical and geographical factors. My results indicate that iPhones\nare significantly (p $<$ 0.01) more expensive in markets where competitions are\nweak or where Apple has a strong market presence. Furthermore, iPhone prices\nare likely to increase (p $<$ 0.01) in developing countries/regions or markets\nwith high income inequality.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.07903v1"
    },
    {
        "title": "Estimation of Conditional Random Coefficient Models using Machine\n  Learning Techniques",
        "authors": [
            "Stephan Martin"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Nonparametric random coefficient (RC)-density estimation has mostly been\nconsidered in the marginal density case under strict independence of RCs and\ncovariates. This paper deals with the estimation of RC-densities conditional on\na (large-dimensional) set of control variables using machine learning\ntechniques. The conditional RC-density allows to disentangle observable from\nunobservable heterogeneity in partial effects of continuous treatments adding\nto a growing literature on heterogeneous effect estimation using machine\nlearning. %It is also informative of the conditional potential outcome\ndistribution. This paper proposes a two-stage sieve estimation procedure. First\na closed-form sieve approximation of the conditional RC density is derived\nwhere each sieve coefficient can be expressed as conditional expectation\nfunction varying with controls. Second, sieve coefficients are estimated with\ngeneric machine learning procedures and under appropriate sample splitting\nrules. The $L_2$-convergence rate of the conditional RC-density estimator is\nderived. The rate is slower by a factor then typical rates of mean regression\nmachine learning estimators which is due to the ill-posedness of the RC density\nestimation problem. The performance and applicability of the estimator is\nillustrated using random forest algorithms over a range of Monte Carlo\nsimulations and with real data from the SOEP-IS. Here behavioral heterogeneity\nin an economic experiment on portfolio choice is studied. The method reveals\ntwo types of behavior in the population, one type complying with economic\ntheory and one not. The assignment to types appears largely based on\nunobservables not available in the data.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.08366v1"
    },
    {
        "title": "High-Dimensional Sparse Multivariate Stochastic Volatility Models",
        "authors": [
            "Benjamin Poignard",
            "Manabu Asai"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Although multivariate stochastic volatility models usually produce more\naccurate forecasts compared to the MGARCH models, their estimation techniques\nsuch as Bayesian MCMC typically suffer from the curse of dimensionality. We\npropose a fast and efficient estimation approach for MSV based on a penalized\nOLS framework. Specifying the MSV model as a multivariate state space model, we\ncarry out a two-step penalized procedure. We provide the asymptotic properties\nof the two-step estimator and the oracle property of the first-step estimator\nwhen the number of parameters diverges. The performances of our method are\nillustrated through simulations and financial data.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.08584v2"
    },
    {
        "title": "Bootstrap inference for fixed-effect models",
        "authors": [
            "Ayden Higgins",
            "Koen Jochmans"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The maximum-likelihood estimator of nonlinear panel data models with fixed\neffects is consistent but asymptotically-biased under rectangular-array\nasymptotics. The literature has thus far concentrated its effort on devising\nmethods to correct the maximum-likelihood estimator for its bias as a means to\nsalvage standard inferential procedures. Instead, we show that the parametric\nbootstrap replicates the distribution of the (uncorrected) maximum-likelihood\nestimator in large samples. This justifies the use of confidence sets\nconstructed via standard bootstrap percentile methods. No adjustment for the\npresence of bias needs to be made.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.11156v1"
    },
    {
        "title": "Standard errors for two-way clustering with serially correlated time\n  effects",
        "authors": [
            "Harold D Chiang",
            "Bruce E Hansen",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose improved standard errors and an asymptotic distribution theory for\ntwo-way clustered panels. Our proposed estimator and theory allow for arbitrary\nserial dependence in the common time effects, which is excluded by existing\ntwo-way methods, including the popular two-way cluster standard errors of\nCameron, Gelbach, and Miller (2011) and the cluster bootstrap of Menzel (2021).\nOur asymptotic distribution theory is the first which allows for this level of\ninter-dependence among the observations. Under weak regularity conditions, we\ndemonstrate that the least squares estimator is asymptotically normal, our\nproposed variance estimator is consistent, and t-ratios are asymptotically\nstandard normal, permitting conventional inference. We present simulation\nevidence that confidence intervals constructed with our proposed standard\nerrors obtain superior coverage performance relative to existing methods. We\nillustrate the relevance of the proposed method in an empirical application to\na standard Fama-French three-factor regression.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.11304v4"
    },
    {
        "title": "On the Use of Instrumental Variables in Mediation Analysis",
        "authors": [
            "Bora Kim"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Empirical researchers are often interested in not only whether a treatment\naffects an outcome of interest, but also how the treatment effect arises.\nCausal mediation analysis provides a formal framework to identify causal\nmechanisms through which a treatment affects an outcome. The most popular\nidentification strategy relies on so-called sequential ignorability (SI)\nassumption which requires that there is no unobserved confounder that lies in\nthe causal paths between the treatment and the outcome. Despite its popularity,\nsuch assumption is deemed to be too strong in many settings as it excludes the\nexistence of unobserved confounders. This limitation has inspired recent\nliterature to consider an alternative identification strategy based on an\ninstrumental variable (IV). This paper discusses the identification of causal\nmediation effects in a setting with a binary treatment and a binary\ninstrumental variable that is both assumed to be random. We show that while IV\nmethods allow for the possible existence of unobserved confounders, additional\nmonotonicity assumptions are required unless the strong constant effect is\nassumed. Furthermore, even when such monotonicity assumptions are satisfied, IV\nestimands are not necessarily equivalent to target parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.12752v1"
    },
    {
        "title": "Partial Sum Processes of Residual-Based and Wald-type Break-Point\n  Statistics in Time Series Regression Models",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We revisit classical asymptotics when testing for a structural break in\nlinear regression models by obtaining the limit theory of residual-based and\nWald-type processes. First, we establish the Brownian bridge limiting\ndistribution of these test statistics. Second, we study the asymptotic\nbehaviour of the partial-sum processes in nonstationary (linear) time series\nregression models. Although, the particular comparisons of these two different\nmodelling environments is done from the perspective of the partial-sum\nprocesses, it emphasizes that the presence of nuisance parameters can change\nthe asymptotic behaviour of the functionals under consideration. Simulation\nexperiments verify size distortions when testing for a break in nonstationary\ntime series regressions which indicates that the Brownian bridge limit cannot\nprovide a suitable asymptotic approximation in this case. Further research is\nrequired to establish the cause of size distortions under the null hypothesis\nof parameter stability.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.00141v2"
    },
    {
        "title": "Estimation of Impulse-Response Functions with Dynamic Factor Models: A\n  New Parametrization",
        "authors": [
            "Juho Koistinen",
            "Bernd Funovits"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose a new parametrization for the estimation and identification of the\nimpulse-response functions (IRFs) of dynamic factor models (DFMs). The\ntheoretical contribution of this paper concerns the problem of observational\nequivalence between different IRFs, which implies non-identification of the IRF\nparameters without further restrictions. We show how the previously proposed\nminimal identification conditions are nested in the new framework and can be\nfurther augmented with overidentifying restrictions leading to efficiency\ngains. The current standard practice for the IRF estimation of DFMs is based on\nprincipal components, compared to which the new parametrization is less\nrestrictive and allows for modelling richer dynamics. As the empirical\ncontribution of the paper, we develop an estimation method based on the EM\nalgorithm, which incorporates the proposed identification restrictions. In the\nempirical application, we use a standard high-dimensional macroeconomic dataset\nto estimate the effects of a monetary policy shock. We estimate a strong\nreaction of the macroeconomic variables, while the benchmark models appear to\ngive qualitatively counterintuitive results. The estimation methods are\nimplemented in the accompanying R package.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.00310v2"
    },
    {
        "title": "Adaptive information-based methods for determining the co-integration\n  rank in heteroskedastic VAR models",
        "authors": [
            "H. Peter Boswijk",
            "Giuseppe Cavaliere",
            "Luca De Angelis",
            "A. M. Robert Taylor"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Standard methods, such as sequential procedures based on Johansen's\n(pseudo-)likelihood ratio (PLR) test, for determining the co-integration rank\nof a vector autoregressive (VAR) system of variables integrated of order one\ncan be significantly affected, even asymptotically, by unconditional\nheteroskedasticity (non-stationary volatility) in the data. Known solutions to\nthis problem include wild bootstrap implementations of the PLR test or the use\nof an information criterion, such as the BIC, to select the co-integration\nrank. Although asymptotically valid in the presence of heteroskedasticity,\nthese methods can display very low finite sample power under some patterns of\nnon-stationary volatility. In particular, they do not exploit potential\nefficiency gains that could be realised in the presence of non-stationary\nvolatility by using adaptive inference methods. Under the assumption of a known\nautoregressive lag length, Boswijk and Zu (2022) develop adaptive PLR test\nbased methods using a non-parameteric estimate of the covariance matrix\nprocess. It is well-known, however, that selecting an incorrect lag length can\nsignificantly impact on the efficacy of both information criteria and bootstrap\nPLR tests to determine co-integration rank in finite samples. We show that\nadaptive information criteria-based approaches can be used to estimate the\nautoregressive lag order to use in connection with bootstrap adaptive PLR\ntests, or to jointly determine the co-integration rank and the VAR lag length\nand that in both cases they are weakly consistent for these parameters in the\npresence of non-stationary volatility provided standard conditions hold on the\npenalty term. Monte Carlo simulations are used to demonstrate the potential\ngains from using adaptive methods and an empirical application to the U.S. term\nstructure is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02532v1"
    },
    {
        "title": "Difference in Differences with Time-Varying Covariates",
        "authors": [
            "Carolina Caetano",
            "Brantly Callaway",
            "Stroud Payne",
            "Hugo Sant'Anna Rodrigues"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper considers identification and estimation of causal effect\nparameters from participating in a binary treatment in a difference in\ndifferences (DID) setup when the parallel trends assumption holds after\nconditioning on observed covariates. Relative to existing work in the\neconometrics literature, we consider the case where the value of covariates can\nchange over time and, potentially, where participating in the treatment can\naffect the covariates themselves. We propose new empirical strategies in both\ncases. We also consider two-way fixed effects (TWFE) regressions that include\ntime-varying regressors, which is the most common way that DID identification\nstrategies are implemented under conditional parallel trends. We show that,\neven in the case with only two time periods, these TWFE regressions are not\ngenerally robust to (i) time-varying covariates being affected by the\ntreatment, (ii) treatment effects and/or paths of untreated potential outcomes\ndepending on the level of time-varying covariates in addition to only the\nchange in the covariates over time, (iii) treatment effects and/or paths of\nuntreated potential outcomes depending on time-invariant covariates, (iv)\ntreatment effect heterogeneity with respect to observed covariates, and (v)\nviolations of strong functional form assumptions, both for outcomes over time\nand the propensity score, that are unlikely to be plausible in most DID\napplications. Thus, TWFE regressions can deliver misleading estimates of causal\neffect parameters in a number of empirically relevant cases. We propose both\ndoubly robust estimands and regression adjustment/imputation strategies that\nare robust to these issues while not being substantially more challenging to\nimplement.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02903v3"
    },
    {
        "title": "Predicting Default Probabilities for Stress Tests: A Comparison of\n  Models",
        "authors": [
            "Martin Guth"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Since the Great Financial Crisis (GFC), the use of stress tests as a tool for\nassessing the resilience of financial institutions to adverse financial and\neconomic developments has increased significantly. One key part in such\nexercises is the translation of macroeconomic variables into default\nprobabilities for credit risk by using macrofinancial linkage models. A key\nrequirement for such models is that they should be able to properly detect\nsignals from a wide array of macroeconomic variables in combination with a\nmostly short data sample. The aim of this paper is to compare a great number of\ndifferent regression models to find the best performing credit risk model. We\nset up an estimation framework that allows us to systematically estimate and\nevaluate a large set of models within the same environment. Our results\nindicate that there are indeed better performing models than the current\nstate-of-the-art model. Moreover, our comparison sheds light on other potential\ncredit risk models, specifically highlighting the advantages of machine\nlearning models and forecast combinations.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.03110v1"
    },
    {
        "title": "Continuous permanent unobserved heterogeneity in dynamic discrete choice\n  models",
        "authors": [
            "Jackson Bunting"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In dynamic discrete choice (DDC) analysis, it is common to use mixture models\nto control for unobserved heterogeneity. However, consistent estimation\ntypically requires both restrictions on the support of unobserved heterogeneity\nand a high-level injectivity condition that is difficult to verify. This paper\nprovides primitive conditions for point identification of a broad class of DDC\nmodels with multivariate continuous permanent unobserved heterogeneity. The\nresults apply to both finite- and infinite-horizon DDC models, do not require a\nfull support assumption, nor a long panel, and place no parametric restriction\non the distribution of unobserved heterogeneity. In addition, I propose a\nseminonparametric estimator that is computationally attractive and can be\nimplemented using familiar parametric methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.03960v3"
    },
    {
        "title": "Dynamic Heterogeneous Distribution Regression Panel Models, with an\n  Application to Labor Income Processes",
        "authors": [
            "Ivan Fernandez-Val",
            "Wayne Yuan Gao",
            "Yuan Liao",
            "Francis Vella"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We consider the estimation of a dynamic distribution regression panel data\nmodel with heterogeneous coefficients across units. The objects of primary\ninterest are specific functionals of these coefficients. These include\npredicted actual and stationary distributions of the outcome variable and\nquantile treatment effects. Coefficients and their functionals are estimated\nvia fixed effect methods. We investigate how these functionals vary in response\nto changes in initial conditions or covariate values. We also identify a\nuniformity issue related to the robustness of inference to the unknown degree\nof heterogeneity, and propose a cross-sectional bootstrap method for uniformly\nvalid inference on function-valued objects. Employing PSID annual labor income\ndata we illustrate some important empirical issues we can address. We first\nquantify the impact of a negative labor income shock on the distribution of\nfuture labor income. We also examine the impact on the distribution of labor\nincome from increasing the education level of a chosen group of workers.\nFinally, we demonstrate the existence of heterogeneity in income mobility, and\nhow this leads to substantial variation in individuals' incidences to be\ntrapped in poverty. We also provide simulation evidence confirming that our\nprocedures work well.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.04154v3"
    },
    {
        "title": "von Mises-Fisher distributions and their statistical divergence",
        "authors": [
            "Toru Kitagawa",
            "Jeff Rowley"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The von Mises-Fisher family is a parametric family of distributions on the\nsurface of the unit ball, summarised by a concentration parameter and a mean\ndirection. As a quasi-Bayesian prior, the von Mises-Fisher distribution is a\nconvenient and parsimonious choice when parameter spaces are isomorphic to the\nhypersphere (e.g., maximum score estimation in semi-parametric discrete choice,\nestimation of single-index treatment assignment rules via empirical welfare\nmaximisation, under-identifying linear simultaneous equation models). Despite a\nlong history of application, measures of statistical divergence have not been\nanalytically characterised for von Mises-Fisher distributions. This paper\nprovides analytical expressions for the $f$-divergence of a von Mises-Fisher\ndistribution from another, distinct, von Mises-Fisher distribution in\n$\\mathbb{R}^p$ and the uniform distribution over the hypersphere. This paper\nalso collect several other results pertaining to the von Mises-Fisher family of\ndistributions, and characterises the limiting behaviour of the measures of\ndivergence that we consider.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.05192v2"
    },
    {
        "title": "Sequential Monte Carlo With Model Tempering",
        "authors": [
            "Marko Mlikota",
            "Frank Schorfheide"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Modern macroeconometrics often relies on time series models for which it is\ntime-consuming to evaluate the likelihood function. We demonstrate how Bayesian\ncomputations for such models can be drastically accelerated by reweighting and\nmutating posterior draws from an approximating model that allows for fast\nlikelihood evaluations, into posterior draws from the model of interest, using\na sequential Monte Carlo (SMC) algorithm. We apply the technique to the\nestimation of a vector autoregression with stochastic volatility and a\nnonlinear dynamic stochastic general equilibrium model. The runtime reductions\nwe obtain range from 27% to 88%.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.07070v1"
    },
    {
        "title": "Semiparametric Estimation of Dynamic Binary Choice Panel Data Models",
        "authors": [
            "Fu Ouyang",
            "Thomas Tao Yang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose a new approach to the semiparametric analysis of panel data binary\nchoice models with fixed effects and dynamics (lagged dependent variables). The\nmodel we consider has the same random utility framework as in Honore and\nKyriazidou (2000). We demonstrate that, with additional serial dependence\nconditions on the process of deterministic utility and tail restrictions on the\nerror distribution, the (point) identification of the model can proceed in two\nsteps, and only requires matching the value of an index function of explanatory\nvariables over time, as opposed to that of each explanatory variable. Our\nidentification approach motivates an easily implementable, two-step maximum\nscore (2SMS) procedure -- producing estimators whose rates of convergence, in\ncontrast to Honore and Kyriazidou's (2000) methods, are independent of the\nmodel dimension. We then derive the asymptotic properties of the 2SMS procedure\nand propose bootstrap-based distributional approximations for inference. Monte\nCarlo evidence indicates that our procedure performs adequately in finite\nsamples.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12062v4"
    },
    {
        "title": "Confidence Intervals of Treatment Effects in Panel Data Models with\n  Interactive Fixed Effects",
        "authors": [
            "Xingyu Li",
            "Yan Shen",
            "Qiankun Zhou"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We consider the construction of confidence intervals for treatment effects\nestimated using panel models with interactive fixed effects. We first use the\nfactor-based matrix completion technique proposed by Bai and Ng (2021) to\nestimate the treatment effects, and then use bootstrap method to construct\nconfidence intervals of the treatment effects for treated units at each\npost-treatment period. Our construction of confidence intervals requires\nneither specific distributional assumptions on the error terms nor large number\nof post-treatment periods. We also establish the validity of the proposed\nbootstrap procedure that these confidence intervals have asymptotically correct\ncoverage probabilities. Simulation studies show that these confidence intervals\nhave satisfactory finite sample performances, and empirical applications using\nclassical datasets yield treatment effect estimates of similar magnitudes and\nreliable confidence intervals.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12078v1"
    },
    {
        "title": "Fast variational Bayes methods for multinomial probit models",
        "authors": [
            "Rubén Loaiza-Maya",
            "Didier Nibbering"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The multinomial probit model is often used to analyze choice behaviour.\nHowever, estimation with existing Markov chain Monte Carlo (MCMC) methods is\ncomputationally costly, which limits its applicability to large choice data\nsets. This paper proposes a variational Bayes method that is accurate and fast,\neven when a large number of choice alternatives and observations are\nconsidered. Variational methods usually require an analytical expression for\nthe unnormalized posterior density and an adequate choice of variational\nfamily. Both are challenging to specify in a multinomial probit, which has a\nposterior that requires identifying restrictions and is augmented with a large\nset of latent utilities. We employ a spherical transformation on the covariance\nmatrix of the latent utilities to construct an unnormalized augmented posterior\nthat identifies the parameters, and use the conditional posterior of the latent\nutilities as part of the variational family. The proposed method is faster than\nMCMC, and can be made scalable to both a large number of choice alternatives\nand a large number of observations. The accuracy and scalability of our method\nis illustrated in numerical experiments and real purchase data with one million\nobservations.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12495v2"
    },
    {
        "title": "Variational inference for large Bayesian vector autoregressions",
        "authors": [
            "Mauro Bernardi",
            "Daniele Bianchi",
            "Nicolas Bianco"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose a novel variational Bayes approach to estimate high-dimensional\nvector autoregression (VAR) models with hierarchical shrinkage priors. Our\napproach does not rely on a conventional structural VAR representation of the\nparameter space for posterior inference. Instead, we elicit hierarchical\nshrinkage priors directly on the matrix of regression coefficients so that (1)\nthe prior structure directly maps into posterior inference on the reduced-form\ntransition matrix, and (2) posterior estimates are more robust to variables\npermutation. An extensive simulation study provides evidence that our approach\ncompares favourably against existing linear and non-linear Markov Chain Monte\nCarlo and variational Bayes methods. We investigate both the statistical and\neconomic value of the forecasts from our variational inference approach within\nthe context of a mean-variance investor allocating her wealth in a large set of\ndifferent industry portfolios. The results show that more accurate estimates\ntranslate into substantial statistical and economic out-of-sample gains. The\nresults hold across different hierarchical shrinkage priors and model\ndimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12644v3"
    },
    {
        "title": "Personalized Subsidy Rules",
        "authors": [
            "Yu-Chang Chen",
            "Haitian Xie"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Subsidies are commonly used to encourage behaviors that can lead to short- or\nlong-term benefits. Typical examples include subsidized job training programs\nand provisions of preventive health products, in which both behavioral\nresponses and associated gains can exhibit heterogeneity. This study uses the\nmarginal treatment effect (MTE) framework to study personalized assignments of\nsubsidies based on individual characteristics. First, we derive the optimality\ncondition for a welfare-maximizing subsidy rule by showing that the welfare can\nbe represented as a function of the MTE. Next, we show that subsidies generally\nresult in better welfare than directly mandating the encouraged behavior\nbecause subsidy rules implicitly target individuals through unobserved\nheterogeneity in the behavioral response. When there is positive selection,\nthat is, when individuals with higher returns are more likely to select the\nencouraged behavior, the optimal subsidy rule achieves the first-best welfare,\nwhich is the optimal welfare if a policy-maker can observe individuals' private\ninformation. We then provide methods to (partially) identify the optimal\nsubsidy rule when the MTE is identified and unidentified. Particularly,\npositive selection allows for the point identification of the optimal subsidy\nrule even when the MTE curve is not. As an empirical application, we study the\noptimal wage subsidy using the experimental data from the Jordan New\nOpportunities for Women pilot study.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.13545v2"
    },
    {
        "title": "Forecasting US Inflation Using Bayesian Nonparametric Models",
        "authors": [
            "Todd E. Clark",
            "Florian Huber",
            "Gary Koop",
            "Massimiliano Marcellino"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The relationship between inflation and predictors such as unemployment is\npotentially nonlinear with a strength that varies over time, and prediction\nerrors error may be subject to large, asymmetric shocks. Inspired by these\nconcerns, we develop a model for inflation forecasting that is nonparametric\nboth in the conditional mean and in the error using Gaussian and Dirichlet\nprocesses, respectively. We discuss how both these features may be important in\nproducing accurate forecasts of inflation. In a forecasting exercise involving\nCPI inflation, we find that our approach has substantial benefits, both overall\nand in the left tail, with nonparametric modeling of the conditional mean being\nof particular importance.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.13793v1"
    },
    {
        "title": "A Classifier-Lasso Approach for Estimating Production Functions with\n  Latent Group Structures",
        "authors": [
            "Daniel Czarnowske"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  I present a new estimation procedure for production functions with latent\ngroup structures. I consider production functions that are heterogeneous across\ngroups but time-homogeneous within groups, and where the group membership of\nthe firms is unknown. My estimation procedure is fully data-driven and embeds\nrecent identification strategies from the production function literature into\nthe classifier-Lasso. Simulation experiments demonstrate that firms are\nassigned to their correct latent group with probability close to one. I apply\nmy estimation procedure to a panel of Chilean firms and find sizable\ndifferences in the estimates compared to the standard approach of\nclassification by industry.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.02220v1"
    },
    {
        "title": "Latent Unbalancedness in Three-Way Gravity Models",
        "authors": [
            "Daniel Czarnowske",
            "Amrei Stammann"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Many panel data sets used for pseudo-poisson estimation of three-way gravity\nmodels are implicitly unbalanced because uninformative observations are\nredundant for the estimation. We show with real data as well as simulations\nthat this phenomenon, which we call latent unbalancedness, amplifies the\ninference problem recently studied by Weidner and Zylkin (2021).\n",
        "pdf_link": "http://arxiv.org/pdf/2203.02235v1"
    },
    {
        "title": "Inference in Linear Dyadic Data Models with Network Spillovers",
        "authors": [
            "Nathan Canen",
            "Ko Sugiura"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  When using dyadic data (i.e., data indexed by pairs of units), researchers\ntypically assume a linear model, estimate it using Ordinary Least Squares and\nconduct inference using ``dyadic-robust\" variance estimators. The latter\nassumes that dyads are uncorrelated if they do not share a common unit (e.g.,\nif the same individual is not present in both pairs of data). We show that this\nassumption does not hold in many empirical applications because indirect links\nmay exist due to network connections, generating correlated outcomes. Hence,\n``dyadic-robust'' estimators can be biased in such situations. We develop a\nconsistent variance estimator for such contexts by leveraging results in\nnetwork statistics. Our estimator has good finite sample properties in\nsimulations, while allowing for decay in spillover effects. We illustrate our\nmessage with an application to politicians' voting behavior when they are\nseating neighbors in the European Parliament.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.03497v5"
    },
    {
        "title": "Non-Existent Moments of Earnings Growth",
        "authors": [
            "Silvia Sarpietro",
            "Yuya Sasaki",
            "Yulong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The literature often employs moment-based earnings risk measures like\nvariance, skewness, and kurtosis. However, under heavy-tailed distributions,\nthese moments may not exist in the population. Our empirical analysis reveals\nthat population kurtosis, skewness, and variance often do not exist for the\nconditional distribution of earnings growth. This challenges moment-based\nanalyses. We propose robust conditional Pareto exponents as novel earnings risk\nmeasures, developing estimation and inference methods. Using the UK New\nEarnings Survey Panel Dataset (NESPD) and US Panel Study of Income Dynamics\n(PSID), we find: 1) Moments often fail to exist; 2) Earnings risk increases\nover the life cycle; 3) Job stayers face higher earnings risk; 4) These\npatterns persist during the 2007--2008 recession and the 2015--2016 positive\ngrowth period.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08014v3"
    },
    {
        "title": "Pairwise Valid Instruments",
        "authors": [
            "Zhenting Sun",
            "Kaspar Wüthrich"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Finding valid instruments is difficult. We propose Validity Set Instrumental\nVariable (VSIV) estimation, a method for estimating local average treatment\neffects (LATEs) in heterogeneous causal effect models when the instruments are\npartially invalid. We consider settings with pairwise valid instruments, that\nis, instruments that are valid for a subset of instrument value pairs. VSIV\nestimation exploits testable implications of instrument validity to remove\ninvalid pairs and provides estimates of the LATEs for all remaining pairs,\nwhich can be aggregated into a single parameter of interest using\nresearcher-specified weights. We show that the proposed VSIV estimators are\nasymptotically normal under weak conditions and remove or reduce the asymptotic\nbias relative to standard LATE estimators (that is, LATE estimators that do not\nuse testable implications to remove invalid variation). We evaluate the finite\nsample properties of VSIV estimation in application-based simulations and apply\nour method to estimate the returns to college education using parental\neducation as an instrument.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08050v4"
    },
    {
        "title": "A Simple and Computationally Trivial Estimator for Grouped Fixed Effects\n  Models",
        "authors": [
            "Martin Mugnier"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper introduces a new fixed effects estimator for linear panel data\nmodels with clustered time patterns of unobserved heterogeneity. The method\navoids non-convex and combinatorial optimization by combining a preliminary\nconsistent estimator of the slope coefficient, an agglomerative\npairwise-differencing clustering of cross-sectional units, and a pooled\nordinary least squares regression. Asymptotic guarantees are established in a\nframework where $T$ can grow at any power of $N$, as both $N$ and $T$ approach\ninfinity. Unlike most existing approaches, the proposed estimator is\ncomputationally straightforward and does not require a known upper bound on the\nnumber of groups. As existing approaches, this method leads to a consistent\nestimation of well-separated groups and an estimator of common parameters\nasymptotically equivalent to the infeasible regression controlling for the true\ngroups. An application revisits the statistical association between income and\ndemocracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08879v4"
    },
    {
        "title": "Selection and parallel trends",
        "authors": [
            "Dalia Ghanem",
            "Pedro H. C. Sant'Anna",
            "Kaspar Wüthrich"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We study the role of selection into treatment in difference-in-differences\n(DiD) designs. We derive necessary and sufficient conditions for parallel\ntrends assumptions under general classes of selection mechanisms. These\nconditions characterize the empirical content of parallel trends. For settings\nwhere the necessary conditions are questionable, we propose tools for\nselection-based sensitivity analysis. We also provide templates for justifying\nDiD in applications with and without covariates. A reanalysis of the causal\neffect of NSW training programs demonstrates the usefulness of our\nselection-based approach to sensitivity analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.09001v11"
    },
    {
        "title": "Indirect Inference for Nonlinear Panel Models with Fixed Effects",
        "authors": [
            "Shuowen Chen"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Fixed effect estimators of nonlinear panel data models suffer from the\nincidental parameter problem. This leads to two undesirable consequences in\napplied research: (1) point estimates are subject to large biases, and (2)\nconfidence intervals have incorrect coverages. This paper proposes a\nsimulation-based method for bias reduction. The method simulates data using the\nmodel with estimated individual effects, and finds values of parameters by\nequating fixed effect estimates obtained from observed and simulated data. The\nasymptotic framework provides consistency, bias correction, and asymptotic\nnormality results. An application and simulations to female labor force\nparticipation illustrates the finite-sample performance of the method.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.10683v2"
    },
    {
        "title": "Bounds for Bias-Adjusted Treatment Effect in Linear Econometric Models",
        "authors": [
            "Deepankar Basu"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In linear econometric models with proportional selection on unobservables,\nomitted variable bias in estimated treatment effects are real roots of a cubic\nequation involving estimated parameters from a short and intermediate\nregression. The roots of the cubic are functions of $\\delta$, the degree of\nselection on unobservables, and $R_{max}$, the R-squared in a hypothetical long\nregression that includes the unobservable confounder and all observable\ncontrols. In this paper I propose and implement a novel algorithm to compute\nroots of the cubic equation over relevant regions of the $\\delta$-$R_{max}$\nplane and use the roots to construct bounding sets for the true treatment\neffect. The algorithm is based on two well-known mathematical results: (a) the\ndiscriminant of the cubic equation can be used to demarcate regions of unique\nreal roots from regions of three real roots, and (b) a small change in the\ncoefficients of a polynomial equation will lead to small change in its roots\nbecause the latter are continuous functions of the former. I illustrate my\nmethod by applying it to the analysis of maternal behavior on child outcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12431v1"
    },
    {
        "title": "Correcting Attrition Bias using Changes-in-Changes",
        "authors": [
            "Dalia Ghanem",
            "Sarojini Hirshleifer",
            "Désiré Kédagni",
            "Karen Ortiz-Becerra"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Attrition is a common and potentially important threat to internal validity\nin treatment effect studies. We extend the changes-in-changes approach to\nidentify the average treatment effect for respondents and the entire study\npopulation in the presence of attrition. Our method, which exploits baseline\noutcome data, can be applied to randomized experiments as well as\nquasi-experimental difference-in-difference designs. A formal comparison\nhighlights that while widely used corrections typically impose restrictions on\nwhether or how response depends on treatment, our proposed attrition correction\nexploits restrictions on the outcome model. We further show that the conditions\nrequired for our correction can accommodate a broad class of response models\nthat depend on treatment in an arbitrary way. We illustrate the implementation\nof the proposed corrections in an application to a large-scale randomized\nexperiment.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12740v5"
    },
    {
        "title": "Estimating Nonlinear Network Data Models with Fixed Effects",
        "authors": [
            "David W. Hughes"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  I introduce a new method for bias correction of dyadic models with\nagent-specific fixed-effects, including the dyadic link formation model with\nhomophily and degree heterogeneity. The proposed approach uses a jackknife\nprocedure to deal with the incidental parameters problem. The method can be\napplied to both directed and undirected networks, allows for non-binary outcome\nvariables, and can be used to bias correct estimates of average effects and\ncounterfactual outcomes. I also show how the jackknife can be used to\nbias-correct fixed effect averages over functions that depend on multiple\nnodes, e.g. triads or tetrads in the network. As an example, I implement\nspecification tests for dependence across dyads, such as reciprocity or\ntransitivity. Finally, I demonstrate the usefulness of the estimator in an\napplication to a gravity model for import/export relationships across\ncountries.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.15603v2"
    },
    {
        "title": "Difference-in-Differences for Policy Evaluation",
        "authors": [
            "Brantly Callaway"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Difference-in-differences is one of the most used identification strategies\nin empirical work in economics. This chapter reviews a number of important,\nrecent developments related to difference-in-differences. First, this chapter\nreviews recent work pointing out limitations of two way fixed effects\nregressions (these are panel data regressions that have been the dominant\napproach to implementing difference-in-differences identification strategies)\nthat arise in empirically relevant settings where there are more than two time\nperiods, variation in treatment timing across units, and treatment effect\nheterogeneity. Second, this chapter reviews recently proposed alternative\napproaches that are able to circumvent these issues without being substantially\nmore complicated to implement. Third, this chapter covers a number of\nextensions to these results, paying particular attention to (i) parallel trends\nassumptions that hold only after conditioning on observed covariates and (ii)\nstrategies to partially identify causal effect parameters in\ndifference-in-differences applications in cases where the parallel trends\nassumption may be violated.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.15646v1"
    },
    {
        "title": "Estimating Separable Matching Models",
        "authors": [
            "Alfred Galichon",
            "Bernard Salanié"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this paper we propose two simple methods to estimate models of matching\nwith transferable and separable utility introduced in Galichon and Salani\\'e\n(2022). The first method is a minimum distance estimator that relies on the\ngeneralized entropy of matching. The second relies on a reformulation of the\nmore special but popular Choo and Siow (2006) model; it uses generalized linear\nmodels (GLMs) with two-way fixed effects.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.00362v1"
    },
    {
        "title": "Finite Sample Inference in Incomplete Models",
        "authors": [
            "Lixiong Li",
            "Marc Henry"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose confidence regions for the parameters of incomplete models with\nexact coverage of the true parameter in finite samples. Our confidence region\ninverts a test, which generalizes Monte Carlo tests to incomplete models. The\ntest statistic is a discrete analogue of a new optimal transport\ncharacterization of the sharp identified region. Both test statistic and\ncritical values rely on simulation drawn from the distribution of latent\nvariables and are computed using solutions to discrete optimal transport, hence\nlinear programming problems. We also propose a fast preliminary search in the\nparameter space with an alternative, more conservative yet consistent test,\nbased on a parameter free critical value.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.00473v2"
    },
    {
        "title": "Decomposition of Differences in Distribution under Sample Selection and\n  the Gender Wage Gap",
        "authors": [
            "Santiago Pereda-Fernández"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  I address the decomposition of the differences between the distribution of\noutcomes of two groups when individuals self-select themselves into\nparticipation. I differentiate between the decomposition for participants and\nthe entire population, highlighting how the primitive components of the model\naffect each of the distributions of outcomes. Additionally, I introduce two\nancillary decompositions that help uncover the sources of differences in the\ndistribution of unobservables and participation between the two groups. The\nestimation is done using existing quantile regression methods, for which I show\nhow to perform uniformly valid inference. I illustrate these methods by\nrevisiting the gender wage gap, finding that changes in female participation\nand self-selection have been the main drivers for reducing the gap.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.00551v2"
    },
    {
        "title": "Asymptotic Theory for Unit Root Moderate Deviations in Quantile\n  Autoregressions and Predictive Regressions",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We establish the asymptotic theory in quantile autoregression when the model\nparameter is specified with respect to moderate deviations from the unit\nboundary of the form (1 + c / k) with a convergence sequence that diverges at a\nrate slower than the sample size n. Then, extending the framework proposed by\nPhillips and Magdalinos (2007), we consider the limit theory for the\nnear-stationary and the near-explosive cases when the model is estimated with a\nconditional quantile specification function and model parameters are\nquantile-dependent. Additionally, a Bahadur-type representation and limiting\ndistributions based on the M-estimators of the model parameters are derived.\nSpecifically, we show that the serial correlation coefficient converges in\ndistribution to a ratio of two independent random variables. Monte Carlo\nsimulations illustrate the finite-sample performance of the estimation\nprocedure under investigation.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.02073v2"
    },
    {
        "title": "Finitely Heterogeneous Treatment Effect in Event-study",
        "authors": [
            "Myungkou Shin"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  A key assumption of the differences-in-differences designs is that the\naverage evolution of untreated potential outcomes is the same across different\ntreatment cohorts: a parallel trends assumption. In this paper, we relax the\nparallel trend assumption by assuming a latent type variable and developing a\ntype-specific parallel trend assumption. With a finite support assumption on\nthe latent type variable and long pretreatment time periods, we show that an\nextremum classifier consistently estimates the type assignment. Based on the\nclassification result, we propose a type-specific diff-in-diff estimator for\ntype-specific ATT. By estimating the type-specific ATT, we study heterogeneity\nin treatment effect, in addition to heterogeneity in baseline outcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.02346v5"
    },
    {
        "title": "Bootstrap Cointegration Tests in ARDL Models",
        "authors": [
            "Stefano Bertelli",
            "Gianmarco Vacca",
            "Maria Grazia Zoia"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The paper proposes a new bootstrap approach to the Pesaran, Shin and Smith's\nbound tests in a conditional equilibrium correction model with the aim to\novercome some typical drawbacks of the latter, such as inconclusive inference\nand distortion in size. The bootstrap tests are worked out under several data\ngenerating processes, including degenerate cases. Monte Carlo simulations\nconfirm the better performance of the bootstrap tests with respect to bound\nones and to the asymptotic F test on the independent variables of the ARDL\nmodel. It is also proved that any inference carried out in misspecified models,\nsuch as unconditional ARDLs, may be misleading. Empirical applications\nhighlight the importance of employing the appropriate specification and provide\ndefinitive answers to the inconclusive inference of the bound tests when\nexploring the long-term equilibrium relationship between economic variables.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.04939v1"
    },
    {
        "title": "Partially Linear Models under Data Combination",
        "authors": [
            "Xavier D'Haultfœuille",
            "Christophe Gaillac",
            "Arnaud Maurel"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We study partially linear models when the outcome of interest and some of the\ncovariates are observed in two different datasets that cannot be linked. This\ntype of data combination problem arises very frequently in empirical\nmicroeconomics. Using recent tools from optimal transport theory, we derive a\nconstructive characterization of the sharp identified set. We then build on\nthis result and develop a novel inference method that exploits the specific\ngeometric properties of the identified set. Our method exhibits good\nperformances in finite samples, while remaining very tractable. We apply our\napproach to study intergenerational income mobility over the period 1850-1930\nin the United States. Our method allows us to relax the exclusion restrictions\nused in earlier work, while delivering confidence regions that are informative.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.05175v3"
    },
    {
        "title": "Tuning Parameter-Free Nonparametric Density Estimation from Tabulated\n  Summary Data",
        "authors": [
            "Ji Hyung Lee",
            "Yuya Sasaki",
            "Alexis Akira Toda",
            "Yulong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Administrative data are often easier to access as tabulated summaries than in\nthe original format due to confidentiality concerns. Motivated by this\npractical feature, we propose a novel nonparametric density estimation method\nfrom tabulated summary data based on maximum entropy and prove its strong\nuniform consistency. Unlike existing kernel-based estimators, our estimator is\nfree from tuning parameters and admits a closed-form density that is convenient\nfor post-estimation analysis. We apply the proposed method to the tabulated\nsummary data of the U.S. tax returns to estimate the income distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.05480v3"
    },
    {
        "title": "Neyman allocation is minimax optimal for best arm identification with\n  two arms",
        "authors": [
            "Karun Adusumilli"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This note describes the optimal policy rule, according to the local\nasymptotic minimax regret criterion, for best arm identification when there are\nonly two treatments. It is shown that the optimal sampling rule is the Neyman\nallocation, which allocates a constant fraction of units to each treatment in a\nmanner that is proportional to the standard deviation of the treatment\noutcomes. When the variances are equal, the optimal ratio is one-half. This\npolicy is independent of the data, so there is no adaptation to previous\noutcomes. At the end of the experiment, the policy maker adopts the treatment\nwith higher average outcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.05527v7"
    },
    {
        "title": "Nonparametric Identification of Differentiated Products Demand Using\n  Micro Data",
        "authors": [
            "Steven T. Berry",
            "Philip A. Haile"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We examine identification of differentiated products demand when one has\n\"micro data\" linking individual consumers' characteristics and choices. Our\nmodel nests standard specifications featuring rich observed and unobserved\nconsumer heterogeneity as well as product/market-level unobservables that\nintroduce the problem of econometric endogeneity. Previous work establishes\nidentification of such models using market-level data and instruments for all\nprices and quantities. Micro data provides a panel structure that facilitates\nricher demand specifications and reduces requirements on both the number and\ntypes of instrumental variables. We address identification of demand in the\nstandard case in which non-price product characteristics are assumed exogenous,\nbut also cover identification of demand elasticities and other key features\nwhen product characteristics are endogenous. We discuss implications of these\nresults for applied work.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.06637v2"
    },
    {
        "title": "Nonlinear and Nonseparable Structural Functions in Fuzzy Regression\n  Discontinuity Designs",
        "authors": [
            "Haitian Xie"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Many empirical examples of regression discontinuity (RD) designs concern a\ncontinuous treatment variable, but the theoretical aspects of such models are\nless studied. This study examines the identification and estimation of the\nstructural function in fuzzy RD designs with a continuous treatment variable.\nThe structural function fully describes the causal impact of the treatment on\nthe outcome. We show that the nonlinear and nonseparable structural function\ncan be nonparametrically identified at the RD cutoff under shape restrictions,\nincluding monotonicity and smoothness conditions. Based on the nonparametric\nidentification equation, we propose a three-step semiparametric estimation\nprocedure and establish the asymptotic normality of the estimator. The\nsemiparametric estimator achieves the same convergence rate as in the case of a\nbinary treatment variable. As an application of the method, we estimate the\ncausal effect of sleep time on health status by using the discontinuity in\nnatural light timing at time zone boundaries.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.08168v2"
    },
    {
        "title": "MTE with Misspecification",
        "authors": [
            "Julián Martínez-Iriarte",
            "Pietro Emilio Spini"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper studies the implication of a fraction of the population not\nresponding to the instrument when selecting into treatment. We show that, in\ngeneral, the presence of non-responders biases the Marginal Treatment Effect\n(MTE) curve and many of its functionals. Yet, we show that, when the propensity\nscore is fully supported on the unit interval, it is still possible to restore\nidentification of the MTE curve and its functionals with an appropriate\nre-weighting.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.10445v1"
    },
    {
        "title": "A One-Covariate-at-a-Time Method for Nonparametric Additive Models",
        "authors": [
            "Liangjun Su",
            "Thomas Tao Yang",
            "Yonghui Zhang",
            "Qiankun Zhou"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper proposes a one-covariate-at-a-time multiple testing (OCMT)\napproach to choose significant variables in high-dimensional nonparametric\nadditive regression models. Similarly to Chudik, Kapetanios and Pesaran (2018),\nwe consider the statistical significance of individual nonparametric additive\ncomponents one at a time and take into account the multiple testing nature of\nthe problem. One-stage and multiple-stage procedures are both considered. The\nformer works well in terms of the true positive rate only if the marginal\neffects of all signals are strong enough; the latter helps to pick up hidden\nsignals that have weak marginal effects. Simulations demonstrate the good\nfinite sample performance of the proposed procedures. As an empirical\napplication, we use the OCMT procedure on a dataset we extracted from the\nLongitudinal Survey on Rural Urban Migration in China. We find that our\nprocedure works well in terms of the out-of-sample forecast root mean square\nerrors, compared with competing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12023v3"
    },
    {
        "title": "GMM is Inadmissible Under Weak Identification",
        "authors": [
            "Isaiah Andrews",
            "Anna Mikusheva"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We consider estimation in moment condition models and show that under any\nbound on identification strength, asymptotically admissible (i.e. undominated)\nestimators in a wide class of estimation problems must be uniformly continuous\nin the sample moment function. GMM estimators are in general discontinuous in\nthe sample moments, and are thus inadmissible. We show, by contrast, that\nbagged, or bootstrap aggregated, GMM estimators as well as quasi-Bayes\nposterior means have superior continuity properties, while results in the\nliterature imply that they are equivalent to GMM when identification is strong.\nIn simulations calibrated to published instrumental variables specifications,\nwe find that these alternatives often outperform GMM.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12462v3"
    },
    {
        "title": "Impulse response estimation via flexible local projections",
        "authors": [
            "Haroon Mumtaz",
            "Michele Piffer"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper introduces a flexible local projection that generalizes the model\nby Jord\\'a (2005) to a non-parametric setting using Bayesian Additive\nRegression Trees. Monte Carlo experiments show that our BART-LP model is able\nto capture non-linearities in the impulse responses. Our first application\nshows that the fiscal multiplier is stronger in recession than in expansion\nonly in response to contractionary fiscal shocks, but not in response to\nexpansionary fiscal shocks. We then show that financial shocks generate effects\non the economy that increase more than proportionately in the size of the shock\nwhen the shock is negative, but not when the shock is positive.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.13150v1"
    },
    {
        "title": "Penalized Sieve Estimation of Structural Models",
        "authors": [
            "Yao Luo",
            "Peijun Sang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Estimating structural models is an essential tool for economists. However,\nexisting methods are often inefficient either computationally or statistically,\ndepending on how equilibrium conditions are imposed. We propose a class of\npenalized sieve estimators that are consistent, asymptotic normal, and\nasymptotically efficient. Instead of solving the model repeatedly, we\napproximate the solution with a linear combination of basis functions and\nimpose equilibrium conditions as a penalty in searching for the best fitting\ncoefficients. We apply our method to an entry game between Walmart and Kmart.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.13488v1"
    },
    {
        "title": "Greenhouse Gas Emissions and its Main Drivers: a Panel Assessment for\n  EU-27 Member States",
        "authors": [
            "I. Jianu",
            "S. M. Jeloaica",
            "M. D. Tudorache"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper assesses the effects of greenhouse gas emissions drivers in EU-27\nover the period 2010-2019, using a Panel EGLS model with period fixed effects.\nIn particular, we focused our research on studying the effects of GDP,\nrenewable energy, households energy consumption and waste on the greenhouse gas\nemissions. In this regard, we found a positive relationship between three\nindependent variables (real GDP per capita, households final consumption per\ncapita and waste generation per capita) and greenhouse gas emissions per\ncapita, while the effect of the share of renewable energy in gross final energy\nconsumption on the dependent variable proved to be negative, but quite low. In\naddition, we demonstrate that the main challenge that affects greenhouse gas\nemissions is related to the structure of households energy consumption, which\nis generally composed by environmentally harmful fuels. This suggests the need\nto make greater efforts to support the shift to a green economy based on a\nhigher energy efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00295v1"
    },
    {
        "title": "Higher-order Expansions and Inference for Panel Data Models",
        "authors": [
            "Jiti Gao",
            "Bin Peng",
            "Yayi Yan"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this paper, we propose a simple inferential method for a wide class of\npanel data models with a focus on such cases that have both serial correlation\nand cross-sectional dependence. In order to establish an asymptotic theory to\nsupport the inferential method, we develop some new and useful higher-order\nexpansions, such as Berry-Esseen bound and Edgeworth Expansion, under a set of\nsimple and general conditions. We further demonstrate the usefulness of these\ntheoretical results by explicitly investigating a panel data model with\ninteractive effects which nests many traditional panel data models as special\ncases. Finally, we show the superiority of our approach over several natural\ncompetitors using extensive numerical studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00577v2"
    },
    {
        "title": "A Note on \"A survey of preference estimation with unobserved choice set\n  heterogeneity\" by Gregory S. Crawford, Rachel Griffith, and Alessandro Iaria",
        "authors": [
            "C. Angelo Guevara"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Crawford's et al. (2021) article on estimation of discrete choice models with\nunobserved or latent consideration sets, presents a unified framework to\naddress the problem in practice by using \"sufficient sets\", defined as a\ncombination of past observed choices. The proposed approach is sustained in a\nre-interpretation of a consistency result by McFadden (1978) for the problem of\nsampling of alternatives, but the usage of that result in Crawford et al.\n(2021) is imprecise in an important matter. It is stated that consistency would\nbe attained if any subset of the true consideration set is used for estimation,\nbut McFadden (1978) shows that, in general, one needs to do a sampling\ncorrection that depends on the protocol used to draw the choice set. This note\nderives the sampling correction that is required when the choice set for\nestimation is built from past choices. Then, it formalizes the conditions under\nwhich such correction would fulfill the uniform condition property and can\ntherefore be ignored when building practical estimators, such as the ones\nanalyzed by Crawford et al. (2021).\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00852v1"
    },
    {
        "title": "A short term credibility index for central banks under inflation\n  targeting: an application to Brazil",
        "authors": [
            "Alain Hecq",
            "Joao Issler",
            "Elisa Voisin"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper uses predictive densities obtained via mixed causal-noncausal\nautoregressive models to evaluate the statistical sustainability of Brazilian\ninflation targeting system with the tolerance bounds. The probabilities give an\nindication of the short-term credibility of the targeting system without\nrequiring modelling people's beliefs. We employ receiver operating\ncharacteristic curves to determine the optimal probability threshold from which\nthe bank is predicted to be credible. We also investigate the added value of\nincluding experts predictions of key macroeconomic variables.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00924v2"
    },
    {
        "title": "Efficient Score Computation and Expectation-Maximization Algorithm in\n  Regime-Switching Models",
        "authors": [
            "Chaojun Li",
            "Shi Qiu"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This study proposes an efficient algorithm for score computation for\nregime-switching models, and derived from which, an efficient\nexpectation-maximization (EM) algorithm. Different from existing algorithms,\nthis algorithm does not rely on the forward-backward filtering for smoothed\nregime probabilities, and only involves forward computation. Moreover, the\nalgorithm to compute score is readily extended to compute the Hessian matrix.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.01565v1"
    },
    {
        "title": "Cluster-Robust Inference: A Guide to Empirical Practice",
        "authors": [
            "James G. MacKinnon",
            "Morten Ørregaard Nielsen",
            "Matthew D. Webb"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Methods for cluster-robust inference are routinely used in economics and many\nother disciplines. However, it is only recently that theoretical foundations\nfor the use of these methods in many empirically relevant situations have been\ndeveloped. In this paper, we use these theoretical results to provide a guide\nto empirical practice. We do not attempt to present a comprehensive survey of\nthe (very large) literature. Instead, we bridge theory and practice by\nproviding a thorough guide on what to do and why, based on recently available\neconometric theory and simulation evidence. To practice what we preach, we\ninclude an empirical analysis of the effects of the minimum wage on labor\nsupply of teenagers using individual data.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.03285v1"
    },
    {
        "title": "Leverage, Influence, and the Jackknife in Clustered Regression Models:\n  Reliable Inference Using summclust",
        "authors": [
            "James G. MacKinnon",
            "Morten Ørregaard Nielsen",
            "Matthew D. Webb"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We introduce a new Stata package called summclust that summarizes the cluster\nstructure of the dataset for linear regression models with clustered\ndisturbances. The key unit of observation for such a model is the cluster. We\ntherefore propose cluster-level measures of leverage, partial leverage, and\ninfluence and show how to compute them quickly in most cases. The measures of\nleverage and partial leverage can be used as diagnostic tools to identify\ndatasets and regression designs in which cluster-robust inference is likely to\nbe challenging. The measures of influence can provide valuable information\nabout how the results depend on the data in the various clusters. We also show\nhow to calculate two jackknife variance matrix estimators efficiently as a\nbyproduct of our other computations. These estimators, which are already\navailable in Stata, are generally more conservative than conventional variance\nmatrix estimators. The summclust package computes all the quantities that we\ndiscuss.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.03288v3"
    },
    {
        "title": "Identification and Estimation of Dynamic Games with Unknown Information\n  Structure",
        "authors": [
            "Konan Hara",
            "Yuki Ito",
            "Paul Koh"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper studies the identification and estimation of dynamic games when\nthe underlying information structure is unknown to the researcher. To tractably\ncharacterize the set of Markov perfect equilibrium predictions while\nmaintaining weak assumptions on players' information, we introduce\n\\textit{Markov correlated equilibrium}, a dynamic analog of Bayes correlated\nequilibrium. The set of Markov correlated equilibrium predictions coincides\nwith the set of Markov perfect equilibrium predictions that can arise when the\nplayers can observe more signals than assumed by the analyst. Using Markov\ncorrelated equilibrium as the solution concept, we propose tractable\ncomputational strategies for informationally robust estimation, inference, and\ncounterfactual analysis that deal with the non-convexities arising in dynamic\nenvironments. We use our method to analyze the dynamic competition between\nStarbucks and Dunkin' in the US and the role of informational assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.03706v4"
    },
    {
        "title": "Dynamic demand for differentiated products with fixed-effects unobserved\n  heterogeneity",
        "authors": [
            "Victor Aguirregabiria"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper studies identification and estimation of a dynamic discrete choice\nmodel of demand for differentiated product using consumer-level panel data with\nfew purchase events per consumer (i.e., short panel). Consumers are\nforward-looking and their preferences incorporate two sources of dynamics: last\nchoice dependence due to habits and switching costs, and duration dependence\ndue to inventory, depreciation, or learning. A key distinguishing feature of\nthe model is that consumer unobserved heterogeneity has a Fixed Effects (FE)\nstructure -- that is, its probability distribution conditional on the initial\nvalues of endogenous state variables is unrestricted. I apply and extend recent\nresults to establish the identification of all the structural parameters as\nlong as the dataset includes four or more purchase events per household. The\nparameters can be estimated using a sufficient statistic - conditional maximum\nlikelihood (CML) method. An attractive feature of CML in this model is that the\nsufficient statistic controls for the forward-looking value of the consumer's\ndecision problem such that the method does not require solving dynamic\nprogramming problems or calculating expected present values.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.03948v2"
    },
    {
        "title": "Policy Choice in Time Series by Empirical Welfare Maximization",
        "authors": [
            "Toru Kitagawa",
            "Weining Wang",
            "Mengshan Xu"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper develops a novel method for policy choice in a dynamic setting\nwhere the available data is a multi-variate time series. Building on the\nstatistical treatment choice framework, we propose Time-series Empirical\nWelfare Maximization (T-EWM) methods to estimate an optimal policy rule by\nmaximizing an empirical welfare criterion constructed using nonparametric\npotential outcome time series. We characterize conditions under which T-EWM\nconsistently learns a policy choice that is optimal in terms of conditional\nwelfare given the time-series history. We derive a nonasymptotic upper bound\nfor conditional welfare regret. To illustrate the implementation and uses of\nT-EWM, we perform simulation studies and apply the method to estimate optimal\nrestriction rules against Covid-19.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.03970v4"
    },
    {
        "title": "A unified diagnostic test for regression discontinuity designs",
        "authors": [
            "Koki Fusejima",
            "Takuya Ishihara",
            "Masayuki Sawada"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Diagnostic tests for regression discontinuity design face a size-control\nproblem. We document a massive over-rejection of the identifying restriction\namong empirical studies in the top five economics journals. At least one\ndiagnostic test was rejected for 21 out of 60 studies, whereas less than 5% of\nthe collected 799 tests rejected the null hypotheses. In other words, more than\none-third of the studies rejected at least one of their diagnostic tests,\nwhereas their underlying identifying restrictions appear valid. Multiple\ntesting causes this problem because the median number of tests per study was as\nhigh as 12. Therefore, we offer unified tests to overcome the size-control\nproblem. Our procedure is based on the new joint asymptotic normality of local\npolynomial mean and density estimates. In simulation studies, our unified tests\noutperformed the Bonferroni correction. We implement the procedure as an R\npackage rdtest with two empirical examples in its vignettes.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04345v4"
    },
    {
        "title": "Distributionally Robust Policy Learning with Wasserstein Distance",
        "authors": [
            "Daido Kido"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The effects of treatments are often heterogeneous, depending on the\nobservable characteristics, and it is necessary to exploit such heterogeneity\nto devise individualized treatment rules (ITRs). Existing estimation methods of\nsuch ITRs assume that the available experimental or observational data are\nderived from the target population in which the estimated policy is\nimplemented. However, this assumption often fails in practice because of\nlimited useful data. In this case, policymakers must rely on the data generated\nin the source population, which differs from the target population.\nUnfortunately, existing estimation methods do not necessarily work as expected\nin the new setting, and strategies that can achieve a reasonable goal in such a\nsituation are required. This study examines the application of distributionally\nrobust optimization (DRO), which formalizes an ambiguity about the target\npopulation and adapts to the worst-case scenario in the set. It is shown that\nDRO with Wasserstein distance-based characterization of ambiguity provides\nsimple intuitions and a simple estimation method. I then develop an estimator\nfor the distributionally robust ITR and evaluate its theoretical performance.\nAn empirical application shows that the proposed approach outperforms the naive\napproach in the target population.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04637v2"
    },
    {
        "title": "Stable Outcomes and Information in Games: An Empirical Framework",
        "authors": [
            "Paul S. Koh"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Empirically, many strategic settings are characterized by stable outcomes in\nwhich players' decisions are publicly observed, yet no player takes the\nopportunity to deviate. To analyze such situations in the presence of\nincomplete information, we build an empirical framework by introducing a novel\nsolution concept that we call Bayes stable equilibrium. Our framework allows\nthe researcher to be agnostic about players' information and the equilibrium\nselection rule. The Bayes stable equilibrium identified set collapses to the\ncomplete information pure strategy Nash equilibrium identified set under strong\nassumptions on players' information. Furthermore, all else equal, it is weakly\ntighter than the Bayes correlated equilibrium identified set. We also propose\ncomputationally tractable approaches for estimation and inference. In an\napplication, we study the strategic entry decisions of McDonald's and Burger\nKing in the US. Our results highlight the identifying power of informational\nassumptions and show that the Bayes stable equilibrium identified set can be\nsubstantially tighter than the Bayes correlated equilibrium identified set. In\na counterfactual experiment, we examine the impact of increasing access to\nhealthy food on the market structures in Mississippi food deserts.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04990v2"
    },
    {
        "title": "Estimating Discrete Games of Complete Information: Bringing Logit Back\n  in the Game",
        "authors": [
            "Paul S. Koh"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Estimating discrete games of complete information is often computationally\ndifficult due to partial identification and the absence of closed-form moment\ncharacterizations. This paper proposes computationally tractable approaches to\nestimation and inference that remove the computational burden associated with\nequilibria enumeration, numerical simulation, and grid search. Separately for\nunordered and ordered-actions games, I construct an identified set\ncharacterized by a finite set of generalized likelihood-based conditional\nmoment inequalities that are convex in (a subvector of) structural model\nparameters under the standard logit assumption on unobservables. I use\nsimulation and empirical examples to show that the proposed approaches generate\ninformative identified sets and can be several orders of magnitude faster than\nexisting estimation methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.05002v4"
    },
    {
        "title": "Causal Estimation of Position Bias in Recommender Systems Using\n  Marketplace Instruments",
        "authors": [
            "Rina Friedberg",
            "Karthik Rajkumar",
            "Jialiang Mao",
            "Qian Yao",
            "YinYin Yu",
            "Min Liu"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Information retrieval systems, such as online marketplaces, news feeds, and\nsearch engines, are ubiquitous in today's digital society. They facilitate\ninformation discovery by ranking retrieved items on predicted relevance, i.e.\nlikelihood of interaction (click, share) between users and items. Typically\nmodeled using past interactions, such rankings have a major drawback:\ninteraction depends on the attention items receive. A highly-relevant item\nplaced outside a user's attention could receive little interaction. This\ndiscrepancy between observed interaction and true relevance is termed the\nposition bias. Position bias degrades relevance estimation and when it\ncompounds over time, it can silo users into false relevant items, causing\nmarketplace inefficiencies. Position bias may be identified with randomized\nexperiments, but such an approach can be prohibitive in cost and feasibility.\nPast research has also suggested propensity score methods, which do not\nadequately address unobserved confounding; and regression discontinuity\ndesigns, which have poor external validity. In this work, we address these\nconcerns by leveraging the abundance of A/B tests in ranking evaluations as\ninstrumental variables. Historical A/B tests allow us to access exogenous\nvariation in rankings without manually introducing them, harming user\nexperience and platform revenue. We demonstrate our methodology in two distinct\napplications at LinkedIn - feed ads and the People-You-May-Know (PYMK)\nrecommender. The marketplaces comprise users and campaigns on the ads side, and\ninvite senders and recipients on PYMK. By leveraging prior experimentation, we\nobtain quasi-experimental variation in item rankings that is orthogonal to user\nrelevance. Our method provides robust position effect estimates that handle\nunobserved confounding well, greater generalizability, and easily extends to\nother information retrieval systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.06363v1"
    },
    {
        "title": "How do Bounce Rates vary according to product sold?",
        "authors": [
            "Himanshu Sharma"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Bounce Rate of different E-commerce websites depends on the different factors\nbased upon the different devices through which traffic share is observed. This\nresearch paper focuses on how the type of products sold by different E-commerce\nwebsites affects the bounce rate obtained through Mobile/Desktop. It tries to\nexplain the observations which counter the general trend of positive relation\nbetween Mobile traffic share and bounce rate and how this is different for the\nDesktop. To estimate the differences created by the types of products sold by\nE-commerce websites on the bounce rate according to the data observed for\ndifferent time, fixed effect model (within group method) is used to determine\nthe difference created by the factors. Along with the effect of the type of\nproducts sold by the E-commerce website on bounce rate, the effect of\nindividual website is also compared to verify the results obtained for type of\nproducts.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.06866v1"
    },
    {
        "title": "Is climate change time reversible?",
        "authors": [
            "Francesco Giancaterini",
            "Alain Hecq",
            "Claudio Morana"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper proposes strategies to detect time reversibility in stationary\nstochastic processes by using the properties of mixed causal and noncausal\nmodels. It shows that they can also be used for non-stationary processes when\nthe trend component is computed with the Hodrick-Prescott filter rendering a\ntime-reversible closed-form solution. This paper also links the concept of an\nenvironmental tipping point to the statistical property of time irreversibility\nand assesses fourteen climate indicators. We find evidence of time\nirreversibility in $GHG$ emissions, global temperature, global sea levels, sea\nice area, and some natural oscillation indices. While not conclusive, our\nfindings urge the implementation of correction policies to avoid the worst\nconsequences of climate change and not miss the opportunity window, which might\nstill be available, despite closing quickly.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.07579v3"
    },
    {
        "title": "2SLS with Multiple Treatments",
        "authors": [
            "Manudeep Bhuller",
            "Henrik Sigstad"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We study what two-stage least squares (2SLS) identifies in models with\nmultiple treatments under treatment effect heterogeneity. Two conditions are\nshown to be necessary and sufficient for the 2SLS to identify positively\nweighted sums of agent-specific effects of each treatment: average conditional\nmonotonicity and no cross effects. Our identification analysis allows for any\nnumber of treatments, any number of continuous or discrete instruments, and the\ninclusion of covariates. We provide testable implications and present\ncharacterizations of choice behavior implied by our identification conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.07836v11"
    },
    {
        "title": "The Power of Tests for Detecting $p$-Hacking",
        "authors": [
            "Graham Elliott",
            "Nikolay Kudrin",
            "Kaspar Wüthrich"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  $p$-Hacking undermines the validity of empirical studies. A flourishing\nempirical literature investigates the prevalence of $p$-hacking based on the\ndistribution of $p$-values across studies. Interpreting results in this\nliterature requires a careful understanding of the power of methods for\ndetecting $p$-hacking. We theoretically study the implications of likely forms\nof $p$-hacking on the distribution of $p$-values to understand the power of\ntests for detecting it. Power depends crucially on the $p$-hacking strategy and\nthe distribution of true effects. Publication bias can enhance the power for\ntesting the joint null of no $p$-hacking and no publication bias.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.07950v3"
    },
    {
        "title": "Nonlinear Fore(Back)casting and Innovation Filtering for\n  Causal-Noncausal VAR Models",
        "authors": [
            "Christian Gourieroux",
            "Joann Jasiak"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We introduce closed-form formulas of out-of-sample predictive densities for\nforecasting and backcasting of mixed causal-noncausal (Structural) Vector\nAutoregressive VAR models. These nonlinear and time irreversible non-Gaussian\nVAR processes are shown to satisfy the Markov property in both calendar and\nreverse time. A post-estimation inference method for assessing the forecast\ninterval uncertainty due to the preliminary estimation step is introduced too.\nThe nonlinear past-dependent innovations of a mixed causal-noncausal VAR model\nare defined and their filtering and identification methods are discussed. Our\napproach is illustrated by a simulation study, and an application to\ncryptocurrency prices.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.09922v3"
    },
    {
        "title": "The Forecasting performance of the Factor model with Martingale\n  Difference errors",
        "authors": [
            "Luca Mattia Rolla",
            "Alessandro Giovannelli"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper analyses the forecasting performance of a new class of factor\nmodels with martingale difference errors (FMMDE) recently introduced by Lee and\nShao (2018). The FMMDE makes it possible to retrieve a transformation of the\noriginal series so that the resulting variables can be partitioned according to\nwhether they are conditionally mean-independent with respect to past\ninformation. We contribute to the literature in two respects. First, we propose\na novel methodology for selecting the number of factors in FMMDE. Through\nsimulation experiments, we show the good performance of our approach for finite\nsamples for various panel data specifications. Second, we compare the\nforecasting performance of FMMDE with alternative factor model specifications\nby conducting an extensive forecasting exercise using FRED-MD, a comprehensive\nmonthly macroeconomic database for the US economy. Our empirical findings\nindicate that FMMDE provides an advantage in predicting the evolution of the\nreal sector of the economy when the novel methodology for factor selection is\nadopted. These results are confirmed for key aggregates such as Production and\nIncome, the Labor Market, and Consumption.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.10256v2"
    },
    {
        "title": "Treatment Effects in Bunching Designs: The Impact of Mandatory Overtime\n  Pay on Hours",
        "authors": [
            "Leonard Goff"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper studies the identifying power of bunching at kinks when the\nresearcher does not assume a parametric choice model. I find that in a general\nchoice model, identifying the average causal response to the policy switch at a\nkink amounts to confronting two extrapolation problems, each about the\ndistribution of a counterfactual choice that is observed only in a censored\nmanner. I apply this insight to partially identify the effect of overtime pay\nregulation on the hours of U.S. workers using administrative payroll data,\nassuming that each distribution satisfies a weak non-parametric shape\nconstraint in the region where it is not observed. The resulting bounds are\ninformative and indicate a relatively small elasticity of demand for weekly\nhours, addressing a long-standing question about the causal effects of the\novertime mandate.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.10310v4"
    },
    {
        "title": "Estimation and Inference for High Dimensional Factor Model with Regime\n  Switching",
        "authors": [
            "Giovanni Urga",
            "Fa Wang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper proposes maximum (quasi)likelihood estimation for high dimensional\nfactor models with regime switching in the loadings. The model parameters are\nestimated jointly by the EM (expectation maximization) algorithm, which in the\ncurrent context only requires iteratively calculating regime probabilities and\nprincipal components of the weighted sample covariance matrix. When regime\ndynamics are taken into account, smoothed regime probabilities are calculated\nusing a recursive algorithm. Consistency, convergence rates and limit\ndistributions of the estimated loadings and the estimated factors are\nestablished under weak cross-sectional and temporal dependence as well as\nheteroscedasticity. It is worth noting that due to high dimension, regime\nswitching can be identified consistently after the switching point with only\none observation. Simulation results show good performance of the proposed\nmethod. An application to the FRED-MD dataset illustrates the potential of the\nproposed method for detection of business cycle turning points.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.12126v2"
    },
    {
        "title": "Identification of Auction Models Using Order Statistics",
        "authors": [
            "Yao Luo",
            "Ruli Xiao"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Auction data often contain information on only the most competitive bids as\nopposed to all bids. The usual measurement error approaches to unobserved\nheterogeneity are inapplicable due to dependence among order statistics. We\nbridge this gap by providing a set of positive identification results. First,\nwe show that symmetric auctions with discrete unobserved heterogeneity are\nidentifiable using two consecutive order statistics and an instrument. Second,\nwe extend the results to ascending auctions with unknown competition and\nunobserved heterogeneity.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.12917v2"
    },
    {
        "title": "Fast Two-Stage Variational Bayesian Approach to Estimating Panel Spatial\n  Autoregressive Models with Unrestricted Spatial Weights Matrices",
        "authors": [
            "Deborah Gefang",
            "Stephen G. Hall",
            "George S. Tavlas"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper proposes a fast two-stage variational Bayesian (VB) algorithm to\nestimate unrestricted panel spatial autoregressive models. Using\nDirichlet-Laplace priors, we are able to uncover the spatial relationships\nbetween cross-sectional units without imposing any a priori restrictions. Monte\nCarlo experiments show that our approach works well for both long and short\npanels. We are also the first in the literature to develop VB methods to\nestimate large covariance matrices with unrestricted sparsity patterns, which\nare useful for popular large data models such as Bayesian vector\nautoregressions. In empirical applications, we examine the spatial\ninterdependence between euro area sovereign bond ratings and spreads. We find\nmarked differences between the spillover behaviours of the northern euro area\ncountries and those of the south.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.15420v3"
    },
    {
        "title": "Estimating spot volatility under infinite variation jumps with dependent\n  market microstructure noise",
        "authors": [
            "Qiang Liu",
            "Zhi Liu"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Jumps and market microstructure noise are stylized features of high-frequency\nfinancial data. It is well known that they introduce bias in the estimation of\nvolatility (including integrated and spot volatilities) of assets, and many\nmethods have been proposed to deal with this problem. When the jumps are\nintensive with infinite variation, the efficient estimation of spot volatility\nunder serially dependent noise is not available and is thus in need. For this\npurpose, we propose a novel estimator of spot volatility with a hybrid use of\nthe pre-averaging technique and the empirical characteristic function. Under\nmild assumptions, the results of consistency and asymptotic normality of our\nestimator are established. Furthermore, we show that our estimator achieves an\nalmost efficient convergence rate with optimal variance when the jumps are\neither less active or active with symmetric structure. Simulation studies\nverify our theoretical conclusions. We apply our proposed estimator to\nempirical analyses, such as estimating the weekly volatility curve using\nsecond-by-second transaction price data.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.15738v2"
    },
    {
        "title": "Time-Varying Multivariate Causal Processes",
        "authors": [
            "Jiti Gao",
            "Bin Peng",
            "Wei Biao Wu",
            "Yayi Yan"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this paper, we consider a wide class of time-varying multivariate causal\nprocesses which nests many classic and new examples as special cases. We first\nprove the existence of a weakly dependent stationary approximation for our\nmodel which is the foundation to initiate the theoretical development.\nAfterwards, we consider the QMLE estimation approach, and provide both\npoint-wise and simultaneous inferences on the coefficient functions. In\naddition, we demonstrate the theoretical findings through both simulated and\nreal data examples. In particular, we show the empirical relevance of our study\nusing an application to evaluate the conditional correlations between the stock\nmarkets of China and U.S. We find that the interdependence between the two\nstock markets is increasing over time.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.00409v1"
    },
    {
        "title": "Human Wellbeing and Machine Learning",
        "authors": [
            "Ekaterina Oparina",
            "Caspar Kaiser",
            "Niccolò Gentile",
            "Alexandre Tkatchenko",
            "Andrew E. Clark",
            "Jan-Emmanuel De Neve",
            "Conchita D'Ambrosio"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  There is a vast literature on the determinants of subjective wellbeing.\nInternational organisations and statistical offices are now collecting such\nsurvey data at scale. However, standard regression models explain surprisingly\nlittle of the variation in wellbeing, limiting our ability to predict it. In\nresponse, we here assess the potential of Machine Learning (ML) to help us\nbetter understand wellbeing. We analyse wellbeing data on over a million\nrespondents from Germany, the UK, and the United States. In terms of predictive\npower, our ML approaches do perform better than traditional models. Although\nthe size of the improvement is small in absolute terms, it turns out to be\nsubstantial when compared to that of key variables like health. We moreover\nfind that drastically expanding the set of explanatory variables doubles the\npredictive power of both OLS and the ML approaches on unseen data. The\nvariables identified as important by our ML algorithms - $i.e.$ material\nconditions, health, and meaningful social relations - are similar to those that\nhave already been identified in the literature. In that sense, our data-driven\nML results validate the findings from conventional approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.00574v1"
    },
    {
        "title": "Randomization Inference Tests for Shift-Share Designs",
        "authors": [
            "Luis Alvarez",
            "Bruno Ferman",
            "Raoni Oliveira"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We consider the problem of inference in shift-share research designs. The\nchoice between existing approaches that allow for unrestricted spatial\ncorrelation involves tradeoffs, varying in terms of their validity when there\nare relatively few or concentrated shocks, and in terms of the assumptions on\nthe shock assignment process and treatment effects heterogeneity. We propose\nalternative randomization inference methods that combine the advantages of\ndifferent approaches. These methods are valid in finite samples under\nrelatively stronger assumptions, while asymptotically valid under weaker\nassumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.00999v1"
    },
    {
        "title": "Causal impact of severe events on electricity demand: The case of\n  COVID-19 in Japan",
        "authors": [
            "Yasunobu Wakashiro"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  As of May 2022, the coronavirus disease 2019 (COVID-19) still has a severe\nglobal impact on people's lives. Previous studies have reported that COVID-19\ndecreased the electricity demand in early 2020. However, our study found that\nthe electricity demand increased in summer and winter even when the infection\nwas widespread. The fact that the event has continued over two years suggests\nthat it is essential to introduce the method which can estimate the impact of\nthe event for long period considering seasonal fluctuations. We employed the\nBayesian structural time-series model to estimate the causal impact of COVID-19\non electricity demand in Japan. The results indicate that behavioral\nrestrictions due to COVID-19 decreased the daily electricity demand (-5.1% in\nweekdays, -6.1% in holidays) in April and May 2020 as indicated by previous\nstudies. However, even in 2020, the results show that the demand increases in\nthe hot summer and cold winter (the increasing rate is +14% in the period from\n1st August to 15th September 2020, and +7.6% from 16th December 2020 to 15th\nJanuary 2021). This study shows that the significant decrease in electricity\ndemand for the business sector exceeded the increase in demand for the\nhousehold sector in April and May 2020; however, the increase in demand for the\nhouseholds exceeded the decrease in demand for the business in hot summer and\ncold winter periods. Our result also implies that it is possible to run out of\nelectricity when people's behavior changes even if they are less active.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.02122v1"
    },
    {
        "title": "On the Performance of the Neyman Allocation with Small Pilots",
        "authors": [
            "Yong Cai",
            "Ahnaf Rafi"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The Neyman Allocation is used in many papers on experimental design, which\ntypically assume that researchers have access to large pilot studies. This may\nbe unrealistic. To understand the properties of the Neyman Allocation with\nsmall pilots, we study its behavior in an asymptotic framework that takes pilot\nsize to be fixed even as the size of the main wave tends to infinity. Our\nanalysis shows that the Neyman Allocation can lead to estimates of the ATE with\nhigher asymptotic variance than with (non-adaptive) balanced randomization. In\nparticular, this happens when the outcome variable is relatively homoskedastic\nwith respect to treatment status or when it exhibits high kurtosis. We provide\na series of empirical examples showing that such situations can arise in\npractice. Our results suggest that researchers with small pilots should not use\nthe Neyman Allocation if they believe that outcomes are homoskedastic or\nheavy-tailed. Finally, we examine some potential methods for improving the\nfinite sample performance of the FNA via simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.04643v4"
    },
    {
        "title": "Machine Learning Inference on Inequality of Opportunity",
        "authors": [
            "Juan Carlos Escanciano",
            "Joël Robert Terschuur"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Equality of opportunity has emerged as an important ideal of distributive\njustice. Empirically, Inequality of Opportunity (IOp) is measured in two steps:\nfirst, an outcome (e.g., income) is predicted given individual circumstances;\nand second, an inequality index (e.g., Gini) of the predictions is computed.\nMachine Learning (ML) methods are tremendously useful in the first step.\nHowever, they can cause sizable biases in IOp since the bias-variance trade-off\nallows the bias to creep in the second step. We propose a simple debiased IOp\nestimator robust to such ML biases and provide the first valid inferential\ntheory for IOp. We demonstrate improved performance in simulations and report\nthe first unbiased measures of income IOp in Europe. Mother's education and\nfather's occupation are the circumstances that explain the most. Plug-in\nestimators are very sensitive to the ML algorithm, while debiased IOp\nestimators are robust. These results are extended to a general U-statistics\nsetting.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.05235v3"
    },
    {
        "title": "A Constructive GAN-based Approach to Exact Estimate Treatment Effect\n  without Matching",
        "authors": [
            "Boyang You",
            "Kerry Papps"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Matching has become the mainstream in counterfactual inference, with which\nselection bias between sample groups can be significantly eliminated. However\nin practice, when estimating average treatment effect on the treated (ATT) via\nmatching, no matter which method, the trade-off between estimation accuracy and\ninformation loss constantly exist. Attempting to completely replace the\nmatching process, this paper proposes the GAN-ATT estimator that integrates\ngenerative adversarial network (GAN) into counterfactual inference framework.\nThrough GAN machine learning, the probability density functions (PDFs) of\nsamples in both treatment group and control group can be approximated. By\ndifferentiating conditional PDFs of the two groups with identical input\ncondition, the conditional average treatment effect (CATE) can be estimated,\nand the ensemble average of corresponding CATEs over all treatment group\nsamples is the estimate of ATT. Utilizing GAN-based infinite sample\naugmentations, problems in the case of insufficient samples or lack of common\nsupport domains can be easily solved. Theoretically, when GAN could perfectly\nlearn the PDFs, our estimators can provide exact estimate of ATT.\n  To check the performance of the GAN-ATT estimator, three sets of data are\nused for ATT estimations: Two toy data sets with 1/2 dimensional covariate\ninputs and constant/covariate-dependent treatment effect are tested. The\nestimates of GAN-ATT are proved close to the ground truth and are better than\ntraditional matching approaches; A real firm-level data set with\nhigh-dimensional input is tested and the applicability towards real data sets\nis evaluated by comparing matching approaches. Through the evidences obtained\nfrom the three tests, we believe that the GAN-ATT estimator has significant\nadvantages over traditional matching methods in estimating ATT.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.06116v1"
    },
    {
        "title": "Clustering coefficients as measures of the complex interactions in a\n  directed weighted multilayer network",
        "authors": [
            "Paolo Bartesaghi",
            "Gian Paolo Clemente",
            "Rosanna Grassi"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this paper, we provide novel definitions of clustering coefficient for\nweighted and directed multilayer networks. We extend in the multilayer\ntheoretical context the clustering coefficients proposed in the literature for\nweighted directed monoplex networks. We quantify how deeply a node is involved\nin a choesive structure focusing on a single node, on a single layer or on the\nentire system. The coefficients convey several characteristics inherent to the\ncomplex topology of the multilayer network. We test their effectiveness\napplying them to a particularly complex structure such as the international\ntrade network. The trade data integrate different aspects and they can be\ndescribed by a directed and weighted multilayer network, where each layer\nrepresents import and export relationships between countries for a given\nsector. The proposed coefficients find successful application in describing the\ninterrelations of the trade network, allowing to disentangle the effects of\ncountries and sectors and jointly consider the interactions between them.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.06309v2"
    },
    {
        "title": "Nowcasting the Portuguese GDP with Monthly Data",
        "authors": [
            "João B. Assunção",
            "Pedro Afonso Fernandes"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this article, we present a method to forecast the Portuguese gross\ndomestic product (GDP) in each current quarter (nowcasting). It combines bridge\nequations of the real GDP on readily available monthly data like the Economic\nSentiment Indicator (ESI), industrial production index, cement sales or exports\nand imports, with forecasts for the jagged missing values computed with the\nwell-known Hodrick and Prescott (HP) filter. As shown, this simple multivariate\napproach can perform as well as a Targeted Diffusion Index (TDI) model and\nslightly better than the univariate Theta method in terms of out-of-sample mean\nerrors.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.06823v1"
    },
    {
        "title": "Likelihood ratio test for structural changes in factor models",
        "authors": [
            "Jushan Bai",
            "Jiangtao Duan",
            "Xu Han"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  A factor model with a break in its factor loadings is observationally\nequivalent to a model without changes in the loadings but a change in the\nvariance of its factors. This effectively transforms a structural change\nproblem of high dimension into a problem of low dimension. This paper considers\nthe likelihood ratio (LR) test for a variance change in the estimated factors.\nThe LR test implicitly explores a special feature of the estimated factors: the\npre-break and post-break variances can be a singular matrix under the\nalternative hypothesis, making the LR test diverging faster and thus more\npowerful than Wald-type tests. The better power property of the LR test is also\nconfirmed by simulations. We also consider mean changes and multiple breaks. We\napply the procedure to the factor modelling and structural change of the US\nemployment using monthly industry-level-data.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.08052v2"
    },
    {
        "title": "Fast and Accurate Variational Inference for Large Bayesian VARs with\n  Stochastic Volatility",
        "authors": [
            "Joshua C. C. Chan",
            "Xuewen Yu"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose a new variational approximation of the joint posterior\ndistribution of the log-volatility in the context of large Bayesian VARs. In\ncontrast to existing approaches that are based on local approximations, the new\nproposal provides a global approximation that takes into account the entire\nsupport of the joint distribution. In a Monte Carlo study we show that the new\nglobal approximation is over an order of magnitude more accurate than existing\nalternatives. We illustrate the proposed methodology with an application of a\n96-variable VAR with stochastic volatility to measure global bank network\nconnectedness.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.08438v1"
    },
    {
        "title": "Semiparametric Single-Index Estimation for Average Treatment Effects",
        "authors": [
            "Difang Huang",
            "Jiti Gao",
            "Tatsushi Oka"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose a semiparametric method to estimate the average treatment effect\nunder the assumption of unconfoundedness given observational data. Our\nestimation method alleviates misspecification issues of the propensity score\nfunction by estimating the single-index link function involved through Hermite\npolynomials. Our approach is computationally tractable and allows for\nmoderately large dimension covariates. We provide the large sample properties\nof the estimator and show its validity. Also, the average treatment effect\nestimator achieves the parametric rate and asymptotic normality. Our extensive\nMonte Carlo study shows that the proposed estimator is valid in finite samples.\nApplying our method to maternal smoking and infant health, we find that\nconventional estimates of smoking's impact on birth weight may be biased due to\npropensity score misspecification, and our analysis of job training programs\nreveals earnings effects that are more precisely estimated than in prior work.\nThese applications demonstrate how addressing model misspecification can\nsubstantively affect our understanding of key policy-relevant treatment\neffects.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.08503v4"
    },
    {
        "title": "Interpretable and Actionable Vehicular Greenhouse Gas Emission\n  Prediction at Road link-level",
        "authors": [
            "S. Roderick Zhang",
            "Bilal Farooq"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  To help systematically lower anthropogenic Greenhouse gas (GHG) emissions,\naccurate and precise GHG emission prediction models have become a key focus of\nthe climate research. The appeal is that the predictive models will inform\npolicymakers, and hopefully, in turn, they will bring about systematic changes.\nSince the transportation sector is constantly among the top GHG emission\ncontributors, especially in populated urban areas, substantial effort has been\ngoing into building more accurate and informative GHG prediction models to help\ncreate more sustainable urban environments. In this work, we seek to establish\na predictive framework of GHG emissions at the urban road segment or link level\nof transportation networks. The key theme of the framework centers around model\ninterpretability and actionability for high-level decision-makers using\neconometric Discrete Choice Modelling (DCM). We illustrate that DCM is capable\nof predicting link-level GHG emission levels on urban road networks in a\nparsimonious and effective manner. Our results show up to 85.4% prediction\naccuracy in the DCM models' performances. We also argue that since the goal of\nmost GHG emission prediction models focuses on involving high-level\ndecision-makers to make changes and curb emissions, the DCM-based GHG emission\nprediction framework is the most suitable framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.09073v1"
    },
    {
        "title": "Unbiased estimation of the OLS covariance matrix when the errors are\n  clustered",
        "authors": [
            "Tom Boot",
            "Gianmaria Niccodemi",
            "Tom Wansbeek"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  When data are clustered, common practice has become to do OLS and use an\nestimator of the covariance matrix of the OLS estimator that comes close to\nunbiasedness. In this paper we derive an estimator that is unbiased when the\nrandom-effects model holds. We do the same for two more general structures. We\nstudy the usefulness of these estimators against others by simulation, the size\nof the $t$-test being the criterion. Our findings suggest that the choice of\nestimator hardly matters when the regressor has the same distribution over the\nclusters. But when the regressor is a cluster-specific treatment variable, the\nchoice does matter and the unbiased estimator we propose for the random-effects\nmodel shows excellent performance, even when the clusters are highly\nunbalanced.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.09644v1"
    },
    {
        "title": "Policy Learning under Endogeneity Using Instrumental Variables",
        "authors": [
            "Yan Liu"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper studies the identification and estimation of individualized\nintervention policies in observational data settings characterized by\nendogenous treatment selection and the availability of instrumental variables.\nWe introduce encouragement rules that manipulate an instrument. Incorporating\nthe marginal treatment effects (MTE) as policy invariant structural parameters,\nwe establish the identification of the social welfare criterion for the optimal\nencouragement rule. Focusing on binary encouragement rules, we propose to\nestimate the optimal policy via the Empirical Welfare Maximization (EWM) method\nand derive convergence rates of the regret (welfare loss). We consider\nextensions to accommodate multiple instruments and budget constraints. Using\ndata from the Indonesian Family Life Survey, we apply the EWM encouragement\nrule to advise on the optimal tuition subsidy assignment. Our framework offers\ninterpretability regarding why a certain subpopulation is targeted.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.09883v3"
    },
    {
        "title": "Estimation and Inference in High-Dimensional Panel Data Models with\n  Interactive Fixed Effects",
        "authors": [
            "Oliver Linton",
            "Maximilian Ruecker",
            "Michael Vogt",
            "Christopher Walsh"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We develop new econometric methods for estimation and inference in\nhigh-dimensional panel data models with interactive fixed effects. Our approach\ncan be regarded as a non-trivial extension of the very popular common\ncorrelated effects (CCE) approach. Roughly speaking, we proceed as follows: We\nfirst construct a projection device to eliminate the unobserved factors from\nthe model by applying a dimensionality reduction transform to the matrix of\ncross-sectionally averaged covariates. The unknown parameters are then\nestimated by applying lasso techniques to the projected model. For inference\npurposes, we derive a desparsified version of our lasso-type estimator. While\nthe original CCE approach is restricted to the low-dimensional case where the\nnumber of regressors is small and fixed, our methods can deal with both low-\nand high-dimensional situations where the number of regressors is large and may\neven exceed the overall sample size. We derive theory for our estimation and\ninference methods both in the large-T-case, where the time series length T\ntends to infinity, and in the small-T-case, where T is a fixed natural number.\nSpecifically, we derive the convergence rate of our estimator and show that its\ndesparsified version is asymptotically normal under suitable regularity\nconditions. The theoretical analysis of the paper is complemented by a\nsimulation study and an empirical application to characteristic based asset\npricing.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.12152v2"
    },
    {
        "title": "Misspecification and Weak Identification in Asset Pricing",
        "authors": [
            "Frank Kleibergen",
            "Zhaoguo Zhan"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The widespread co-existence of misspecification and weak identification in\nasset pricing has led to an overstated performance of risk factors. Because the\nconventional Fama and MacBeth (1973) methodology is jeopardized by\nmisspecification and weak identification, we infer risk premia by using a\ndouble robust Lagrange multiplier test that remains reliable in the presence of\nthese two empirically relevant issues. Moreover, we show how the\nidentification, and the resulting appropriate interpretation, of the risk\npremia is governed by the relative magnitudes of the misspecification\nJ-statistic and the identification IS-statistic. We revisit several prominent\nempirical applications and all specifications with one to six factors from the\nfactor zoo of Feng, Giglio, and Xiu (2020) to emphasize the widespread\noccurrence of misspecification and weak identification.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.13600v1"
    },
    {
        "title": "Unique futures in China: studys on volatility spillover effects of\n  ferrous metal futures",
        "authors": [
            "Tingting Cao",
            "Weiqing Sun",
            "Cuiping Sun",
            "Lin Hao"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Ferrous metal futures have become unique commodity futures with Chinese\ncharacteristics. Due to the late listing time, it has received less attention\nfrom scholars. Our research focuses on the volatility spillover effects,\ndefined as the intensity of price volatility in financial instruments. We use\nDCC-GARCH, BEKK-GARCH, and DY(2012) index methods to conduct empirical tests on\nthe volatility spillover effects of the Chinese ferrous metal futures market\nand other parts of the Chinese commodity futures market, as well as industries\nrelated to the steel industry chain in stock markets. It can be seen that there\nis a close volatility spillover relationship between ferrous metal futures and\nnonferrous metal futures. Energy futures and chemical futures have a\nsignificant transmission effect on the fluctuations of ferrous metals. In\naddition, ferrous metal futures have a significant spillover effect on the\nstock index of the steel industry, real estate industry, building materials\nindustry, machinery equipment industry, and household appliance industry.\nStudying the volatility spillover effect of the ferrous metal futures market\ncan reveal the operating laws of this field and provide ideas and theoretical\nreferences for investors to hedge their risks. It shows that the ferrous metal\nfutures market has an essential role as a \"barometer\" for the Chinese commodity\nfutures market and the stock market.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.15039v1"
    },
    {
        "title": "Valid and Unobtrusive Measurement of Returns to Advertising through\n  Asymmetric Budget Split",
        "authors": [
            "Johannes Hermle",
            "Giorgio Martini"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Ad platforms require reliable measurement of advertising returns: what\nincrease in performance (such as clicks or conversions) can an advertiser\nexpect in return for additional budget on the platform? Even from the\nperspective of the platform, accurately measuring advertising returns is hard.\nSelection and omitted variable biases make estimates from observational methods\nunreliable, and straightforward experimentation is often costly or infeasible.\nWe introduce Asymmetric Budget Split, a novel methodology for valid measurement\nof ad returns from the perspective of the platform. Asymmetric budget split\ncreates small asymmetries in ad budget allocation across comparable partitions\nof the platform's userbase. By observing performance of the same ad at\ndifferent budget levels while holding all other factors constant, the platform\ncan obtain a valid measure of ad returns. The methodology is unobtrusive and\ncost-effective in that it does not require holdout groups or sacrifices in ad\nor marketplace performance. We discuss a successful deployment of asymmetric\nbudget split to LinkedIn's Jobs Marketplace, an ad marketplace where it is used\nto measure returns from promotion budgets in terms of incremental job\napplicants. We outline operational considerations for practitioners and discuss\nfurther use cases such as budget-aware performance forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.00206v1"
    },
    {
        "title": "csa2sls: A complete subset approach for many instruments using Stata",
        "authors": [
            "Seojeong Lee",
            "Siha Lee",
            "Julius Owusu",
            "Youngki Shin"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We develop a Stata command $\\texttt{csa2sls}$ that implements the complete\nsubset averaging two-stage least squares (CSA2SLS) estimator in Lee and Shin\n(2021). The CSA2SLS estimator is an alternative to the two-stage least squares\nestimator that remedies the bias issue caused by many correlated instruments.\nWe conduct Monte Carlo simulations and confirm that the CSA2SLS estimator\nreduces both the mean squared error and the estimation bias substantially when\ninstruments are correlated. We illustrate the usage of $\\texttt{csa2sls}$ in\nStata by an empirical application.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.01533v2"
    },
    {
        "title": "Identification and Inference for Welfare Gains without Unconfoundedness",
        "authors": [
            "Undral Byambadalai"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper studies identification and inference of the welfare gain that\nresults from switching from one policy (such as the status quo policy) to\nanother policy. The welfare gain is not point identified in general when data\nare obtained from an observational study or a randomized experiment with\nimperfect compliance. I characterize the sharp identified region of the welfare\ngain and obtain bounds under various assumptions on the unobservables with and\nwithout instrumental variables. Estimation and inference of the lower and upper\nbounds are conducted using orthogonalized moment conditions to deal with the\npresence of infinite-dimensional nuisance parameters. I illustrate the analysis\nby considering hypothetical policies of assigning individuals to job training\nprograms using experimental data from the National Job Training Partnership Act\nStudy. Monte Carlo simulations are conducted to assess the finite sample\nperformance of the estimators.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.04314v1"
    },
    {
        "title": "Two-stage differences in differences",
        "authors": [
            "John Gardner"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  A recent literature has shown that when adoption of a treatment is staggered\nand average treatment effects vary across groups and over time,\ndifference-in-differences regression does not identify an easily interpretable\nmeasure of the typical effect of the treatment. In this paper, I extend this\nliterature in two ways. First, I provide some simple underlying intuition for\nwhy difference-in-differences regression does not identify a\ngroup$\\times$period average treatment effect. Second, I propose an alternative\ntwo-stage estimation framework, motivated by this intuition. In this framework,\ngroup and period effects are identified in a first stage from the sample of\nuntreated observations, and average treatment effects are identified in a\nsecond stage by comparing treated and untreated outcomes, after removing these\ngroup and period effects. The two-stage approach is robust to treatment-effect\nheterogeneity under staggered adoption, and can be used to identify a host of\ndifferent average treatment effect measures. It is also simple, intuitive, and\neasy to implement. I establish the theoretical properties of the two-stage\napproach and demonstrate its effectiveness and applicability using Monte-Carlo\nevidence and an example from the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.05943v1"
    },
    {
        "title": "Parallel Trends and Dynamic Choices",
        "authors": [
            "Philip Marx",
            "Elie Tamer",
            "Xun Tang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Difference-in-differences is a common method for estimating treatment\neffects, and the parallel trends condition is its main identifying assumption:\nthe trend in mean untreated outcomes is independent of the observed treatment\nstatus. In observational settings, treatment is often a dynamic choice made or\ninfluenced by rational actors, such as policy-makers, firms, or individual\nagents. This paper relates parallel trends to economic models of dynamic\nchoice. We clarify the implications of parallel trends on agent behavior and\nstudy when dynamic selection motives lead to violations of parallel trends.\nFinally, we consider identification under alternative assumptions that\naccommodate features of dynamic choice.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.06564v3"
    },
    {
        "title": "Flexible global forecast combinations",
        "authors": [
            "Ryan Thompson",
            "Yilin Qian",
            "Andrey L. Vasnev"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Forecast combination -- the aggregation of individual forecasts from multiple\nexperts or models -- is a proven approach to economic forecasting. To date,\nresearch on economic forecasting has concentrated on local combination methods,\nwhich handle separate but related forecasting tasks in isolation. Yet, it has\nbeen known for over two decades in the machine learning community that global\nmethods, which exploit task-relatedness, can improve on local methods that\nignore it. Motivated by the possibility for improvement, this paper introduces\na framework for globally combining forecasts while being flexible to the level\nof task-relatedness. Through our framework, we develop global versions of\nseveral existing forecast combinations. To evaluate the efficacy of these new\nglobal forecast combinations, we conduct extensive comparisons using synthetic\nand real data. Our real data comparisons, which involve forecasts of core\neconomic indicators in the Eurozone, provide empirical evidence that the\naccuracy of global combinations of economic forecasts can surpass local\ncombinations.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.07318v3"
    },
    {
        "title": "Simultaneity in Binary Outcome Models with an Application to Employment\n  for Couples",
        "authors": [
            "Bo E. Honoré",
            "Luojia Hu",
            "Ekaterini Kyriazidou",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Two of Peter Schmidt's many contributions to econometrics have been to\nintroduce a simultaneous logit model for bivariate binary outcomes and to study\nestimation of dynamic linear fixed effects panel data models using short\npanels. In this paper, we study a dynamic panel data version of the bivariate\nmodel introduced in Schmidt and Strauss (1975) that allows for lagged dependent\nvariables and fixed effects as in Ahn and Schmidt (1995). We combine a\nconditional likelihood approach with a method of moments approach to obtain an\nestimation strategy for the resulting model. We apply this estimation strategy\nto a simple model for the intra-household relationship in employment. Our main\nconclusion is that the within-household dependence in employment differs\nsignificantly by the ethnicity composition of the couple even after one allows\nfor unobserved household specific heterogeneity.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.07343v2"
    },
    {
        "title": "Bias correction and uniform inference for the quantile density function",
        "authors": [
            "Grigory Franguridi"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  For the kernel estimator of the quantile density function (the derivative of\nthe quantile function), I show how to perform the boundary bias correction,\nestablish the rate of strong uniform consistency of the bias-corrected\nestimator, and construct the confidence bands that are asymptotically exact\nuniformly over the entire domain $[0,1]$. The proposed procedures rely on the\npivotality of the studentized bias-corrected estimator and known\nanti-concentration properties of the Gaussian approximation for its supremum.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.09004v1"
    },
    {
        "title": "Asymptotic Properties of Endogeneity Corrections Using Nonlinear\n  Transformations",
        "authors": [
            "Jörg Breitung",
            "Alexander Mayer",
            "Dominik Wied"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper considers a linear regression model with an endogenous regressor\nwhich arises from a nonlinear transformation of a latent variable. It is shown\nthat the corresponding coefficient can be consistently estimated without\nexternal instruments by adding a rank-based transformation of the regressor to\nthe model and performing standard OLS estimation. In contrast to other\napproaches, our nonparametric control function approach does not rely on a\nconformably specified copula. Furthermore, the approach allows for the presence\nof additional exogenous regressors which may be (linearly) correlated with the\nendogenous regressor(s). Consistency and asymptotic normality of the estimator\nare proved and the estimator is compared with copula based approaches by means\nof Monte Carlo simulations. An empirical application on wage data of the US\ncurrent population survey demonstrates the usefulness of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.09246v3"
    },
    {
        "title": "Testing for a Threshold in Models with Endogenous Regressors",
        "authors": [
            "Mario P. Rothfelder",
            "Otilia Boldea"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We show by simulation that the test for an unknown threshold in models with\nendogenous regressors - proposed in Caner and Hansen (2004) - can exhibit\nsevere size distortions both in small and in moderately large samples,\npertinent to empirical applications. We propose three new tests that rectify\nthese size distortions. The first test is based on GMM estimators. The other\ntwo are based on unconventional 2SLS estimators, that use additional\ninformation about the linearity (or lack of linearity) of the first stage. Just\nlike the test in Caner and Hansen (2004), our tests are non-pivotal, and we\nprove their bootstrap validity. The empirical application revisits the question\nin Ramey and Zubairy (2018) whether government spending multipliers are larger\nin recessions, but using tests for an unknown threshold. Consistent with Ramey\nand Zubairy (2018), we do not find strong evidence that these multipliers are\nlarger in recessions.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.10076v1"
    },
    {
        "title": "A Conditional Linear Combination Test with Many Weak Instruments",
        "authors": [
            "Dennis Lim",
            "Wenjie Wang",
            "Yichong Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We consider a linear combination of jackknife Anderson-Rubin (AR), jackknife\nLagrangian multiplier (LM), and orthogonalized jackknife LM tests for inference\nin IV regressions with many weak instruments and heteroskedasticity. Following\nI.Andrews (2016), we choose the weights in the linear combination based on a\ndecision-theoretic rule that is adaptive to the identification strength. Under\nboth weak and strong identifications, the proposed test controls asymptotic\nsize and is admissible among certain class of tests. Under strong\nidentification, our linear combination test has optimal power against local\nalternatives among the class of invariant or unbiased tests which are\nconstructed based on jackknife AR and LM tests. Simulations and an empirical\napplication to Angrist and Krueger's (1991) dataset confirm the good power\nproperties of our test.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.11137v3"
    },
    {
        "title": "Detecting common bubbles in multivariate mixed causal-noncausal models",
        "authors": [
            "Gianluca Cubadda",
            "Alain Hecq",
            "Elisa Voisin"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper proposes methods to investigate whether the bubble patterns\nobserved in individual series are common to various series. We detect the\nnon-linear dynamics using the recent mixed causal and noncausal models. Both a\nlikelihood ratio test and information criteria are investigated, the former\nhaving better performances in our Monte Carlo simulations. Implementing our\napproach on three commodity prices we do not find evidence of commonalities\nalthough some series look very similar.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.11557v1"
    },
    {
        "title": "Difference-in-Differences with a Misclassified Treatment",
        "authors": [
            "Akanksha Negi",
            "Digvijay Singh Negi"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper studies identification and estimation of the average treatment\neffect on the treated (ATT) in difference-in-difference (DID) designs when the\nvariable that classifies individuals into treatment and control groups\n(treatment status, D) is endogenously misclassified. We show that\nmisclassification in D hampers consistent estimation of ATT because 1) it\nrestricts us from identifying the truly treated from those misclassified as\nbeing treated and 2) differential misclassification in counterfactual trends\nmay result in parallel trends being violated with D even when they hold with\nthe true but unobserved D*. We propose a solution to correct for endogenous\none-sided misclassification in the context of a parametric DID regression which\nallows for considerable heterogeneity in treatment effects and establish its\nasymptotic properties in panel and repeated cross section settings.\nFurthermore, we illustrate the method by using it to estimate the insurance\nimpact of a large-scale in-kind food transfer program in India which is known\nto suffer from large targeting errors.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.02412v1"
    },
    {
        "title": "Factor Network Autoregressions",
        "authors": [
            "Matteo Barigozzi",
            "Giuseppe Cavaliere",
            "Graziano Moramarco"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose a factor network autoregressive (FNAR) model for time series with\ncomplex network structures. The coefficients of the model reflect many\ndifferent types of connections between economic agents (``multilayer network\"),\nwhich are summarized into a smaller number of network matrices (``network\nfactors\") through a novel tensor-based principal component approach. We provide\nconsistency and asymptotic normality results for the estimation of the factors\nand the coefficients of the FNAR. Our approach combines two different\ndimension-reduction techniques and can be applied to ultra-high-dimensional\ndatasets. Simulation results show the goodness of our approach in finite\nsamples. In an empirical application, we use the FNAR to investigate the\ncross-country interdependence of GDP growth rates based on a variety of\ninternational trade and financial linkages. The model provides a rich\ncharacterization of macroeconomic network effects.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.02925v4"
    },
    {
        "title": "Partial Identification of Personalized Treatment Response with\n  Trial-reported Analyses of Binary Subgroups",
        "authors": [
            "Sheyu Li",
            "Valentyn Litvin",
            "Charles F. Manski"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Medical journals have adhered to a reporting practice that seriously limits\nthe usefulness of published trial findings. Medical decision makers commonly\nobserve many patient covariates and seek to use this information to personalize\ntreatment choices. Yet standard summaries of trial findings only partition\nsubjects into broad subgroups, typically into binary categories. Given this\nreporting practice, we study the problem of inference on long mean treatment\noutcomes E[y(t)|x], where t is a treatment, y(t) is a treatment outcome, and\nthe covariate vector x has length K, each component being a binary variable.\nThe available data are estimates of {E[y(t)|xk = 0], E[y(t)|xk = 1], P(xk)}, k\n= 1, . . . , K reported in journal articles. We show that reported trial\nfindings partially identify {E[y(t)|x], P(x)}. Illustrative computations\ndemonstrate that the summaries of trial findings in journal articles may imply\nonly wide bounds on long mean outcomes. One can realistically tighten\ninferences if one can combine reported trial findings with credible assumptions\nhaving identifying power, such as bounded-variation assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03381v2"
    },
    {
        "title": "Quantile Random-Coefficient Regression with Interactive Fixed Effects:\n  Heterogeneous Group-Level Policy Evaluation",
        "authors": [
            "Ruofan Xu",
            "Jiti Gao",
            "Tatsushi Oka",
            "Yoon-Jae Whang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose a quantile random-coefficient regression with interactive fixed\neffects to study the effects of group-level policies that are heterogeneous\nacross individuals. Our approach is the first to use a latent factor structure\nto handle the unobservable heterogeneities in the random coefficient. The\nasymptotic properties and an inferential method for the policy estimators are\nestablished. The model is applied to evaluate the effect of the minimum wage\npolicy on earnings between 1967 and 1980 in the United States. Our results\nsuggest that the minimum wage policy has significant and persistent positive\neffects on black workers and female workers up to the median. Our results also\nindicate that the policy helps reduce income disparity up to the median between\ntwo groups: black, female workers versus white, male workers. However, the\npolicy is shown to have little effect on narrowing the income gap between low-\nand high-income workers within the subpopulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03632v3"
    },
    {
        "title": "Endogeneity in Weakly Separable Models without Monotonicity",
        "authors": [
            "Songnian Chen",
            "Shakeeb Khan",
            "Xun Tang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We identify and estimate treatment effects when potential outcomes are weakly\nseparable with a binary endogenous treatment. Vytlacil and Yildiz (2007)\nproposed an identification strategy that exploits the mean of observed\noutcomes, but their approach requires a monotonicity condition. In comparison,\nwe exploit full information in the entire outcome distribution, instead of just\nits mean. As a result, our method does not require monotonicity and is also\napplicable to general settings with multiple indices. We provide examples where\nour approach can identify treatment effect parameters of interest whereas\nexisting methods would fail. These include models where potential outcomes\ndepend on multiple unobserved disturbance terms, such as a Roy model, a\nmultinomial choice model, as well as a model with endogenous random\ncoefficients. We establish consistency and asymptotic normality of our\nestimators.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.05047v1"
    },
    {
        "title": "Understanding Volatility Spillover Relationship Among G7 Nations And\n  India During Covid-19",
        "authors": [
            "Avik Das",
            "Devanjali Nandi Das"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Purpose: In the context of a COVID pandemic in 2020-21, this paper attempts\nto capture the interconnectedness and volatility transmission dynamics. The\nnature of change in volatility spillover effects and time-varying conditional\ncorrelation among the G7 countries and India is investigated. Methodology: To\nassess the volatility spillover effects, the bivariate BEKK and t- DCC (1,1)\nGARCH (1,1) models have been used. Our research shows how the dynamics of\nvolatility spillover between India and the G7 countries shift before and during\nCOVID-19. Findings: The findings reveal that the extent of volatility spillover\nhas altered during COVID compared to the pre-COVID environment. During this\npandemic, a sharp increase in conditional correlation indicates an increase in\nsystematic risk between countries. Originality: The study contributes to a\nbetter understanding of the dynamics of volatility spillover between G7\ncountries and India. Asset managers and foreign corporations can use the\nchanging spillover dynamics to improve investment decisions and implement\neffective hedging measures to protect their interests. Furthermore, this\nresearch will assist financial regulators in assessing market risk in the\nfuture owing to crises like as COVID-19.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.09148v1"
    },
    {
        "title": "Beta-Sorted Portfolios",
        "authors": [
            "Matias D. Cattaneo",
            "Richard K. Crump",
            "Weining Wang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Beta-sorted portfolios -- portfolios comprised of assets with similar\ncovariation to selected risk factors -- are a popular tool in empirical finance\nto analyze models of (conditional) expected returns. Despite their widespread\nuse, little is known of their statistical properties in contrast to comparable\nprocedures such as two-pass regressions. We formally investigate the properties\nof beta-sorted portfolio returns by casting the procedure as a two-step\nnonparametric estimator with a nonparametric first step and a beta-adaptive\nportfolios construction. Our framework rationalize the well-known estimation\nalgorithm with precise economic and statistical assumptions on the general data\ngenerating process and characterize its key features. We study beta-sorted\nportfolios for both a single cross-section as well as for aggregation over time\n(e.g., the grand mean), offering conditions that ensure consistency and\nasymptotic normality along with new uniform inference procedures allowing for\nuncertainty quantification and testing of various relevant hypotheses in\nfinancial applications. We also highlight some limitations of current empirical\npractices and discuss what inferences can and cannot be drawn from returns to\nbeta-sorted portfolios for either a single cross-section or across the whole\nsample. Finally, we illustrate the functionality of our new procedures in an\nempirical application.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.10974v3"
    },
    {
        "title": "What Impulse Response Do Instrumental Variables Identify?",
        "authors": [
            "Bonsoo Koo",
            "Seojeong Lee",
            "Myung Hwan Seo"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Macro shocks are often composites, yet overlooked in the impulse response\nanalysis. When an instrumental variable (IV) is used to identify a composite\nshock, it violates the common IV exclusion restriction. We show that the Local\nProjection-IV estimand is represented as a weighted average of component-wise\nimpulse responses but with possibly negative weights, which occur when the IV\nand shock components have opposite correlations. We further develop alternative\n(set-) identification strategies for the LP-IV based on sign restrictions or\nadditional granular information. Our applications confirm the composite nature\nof monetary policy shocks and reveal a non-defense spending multiplier\nexceeding one.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.11828v2"
    },
    {
        "title": "Large Volatility Matrix Analysis Using Global and National Factor Models",
        "authors": [
            "Sung Hoon Choi",
            "Donggyu Kim"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Several large volatility matrix inference procedures have been developed,\nbased on the latent factor model. They often assumed that there are a few of\ncommon factors, which can account for volatility dynamics. However, several\nstudies have demonstrated the presence of local factors. In particular, when\nanalyzing the global stock market, we often observe that nation-specific\nfactors explain their own country's volatility dynamics. To account for this,\nwe propose the Double Principal Orthogonal complEment Thresholding\n(Double-POET) method, based on multi-level factor models, and also establish\nits asymptotic properties. Furthermore, we demonstrate the drawback of using\nthe regular principal orthogonal component thresholding (POET) when the local\nfactor structure exists. We also describe the blessing of dimensionality using\nDouble-POET for local covariance matrix estimation. Finally, we investigate the\nperformance of the Double-POET estimator in an out-of-sample portfolio\nallocation study using international stocks from 20 financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.12323v2"
    },
    {
        "title": "A restricted eigenvalue condition for unit-root non-stationary data",
        "authors": [
            "Etienne Wijler"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this paper, we develop a restricted eigenvalue condition for unit-root\nnon-stationary data and derive its validity under the assumption of independent\nGaussian innovations that may be contemporaneously correlated. The method of\nproof relies on matrix concentration inequalities and offers sufficient\nflexibility to enable extensions of our results to alternative time series\nsettings. As an application of this result, we show the consistency of the\nlasso estimator on ultra high-dimensional cointegrated data in which the number\nof integrated regressors may grow exponentially in relation to the sample size.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.12990v1"
    },
    {
        "title": "A Descriptive Method of Firm Size Transition Dynamics Using Markov Chain",
        "authors": [
            "Boyang You",
            "Kerry Papps"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Social employment, which is mostly carried by firms of different types,\ndetermines the prosperity and stability of a country. As time passing, the\nfluctuations of firm employment can reflect the process of creating or\ndestroying jobs. Therefore, it is instructive to investigate the firm\nemployment (size) dynamics. Drawing on the firm-level panel data extracted from\nthe Chinese Industrial Enterprises Database 1998-2013, this paper proposes a\nMarkov-chain-based descriptive approach to clearly demonstrate the firm size\ntransfer dynamics between different size categories. With this method, any firm\nsize transition path in a short time period can be intuitively demonstrated.\nFurthermore, by utilizing the properties of Markov transfer matrices, the\ndefinition of transition trend and the transition entropy are introduced and\nestimated. As a result, the tendency of firm size transfer between small,\nmedium and large can be exactly revealed, and the uncertainty of size change\ncan be quantified. Generally from the evidence of this paper, it can be\ninferred that small and medium manufacturing firms in China have greater job\ncreation potentials compared to large firms over this time period.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.13012v1"
    },
    {
        "title": "Instrumental variables with unordered treatments: Theory and evidence\n  from returns to fields of study",
        "authors": [
            "Eskil Heinesen",
            "Christian Hvid",
            "Lars Kirkebøen",
            "Edwin Leuven",
            "Magne Mogstad"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We revisit the identification argument of Kirkeboen et al. (2016) who showed\nhow one may combine instruments for multiple unordered treatments with\ninformation about individuals' ranking of these treatments to achieve\nidentification while allowing for both observed and unobserved heterogeneity in\ntreatment effects. We show that the key assumptions underlying their\nidentification argument have testable implications. We also provide a new\ncharacterization of the bias that may arise if these assumptions are violated.\nTaken together, these results allow researchers not only to test the underlying\nassumptions, but also to argue whether the bias from violation of these\nassumptions are likely to be economically meaningful. Guided and motivated by\nthese results, we estimate and compare the earnings payoffs to post-secondary\nfields of study in Norway and Denmark. In each country, we apply the\nidentification argument of Kirkeboen et al. (2016) to data on individuals'\nranking of fields of study and field-specific instruments from discontinuities\nin the admission systems. We empirically examine whether and why the payoffs to\nfields of study differ across the two countries. We find strong cross-country\ncorrelation in the payoffs to fields of study, especially after removing fields\nwith violations of the assumptions underlying the identification argument.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.00417v3"
    },
    {
        "title": "Combining Forecasts under Structural Breaks Using Graphical LASSO",
        "authors": [
            "Tae-Hwy Lee",
            "Ekaterina Seregina"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this paper we develop a novel method of combining many forecasts based on\na machine learning algorithm called Graphical LASSO (GL). We visualize forecast\nerrors from different forecasters as a network of interacting entities and\ngeneralize network inference in the presence of common factor structure and\nstructural breaks. First, we note that forecasters often use common information\nand hence make common mistakes, which makes the forecast errors exhibit common\nfactor structures. We use the Factor Graphical LASSO (FGL, Lee and Seregina\n(2023)) to separate common forecast errors from the idiosyncratic errors and\nexploit sparsity of the precision matrix of the latter. Second, since the\nnetwork of experts changes over time as a response to unstable environments\nsuch as recessions, it is unreasonable to assume constant forecast combination\nweights. Hence, we propose Regime-Dependent Factor Graphical LASSO (RD-FGL)\nthat allows factor loadings and idiosyncratic precision matrix to be\nregime-dependent. We develop its scalable implementation using the Alternating\nDirection Method of Multipliers (ADMM) to estimate regime-dependent forecast\ncombination weights. The empirical application to forecasting macroeconomic\nseries using the data of the European Central Bank's Survey of Professional\nForecasters (ECB SPF) demonstrates superior performance of a combined forecast\nusing FGL and RD-FGL.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.01697v2"
    },
    {
        "title": "A Ridge-Regularised Jackknifed Anderson-Rubin Test",
        "authors": [
            "Max-Sebastian Dovì",
            "Anders Bredahl Kock",
            "Sophocles Mavroeidis"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We consider hypothesis testing in instrumental variable regression models\nwith few included exogenous covariates but many instruments -- possibly more\nthan the number of observations. We show that a ridge-regularised version of\nthe jackknifed Anderson Rubin (1949, henceforth AR) test controls asymptotic\nsize in the presence of heteroskedasticity, and when the instruments may be\narbitrarily weak. Asymptotic size control is established under weaker\nassumptions than those imposed for recently proposed jackknifed AR tests in the\nliterature. Furthermore, ridge-regularisation extends the scope of jackknifed\nAR tests to situations in which there are more instruments than observations.\nMonte-Carlo simulations indicate that our method has favourable finite-sample\nsize and power properties compared to recently proposed alternative approaches\nin the literature. An empirical application on the elasticity of substitution\nbetween immigrants and natives in the US illustrates the usefulness of the\nproposed method for practitioners.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.03259v2"
    },
    {
        "title": "Modified Causal Forest",
        "authors": [
            "Michael Lechner",
            "Jana Mareckova"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Uncovering the heterogeneity of causal effects of policies and business\ndecisions at various levels of granularity provides substantial value to\ndecision makers. This paper develops estimation and inference procedures for\nmultiple treatment models in a selection-on-observed-variables framework by\nmodifying the Causal Forest approach (Wager and Athey, 2018) in several\ndimensions. The new estimators have desirable theoretical, computational, and\npractical properties for various aggregation levels of the causal effects.\nWhile an Empirical Monte Carlo study suggests that they outperform previously\nsuggested estimators, an application to the evaluation of an active labour\nmarket pro-gramme shows their value for applied research.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.03744v1"
    },
    {
        "title": "Evidence and Strategy on Economic Distance in Spatially Augmented\n  Solow-Swan Growth Model",
        "authors": [
            "Jieun Lee"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Economists' interests in growth theory have a very long history (Harrod,\n1939; Domar, 1946; Solow, 1956; Swan 1956; Mankiw, Romer, and Weil, 1992).\nRecently, starting from the neoclassical growth model, Ertur and Koch (2007)\ndeveloped the spatially augmented Solow-Swan growth model with the exogenous\nspatial weights matrices ($W$). While the exogenous $W$ assumption could be\ntrue only with the geographical/physical distance, it may not be true when\neconomic/social distances play a role. Using Penn World Table version 7.1,\nwhich covers year 1960-2010, I conducted the robust Rao's score test (Bera,\nDogan, and Taspinar, 2018) to determine if $W$ is endogeonus and used the\nmaximum likelihood estimation (Qu and Lee, 2015). The key finding is that the\nsignificance and positive effects of physical capital externalities and spatial\nexternalities (technological interdependence) in Ertur and Koch (2007) were no\nlonger found with the exogenous $W$, but still they were with the endogenous\n$W$ models. I also found an empirical strategy on which economic distance to\nuse when the data recently has been under heavy shocks of the worldwide\nfinancial crises during year 1996-2010.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.05562v1"
    },
    {
        "title": "Testing Endogeneity of Spatial Weights Matrices in Spatial Dynamic Panel\n  Data Models",
        "authors": [
            "Jieun Lee"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  I propose Robust Rao's Score (RS) test statistic to determine endogeneity of\nspatial weights matrices in a spatial dynamic panel data (SDPD) model (Qu, Lee,\nand Yu, 2017). I firstly introduce the bias-corrected score function since the\nscore function is not centered around zero due to the two-way fixed effects. I\nfurther adjust score functions to rectify the over-rejection of the null\nhypothesis under a presence of local misspecification in contemporaneous\ndependence over space, dependence over time, or spatial time dependence. I then\nderive the explicit forms of our test statistic. A Monte Carlo simulation\nsupports the analytics and shows nice finite sample properties. Finally, an\nempirical illustration is provided using data from Penn World Table version\n6.1.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.05563v1"
    },
    {
        "title": "Estimation of Average Derivatives of Latent Regressors: With an\n  Application to Inference on Buffer-Stock Saving",
        "authors": [
            "Hao Dong",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper proposes a density-weighted average derivative estimator based on\ntwo noisy measures of a latent regressor. Both measures have classical errors\nwith possibly asymmetric distributions. We show that the proposed estimator\nachieves the root-n rate of convergence, and derive its asymptotic normal\ndistribution for statistical inference. Simulation studies demonstrate\nexcellent small-sample performance supporting the root-n asymptotic normality.\nBased on the proposed estimator, we construct a formal test on the sub-unity of\nthe marginal propensity to consume out of permanent income (MPCP) under a\nnonparametric consumption model and a permanent-transitory model of income\ndynamics with nonparametric distribution. Applying the test to four recent\nwaves of U.S. Panel Study of Income Dynamics (PSID), we reject the null\nhypothesis of the unit MPCP in favor of a sub-unit MPCP, supporting the\nbuffer-stock model of saving.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.05914v1"
    },
    {
        "title": "Sample Fit Reliability",
        "authors": [
            "Gabriel Okasa",
            "Kenneth A. Younge"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Researchers frequently test and improve model fit by holding a sample\nconstant and varying the model. We propose methods to test and improve sample\nfit by holding a model constant and varying the sample. Much as the bootstrap\nis a well-known method to re-sample data and estimate the uncertainty of the\nfit of parameters in a model, we develop Sample Fit Reliability (SFR) as a set\nof computational methods to re-sample data and estimate the reliability of the\nfit of observations in a sample. SFR uses Scoring to assess the reliability of\neach observation in a sample, Annealing to check the sensitivity of results to\nremoving unreliable data, and Fitting to re-weight observations for more robust\nanalysis. We provide simulation evidence to demonstrate the advantages of using\nSFR, and we replicate three empirical studies with treatment effects to\nillustrate how SFR reveals new insights about each study.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.06631v1"
    },
    {
        "title": "Statistical Treatment Rules under Social Interaction",
        "authors": [
            "Seungjin Han",
            "Julius Owusu",
            "Youngki Shin"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this paper we study treatment assignment rules in the presence of social\ninteraction. We construct an analytical framework under the anonymous\ninteraction assumption, where the decision problem becomes choosing a treatment\nfraction. We propose a multinomial empirical success (MES) rule that includes\nthe empirical success rule of Manski (2004) as a special case. We investigate\nthe non-asymptotic bounds of the expected utility based on the MES rule.\nFinally, we prove that the MES rule achieves the asymptotic optimality with the\nminimax regret criterion.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.09077v2"
    },
    {
        "title": "Multiscale Comparison of Nonparametric Trend Curves",
        "authors": [
            "Marina Khismatullina",
            "Michael Vogt"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We develop new econometric methods for the comparison of nonparametric time\ntrends. In many applications, practitioners are interested in whether the\nobserved time series all have the same time trend. Moreover, they would often\nlike to know which trends are different and in which time intervals they\ndiffer. We design a multiscale test to formally approach these questions.\nSpecifically, we develop a test which allows to make rigorous confidence\nstatements about which time trends are different and where (that is, in which\ntime intervals) they differ. Based on our multiscale test, we further develop a\nclustering algorithm which allows to cluster the observed time series into\ngroups with the same trend. We derive asymptotic theory for our test and\nclustering methods. The theory is complemented by a simulation study and two\napplications to GDP growth data and house pricing data.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.10841v1"
    },
    {
        "title": "Treatment Effects with Multidimensional Unobserved Heterogeneity:\n  Identification of the Marginal Treatment Effect",
        "authors": [
            "Toshiki Tsuda"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper establishes sufficient conditions for the identification of the\nmarginal treatment effects with multivalued treatments. Our model is based on a\nmultinomial choice model with utility maximization. Our MTE generalizes the MTE\ndefined in Heckman and Vytlacil (2005) in binary treatment models. As in the\nbinary case, we can interpret the MTE as the treatment effect for persons who\nare indifferent between two treatments at a particular level. Our MTE enables\none to obtain the treatment effects of those with specific preference orders\nover the choice set. Further, our results can identify other parameters such as\nthe marginal distribution of potential outcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.11444v5"
    },
    {
        "title": "Bayesian Modeling of TVP-VARs Using Regression Trees",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber",
            "Gary Koop",
            "James Mitchell"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In light of widespread evidence of parameter instability in macroeconomic\nmodels, many time-varying parameter (TVP) models have been proposed. This paper\nproposes a nonparametric TVP-VAR model using Bayesian additive regression trees\n(BART) that models the TVPs as an unknown function of effect modifiers. The\nnovelty of this model arises from the fact that the law of motion driving the\nparameters is treated nonparametrically. This leads to great flexibility in the\nnature and extent of parameter change, both in the conditional mean and in the\nconditional variance. Parsimony is achieved through adopting nonparametric\nfactor structures and use of shrinkage priors. In an application to US\nmacroeconomic data, we illustrate the use of our model in tracking both the\nevolving nature of the Phillips curve and how the effects of business cycle\nshocks on inflation measures vary nonlinearly with changes in the effect\nmodifiers.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.11970v3"
    },
    {
        "title": "Linear estimation of global average treatment effects",
        "authors": [
            "Stefan Faridani",
            "Paul Niehaus"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We study the problem of estimating the average causal effect of treating\nevery member of a population, as opposed to none, using an experiment that\ntreats only some. We consider settings where spillovers have global support and\ndecay slowly with (a generalized notion of) distance. We derive the minimax\nrate over both estimators and designs, and show that it increases with the\nspatial rate of spillover decay. Estimators based on OLS regressions like those\nused to analyze recent large-scale experiments are consistent (though only\nafter de-weighting), achieve the minimax rate when the DGP is linear, and\nconverge faster than IPW-based alternatives when treatment clusters are small,\nproviding one justification for OLS's ubiquity. When the DGP is nonlinear they\nremain consistent but converge slowly. We further address inference and\nbandwidth selection. Applied to the cash transfer experiment studied by Egger\net al. (2022) these methods yield a 20% larger estimated effect on consumption.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.14181v6"
    },
    {
        "title": "The Network Propensity Score: Spillovers, Homophily, and Selection into\n  Treatment",
        "authors": [
            "Alejandro Sanchez-Becerra"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  I establish primitive conditions for unconfoundedness in a coherent model\nthat features heterogeneous treatment effects, spillovers,\nselection-on-observables, and network formation. I identify average partial\neffects under minimal exchangeability conditions. If social interactions are\nalso anonymous, I derive a three-dimensional network propensity score,\ncharacterize its support conditions, relate it to recent work on network\npseudo-metrics, and study extensions. I propose a two-step semiparametric\nestimator for a random coefficients model which is consistent and\nasymptotically normal as the number and size of the networks grows. I apply my\nestimator to a political participation intervention Uganda and a microfinance\napplication in India.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.14391v1"
    },
    {
        "title": "With big data come big problems: pitfalls in measuring basis risk for\n  crop index insurance",
        "authors": [
            "Matthieu Stigler",
            "Apratim Dey",
            "Andrew Hobbs",
            "David Lobell"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  New satellite sensors will soon make it possible to estimate field-level crop\nyields, showing a great potential for agricultural index insurance. This paper\nidentifies an important threat to better insurance from these new technologies:\ndata with many fields and few years can yield downward biased estimates of\nbasis risk, a fundamental metric in index insurance. To demonstrate this bias,\nwe use state-of-the-art satellite-based data on agricultural yields in the US\nand in Kenya to estimate and simulate basis risk. We find a substantive\ndownward bias leading to a systematic overestimation of insurance quality.\n  In this paper, we argue that big data in crop insurance can lead to a new\nsituation where the number of variables $N$ largely exceeds the number of\nobservations $T$. In such a situation where $T\\ll N$, conventional asymptotics\nbreak, as evidenced by the large bias we find in simulations. We show how the\nhigh-dimension, low-sample-size (HDLSS) asymptotics, together with the spiked\ncovariance model, provide a more relevant framework for the $T\\ll N$ case\nencountered in index insurance. More precisely, we derive the asymptotic\ndistribution of the relative share of the first eigenvalue of the covariance\nmatrix, a measure of systematic risk in index insurance. Our formula accurately\napproximates the empirical bias simulated from the satellite data, and provides\na useful tool for practitioners to quantify bias in insurance quality.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.14611v1"
    },
    {
        "title": "Sentiment Analysis on Inflation after Covid-19",
        "authors": [
            "Xinyu Li",
            "Zihan Tang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We implement traditional machine learning and deep learning methods for\nglobal tweets from 2017-2022 to build a high-frequency measure of the public's\nsentiment index on inflation and analyze its correlation with other online data\nsources such as google trend and market-oriented inflation index. We use\nmanually labeled trigrams to test the prediction performance of several machine\nlearning models(logistic regression,random forest etc.) and choose Bert model\nfor final demonstration. Later, we sum daily tweets' sentiment scores gained\nfrom Bert model to obtain the predicted inflation sentiment index, and we\nfurther analyze the regional and pre/post covid patterns of these inflation\nindexes. Lastly, we take other empirical inflation-related data as references\nand prove that twitter-based inflation sentiment analysis method has an\noutstanding capability to predict inflation. The results suggest that Twitter\ncombined with deep learning methods can be a novel and timely method to utilize\nexisting abundant data sources on inflation expectations and provide daily\nindicators of consumers' perception on inflation.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.14737v2"
    },
    {
        "title": "Economic effects of Chile FTAs and an eventual CTPP accession",
        "authors": [
            "Vargas Sepulveda",
            "Mauricio \"Pacha\""
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this article, we show the benefits derived from the Chile-USA (in-force\nJan, 2004) and Chile-China (in-force Oct, 2006) FTA on GDP consumer and\nproducers to conclude that Chile improved its welfare improved after its\nsubscription. From that point, we extrapolate to show the direct and indirect\nbenefits of CTPP accession.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.14748v1"
    },
    {
        "title": "Large-Scale Allocation of Personalized Incentives",
        "authors": [
            "Lucas Javaudin",
            "Andrea Araldo",
            "André de Palma"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We consider a regulator willing to drive individual choices towards\nincreasing social welfare by providing incentives to a large population of\nindividuals.\n  For that purpose, we formalize and solve the problem of finding an optimal\npersonalized-incentive policy: optimal in the sense that it maximizes social\nwelfare under an incentive budget constraint, personalized in the sense that\nthe incentives proposed depend on the alternatives available to each\nindividual, as well as her preferences.\n  We propose a polynomial time approximation algorithm that computes a policy\nwithin few seconds and we analytically prove that it is boundedly close to the\noptimum.\n  We then extend the problem to efficiently calculate the Maximum Social\nWelfare Curve, which gives the maximum social welfare achievable for a range of\nincentive budgets (not just one value).\n  This curve is a valuable practical tool for the regulator to determine the\nright incentive budget to invest.\n  Finally, we simulate a large-scale application to mode choice in a French\ndepartment (about 200 thousands individuals) and illustrate the effectiveness\nof the proposed personalized-incentive policy in reducing CO2 emissions.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.00463v1"
    },
    {
        "title": "Bikeability and the induced demand for cycling",
        "authors": [
            "Mogens Fosgerau",
            "Miroslawa Lukawska",
            "Mads Paulsen",
            "Thomas Kjær Rasmussen"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  To what extent is the volume of urban bicycle traffic affected by the\nprovision of bicycle infrastructure? In this study, we exploit a large dataset\nof observed bicycle trajectories in combination with a fine-grained\nrepresentation of the Copenhagen bicycle-relevant network. We apply a novel\nmodel for bicyclists' choice of route from origin to destination that takes the\ncomplete network into account. This enables us to determine bicyclists'\npreferences for a range of infrastructure and land-use types. We use the\nestimated preferences to compute a subjective cost of bicycle travel, which we\ncorrelate with the number of bicycle trips across a large number of\norigin-destination pairs. Simulations suggest that the extensive Copenhagen\nbicycle lane network has caused the number of bicycle trips and the bicycle\nkilometers traveled to increase by 60% and 90%, respectively, compared with a\ncounterfactual without the bicycle lane network. This translates into an annual\nbenefit of EUR 0.4M per km of bicycle lane owing to changes in subjective\ntravel cost, health, and accidents. Our results thus strongly support the\nprovision of bicycle infrastructure.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.02504v2"
    },
    {
        "title": "Testing the Number of Components in Finite Mixture Normal Regression\n  Model with Panel Data",
        "authors": [
            "Yu Hao",
            "Hiroyuki Kasahara"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper develops the likelihood ratio-based test of the null hypothesis of\na M0-component model against an alternative of (M0 + 1)-component model in the\nnormal mixture panel regression by extending the Expectation-Maximization (EM)\ntest of Chen and Li (2009a) and Kasahara and Shimotsu (2015) to the case of\npanel data. We show that, unlike the cross-sectional normal mixture, the\nfirst-order derivative of the density function for the variance parameter in\nthe panel normal mixture is linearly independent of its second-order\nderivatives for the mean parameter. On the other hand, like the cross-sectional\nnormal mixture, the likelihood ratio test statistic of the panel normal mixture\nis unbounded. We consider the Penalized Maximum Likelihood Estimator to deal\nwith the unboundedness, where we obtain the data-driven penalty function via\ncomputational experiments. We derive the asymptotic distribution of the\nPenalized Likelihood Ratio Test (PLRT) and EM test statistics by expanding the\nlog-likelihood function up to five times for the reparameterized parameters.\nThe simulation experiment indicates good finite sample performance of the\nproposed EM test. We apply our EM test to estimate the number of production\ntechnology types for the finite mixture Cobb-Douglas production function model\nstudied by Kasahara et al. (2022) used the panel data of the Japanese and\nChilean manufacturing firms. We find the evidence of heterogeneity in\nelasticities of output for intermediate goods, suggesting that production\nfunction is heterogeneous across firms beyond their Hicks-neutral productivity\nterms.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.02824v2"
    },
    {
        "title": "Order Statistics Approaches to Unobserved Heterogeneity in Auctions",
        "authors": [
            "Yao Luo",
            "Peijun Sang",
            "Ruli Xiao"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We establish nonparametric identification of auction models with continuous\nand nonseparable unobserved heterogeneity using three consecutive order\nstatistics of bids. We then propose sieve maximum likelihood estimators for the\njoint distribution of unobserved heterogeneity and the private value, as well\nas their conditional and marginal distributions. Lastly, we apply our\nmethodology to a novel dataset from judicial auctions in China. Our estimates\nsuggest substantial gains from accounting for unobserved heterogeneity when\nsetting reserve prices. We propose a simple scheme that achieves nearly optimal\nrevenue by using the appraisal value as the reserve price.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.03547v1"
    },
    {
        "title": "An identification and testing strategy for proxy-SVARs with weak proxies",
        "authors": [
            "Giovanni Angelini",
            "Giuseppe Cavaliere",
            "Luca Fanelli"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  When proxies (external instruments) used to identify target structural shocks\nare weak, inference in proxy-SVARs (SVAR-IVs) is nonstandard and the\nconstruction of asymptotically valid confidence sets for the impulse responses\nof interest requires weak-instrument robust methods. In the presence of\nmultiple target shocks, test inversion techniques require extra restrictions on\nthe proxy-SVAR parameters other those implied by the proxies that may be\ndifficult to interpret and test. We show that frequentist asymptotic inference\nin these situations can be conducted through Minimum Distance estimation and\nstandard asymptotic methods if the proxy-SVAR can be identified by using\n`strong' instruments for the non-target shocks; i.e. the shocks which are not\nof primary interest in the analysis. The suggested identification strategy\nhinges on a novel pre-test for the null of instrument relevance based on\nbootstrap resampling which is not subject to pre-testing issues, in the sense\nthat the validity of post-test asymptotic inferences is not affected by the\noutcomes of the test. The test is robust to conditionally heteroskedasticity\nand/or zero-censored proxies, is computationally straightforward and applicable\nregardless of the number of shocks being instrumented. Some illustrative\nexamples show the empirical usefulness of the suggested identification and\ntesting strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.04523v4"
    },
    {
        "title": "Policy Learning with New Treatments",
        "authors": [
            "Samuel Higbee"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  I study the problem of a decision maker choosing a policy which allocates\ntreatment to a heterogeneous population on the basis of experimental data that\nincludes only a subset of possible treatment values. The effects of new\ntreatments are partially identified by shape restrictions on treatment\nresponse. Policies are compared according to the minimax regret criterion, and\nI show that the empirical analog of the population decision problem has a\ntractable linear- and integer-programming formulation. I prove the maximum\nregret of the estimated policy converges to the lowest possible maximum regret\nat a rate which is the maximum of N^-1/2 and the rate at which conditional\naverage treatment effects are estimated in the experimental data. I apply my\nresults to design targeted subsidies for electrical grid connections in rural\nKenya, and estimate that 97% of the population should be given a treatment not\nimplemented in the experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.04703v2"
    },
    {
        "title": "Bayesian analysis of mixtures of lognormal distribution with an unknown\n  number of components from grouped data",
        "authors": [
            "Kazuhiko Kakamu"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This study proposes a reversible jump Markov chain Monte Carlo method for\nestimating parameters of lognormal distribution mixtures for income. Using\nsimulated data examples, we examined the proposed algorithm's performance and\nthe accuracy of posterior distributions of the Gini coefficients. Results\nsuggest that the parameters were estimated accurately. Therefore, the posterior\ndistributions are close to the true distributions even when the different data\ngenerating process is accounted for. Moreover, promising results for Gini\ncoefficients encouraged us to apply our method to real data from Japan. The\nempirical examples indicate two subgroups in Japan (2020) and the Gini\ncoefficients' integrity.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.05115v3"
    },
    {
        "title": "On estimating Armington elasticities for Japan's meat imports",
        "authors": [
            "Satoshi Nakano",
            "Kazuhiko Nishimura"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  By fully accounting for the distinct tariff regimes levied on imported meat,\nwe estimate substitution elasticities of Japan's two-stage import aggregation\nfunctions for beef, chicken and pork. While the regression analysis crucially\ndepends on the price that consumers face, the post-tariff price of imported\nmeat depends not only on ad valorem duties but also on tariff rate quotas and\ngate price system regimes. The effective tariff rate is consequently evaluated\nby utilizing monthly transaction data. To address potential endogeneity\nproblems, we apply exchange rates that we believe to be independent of the\ndemand shocks for imported meat. The panel nature of the data allows us to\nretrieve the first-stage aggregates via time dummy variables, free of demand\nshocks, to be used as part of the explanatory variable and as an instrument in\nthe second-stage regression.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.05358v2"
    },
    {
        "title": "Estimating Option Pricing Models Using a Characteristic Function-Based\n  Linear State Space Representation",
        "authors": [
            "H. Peter Boswijk",
            "Roger J. A. Laeven",
            "Evgenii Vladimirov"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We develop a novel filtering and estimation procedure for parametric option\npricing models driven by general affine jump-diffusions. Our procedure is based\non the comparison between an option-implied, model-free representation of the\nconditional log-characteristic function and the model-implied conditional\nlog-characteristic function, which is functionally affine in the model's state\nvector. We formally derive an associated linear state space representation and\nestablish the asymptotic properties of the corresponding measurement errors.\nThe state space representation allows us to use a suitably modified Kalman\nfiltering technique to learn about the latent state vector and a quasi-maximum\nlikelihood estimator of the model parameters, which brings important\ncomputational advantages. We analyze the finite-sample behavior of our\nprocedure in Monte Carlo simulations. The applicability of our procedure is\nillustrated in two case studies that analyze S&P 500 option prices and the\nimpact of exogenous state variables capturing Covid-19 reproduction and\neconomic policy uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.06217v1"
    },
    {
        "title": "Modified Wilcoxon-Mann-Whitney tests of stochastic dominance",
        "authors": [
            "Brendan K. Beare",
            "Jackson D. Clarke"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Given independent samples from two univariate distributions, one-sided\nWilcoxon-Mann-Whitney statistics may be used to conduct rank-based tests of\nstochastic dominance. We broaden the scope of applicability of such tests by\nshowing that the bootstrap may be used to conduct valid inference in a matched\npairs sampling framework permitting dependence between the two samples.\nFurther, we show that a modified bootstrap incorporating an implicit estimate\nof a contact set may be used to improve power. Numerical simulations indicate\nthat our test using the modified bootstrap effectively controls the null\nrejection rates and can deliver more or less power than that of the Donald-Hsu\ntest. In the course of establishing our results we obtain a weak approximation\nto the empirical ordinance dominance curve permitting its population density to\ndiverge to infinity at zero or one at arbitrary rates.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.08892v1"
    },
    {
        "title": "Party On: The Labor Market Returns to Social Networks in Adolescence",
        "authors": [
            "Adriana Lleras-Muney",
            "Matthew Miller",
            "Shuyang Sheng",
            "Veronica Sovero"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We investigate the returns to adolescent friendships on earnings in adulthood\nusing data from the National Longitudinal Study of Adolescent to Adult Health.\nBecause both education and friendships are jointly determined in adolescence,\nOLS estimates of their returns are likely biased. We implement a novel\nprocedure to obtain bounds on the causal returns to friendships: we assume that\nthe returns to schooling range from 5 to 15% (based on prior literature), and\ninstrument for friendships using similarity in age among peers. Having one more\nfriend in adolescence increases earnings between 7 and 14%, substantially more\nthan OLS estimates would suggest.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.09426v5"
    },
    {
        "title": "Modelling Large Dimensional Datasets with Markov Switching Factor Models",
        "authors": [
            "Matteo Barigozzi",
            "Daniele Massacci"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We study a novel large dimensional approximate factor model with regime\nchanges in the loadings driven by a latent first order Markov process. By\nexploiting the equivalent linear representation of the model, we first recover\nthe latent factors by means of Principal Component Analysis. We then cast the\nmodel in state-space form, and we estimate loadings and transition\nprobabilities through an EM algorithm based on a modified version of the\nBaum-Lindgren-Hamilton-Kim filter and smoother that makes use of the factors\npreviously estimated. Our approach is appealing as it provides closed form\nexpressions for all estimators. More importantly, it does not require knowledge\nof the true number of factors. We derive the theoretical properties of the\nproposed estimation procedure, and we show their good finite sample performance\nthrough a comprehensive set of Monte Carlo experiments. The empirical\nusefulness of our approach is illustrated through three applications to large\nU.S. datasets of stock returns, macroeconomic variables, and inflation indexes.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.09828v5"
    },
    {
        "title": "Linear Regression with Centrality Measures",
        "authors": [
            "Yong Cai"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper studies the properties of linear regression on centrality measures\nwhen network data is sparse -- that is, when there are many more agents than\nlinks per agent -- and when they are measured with error. We make three\ncontributions in this setting: (1) We show that OLS estimators can become\ninconsistent under sparsity and characterize the threshold at which this\noccurs, with and without measurement error. This threshold depends on the\ncentrality measure used. Specifically, regression on eigenvector is less robust\nto sparsity than on degree and diffusion. (2) We develop distributional theory\nfor OLS estimators under measurement error and sparsity, finding that OLS\nestimators are subject to asymptotic bias even when they are consistent.\nMoreover, bias can be large relative to their variances, so that bias\ncorrection is necessary for inference. (3) We propose novel bias correction and\ninference methods for OLS with sparse noisy networks. Simulation evidence\nsuggests that our theory and methods perform well, particularly in settings\nwhere the usual OLS estimators and heteroskedasticity-consistent/robust t-tests\nare deficient. Finally, we demonstrate the utility of our results in an\napplication inspired by De Weerdt and Deacon (2006), in which we consider\nconsumption smoothing and social insurance in Nyakatoke, Tanzania.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.10024v1"
    },
    {
        "title": "Allowing for weak identification when testing GARCH-X type models",
        "authors": [
            "Philipp Ketz"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this paper, we use the results in Andrews and Cheng (2012), extended to\nallow for parameters to be near or at the boundary of the parameter space, to\nderive the asymptotic distributions of the two test statistics that are used in\nthe two-step (testing) procedure proposed by Pedersen and Rahbek (2019). The\nlatter aims at testing the null hypothesis that a GARCH-X type model, with\nexogenous covariates (X), reduces to a standard GARCH type model, while\nallowing the \"GARCH parameter\" to be unidentified. We then provide a\ncharacterization result for the asymptotic size of any test for testing this\nnull hypothesis before numerically establishing a lower bound on the asymptotic\nsize of the two-step procedure at the 5% nominal level. This lower bound\nexceeds the nominal level, revealing that the two-step procedure does not\ncontrol asymptotic size. In a simulation study, we show that this finding is\nrelevant for finite samples, in that the two-step procedure can suffer from\noverrejection in finite samples. We also propose a new test that, by\nconstruction, controls asymptotic size and is found to be more powerful than\nthe two-step procedure when the \"ARCH parameter\" is \"very small\" (in which case\nthe two-step procedure underrejects).\n",
        "pdf_link": "http://arxiv.org/pdf/2210.11398v1"
    },
    {
        "title": "Choosing The Best Incentives for Belief Elicitation with an Application\n  to Political Protests",
        "authors": [
            "Nathan Canen",
            "Anujit Chakraborty"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Many experiments elicit subjects' prior and posterior beliefs about a random\nvariable to assess how information affects one's own actions. However, beliefs\nare multi-dimensional objects, and experimenters often only elicit a single\nresponse from subjects. In this paper, we discuss how the incentives offered by\nexperimenters map subjects' true belief distributions to what profit-maximizing\nsubjects respond in the elicitation task. In particular, we show how slightly\ndifferent incentives may induce subjects to report the mean, mode, or median of\ntheir belief distribution. If beliefs are not symmetric and unimodal, then\nusing an elicitation scheme that is mismatched with the research question may\naffect both the magnitude and the sign of identified effects, or may even make\nidentification impossible. As an example, we revisit Cantoni et al.'s (2019)\nstudy of whether political protests are strategic complements or substitutes.\nWe show that they elicit modal beliefs, while modal and mean beliefs may be\nupdated in opposite directions following their experiment. Hence, the sign of\ntheir effects may change, allowing an alternative interpretation of their\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.12549v1"
    },
    {
        "title": "Prediction intervals for economic fixed-event forecasts",
        "authors": [
            "Fabian Krüger",
            "Hendrik Plett"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The fixed-event forecasting setup is common in economic policy. It involves a\nsequence of forecasts of the same (`fixed') predictand, so that the difficulty\nof the forecasting problem decreases over time. Fixed-event point forecasts are\ntypically published without a quantitative measure of uncertainty. To construct\nsuch a measure, we consider forecast postprocessing techniques tailored to the\nfixed-event case. We develop regression methods that impose constraints\nmotivated by the problem at hand, and use these methods to construct prediction\nintervals for gross domestic product (GDP) growth in Germany and the US.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.13562v3"
    },
    {
        "title": "Unit Averaging for Heterogeneous Panels",
        "authors": [
            "Christian Brownlees",
            "Vladislav Morozov"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this work we introduce a unit averaging procedure to efficiently recover\nunit-specific parameters in a heterogeneous panel model. The procedure consists\nin estimating the parameter of a given unit using a weighted average of all the\nunit-specific parameter estimators in the panel. The weights of the average are\ndetermined by minimizing an MSE criterion we derive. We analyze the properties\nof the resulting minimum MSE unit averaging estimator in a local heterogeneity\nframework inspired by the literature on frequentist model averaging, and we\nderive the local asymptotic distribution of the estimator and the corresponding\nweights. The benefits of the procedure are showcased with an application to\nforecasting unemployment rates for a panel of German regions.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.14205v3"
    },
    {
        "title": "Estimation of Heterogeneous Treatment Effects Using a Conditional Moment\n  Based Approach",
        "authors": [
            "Xiaolin Sun"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose a new estimator for heterogeneous treatment effects in a partially\nlinear model (PLM) with multiple exogenous covariates and a potentially\nendogenous treatment variable. Our approach integrates a Robinson\ntransformation to handle the nonparametric component, the Smooth Minimum\nDistance (SMD) method to leverage conditional mean independence restrictions,\nand a Neyman-Orthogonalized first-order condition (FOC). By employing\nregularized model selection techniques like the Lasso method, our estimator\naccommodates numerous covariates while exhibiting reduced bias, consistency,\nand asymptotic normality. Simulations demonstrate its robust performance with\ndiverse instrument sets compared to traditional GMM-type estimators. Applying\nthis method to estimate Medicaid's heterogeneous treatment effects from the\nOregon Health Insurance Experiment reveals more robust and reliable results\nthan conventional GMM approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.15829v4"
    },
    {
        "title": "How to sample and when to stop sampling: The generalized Wald problem\n  and minimax policies",
        "authors": [
            "Karun Adusumilli"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The aim of this paper is to develop techniques for incorporating the cost of\ninformation into experimental design. Specifically, we study sequential\nexperiments where sampling is costly and a decision-maker aims to determine the\nbest treatment for full scale implementation by (1) adaptively allocating units\nto two possible treatments, and (2) stopping the experiment when the expected\nwelfare (inclusive of sampling costs) from implementing the chosen treatment is\nmaximized. Working under the diffusion limit, we describe the optimal policies\nunder the minimax regret criterion. Under small cost asymptotics, the same\npolicies are also optimal under parametric and non-parametric distributions of\noutcomes. The minimax optimal sampling rule is just the Neyman allocation; it\nis independent of sampling costs and does not adapt to previous outcomes. The\ndecision-maker stops sampling when the average difference between the treatment\noutcomes, multiplied by the number of observations collected until that point,\nexceeds a specific threshold. The results derived here also apply to best arm\nidentification with two arms.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.15841v6"
    },
    {
        "title": "Non-Robustness of the Cluster-Robust Inference: with a Proposal of a New\n  Robust Method",
        "authors": [
            "Yuya Sasaki",
            "Yulong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The conventional cluster-robust (CR) standard errors may not be robust. They\nare vulnerable to data that contain a small number of large clusters. When a\nresearcher uses the 51 states in the U.S. as clusters, the largest cluster\n(California) consists of about 10% of the total sample. Such a case in fact\nviolates the assumptions under which the widely used CR methods are guaranteed\nto work. We formally show that the conventional CR methods fail if the\ndistribution of cluster sizes follows a power law with exponent less than two.\nBesides the example of 51 state clusters, some examples are drawn from a list\nof recent original research articles published in a top journal. In light of\nthese negative results about the existing CR methods, we propose a weighted CR\n(WCR) method as a simple fix. Simulation studies support our arguments that the\nWCR method is robust while the conventional CR methods are not.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.16991v2"
    },
    {
        "title": "Weak Identification in Low-Dimensional Factor Models with One or Two\n  Factors",
        "authors": [
            "Gregory Cox"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper describes how to reparameterize low-dimensional factor models with\none or two factors to fit weak identification theory developed for generalized\nmethod of moments models. Some identification-robust tests, here called\n\"plug-in\" tests, require a reparameterization to distinguish weakly identified\nparameters from strongly identified parameters. The reparameterizations in this\npaper make plug-in tests available for subvector hypotheses in low-dimensional\nfactor models with one or two factors. Simulations show that the plug-in tests\nare less conservative than identification-robust tests that use the original\nparameterization. An empirical application to a factor model of parental\ninvestments in children is included.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.00329v2"
    },
    {
        "title": "Reservoir Computing for Macroeconomic Forecasting with Mixed Frequency\n  Data",
        "authors": [
            "Giovanni Ballarin",
            "Petros Dellaportas",
            "Lyudmila Grigoryeva",
            "Marcel Hirt",
            "Sophie van Huellen",
            "Juan-Pablo Ortega"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Macroeconomic forecasting has recently started embracing techniques that can\ndeal with large-scale datasets and series with unequal release periods.\nMIxed-DAta Sampling (MIDAS) and Dynamic Factor Models (DFM) are the two main\nstate-of-the-art approaches that allow modeling series with non-homogeneous\nfrequencies. We introduce a new framework called the Multi-Frequency Echo State\nNetwork (MFESN) based on a relatively novel machine learning paradigm called\nreservoir computing. Echo State Networks (ESN) are recurrent neural networks\nformulated as nonlinear state-space systems with random state coefficients\nwhere only the observation map is subject to estimation. MFESNs are\nconsiderably more efficient than DFMs and allow for incorporating many series,\nas opposed to MIDAS models, which are prone to the curse of dimensionality. All\nmethods are compared in extensive multistep forecasting exercises targeting US\nGDP growth. We find that our MFESN models achieve superior or comparable\nperformance over MIDAS and DFMs at a much lower computational cost.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.00363v3"
    },
    {
        "title": "Population and Technological Growth: Evidence from Roe v. Wade",
        "authors": [
            "John T. H. Wong",
            "Matthias Hei Man",
            "Alex Li Cheuk Hung"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We exploit the heterogeneous impact of the Roe v. Wade ruling by the US\nSupreme Court, which ruled most abortion restrictions unconstitutional. Our\nidentifying assumption is that states which had not liberalized their abortion\nlaws prior to Roe would experience a negative birth shock of greater proportion\nthan states which had undergone pre-Roe reforms. We estimate the\ndifference-in-difference in births and use estimated births as an exogenous\ntreatment variable to predict patents per capita. Our results show that one\nstandard deviation increase in cohort starting population increases per capita\npatents by 0.24 standard deviation. These results suggest that at the margins,\nincreasing fertility can increase patent production. Insofar as patent\nproduction is a sufficient proxy for technological growth, increasing births\nhas a positive impact on technological growth. This paper and its results do\nnot pertain to the issue of abortion itself.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.00410v1"
    },
    {
        "title": "A New Test for Market Efficiency and Uncovered Interest Parity",
        "authors": [
            "Richard T. Baillie",
            "Francis X. Diebold",
            "George Kapetanios",
            "Kun Ho Kim"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We suggest a new single-equation test for Uncovered Interest Parity (UIP)\nbased on a dynamic regression approach. The method provides consistent and\nasymptotically efficient parameter estimates, and is not dependent on\nassumptions of strict exogeneity. This new approach is asymptotically more\nefficient than the common approach of using OLS with HAC robust standard errors\nin the static forward premium regression. The coefficient estimates when spot\nreturn changes are regressed on the forward premium are all positive and\nremarkably stable across currencies. These estimates are considerably larger\nthan those of previous studies, which frequently find negative coefficients.\nThe method also has the advantage of showing dynamic effects of risk premia, or\nother events that may lead to rejection of UIP or the efficient markets\nhypothesis.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.01344v1"
    },
    {
        "title": "Stochastic Treatment Choice with Empirical Welfare Updating",
        "authors": [
            "Toru Kitagawa",
            "Hugo Lopez",
            "Jeff Rowley"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper proposes a novel method to estimate individualised treatment\nassignment rules. The method is designed to find rules that are stochastic,\nreflecting uncertainty in estimation of an assignment rule and about its\nwelfare performance. Our approach is to form a prior distribution over\nassignment rules, not over data generating processes, and to update this prior\nbased upon an empirical welfare criterion, not likelihood. The social planner\nthen assigns treatment by drawing a policy from the resulting posterior. We\nshow analytically a welfare-optimal way of updating the prior using empirical\nwelfare; this posterior is not feasible to compute, so we propose a variational\nBayes approximation for the optimal posterior. We characterise the welfare\nregret convergence of the assignment rule based upon this variational Bayes\napproximation, showing that it converges to zero at a rate of ln(n)/sqrt(n). We\napply our methods to experimental data from the Job Training Partnership Act\nStudy to illustrate the implementation of our methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.01537v3"
    },
    {
        "title": "Estimating interaction effects with panel data",
        "authors": [
            "Chris Muris",
            "Konstantin Wacker"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  A common task in empirical economics is to estimate \\emph{interaction\neffects} that measure how the effect of one variable $X$ on another variable\n$Y$ depends on a third variable $H$. This paper considers the estimation of\ninteraction effects in linear panel models with a fixed number of time periods.\nThere are at least two ways to estimate interaction effects in this setting,\nboth common in applied work. Our theoretical results show that these two\napproaches are distinct, and only coincide under strong conditions on\nunobserved effect heterogeneity. Our empirical results show that the difference\nbetween the two approaches is large, leading to conflicting conclusions about\nthe sign of the interaction effect. Taken together, our findings may guide the\nchoice between the two approaches in empirical work.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.01557v1"
    },
    {
        "title": "On Estimation and Inference of Large Approximate Dynamic Factor Models\n  via the Principal Component Analysis",
        "authors": [
            "Matteo Barigozzi"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We provide an alternative derivation of the asymptotic results for the\nPrincipal Components estimator of a large approximate factor model. Results are\nderived under a minimal set of assumptions and, in particular, we require only\nthe existence of 4th order moments. A special focus is given to the time series\nsetting, a case considered in almost all recent econometric applications of\nfactor models. Hence, estimation is based on the classical $n\\times n$ sample\ncovariance matrix and not on a $T\\times T$ covariance matrix often considered\nin the literature. Indeed, despite the two approaches being asymptotically\nequivalent, the former is more coherent with a time series setting and it\nimmediately allows us to write more intuitive asymptotic expansions for the\nPrincipal Component estimators showing that they are equivalent to OLS as long\nas $\\sqrt n/T\\to 0$ and $\\sqrt T/n\\to 0$, that is the loadings are estimated in\na time series regression as if the factors were known, while the factors are\nestimated in a cross-sectional regression as if the loadings were known.\nFinally, we give some alternative sets of primitive sufficient conditions for\nmean-squared consistency of the sample covariance matrix of the factors, of the\nidiosyncratic components, and of the observed time series, which is the\nstarting point for Principal Component Analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.01921v3"
    },
    {
        "title": "Boosted p-Values for High-Dimensional Vector Autoregression",
        "authors": [
            "Xiao Huang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Assessing the statistical significance of parameter estimates is an important\nstep in high-dimensional vector autoregression modeling. Using the\nleast-squares boosting method, we compute the p-value for each selected\nparameter at every boosting step in a linear model. The p-values are\nasymptotically valid and also adapt to the iterative nature of the boosting\nprocedure. Our simulation experiment shows that the p-values can keep false\npositive rate under control in high-dimensional vector autoregressions. In an\napplication with more than 100 macroeconomic time series, we further show that\nthe p-values can not only select a sparser model with good prediction\nperformance but also help control model stability. A companion R package\nboostvar is developed.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.02215v2"
    },
    {
        "title": "Fast, Robust Inference for Linear Instrumental Variables Models using\n  Self-Normalized Moments",
        "authors": [
            "Eric Gautier",
            "Christiern Rose"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose and implement an approach to inference in linear instrumental\nvariables models which is simultaneously robust and computationally tractable.\nInference is based on self-normalization of sample moment conditions, and\nallows for (but does not require) many (relative to the sample size), weak,\npotentially invalid or potentially endogenous instruments, as well as for many\nregressors and conditional heteroskedasticity. Our coverage results are uniform\nand can deliver a small sample guarantee. We develop a new computational\napproach based on semidefinite programming, which we show can equally be\napplied to rapidly invert existing tests (e.g,. AR, LM, CLR, etc.).\n",
        "pdf_link": "http://arxiv.org/pdf/2211.02249v3"
    },
    {
        "title": "Bootstraps for Dynamic Panel Threshold Models",
        "authors": [
            "Woosik Gong",
            "Myung Hwan Seo"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper develops valid bootstrap inference methods for the dynamic short\npanel threshold regression. We demonstrate that the standard nonparametric\nbootstrap is inconsistent for the first-differenced generalized method of\nmoments (GMM) estimator. The inconsistency is due to an $n^{1/4}$-consistent\nnon-normal asymptotic distribution for the threshold estimate when the\nparameter resides within the continuity region of the parameter space. It stems\nfrom the rank deficiency of the approximate Jacobian of the sample moment\nconditions on the continuity region. To address this, we propose a grid\nbootstrap to construct confidence intervals of the threshold, a residual\nbootstrap to construct confidence intervals of the coefficients, and a\nbootstrap for testing continuity. They are shown to be valid under uncertain\ncontinuity, while the grid bootstrap is additionally shown to be uniformly\nvalid. A set of Monte Carlo experiments demonstrate that the proposed\nbootstraps perform well in the finite samples and improve upon the standard\nnonparametric bootstrap.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.04027v3"
    },
    {
        "title": "Crises Do Not Cause Lower Short-Term Growth",
        "authors": [
            "Kaiwen Hou",
            "David Hou",
            "Yang Ouyang",
            "Lulu Zhang",
            "Aster Liu"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  It is commonly believed that financial crises \"lead to\" lower growth of a\ncountry during the two-year recession period, which can be reflected by their\npost-crisis GDP growth. However, by contrasting a causal model with a standard\nprediction model, this paper argues that such a belief is non-causal. To make\ncausal inferences, we design a two-stage staggered difference-in-differences\nmodel to estimate the average treatment effects. Interpreting the residuals as\nthe contribution of each crisis to the treatment effects, we astonishingly\nconclude that cross-sectional crises are often limited to providing relevant\ncausal information to policymakers.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.04558v3"
    },
    {
        "title": "Multiple Structural Breaks in Interactive Effects Panel Data and the\n  Impact of Quantitative Easing on Bank Lending",
        "authors": [
            "Jan Ditzen",
            "Yiannis Karavias",
            "Joakim Westerlund"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper develops a new toolbox for multiple structural break detection in\npanel data models with interactive effects. The toolbox includes tests for the\npresence of structural breaks, a break date estimator, and a break date\nconfidence interval. The new toolbox is applied to a large panel of US banks\nfor a period characterized by massive quantitative easing programs aimed at\nlessening the impact of the global financial crisis and the COVID--19 pandemic.\nThe question we ask is: Have these programs been successful in spurring bank\nlending in the US economy? The short answer turns out to be: ``No''.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.06707v2"
    },
    {
        "title": "Robust Difference-in-differences Models",
        "authors": [
            "Kyunghoon Ban",
            "Désiré Kédagni"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The difference-in-differences (DID) method identifies the average treatment\neffects on the treated (ATT) under mainly the so-called parallel trends (PT)\nassumption. The most common and widely used approach to justify the PT\nassumption is the pre-treatment period examination. If a null hypothesis of the\nsame trend in the outcome means for both treatment and control groups in the\npre-treatment periods is rejected, researchers believe less in PT and the DID\nresults. This paper develops a robust generalized DID method that utilizes all\nthe information available not only from the pre-treatment periods but also from\nmultiple data sources. Our approach interprets PT in a different way using a\nnotion of selection bias, which enables us to generalize the standard DID\nestimand by defining an information set that may contain multiple pre-treatment\nperiods or other baseline covariates. Our main assumption states that the\nselection bias in the post-treatment period lies within the convex hull of all\nselection biases in the pre-treatment periods. We provide a sufficient\ncondition for this assumption to hold. Based on the baseline information set we\nconstruct, we provide an identified set for the ATT that always contains the\ntrue ATT under our identifying assumption, and also the standard DID estimand.\nWe extend our proposed approach to multiple treatment periods DID settings. We\npropose a flexible and easy way to implement the method. Finally, we illustrate\nour methodology through some numerical and empirical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.06710v5"
    },
    {
        "title": "Type I Tobit Bayesian Additive Regression Trees for Censored Outcome\n  Regression",
        "authors": [
            "Eoghan O'Neill"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Censoring occurs when an outcome is unobserved beyond some threshold value.\nMethods that do not account for censoring produce biased predictions of the\nunobserved outcome. This paper introduces Type I Tobit Bayesian Additive\nRegression Tree (TOBART-1) models for censored outcomes. Simulation results and\nreal data applications demonstrate that TOBART-1 produces accurate predictions\nof censored outcomes. TOBART-1 provides posterior intervals for the conditional\nexpectation and other quantities of interest. The error term distribution can\nhave a large impact on the expectation of the censored outcome. Therefore the\nerror is flexibly modeled as a Dirichlet process mixture of normal\ndistributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.07506v4"
    },
    {
        "title": "Identification and Auto-debiased Machine Learning for Outcome\n  Conditioned Average Structural Derivatives",
        "authors": [
            "Zequn Jin",
            "Lihua Lin",
            "Zhengyu Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper proposes a new class of heterogeneous causal quantities, named\n\\textit{outcome conditioned} average structural derivatives (OASD) in a general\nnonseparable model. OASD is the average partial effect of a marginal change in\na continuous treatment on the individuals located at different parts of the\noutcome distribution, irrespective of individuals' characteristics. OASD\ncombines both features of ATE and QTE: it is interpreted as straightforwardly\nas ATE while at the same time more granular than ATE by breaking the entire\npopulation up according to the rank of the outcome distribution.\n  One contribution of this paper is that we establish some close relationships\nbetween the \\textit{outcome conditioned average partial effects} and a class of\nparameters measuring the effect of counterfactually changing the distribution\nof a single covariate on the unconditional outcome quantiles. By exploiting\nsuch relationship, we can obtain root-$n$ consistent estimator and calculate\nthe semi-parametric efficiency bound for these counterfactual effect\nparameters. We illustrate this point by two examples: equivalence between OASD\nand the unconditional partial quantile effect (Firpo et al. (2009)), and\nequivalence between the marginal partial distribution policy effect (Rothe\n(2012)) and a corresponding outcome conditioned parameter.\n  Because identification of OASD is attained under a conditional exogeneity\nassumption, by controlling for a rich information about covariates, a\nresearcher may ideally use high-dimensional controls in data. We propose for\nOASD a novel automatic debiased machine learning estimator, and present\nasymptotic statistical guarantees for it. We prove our estimator is root-$n$\nconsistent, asymptotically normal, and semiparametrically efficient. We also\nprove the validity of the bootstrap procedure for uniform inference on the OASD\nprocess.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.07903v1"
    },
    {
        "title": "Causal Bandits: Online Decision-Making in Endogenous Settings",
        "authors": [
            "Jingwen Zhang",
            "Yifang Chen",
            "Amandeep Singh"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The deployment of Multi-Armed Bandits (MAB) has become commonplace in many\neconomic applications. However, regret guarantees for even state-of-the-art\nlinear bandit algorithms (such as Optimism in the Face of Uncertainty Linear\nbandit (OFUL)) make strong exogeneity assumptions w.r.t. arm covariates. This\nassumption is very often violated in many economic contexts and using such\nalgorithms can lead to sub-optimal decisions. Further, in social science\nanalysis, it is also important to understand the asymptotic distribution of\nestimated parameters. To this end, in this paper, we consider the problem of\nonline learning in linear stochastic contextual bandit problems with endogenous\ncovariates. We propose an algorithm we term $\\epsilon$-BanditIV, that uses\ninstrumental variables to correct for this bias, and prove an\n$\\tilde{\\mathcal{O}}(k\\sqrt{T})$ upper bound for the expected regret of the\nalgorithm. Further, we demonstrate the asymptotic consistency and normality of\nthe $\\epsilon$-BanditIV estimator. We carry out extensive Monte Carlo\nsimulations to demonstrate the performance of our algorithms compared to other\nmethods. We show that $\\epsilon$-BanditIV significantly outperforms other\nexisting methods in endogenous settings. Finally, we use data from real-time\nbidding (RTB) system to demonstrate how $\\epsilon$-BanditIV can be used to\nestimate the causal impact of advertising in such settings and compare its\nperformance with other existing methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.08649v2"
    },
    {
        "title": "Estimating Dynamic Spillover Effects along Multiple Networks in a Linear\n  Panel Model",
        "authors": [
            "Clemens Possnig",
            "Andreea Rotărescu",
            "Kyungchul Song"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Spillover of economic outcomes often arises over multiple networks, and\ndistinguishing their separate roles is important in empirical research. For\nexample, the direction of spillover between two groups (such as banks and\nindustrial sectors linked in a bipartite graph) has important economic\nimplications, and a researcher may want to learn which direction is supported\nin the data. For this, we need to have an empirical methodology that allows for\nboth directions of spillover simultaneously. In this paper, we develop a\ndynamic linear panel model and asymptotic inference with large $n$ and small\n$T$, where both directions of spillover are accommodated through multiple\nnetworks. Using the methodology developed here, we perform an empirical study\nof spillovers between bank weakness and zombie-firm congestion in industrial\nsectors, using firm-bank matched data from Spain between 2005 and 2012.\nOverall, we find that there is positive spillover in both directions between\nbanks and sectors.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.08995v1"
    },
    {
        "title": "On the Role of the Zero Conditional Mean Assumption for Causal Inference\n  in Linear Models",
        "authors": [
            "Federico Crudu",
            "Michael C. Knaus",
            "Giovanni Mellace",
            "Joeri Smits"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Many econometrics textbooks imply that under mean independence of the\nregressors and the error term, the OLS parameters have a causal interpretation.\nWe show that even when this assumption is satisfied, OLS might identify a\npseudo-parameter that does not have a causal interpretation. Even assuming that\nthe linear model is \"structural\" creates some ambiguity in what the regression\nerror represents and whether the OLS estimand is causal. This issue applies\nequally to linear IV and panel data models. To give these estimands a causal\ninterpretation, one needs to impose assumptions on a \"causal\" model, e.g.,\nusing the potential outcome framework. This highlights that causal inference\nrequires causal, and not just stochastic, assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.09502v1"
    },
    {
        "title": "A Misuse of Specification Tests",
        "authors": [
            "Naoya Sueishi"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Empirical researchers often perform model specification tests, such as the\nHausman test and the overidentifying restrictions test, to confirm the validity\nof estimators rather than the validity of models. This paper examines the\neffectiveness of specification pretests in finding invalid estimators. We study\nthe local asymptotic properties of test statistics and estimators and show that\nlocally unbiased specification tests cannot determine whether asymptotically\nefficient estimators are asymptotically biased. The main message of the paper\nis that correct specification and valid estimation are different issues.\nCorrect specification is neither necessary nor sufficient for asymptotically\nunbiased estimation under local overidentification.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.11915v1"
    },
    {
        "title": "Peer Effects in Labor Market Training",
        "authors": [
            "Ulrike Unterhofer"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper shows that the group composition matters for the effectiveness of\nlabor market training programs for jobseekers. Using rich administrative data\nfrom Germany, I document that greater average exposure to highly employable\npeers leads to increased employment stability after program participation. Peer\neffects on earnings are positive and long-lasting in classic vocational\ntraining and negative but of short duration in retraining, pointing to\ndifferent mechanisms. Finally, I also find evidence for non-linearities in\neffects and show that more heterogeneity in the peer group is detrimental.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.12366v2"
    },
    {
        "title": "Macroeconomic Effects of Active Labour Market Policies: A Novel\n  Instrumental Variables Approach",
        "authors": [
            "Ulrike Unterhofer",
            "Conny Wunsch"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This study evaluates the macroeconomic effects of active labour market\npolicies (ALMP) in Germany over the period 2005 to 2018. We propose a novel\nidentification strategy to overcome the simultaneity of ALMP and labour market\noutcomes at the regional level. It exploits the imperfect overlap of local\nlabour markets and local employment agencies that decide on the local\nimplementation of policies. Specifically, we instrument for the use of ALMP in\na local labour market with the mix of ALMP implemented outside this market but\nin local employment agencies that partially overlap with this market. We find\nno effects of short-term activation measures and further vocational training on\naggregate labour market outcomes. In contrast, wage subsidies substantially\nincrease the share of workers in unsubsidised employment while lowering\nlong-term unemployment and welfare dependency. Our results suggest that\nnegative externalities of ALMP partially offset the effects for program\nparticipants and that some segments of the labour market benefit more than\nothers.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.12437v1"
    },
    {
        "title": "Cross-Sectional Dynamics Under Network Structure: Theory and\n  Macroeconomic Applications",
        "authors": [
            "Marko Mlikota"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Many environments in economics feature a cross-section of units linked by\nbilateral ties. I develop a framework for studying dynamics of cross-sectional\nvariables that exploits this network structure. The Network-VAR (NVAR) is a\nvector autoregression in which innovations transmit cross-sectionally via\nbilateral links and which can accommodate rich patterns of how network effects\nof higher order accumulate over time. It can be used to estimate dynamic\nnetwork effects, with the network given or inferred from dynamic\ncross-correlations in the data. It also offers a dimensionality-reduction\ntechnique for modeling high-dimensional (cross-sectional) processes, owing to\nnetworks' ability to summarize complex relations among variables (units) by\nrelatively few bilateral links. In a first application, consistent with an RBC\neconomy with lagged input-output conversion, I estimate how sectoral\nproductivity shocks transmit along supply chains and affect sectoral prices in\nthe US economy. In a second application, I forecast monthly industrial\nproduction growth across 44 countries by assuming and estimating a network\nunderlying the dynamics. This reduces out-of-sample mean squared errors by up\nto 23% relative to a factor model, consistent with an equivalence result I\nderive.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.13610v4"
    },
    {
        "title": "Spectral estimation for mixed causal-noncausal autoregressive models",
        "authors": [
            "Alain Hecq",
            "Daniel Velasquez-Gaviria"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper investigates new ways of estimating and identifying causal,\nnoncausal, and mixed causal-noncausal autoregressive models driven by a\nnon-Gaussian error sequence. We do not assume any parametric distribution\nfunction for the innovations. Instead, we use the information of higher-order\ncumulants, combining the spectrum and the bispectrum in a minimum distance\nestimation. We show how to circumvent the nonlinearity of the parameters and\nthe multimodality in the noncausal and mixed models by selecting the\nappropriate initial values in the estimation. In addition, we propose a method\nof identification using a simple comparison criterion based on the global\nminimum of the estimation function. By means of a Monte Carlo study, we find\nunbiased estimated parameters and a correct identification as the data depart\nfrom normality. We propose an empirical application on eight monthly commodity\nprices, finding noncausal and mixed causal-noncausal dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.13830v1"
    },
    {
        "title": "A Design-Based Approach to Spatial Correlation",
        "authors": [
            "Ruonan Xu",
            "Jeffrey M. Wooldridge"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  When observing spatial data, what standard errors should we report? With the\nfinite population framework, we identify three channels of spatial correlation:\nsampling scheme, assignment design, and model specification. The\nEicker-Huber-White standard error, the cluster-robust standard error, and the\nspatial heteroskedasticity and autocorrelation consistent standard error are\ncompared under different combinations of the three channels. Then, we provide\nguidelines for whether standard errors should be adjusted for spatial\ncorrelation for both linear and nonlinear estimators. As it turns out, the\nanswer to this question also depends on the magnitude of the sampling\nprobability.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.14354v1"
    },
    {
        "title": "Extreme Changes in Changes",
        "authors": [
            "Yuya Sasaki",
            "Yulong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Policy analysts are often interested in treating the units with extreme\noutcomes, such as infants with extremely low birth weights. Existing\nchanges-in-changes (CIC) estimators are tailored to middle quantiles and do not\nwork well for such subpopulations. This paper proposes a new CIC estimator to\naccurately estimate treatment effects at extreme quantiles. With its asymptotic\nnormality, we also propose a method of statistical inference, which is simple\nto implement. Based on simulation studies, we propose to use our extreme CIC\nestimator for extreme, such as below 5% and above 95%, quantiles, while the\nconventional CIC estimator should be used for intermediate quantiles. Applying\nthe proposed method, we study the effects of income gains from the 1993 EITC\nreform on infant birth weights for those in the most critical conditions. This\npaper is accompanied by a Stata command.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.14870v2"
    },
    {
        "title": "Incorporating Prior Knowledge of Latent Group Structure in Panel Data\n  Models",
        "authors": [
            "Boyuan Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The assumption of group heterogeneity has become popular in panel data\nmodels. We develop a constrained Bayesian grouped estimator that exploits\nresearchers' prior beliefs on groups in a form of pairwise constraints,\nindicating whether a pair of units is likely to belong to a same group or\ndifferent groups. We propose a prior to incorporate the pairwise constraints\nwith varying degrees of confidence. The whole framework is built on the\nnonparametric Bayesian method, which implicitly specifies a distribution over\nthe group partitions, and so the posterior analysis takes the uncertainty of\nthe latent group structure into account. Monte Carlo experiments reveal that\nadding prior knowledge yields more accurate estimates of coefficient and scores\npredictive gains over alternative estimators. We apply our method to two\nempirical applications. In a first application to forecasting U.S. CPI\ninflation, we illustrate that prior knowledge of groups improves density\nforecasts when the data is not entirely informative. A second application\nrevisits the relationship between a country's income and its democratic\ntransition; we identify heterogeneous income effects on democracy with five\ndistinct groups over ninety countries.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.16714v3"
    },
    {
        "title": "A Data Fusion Approach for Ride-sourcing Demand Estimation: A Discrete\n  Choice Model with Sampling and Endogeneity Corrections",
        "authors": [
            "Rico Krueger",
            "Michel Bierlaire",
            "Prateek Bansal"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Ride-sourcing services offered by companies like Uber and Didi have grown\nrapidly in the last decade. Understanding the demand for these services is\nessential for planning and managing modern transportation systems. Existing\nstudies develop statistical models for ride-sourcing demand estimation at an\naggregate level due to limited data availability. These models lack foundations\nin microeconomic theory, ignore competition of ride-sourcing with other travel\nmodes, and cannot be seamlessly integrated into existing individual-level\n(disaggregate) activity-based models to evaluate system-level impacts of\nride-sourcing services. In this paper, we present and apply an approach for\nestimating ride-sourcing demand at a disaggregate level using discrete choice\nmodels and multiple data sources. We first construct a sample of trip-based\nmode choices in Chicago, USA by enriching household travel survey with publicly\navailable ride-sourcing and taxi trip records. We then formulate a multivariate\nextreme value-based discrete choice with sampling and endogeneity corrections\nto account for the construction of the estimation sample from multiple data\nsources and endogeneity biases arising from supply-side constraints and surge\npricing mechanisms in ride-sourcing systems. Our analysis of the constructed\ndataset reveals insights into the influence of various socio-economic, land use\nand built environment features on ride-sourcing demand. We also derive\nelasticities of ride-sourcing demand relative to travel cost and time. Finally,\nwe illustrate how the developed model can be employed to quantify the welfare\nimplications of ride-sourcing policies and regulations such as terminating\ncertain types of services and introducing ride-sourcing taxes.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.02178v1"
    },
    {
        "title": "Educational Inequality of Opportunity and Mobility in Europe",
        "authors": [
            "Joël Terschuur"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Educational attainment generates labor market returns, societal gains and has\nintrinsic value for individuals. We study Inequality of Opportunity (IOp) and\nintergenerational mobility in the distribution of educational attainment. We\npropose to use debiased IOp estimators based on the Gini coefficient and the\nMean Logarithmic Deviation (MLD) which are robust to machine learning biases.\nWe also measure the effect of each circumstance on IOp, we provide tests to\ncompare IOp in two populations and to test joint significance of a group of\ncircumstances. We find that circumstances explain between 38\\% and 74\\% of\ntotal educational inequality in European countries. Mother's education is the\nmost important circumstance in most countries. There is high intergenerational\npersistence and there is evidence of an educational Great Gatsby curve. We also\nconstruct IOp aware educational Great Gatsby curves and find that high income\nIOp countries are also high educational IOp and less mobile countries.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.02407v3"
    },
    {
        "title": "The long-term effect of childhood exposure to technology using\n  surrogates",
        "authors": [
            "Sylvia Klosin",
            "Nicolaj Søndergaard Mühlbach"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We study how childhood exposure to technology at ages 5-15 via the occupation\nof the parents affects the ability to climb the social ladder in terms of\nincome at ages 45-49 using the Danish micro data from years 1961-2019. Our\nmeasure of technology exposure covers the degree to which using computers\n(hardware and software) is required to perform an occupation, and it is created\nby merging occupational codes with detailed data from O*NET. The challenge in\nestimating this effect is that long-term outcome is observed over a different\ntime horizon than our treatment of interest. We therefore adapt the surrogate\nindex methodology, linking the effect of our childhood treatment on\nintermediate surrogates, such as income and education at ages 25-29, to the\neffect on adulthood income. We estimate that a one standard error increase in\nexposure to technology increases the income rank by 2\\%-points, which is\neconomically and statistically significant and robust to cluster-correlation\nwithin families. The derived policy recommendation is to update the educational\ncurriculum to expose children to computers to a higher degree, which may then\nact as a social leveler.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.03351v2"
    },
    {
        "title": "Semiparametric Distribution Regression with Instruments and Monotonicity",
        "authors": [
            "Dominik Wied"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper proposes IV-based estimators for the semiparametric distribution\nregression model in the presence of an endogenous regressor, which are based on\nan extension of IV probit estimators. We discuss the causal interpretation of\nthe estimators and two methods (monotone rearrangement and isotonic regression)\nto ensure a monotonically increasing distribution function. Asymptotic\nproperties and simulation evidence are provided. An application to wage\nequations reveals statistically significant and heterogeneous differences to\nthe inconsistent OLS-based estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.03704v1"
    },
    {
        "title": "Optimal Model Selection in RDD and Related Settings Using Placebo Zones",
        "authors": [
            "Nathan Kettlewell",
            "Peter Siminski"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose a new model-selection algorithm for Regression Discontinuity\nDesign, Regression Kink Design, and related IV estimators. Candidate models are\nassessed within a 'placebo zone' of the running variable, where the true\neffects are known to be zero. The approach yields an optimal combination of\nbandwidth, polynomial, and any other choice parameters. It can also inform\nchoices between classes of models (e.g. RDD versus cohort-IV) and any other\nchoices, such as covariates, kernel, or other weights. We outline sufficient\nconditions under which the approach is asymptotically optimal. The approach\nalso performs favorably under more general conditions in a series of Monte\nCarlo simulations. We demonstrate the approach in an evaluation of changes to\nMinimum Supervised Driving Hours in the Australian state of New South Wales. We\nalso re-evaluate evidence on the effects of Head Start and Minimum Legal\nDrinking Age. Our Stata commands implement the procedure and compare its\nperformance to other approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.04043v1"
    },
    {
        "title": "On the Non-Identification of Revenue Production Functions",
        "authors": [
            "David Van Dijcke"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Production functions are potentially misspecified when revenue is used as a\nproxy for output. I formalize and strengthen this common knowledge by showing\nthat neither the production function nor Hicks-neutral productivity can be\nidentified with such a revenue proxy. This result obtains when relaxing the\nstandard assumptions used in the literature to allow for imperfect competition.\nIt holds for a large class of production functions, including all commonly used\nparametric forms. Among the prevalent approaches to address this issue, only\nthose that impose assumptions on the underlying demand system can possibly\nidentify the production function.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.04620v3"
    },
    {
        "title": "Dominant Drivers of National Inflation",
        "authors": [
            "Jan Ditzen",
            "Francesco Ravazzolo"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  For western economies a long-forgotten phenomenon is on the horizon: rising\ninflation rates. We propose a novel approach christened D2ML to identify\ndrivers of national inflation. D2ML combines machine learning for model\nselection with time dependent data and graphical models to estimate the inverse\nof the covariance matrix, which is then used to identify dominant drivers.\nUsing a dataset of 33 countries, we find that the US inflation rate and oil\nprices are dominant drivers of national inflation rates. For a more general\nframework, we carry out Monte Carlo simulations to show that our estimator\ncorrectly identifies dominant drivers.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.05841v1"
    },
    {
        "title": "Robust Estimation of the non-Gaussian Dimension in Structural Linear\n  Models",
        "authors": [
            "Miguel Cabello"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Statistical identification of possibly non-fundamental SVARMA models requires\nstructural errors: (i) to be an i.i.d process, (ii) to be mutually independent\nacross components, and (iii) each of them must be non-Gaussian distributed.\nHence, provided the first two requisites, it is crucial to evaluate the\nnon-Gaussian identification condition. We address this problem by relating the\nnon-Gaussian dimension of structural errors vector to the rank of a matrix\nbuilt from the higher-order spectrum of reduced-form errors. This makes our\nproposal robust to the roots location of the lag polynomials, and generalizes\nthe current procedures designed for the restricted case of a causal structural\nVAR model. Simulation exercises show that our procedure satisfactorily\nestimates the number of non-Gaussian components.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.07263v2"
    },
    {
        "title": "Smoothing volatility targeting",
        "authors": [
            "Mauro Bernardi",
            "Daniele Bianchi",
            "Nicolas Bianco"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We propose an alternative approach towards cost mitigation in\nvolatility-managed portfolios based on smoothing the predictive density of an\notherwise standard stochastic volatility model. Specifically, we develop a\nnovel variational Bayes estimation method that flexibly encompasses different\nsmoothness assumptions irrespective of the persistence of the underlying latent\nstate. Using a large set of equity trading strategies, we show that smoothing\nvolatility targeting helps to regularise the extreme leverage/turnover that\nresults from commonly used realised variance estimates. This has important\nimplications for both the risk-adjusted returns and the mean-variance\nefficiency of volatility-managed portfolios, once transaction costs are\nfactored in. An extensive simulation study shows that our variational inference\nscheme compares favourably against existing state-of-the-art Bayesian\nestimation methods for stochastic volatility models.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.07288v1"
    },
    {
        "title": "The finite sample performance of instrumental variable-based estimators\n  of the Local Average Treatment Effect when controlling for covariates",
        "authors": [
            "Hugo Bodory",
            "Martin Huber",
            "Michael Lechner"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper investigates the finite sample performance of a range of\nparametric, semi-parametric, and non-parametric instrumental variable\nestimators when controlling for a fixed set of covariates to evaluate the local\naverage treatment effect. Our simulation designs are based on empirical labor\nmarket data from the US and vary in several dimensions, including effect\nheterogeneity, instrument selectivity, instrument strength, outcome\ndistribution, and sample size. Among the estimators and simulations considered,\nnon-parametric estimation based on the random forest (a machine learner\ncontrolling for covariates in a data-driven way) performs competitive in terms\nof the average coverage rates of the (bootstrap-based) 95% confidence\nintervals, while also being relatively precise. Non-parametric kernel\nregression as well as certain versions of semi-parametric radius matching on\nthe propensity score, pair matching on the covariates, and inverse probability\nweighting also have a decent coverage, but are less precise than the random\nforest-based method. In terms of the average root mean squared error of LATE\nestimation, kernel regression performs best, closely followed by the random\nforest method, which has the lowest average absolute bias.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.07379v1"
    },
    {
        "title": "PAC-Bayesian Treatment Allocation Under Budget Constraints",
        "authors": [
            "Daniel F. Pellatt"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  This paper considers the estimation of treatment assignment rules when the\npolicy maker faces a general budget or resource constraint. Utilizing the\nPAC-Bayesian framework, we propose new treatment assignment rules that allow\nfor flexible notions of treatment outcome, treatment cost, and a budget\nconstraint. For example, the constraint setting allows for cost-savings, when\nthe costs of non-treatment exceed those of treatment for a subpopulation, to be\nfactored into the budget. It also accommodates simpler settings, such as\nquantity constraints, and doesn't require outcome responses and costs to have\nthe same unit of measurement. Importantly, the approach accounts for settings\nwhere budget or resource limitations may preclude treating all that can\nbenefit, where costs may vary with individual characteristics, and where there\nmay be uncertainty regarding the cost of treatment rules of interest. Despite\nthe nomenclature, our theoretical analysis examines frequentist properties of\nthe proposed rules. For stochastic rules that typically approach\nbudget-penalized empirical welfare maximizing policies in larger samples, we\nderive non-asymptotic generalization bounds for the target population costs and\nsharp oracle-type inequalities that compare the rules' welfare regret to that\nof optimal policies in relevant budget categories. A closely related,\nnon-stochastic, model aggregation treatment assignment rule is shown to inherit\ndesirable attributes.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.09007v2"
    },
    {
        "title": "Identification of time-varying counterfactual parameters in nonlinear\n  panel models",
        "authors": [
            "Irene Botosaru",
            "Chris Muris"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  We develop a general framework for the identification of counterfactual\nparameters in a class of nonlinear semiparametric panel models with fixed\neffects and time effects. Our method applies to models for discrete outcomes\n(e.g., two-way fixed effects binary choice) or continuous outcomes (e.g.,\ncensored regression), with discrete or continuous regressors. Our results do\nnot require parametric assumptions on the error terms or time-homogeneity on\nthe outcome equation. Our main results focus on static models, with a set of\nresults applying to models without any exogeneity conditions. We show that the\nsurvival distribution of counterfactual outcomes is identified (point or\npartial) in this class of models. This parameter is a building block for most\npartial and marginal effects of interest in applied practice that are based on\nthe average structural function as defined by Blundell and Powell (2003, 2004).\nTo the best of our knowledge, ours are the first results on average partial and\nmarginal effects for binary choice and ordered choice models with two-way fixed\neffects and non-logistic errors.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.09193v2"
    },
    {
        "title": "Quantifying fairness and discrimination in predictive models",
        "authors": [
            "Arthur Charpentier"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  The analysis of discrimination has long interested economists and lawyers. In\nrecent years, the literature in computer science and machine learning has\nbecome interested in the subject, offering an interesting re-reading of the\ntopic. These questions are the consequences of numerous criticisms of\nalgorithms used to translate texts or to identify people in images. With the\narrival of massive data, and the use of increasingly opaque algorithms, it is\nnot surprising to have discriminatory algorithms, because it has become easy to\nhave a proxy of a sensitive variable, by enriching the data indefinitely.\nAccording to Kranzberg (1986), \"technology is neither good nor bad, nor is it\nneutral\", and therefore, \"machine learning won't give you anything like gender\nneutrality `for free' that you didn't explicitely ask for\", as claimed by\nKearns et a. (2019). In this article, we will come back to the general context,\nfor predictive models in classification. We will present the main concepts of\nfairness, called group fairness, based on independence between the sensitive\nvariable and the prediction, possibly conditioned on this or that information.\nWe will finish by going further, by presenting the concepts of individual\nfairness. Finally, we will see how to correct a potential discrimination, in\norder to guarantee that a model is more ethical\n",
        "pdf_link": "http://arxiv.org/pdf/2212.09868v1"
    },
    {
        "title": "Forward Orthogonal Deviations GMM and the Absence of Large Sample Bias",
        "authors": [
            "Robert F. Phillips"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  It is well known that generalized method of moments (GMM) estimators of\ndynamic panel data regressions can have significant bias when the number of\ntime periods ($T$) is not small compared to the number of cross-sectional units\n($n$). The bias is attributed to the use of many instrumental variables. This\npaper shows that if the maximum number of instrumental variables used in a\nperiod increases with $T$ at a rate slower than $T^{1/2}$, then GMM estimators\nthat exploit the forward orthogonal deviations (FOD) transformation do not have\nasymptotic bias, regardless of how fast $T$ increases relative to $n$. This\nconclusion is specific to using the FOD transformation. A similar conclusion\ndoes not necessarily apply when other transformations are used to remove fixed\neffects. Monte Carlo evidence illustrating the analytical results is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.14075v2"
    },
    {
        "title": "Supercompliers",
        "authors": [
            "Matthew L. Comey",
            "Amanda R. Eng",
            "Pauline Leung",
            "Zhuan Pei"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In a binary-treatment instrumental variable framework, we define\nsupercompliers as the subpopulation whose treatment take-up positively responds\nto eligibility and whose outcome positively responds to take-up. Supercompliers\nare the only subpopulation to benefit from treatment eligibility and, hence,\nare important for policy. We provide tools to characterize supercompliers under\na set of jointly testable assumptions. Specifically, we require standard\nassumptions from the local average treatment effect literature plus an outcome\nmonotonicity assumption. Estimation and inference can be conducted with\ninstrumental variable regression. In two job-training experiments, we\ndemonstrate our machinery's utility, particularly in incorporating social\nwelfare weights into marginal-value-of-public-funds analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.14105v3"
    },
    {
        "title": "Identifying causal effects with subjective ordinal outcomes",
        "authors": [
            "Leonard Goff"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  Survey questions often ask respondents to select from ordered scales where\nthe meanings of the categories are subjective, leaving each individual free to\napply their own definitions when answering. This paper studies the use of these\nresponses as an outcome variable in causal inference, accounting for variation\nin the interpretation of categories across individuals. I find that when a\ncontinuous treatment is statistically independent of both i) potential\noutcomes; and ii) heterogeneity in reporting styles, a nonparametric regression\nof response category number on that treatment variable recovers a quantity\nproportional to an average causal effect among individuals who are on the\nmargin between successive response categories. The magnitude of a given\nregression coefficient is not meaningful on its own, but the ratio of local\nregression derivatives with respect to two such treatment variables identifies\nthe relative magnitudes of convex averages of their effects. I find that\ncomparisons involving discrete treatment variables are not as readily\ninterpretable, but obtain a partial identification result for such cases under\nadditional assumptions. I illustrate the results by revisiting the effects of\nincome comparisons on subjective well-being, without assuming cardinality or\ninterpersonal comparability of responses.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.14622v3"
    },
    {
        "title": "Regression adjustment in randomized controlled trials with many\n  covariates",
        "authors": [
            "Harold D Chiang",
            "Yukitoshi Matsushita",
            "Taisuke Otsu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper is concerned with estimation and inference on average treatment\neffects in randomized controlled trials when researchers observe potentially\nmany covariates. By employing Neyman's (1923) finite population perspective, we\npropose a bias-corrected regression adjustment estimator using cross-fitting,\nand show that the proposed estimator has favorable properties over existing\nalternatives. For inference, we derive the first and second order terms in the\nstochastic component of the regression adjustment estimators, study higher\norder properties of the existing inference methods, and propose a\nbias-corrected version of the HC3 standard error. The proposed methods readily\nextend to stratified experiments with large strata. Simulation studies show our\ncross-fitted estimator, combined with the bias-corrected HC3, delivers precise\npoint estimates and robust size controls over a wide range of DGPs. To\nillustrate, the proposed methods are applied to real dataset on randomized\nexperiments of incentives and services for college achievement following\nAngrist, Lang, and Oreopoulos (2009).\n",
        "pdf_link": "http://arxiv.org/pdf/2302.00469v3"
    },
    {
        "title": "Inference in Non-stationary High-Dimensional VARs",
        "authors": [
            "Alain Hecq",
            "Luca Margaritella",
            "Stephan Smeekes"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this paper we construct an inferential procedure for Granger causality in\nhigh-dimensional non-stationary vector autoregressive (VAR) models. Our method\ndoes not require knowledge of the order of integration of the time series under\nconsideration. We augment the VAR with at least as many lags as the suspected\nmaximum order of integration, an approach which has been proven to be robust\nagainst the presence of unit roots in low dimensions. We prove that we can\nrestrict the augmentation to only the variables of interest for the testing,\nthereby making the approach suitable for high dimensions. We combine this lag\naugmentation with a post-double-selection procedure in which a set of initial\npenalized regressions is performed to select the relevant variables for both\nthe Granger causing and caused variables. We then establish uniform asymptotic\nnormality of a second-stage regression involving only the selected variables.\nFinite sample simulations show good performance, an application to investigate\nthe (predictive) causes and effects of economic uncertainty illustrates the\nneed to allow for unknown orders of integration.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.01434v2"
    },
    {
        "title": "Agreed and Disagreed Uncertainty",
        "authors": [
            "Luca Gambetti",
            "Dimitris Korobilis",
            "John Tsoukalas",
            "Francesco Zanetti"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  When agents' information is imperfect and dispersed, existing measures of\nmacroeconomic uncertainty based on the forecast error variance have two\ndistinct drivers: the variance of the economic shock and the variance of the\ninformation dispersion. The former driver increases uncertainty and reduces\nagents' disagreement (agreed uncertainty). The latter increases both\nuncertainty and disagreement (disagreed uncertainty). We use these implications\nto identify empirically the effects of agreed and disagreed uncertainty shocks,\nbased on a novel measure of consumer disagreement derived from survey\nexpectations. Disagreed uncertainty has no discernible economic effects and is\nbenign for economic activity, but agreed uncertainty exerts significant\ndepressing effects on a broad spectrum of macroeconomic indicators.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.01621v1"
    },
    {
        "title": "Using bayesmixedlogit and bayesmixedlogitwtp in Stata",
        "authors": [
            "Matthew J. Baker"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This document presents an overview of the bayesmixedlogit and\nbayesmixedlogitwtp Stata packages. It mirrors closely the helpfile obtainable\nin Stata (i.e., through help bayesmixedlogit or help bayesmixedlogitwtp).\nFurther background for the packages can be found in Baker(2014).\n",
        "pdf_link": "http://arxiv.org/pdf/2302.01775v1"
    },
    {
        "title": "Testing for Structural Change under Nonstationarity",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This Appendix (dated: July 2021) includes supplementary derivations related\nto the main limit results of the econometric framework for structural break\ntesting in predictive regression models based on the OLS-Wald and IVX-Wald test\nstatistics, developed by Katsouris C (2021). In particular, we derive the\nasymptotic distributions of the test statistics when the predictive regression\nmodel includes either mildly integrated or persistent regressors. Moreover, we\nconsider the case in which a model intercept is included in the model vis-a-vis\nthe case that the predictive regression model has no model intercept. In a\nsubsequent version of this study we reexamine these particular aspects in more\ndepth with respect to the demeaned versions of the variables of the predictive\nregression.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.02370v1"
    },
    {
        "title": "Out of Sample Predictability in Predictive Regressions with Many\n  Predictor Candidates",
        "authors": [
            "Jesus Gonzalo",
            "Jean-Yves Pitarakis"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper is concerned with detecting the presence of out of sample\npredictability in linear predictive regressions with a potentially large set of\ncandidate predictors. We propose a procedure based on out of sample MSE\ncomparisons that is implemented in a pairwise manner using one predictor at a\ntime and resulting in an aggregate test statistic that is standard normally\ndistributed under the global null hypothesis of no linear predictability.\nPredictors can be highly persistent, purely stationary or a combination of\nboth. Upon rejection of the null hypothesis we subsequently introduce a\npredictor screening procedure designed to identify the most active predictors.\nAn empirical application to key predictors of US economic activity illustrates\nthe usefulness of our methods and highlights the important forward looking role\nplayed by the series of manufacturing new orders.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.02866v2"
    },
    {
        "title": "Penalized Quasi-likelihood Estimation and Model Selection in Time Series\n  Models with Parameters on the Boundary",
        "authors": [
            "Heino Bohn Nielsen",
            "Anders Rahbek"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We extend the theory from Fan and Li (2001) on penalized likelihood-based\nestimation and model-selection to statistical and econometric models which\nallow for non-negativity constraints on some or all of the parameters, as well\nas time-series dependence. It differs from classic non-penalized likelihood\nestimation, where limiting distributions of likelihood-based estimators and\ntest-statistics are non-standard, and depend on the unknown number of\nparameters on the boundary of the parameter space. Specifically, we establish\nthat the joint model selection and estimation, results in standard asymptotic\nGaussian distributed estimators. The results are applied to the rich class of\nautoregressive conditional heteroskedastic (ARCH) models for the modelling of\ntime-varying volatility. We find from simulations that the penalized estimation\nand model-selection works surprisingly well even for a large number of\nparameters. A simple empirical illustration for stock-market returns data\nconfirms the ability of the penalized estimation to select ARCH models which\nfit nicely the autocorrelation function, as well as confirms the stylized fact\nof long-memory in financial time series data.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.02867v1"
    },
    {
        "title": "Asymptotic Representations for Sequential Decisions, Adaptive\n  Experiments, and Batched Bandits",
        "authors": [
            "Keisuke Hirano",
            "Jack R. Porter"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We develop asymptotic approximation results that can be applied to sequential\nestimation and inference problems, adaptive randomized controlled trials, and\nother statistical decision problems that involve multiple decision nodes with\nstructured and possibly endogenous information sets. Our results extend the\nclassic asymptotic representation theorem used extensively in efficiency bound\ntheory and local power analysis. In adaptive settings where the decision at one\nstage can affect the observation of variables in later stages, we show that a\nlimiting data environment characterizes all limit distributions attainable\nthrough a joint choice of an adaptive design rule and statistics applied to the\nadaptively generated data, under local alternatives. We illustrate how the\ntheory can be applied to study the choice of adaptive rules and end-of-sample\nstatistical inference in batched (groupwise) sequential adaptive experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.03117v1"
    },
    {
        "title": "Extensions for Inference in Difference-in-Differences with Few Treated\n  Clusters",
        "authors": [
            "Luis Alvarez",
            "Bruno Ferman"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In settings with few treated units, Difference-in-Differences (DID)\nestimators are not consistent, and are not generally asymptotically normal.\nThis poses relevant challenges for inference. While there are inference methods\nthat are valid in these settings, some of these alternatives are not readily\navailable when there is variation in treatment timing and heterogeneous\ntreatment effects; or for deriving uniform confidence bands for event-study\nplots. We present alternatives in settings with few treated units that are\nvalid with variation in treatment timing and/or that allow for uniform\nconfidence bands.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.03131v1"
    },
    {
        "title": "High-Dimensional Granger Causality for Climatic Attribution",
        "authors": [
            "Marina Friedrich",
            "Luca Margaritella",
            "Stephan Smeekes"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this paper we test for Granger causality in high-dimensional vector\nautoregressive models (VARs) to disentangle and interpret the complex causal\nchains linking radiative forcings and global temperatures. By allowing for high\ndimensionality in the model, we can enrich the information set with relevant\nnatural and anthropogenic forcing variables to obtain reliable causal\nrelations. This provides a step forward from existing climatology literature,\nwhich has mostly treated these variables in isolation in small models.\nAdditionally, our framework allows to disregard the order of integration of the\nvariables by directly estimating the VAR in levels, thus avoiding accumulating\nbiases coming from unit-root and cointegration tests. This is of particular\nappeal for climate time series which are well known to contain stochastic\ntrends and long memory. We are thus able to establish causal networks linking\nradiative forcings to global temperatures and to connect radiative forcings\namong themselves, thereby allowing for tracing the path of dynamic causal\neffects through the system.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.03996v2"
    },
    {
        "title": "Covariate Adjustment in Experiments with Matched Pairs",
        "authors": [
            "Yuehao Bai",
            "Liang Jiang",
            "Joseph P. Romano",
            "Azeem M. Shaikh",
            "Yichong Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies inference on the average treatment effect in experiments\nin which treatment status is determined according to \"matched pairs\" and it is\nadditionally desired to adjust for observed, baseline covariates to gain\nfurther precision. By a \"matched pairs\" design, we mean that units are sampled\ni.i.d. from the population of interest, paired according to observed, baseline\ncovariates and finally, within each pair, one unit is selected at random for\ntreatment. Importantly, we presume that not all observed, baseline covariates\nare used in determining treatment assignment. We study a broad class of\nestimators based on a \"doubly robust\" moment condition that permits us to study\nestimators with both finite-dimensional and high-dimensional forms of covariate\nadjustment. We find that estimators with finite-dimensional, linear adjustments\nneed not lead to improvements in precision relative to the unadjusted\ndifference-in-means estimator. This phenomenon persists even if the adjustments\nare interacted with treatment; in fact, doing so leads to no changes in\nprecision. However, gains in precision can be ensured by including fixed\neffects for each of the pairs. Indeed, we show that this adjustment is the\n\"optimal\" finite-dimensional, linear adjustment. We additionally study two\nestimators with high-dimensional forms of covariate adjustment based on the\nLASSO. For each such estimator, we show that it leads to improvements in\nprecision relative to the unadjusted difference-in-means estimator and also\nprovide conditions under which it leads to the \"optimal\" nonparametric,\ncovariate adjustment. A simulation study confirms the practical relevance of\nour theoretical analysis, and the methods are employed to reanalyze data from\nan experiment using a \"matched pairs\" design to study the effect of\nmacroinsurance on microenterprise.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.04380v3"
    },
    {
        "title": "On semiparametric estimation of the intercept of the sample selection\n  model: a kernel approach",
        "authors": [
            "Zhewen Pan"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper presents a new perspective on the identification at infinity for\nthe intercept of the sample selection model as identification at the boundary\nvia a transformation of the selection index. This perspective suggests\ngeneralizations of estimation at infinity to kernel regression estimation at\nthe boundary and further to local linear estimation at the boundary. The\nproposed kernel-type estimators with an estimated transformation are proven to\nbe nonparametric-rate consistent and asymptotically normal under mild\nregularity conditions. A fully data-driven method of selecting the optimal\nbandwidths for the estimators is developed. The Monte Carlo simulation shows\nthe desirable finite sample properties of the proposed estimators and bandwidth\nselection procedures.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.05089v1"
    },
    {
        "title": "Structural Break Detection in Quantile Predictive Regression Models with\n  Persistent Covariates",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We propose an econometric environment for structural break detection in\nnonstationary quantile predictive regressions. We establish the limit\ndistributions for a class of Wald and fluctuation type statistics based on both\nthe ordinary least squares estimator and the endogenous instrumental regression\nestimator proposed by Phillips and Magdalinos (2009a, Econometric Inference in\nthe Vicinity of Unity. Working paper, Singapore Management University).\nAlthough the asymptotic distribution of these test statistics appears to depend\non the chosen estimator, the IVX based tests are shown to be asymptotically\nnuisance parameter-free regardless of the degree of persistence and consistent\nunder local alternatives. The finite-sample performance of both tests is\nevaluated via simulation experiments. An empirical application to house pricing\nindex returns demonstrates the practicality of the proposed break tests for\nregression quantiles of nonstationary time series data.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.05193v1"
    },
    {
        "title": "Policy Learning with Rare Outcomes",
        "authors": [
            "Julia Hatamyar",
            "Noemi Kreif"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Machine learning (ML) estimates of conditional average treatment effects\n(CATE) can guide policy decisions, either by allowing targeting of individuals\nwith beneficial CATE estimates, or as inputs to decision trees that optimise\noverall outcomes. There is limited information available regarding how well\nthese algorithms perform in real-world policy evaluation scenarios. Using\nsynthetic data, we compare the finite sample performance of different policy\nlearning algorithms, machine learning techniques employed during their learning\nphases, and methods for presenting estimated policy values. For each algorithm,\nwe assess the resulting treatment allocation by measuring deviation from the\nideal (\"oracle\") policy. Our main finding is that policy trees based on\nestimated CATEs outperform trees learned from doubly-robust scores. Across\nsettings, Causal Forests and the Normalised Double-Robust Learner perform\nconsistently well, while Bayesian Additive Regression Trees perform poorly.\nThese methods are then applied to a case study targeting optimal allocation of\nsubsidised health insurance, with the goal of reducing infant mortality in\nIndonesia.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.05260v2"
    },
    {
        "title": "Individualized Treatment Allocation in Sequential Network Games",
        "authors": [
            "Toru Kitagawa",
            "Guanyi Wang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Designing individualized allocation of treatments so as to maximize the\nequilibrium welfare of interacting agents has many policy-relevant\napplications. Focusing on sequential decision games of interacting agents, this\npaper develops a method to obtain optimal treatment assignment rules that\nmaximize a social welfare criterion by evaluating stationary distributions of\noutcomes. Stationary distributions in sequential decision games are given by\nGibbs distributions, which are difficult to optimize with respect to a\ntreatment allocation due to analytical and computational complexity. We apply a\nvariational approximation to the stationary distribution and optimize the\napproximated equilibrium welfare with respect to treatment allocation using a\ngreedy optimization algorithm. We characterize the performance of the\nvariational approximation, deriving a performance guarantee for the greedy\noptimization algorithm via a welfare regret bound. We implement our proposed\nmethod in simulation exercises and an empirical application using the Indian\nmicrofinance data (Banerjee et al., 2013), and show it delivers significant\nwelfare gains.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.05747v4"
    },
    {
        "title": "Sequential Estimation of Multivariate Factor Stochastic Volatility\n  Models",
        "authors": [
            "Giorgio Calzolari",
            "Roxana Halbleib",
            "Christian Mücher"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We provide a simple method to estimate the parameters of multivariate\nstochastic volatility models with latent factor structures. These models are\nvery useful as they alleviate the standard curse of dimensionality, allowing\nthe number of parameters to increase only linearly with the number of the\nreturn series. Although theoretically very appealing, these models have only\nfound limited practical application due to huge computational burdens. Our\nestimation method is simple in implementation as it consists of two steps:\nfirst, we estimate the loadings and the unconditional variances by maximum\nlikelihood, and then we use the efficient method of moments to estimate the\nparameters of the stochastic volatility structure with GARCH as an auxiliary\nmodel. In a comprehensive Monte Carlo study we show the good performance of our\nmethod to estimate the parameters of interest accurately. The simulation study\nand an application to real vectors of daily returns of dimensions up to 148\nshow the method's computation advantage over the existing estimation\nprocedures.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.07052v1"
    },
    {
        "title": "Identification-robust inference for the LATE with high-dimensional\n  covariates",
        "authors": [
            "Yukun Ma"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper presents an inference method for the local average treatment\neffect (LATE) in the presence of high-dimensional covariates, irrespective of\nthe strength of identification. We propose a novel high-dimensional conditional\ntest statistic with uniformly correct asymptotic size. We provide an\neasy-to-implement algorithm to infer the high-dimensional LATE by inverting our\ntest statistic and employing the double/debiased machine learning method.\nSimulations indicate that our test is robust against both weak identification\nand high dimensionality concerning size control and power performance,\noutperforming other conventional tests. Applying the proposed method to\nrailroad and population data to study the effect of railroad access on urban\npopulation growth, we observe that our methodology yields confidence intervals\nthat are 49% to 92% shorter than conventional results, depending on\nspecifications.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.09756v4"
    },
    {
        "title": "Decomposition and Interpretation of Treatment Effects in Settings with\n  Delayed Outcomes",
        "authors": [
            "Federico A. Bugni",
            "Ivan A. Canay",
            "Steve McBride"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies settings where the analyst is interested in identifying\nand estimating the average causal effect of a binary treatment on an outcome.\nWe consider a setup in which the outcome realization does not get immediately\nrealized after the treatment assignment, a feature that is ubiquitous in\nempirical settings. The period between the treatment and the realization of the\noutcome allows other observed actions to occur and affect the outcome. In this\ncontext, we study several regression-based estimands routinely used in\nempirical work to capture the average treatment effect and shed light on\ninterpreting them in terms of ceteris paribus effects, indirect causal effects,\nand selection terms. We obtain three main and related takeaways. First, the\nthree most popular estimands do not generally satisfy what we call \\emph{strong\nsign preservation}, in the sense that these estimands may be negative even when\nthe treatment positively affects the outcome conditional on any possible\ncombination of other actions. Second, the most popular regression that includes\nthe other actions as controls satisfies strong sign preservation \\emph{if and\nonly if} these actions are mutually exclusive binary variables. Finally, we\nshow that a linear regression that fully stratifies the other actions leads to\nestimands that satisfy strong sign preservation.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.11505v4"
    },
    {
        "title": "Estimating Fiscal Multipliers by Combining Statistical Identification\n  with Potentially Endogenous Proxies",
        "authors": [
            "Sascha A. Keweloh",
            "Mathias Klein",
            "Jan Prüser"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Different proxy variables used in fiscal policy SVARs lead to contradicting\nconclusions regarding the size of fiscal multipliers. We show that the\nconflicting results are due to violations of the exogeneity assumptions, i.e.\nthe commonly used proxies are endogenously related to the structural shocks. We\npropose a novel approach to include proxy variables into a Bayesian\nnon-Gaussian SVAR, tailored to accommodate for potentially endogenous proxy\nvariables. Using our model, we show that increasing government spending is a\nmore effective tool to stimulate the economy than reducing taxes.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.13066v5"
    },
    {
        "title": "Nickell Bias in Panel Local Projection: Financial Crises Are Worse Than\n  You Think",
        "authors": [
            "Ziwei Mei",
            "Liugang Sheng",
            "Zhentao Shi"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Local Projection is widely used for impulse response estimation, with the\nFixed Effect (FE) estimator being the default for panel data. This paper\nhighlights the presence of Nickell bias for all regressors in the FE estimator,\neven if lagged dependent variables are absent in the regression. This bias is\nthe consequence of the inherent panel predictive specification. We recommend\nusing the split-panel jackknife estimator to eliminate the asymptotic bias and\nrestore the standard statistical inference. Revisiting three macro-finance\nstudies on the linkage between financial crises and economic contraction, we\nfind that the FE estimator substantially underestimates the post-crisis\neconomic losses.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.13455v3"
    },
    {
        "title": "Multi-cell experiments for marginal treatment effect estimation of\n  digital ads",
        "authors": [
            "Caio Waisman",
            "Brett R. Gordon"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Randomized experiments with treatment and control groups are an important\ntool to measure the impacts of interventions. However, in experimental settings\nwith one-sided noncompliance, extant empirical approaches may not produce the\nestimands a decision-maker needs to solve their problem of interest. For\nexample, these experimental designs are common in digital advertising settings,\nbut typical methods do not yield effects that inform the intensive margin --\nhow many consumers should be reached or how much should be spent on a campaign.\nWe propose a solution that combines a novel multi-cell experimental design with\nmodern estimation techniques that enables decision-makers to recover enough\ninformation to solve problems with an intensive margin. Our design is\nstraightforward to implement and does not require any additional budget to be\ncarried out. We illustrate our approach through a series of simulations that\nare calibrated using an advertising experiment at Facebook, finding that our\nmethod outperforms standard techniques in generating better decisions.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.13857v3"
    },
    {
        "title": "Forecasting Macroeconomic Tail Risk in Real Time: Do Textual Data Add\n  Value?",
        "authors": [
            "Philipp Adämmer",
            "Jan Prüser",
            "Rainer Schüssler"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We examine the incremental value of news-based data relative to the FRED-MD\neconomic indicators for quantile predictions of employment, output, inflation\nand consumer sentiment in a high-dimensional setting. Our results suggest that\nnews data contain valuable information that is not captured by a large set of\neconomic indicators. We provide empirical evidence that this information can be\nexploited to improve tail risk predictions. The added value is largest when\nmedia coverage and sentiment are combined to compute text-based predictors.\nMethods that capture quantile-specific non-linearities produce overall superior\nforecasts relative to methods that feature linear predictive relationships. The\nresults are robust along different modeling choices.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.13999v2"
    },
    {
        "title": "Macroeconomic Forecasting using Dynamic Factor Models: The Case of\n  Morocco",
        "authors": [
            "Daoui Marouane"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This article discusses the use of dynamic factor models in macroeconomic\nforecasting, with a focus on the Factor-Augmented Error Correction Model\n(FECM). The FECM combines the advantages of cointegration and dynamic factor\nmodels, providing a flexible and reliable approach to macroeconomic\nforecasting, especially for non-stationary variables. We evaluate the\nforecasting performance of the FECM model on a large dataset of 117 Moroccan\neconomic series with quarterly frequency. Our study shows that FECM outperforms\ntraditional econometric models in terms of forecasting accuracy and robustness.\nThe inclusion of long-term information and common factors in FECM enhances its\nability to capture economic dynamics and leads to better forecasting\nperformance than other competing models. Our results suggest that FECM can be a\nvaluable tool for macroeconomic forecasting in Morocco and other similar\neconomies.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.14180v3"
    },
    {
        "title": "Identification and Estimation of Categorical Random Coefficient Models",
        "authors": [
            "Zhan Gao",
            "M. Hashem Pesaran"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a linear categorical random coefficient model, in which\nthe random coefficients follow parametric categorical distributions. The\ndistributional parameters are identified based on a linear recurrence structure\nof moments of the random coefficients. A Generalized Method of Moments\nestimation procedure is proposed also employed by Peter Schmidt and his\ncoauthors to address heterogeneity in time effects in panel data models. Using\nMonte Carlo simulations, we find that moments of the random coefficients can be\nestimated reasonably accurately, but large samples are required for estimation\nof the parameters of the underlying categorical distribution. The utility of\nthe proposed estimator is illustrated by estimating the distribution of returns\nto education in the U.S. by gender and educational levels. We find that rising\nheterogeneity between educational groups is mainly due to the increasing\nreturns to education for those with postsecondary education, whereas within\ngroup heterogeneity has been rising mostly in the case of individuals with high\nschool or less education.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.14380v1"
    },
    {
        "title": "Unified and robust Lagrange multiplier type tests for cross-sectional\n  independence in large panel data models",
        "authors": [
            "Zhenhong Huang",
            "Zhaoyuan Li",
            "Jianfeng Yao"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper revisits the Lagrange multiplier type test for the null hypothesis\nof no cross-sectional dependence in large panel data models. We propose a\nunified test procedure and its power enhancement version, which show robustness\nfor a wide class of panel model contexts. Specifically, the two procedures are\napplicable to both heterogeneous and fixed effects panel data models with the\npresence of weakly exogenous as well as lagged dependent regressors, allowing\nfor a general form of nonnormal error distribution. With the tools from Random\nMatrix Theory, the asymptotic validity of the test procedures is established\nunder the simultaneous limit scheme where the number of time periods and the\nnumber of cross-sectional units go to infinity proportionally. The derived\ntheories are accompanied by detailed Monte Carlo experiments, which confirm the\nrobustness of the two tests and also suggest the validity of the power\nenhancement technique.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.14387v1"
    },
    {
        "title": "A specification test for the strength of instrumental variables",
        "authors": [
            "Zhenhong Huang",
            "Chen Wang",
            "Jianfeng Yao"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper develops a new specification test for the instrument weakness when\nthe number of instruments $K_n$ is large with a magnitude comparable to the\nsample size $n$. The test relies on the fact that the difference between the\ntwo-stage least squares (2SLS) estimator and the ordinary least squares (OLS)\nestimator asymptotically disappears when there are many weak instruments, but\notherwise converges to a non-zero limit. We establish the limiting distribution\nof the difference within the above two specifications, and introduce a\ndelete-$d$ Jackknife procedure to consistently estimate the asymptotic\nvariance/covariance of the difference. Monte Carlo experiments demonstrate the\ngood performance of the test procedure for both cases of single and multiple\nendogenous variables. Additionally, we re-examine the analysis of returns to\neducation data in Angrist and Keueger (1991) using our proposed test. Both the\nsimulation results and empirical analysis indicate the reliability of the test.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.14396v1"
    },
    {
        "title": "The First-stage F Test with Many Weak Instruments",
        "authors": [
            "Zhenhong Huang",
            "Chen Wang",
            "Jianfeng Yao"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  A widely adopted approach for detecting weak instruments is to use the\nfirst-stage $F$ statistic. While this method was developed with a fixed number\nof instruments, its performance with many instruments remains insufficiently\nexplored. We show that the first-stage $F$ test exhibits distorted sizes for\ndetecting many weak instruments, regardless of the choice of pretested\nestimators or Wald tests. These distortions occur due to the inadequate\napproximation using classical noncentral Chi-squared distributions. As a\nbyproduct of our main result, we present an alternative approach to pre-test\nmany weak instruments with the corrected first-stage $F$ statistic. An\nempirical illustration with Angrist and Keueger (1991)'s returns to education\ndata confirms its usefulness.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.14423v2"
    },
    {
        "title": "Transition Probabilities and Moment Restrictions in Dynamic Fixed\n  Effects Logit Models",
        "authors": [
            "Kevin Dano"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Dynamic logit models are popular tools in economics to measure state\ndependence. This paper introduces a new method to derive moment restrictions in\na large class of such models with strictly exogenous regressors and fixed\neffects. We exploit the common structure of logit-type transition probabilities\nand elementary properties of rational fractions, to formulate a systematic\nprocedure that scales naturally with model complexity (e.g the lag order or the\nnumber of observed time periods). We detail the construction of moment\nrestrictions in binary response models of arbitrary lag order as well as\nfirst-order panel vector autoregressions and dynamic multinomial logit models.\nIdentification of common parameters and average marginal effects is also\ndiscussed for the binary response case. Finally, we illustrate our results by\nstudying the dynamics of drug consumption amongst young people inspired by Deza\n(2015).\n",
        "pdf_link": "http://arxiv.org/pdf/2303.00083v2"
    },
    {
        "title": "Aggregated Intersection Bounds and Aggregated Minimax Values",
        "authors": [
            "Vira Semenova"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a novel framework of aggregated intersection bounds,\nwhere the target parameter is obtained by averaging the minimum (or maximum) of\na collection of regression functions over the covariate space. Examples of such\nquantities include the lower and upper bounds on distributional effects\n(Fr\\'echet-Hoeffding, Makarov) as well as the optimal welfare in statistical\ntreatment choice problems. The proposed estimator -- the envelope score\nestimator -- is shown to have an oracle property, where the oracle knows the\nidentity of the minimizer for each covariate value. Next, the result is\nextended to the aggregated minimax values of a collection of regression\nfunctions, covering optimal distributional welfare in worst-case and best-case,\nrespectively. This proposed estimator -- the envelope saddle value estimator --\nis shown to have an oracle property, where the oracle knows the identity of the\nsaddle point.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.00982v2"
    },
    {
        "title": "Constructing High Frequency Economic Indicators by Imputation",
        "authors": [
            "Serena Ng",
            "Susannah Scanlan"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Monthly and weekly economic indicators are often taken to be the largest\ncommon factor estimated from high and low frequency data, either separately or\njointly. To incorporate mixed frequency information without directly modeling\nthem, we target a low frequency diffusion index that is already available, and\ntreat high frequency values as missing. We impute these values using multiple\nfactors estimated from the high frequency data. In the empirical examples\nconsidered, static matrix completion that does not account for serial\ncorrelation in the idiosyncratic errors yields imprecise estimates of the\nmissing values irrespective of how the factors are estimated. Single equation\nand systems-based dynamic procedures that account for serial correlation yield\nimputed values that are closer to the observed low frequency ones. This is the\ncase in the counterfactual exercise that imputes the monthly values of consumer\nsentiment series before 1978 when the data was released only on a quarterly\nbasis. This is also the case for a weekly version of the CFNAI index of\neconomic activity that is imputed using seasonally unadjusted data. The imputed\nseries reveals episodes of increased variability of weekly economic information\nthat are masked by the monthly data, notably around the 2014-15 collapse in oil\nprices.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.01863v3"
    },
    {
        "title": "Censored Quantile Regression with Many Controls",
        "authors": [
            "Seoyun Hong"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper develops estimation and inference methods for censored quantile\nregression models with high-dimensional controls. The methods are based on the\napplication of double/debiased machine learning (DML) framework to the censored\nquantile regression estimator of Buchinsky and Hahn (1998). I provide valid\ninference for low-dimensional parameters of interest in the presence of\nhigh-dimensional nuisance parameters when implementing machine learning\nestimators. The proposed estimator is shown to be consistent and asymptotically\nnormal. The performance of the estimator with high-dimensional controls is\nillustrated with numerical simulation and an empirical application that\nexamines the effect of 401(k) eligibility on savings.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.02784v1"
    },
    {
        "title": "EnsembleIV: Creating Instrumental Variables from Ensemble Learners for\n  Robust Statistical Inference",
        "authors": [
            "Gordon Burtch",
            "Edward McFowland III",
            "Mochen Yang",
            "Gediminas Adomavicius"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Despite increasing popularity in empirical studies, the integration of\nmachine learning generated variables into regression models for statistical\ninference suffers from the measurement error problem, which can bias estimation\nand threaten the validity of inferences. In this paper, we develop a novel\napproach to alleviate associated estimation biases. Our proposed approach,\nEnsembleIV, creates valid and strong instrumental variables from weak learners\nin an ensemble model, and uses them to obtain consistent estimates that are\nrobust against the measurement error problem. Our empirical evaluations, using\nboth synthetic and real-world datasets, show that EnsembleIV can effectively\nreduce estimation biases across several common regression specifications, and\ncan be combined with modern deep learning techniques when dealing with\nunstructured data.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.02820v2"
    },
    {
        "title": "Identification of Ex ante Returns Using Elicited Choice Probabilities:\n  an Application to Preferences for Public-sector Jobs",
        "authors": [
            "Romuald Meango",
            "Esther Mirjam Girsberger"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Ex ante returns, the net value that agents perceive before they take an\ninvestment decision, are understood as the main drivers of individual\ndecisions. Hence, their distribution in a population is an important tool for\ncounterfactual analysis and policy evaluation. This paper studies the\nidentification of the population distribution of ex ante returns using stated\nchoice experiments, in the context of binary investment decisions. The\nenvironment is characterised by uncertainty about future outcomes, with some\nuncertainty being resolved over time. In this context, each individual holds a\nprobability distribution over different levels of returns. The paper provides\nnovel, nonparametric identification results for the population distribution of\nreturns, accounting for uncertainty. It complements these with a\nnonparametric/semiparametric estimation methodology, which is new to the\nstated-preference literature. Finally, it uses these results to study the\npreference of high ability students in Cote d'Ivoire for public-sector jobs and\nhow the competition for talent affects the expansion of the private sector.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.03009v2"
    },
    {
        "title": "Counterfactual Copula and Its Application to the Effects of College\n  Education on Intergenerational Mobility",
        "authors": [
            "Tsung-Chih Lai",
            "Jiun-Hua Su"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a nonparametric estimator of the counterfactual copula of\ntwo outcome variables that would be affected by a policy intervention. The\nproposed estimator allows policymakers to conduct ex-ante evaluations by\ncomparing the estimated counterfactual and actual copulas as well as their\ncorresponding measures of association. Asymptotic properties of the\ncounterfactual copula estimator are established under regularity conditions.\nThese conditions are also used to validate the nonparametric bootstrap for\ninference on counterfactual quantities. Simulation results indicate that our\nestimation and inference procedures perform well in moderately sized samples.\nApplying the proposed method to studying the effects of college education on\nintergenerational income mobility under two counterfactual scenarios, we find\nthat while providing some college education to all children is unlikely to\npromote mobility, offering a college degree to children from less educated\nfamilies can significantly reduce income persistence across generations.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.06658v1"
    },
    {
        "title": "Identification- and many instrument-robust inference via invariant\n  moment conditions",
        "authors": [
            "Tom Boot",
            "Johannes W. Ligtenberg"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Identification-robust hypothesis tests are commonly based on the continuous\nupdating objective function or its score. When the number of moment conditions\ngrows proportionally with the sample size, the large-dimensional weighting\nmatrix prohibits the use of conventional asymptotic approximations and the\nbehavior of these tests remains unknown. We show that the structure of the\nweighting matrix opens up an alternative route to asymptotic results when,\nunder the null hypothesis, the distribution of the moment conditions is\nreflection invariant. In a heteroskedastic linear instrumental variables model,\nwe then establish asymptotic normality of conventional tests statistics under\nmany instrument sequences. A key result is that the additional terms that\nappear in the variance are negative. Revisiting a study on the elasticity of\nsubstitution between immigrant and native workers where the number of\ninstruments is over a quarter of the sample size, the many instrument-robust\napproximation indeed leads to substantially narrower confidence intervals.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.07822v2"
    },
    {
        "title": "Identifying an Earnings Process With Dependent Contemporaneous Income\n  Shocks",
        "authors": [
            "Dan Ben-Moshe"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a novel approach for identifying coefficients in an\nearnings dynamics model with arbitrarily dependent contemporaneous income\nshocks. Traditional methods relying on second moments fail to identify these\ncoefficients, emphasizing the need for nongaussianity assumptions that capture\ninformation from higher moments. Our results contribute to the literature on\nearnings dynamics by allowing models of earnings to have, for example, the\npermanent income shock of a job change to be linked to the contemporaneous\ntransitory income shock of a relocation bonus.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.08460v2"
    },
    {
        "title": "Bootstrap based asymptotic refinements for high-dimensional nonlinear\n  models",
        "authors": [
            "Joel L. Horowitz",
            "Ahnaf Rafi"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We consider penalized extremum estimation of a high-dimensional, possibly\nnonlinear model that is sparse in the sense that most of its parameters are\nzero but some are not. We use the SCAD penalty function, which provides model\nselection consistent and oracle efficient estimates under suitable conditions.\nHowever, asymptotic approximations based on the oracle model can be inaccurate\nwith the sample sizes found in many applications. This paper gives conditions\nunder which the bootstrap, based on estimates obtained through SCAD\npenalization with thresholding, provides asymptotic refinements of size \\(O\n\\left( n^{- 2} \\right)\\) for the error in the rejection (coverage) probability\nof a symmetric hypothesis test (confidence interval) and \\(O \\left( n^{- 1}\n\\right)\\) for the error in the rejection (coverage) probability of a one-sided\nor equal tailed test (confidence interval). The results of Monte Carlo\nexperiments show that the bootstrap can provide large reductions in errors in\nrejection and coverage probabilities. The bootstrap is consistent, though it\ndoes not necessarily provide asymptotic refinements, even if some parameters\nare close but not equal to zero. Random-coefficients logit and probit models\nand nonlinear moment models are examples of models to which the procedure\napplies.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.09680v2"
    },
    {
        "title": "Standard errors when a regressor is randomly assigned",
        "authors": [
            "Denis Chetverikov",
            "Jinyong Hahn",
            "Zhipeng Liao",
            "Andres Santos"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We examine asymptotic properties of the OLS estimator when the values of the\nregressor of interest are assigned randomly and independently of other\nregressors. We find that the OLS variance formula in this case is often\nsimplified, sometimes substantially. In particular, when the regressor of\ninterest is independent not only of other regressors but also of the error\nterm, the textbook homoskedastic variance formula is valid even if the error\nterm and auxiliary regressors exhibit a general dependence structure. In the\ncontext of randomized controlled trials, this conclusion holds in completely\nrandomized experiments with constant treatment effects. When the error term is\nheteroscedastic with respect to the regressor of interest, the variance formula\nhas to be adjusted not only for heteroscedasticity but also for correlation\nstructure of the error term. However, even in the latter case, some\nsimplifications are possible as only a part of the correlation structure of the\nerror term should be taken into account. In the context of randomized control\ntrials, this implies that the textbook homoscedastic variance formula is\ntypically not valid if treatment effects are heterogenous but\nheteroscedasticity-robust variance formulas are valid if treatment effects are\nindependent across units, even if the error term exhibits a general dependence\nstructure. In addition, we extend the results to the case when the regressor of\ninterest is assigned randomly at a group level, such as in randomized control\ntrials with treatment assignment determined at a group (e.g., school/village)\nlevel.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10306v1"
    },
    {
        "title": "On the Existence and Information of Orthogonal Moments",
        "authors": [
            "Facundo Argañaraz",
            "Juan Carlos Escanciano"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Locally Robust (LR)/Orthogonal/Debiased moments have proven useful with\nmachine learning first steps, but their existence has not been investigated for\ngeneral parameters. In this paper, we provide a necessary and sufficient\ncondition, referred to as Restricted Local Non-surjectivity (RLN), for the\nexistence of such orthogonal moments to conduct robust inference on general\nparameters of interest in regular semiparametric models. Importantly, RLN does\nnot require either identification of the parameters of interest or the nuisance\nparameters. However, for orthogonal moments to be informative, the efficient\nFisher Information matrix for the parameter must be non-zero (though possibly\nsingular). Thus, orthogonal moments exist and are informative under more\ngeneral conditions than previously recognized. We demonstrate the utility of\nour general results by characterizing orthogonal moments in a class of models\nwith Unobserved Heterogeneity (UH). For this class of models our method\ndelivers functional differencing as a special case. Orthogonality for general\nsmooth functionals of the distribution of UH is also characterized. As a second\nmajor application, we investigate the existence of orthogonal moments and their\nrelevance for models defined by moment restrictions with possibly different\nconditioning variables. We find orthogonal moments for the fully saturated two\nstage least squares, for heterogeneous parameters in treatment effects, for\nsample selection models, and for popular models of demand for differentiated\nproducts. We apply our results to the Oregon Health Experiment to study\nheterogeneous treatment effects of Medicaid on different health outcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.11418v2"
    },
    {
        "title": "Using Forests in Multivariate Regression Discontinuity Designs",
        "authors": [
            "Yiqi Liu",
            "Yuan Qi"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We discuss estimation and inference of conditional treatment effects in\nregression discontinuity designs with multiple scores. Aside from the commonly\nused local linear regression approach and a minimax-optimal estimator recently\nproposed by Imbens and Wager (2019), we consider two estimators based on random\nforests -- honest regression forests and local linear forests -- whose\nconstruction resembles that of standard local regressions, with theoretical\nvalidity following from results in Wager and Athey (2018) and Friedberg et al.\n(2020). We design a systematic Monte Carlo study with data generating processes\nbuilt both from functional forms that we specify and from Wasserstein\nGenerative Adversarial Networks that can closely mimic the observed data. We\nfind that no single estimator dominates across all simulations: (i) local\nlinear regressions perform well in univariate settings, but can undercover when\nmultivariate scores are transformed into a univariate score -- which is\ncommonly done in practice -- possibly due to the \"zero-density\" issue of the\ncollapsed univariate score at the transformed cutoff; (ii) good performance of\nthe minimax-optimal estimator depends on accurate estimation of a nuisance\nparameter and its current implementation only accepts up to two scores; (iii)\nforest-based estimators are not designed for estimation at boundary points and\ncan suffer from bias in finite sample, but their flexibility in modeling\nmultivariate scores opens the door to a wide range of empirical applications in\nmultivariate regression discontinuity designs.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.11721v2"
    },
    {
        "title": "Don't (fully) exclude me, it's not necessary! Identification with\n  semi-IVs",
        "authors": [
            "Christophe Bruneel-Zupanc"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a novel tool to nonparametrically identify models with a\ndiscrete endogenous variable or treatment: semi-instrumental variables\n(semi-IVs). A semi-IV is a variable that is relevant but only partially\nexcluded from the potential outcomes, i.e., excluded from at least one, but not\nnecessarily all, potential outcome equations. It follows that standard\ninstrumental variables (IVs), which are fully excluded from all the potential\noutcomes, are a special (extreme) case of semi-IVs. I show that full exclusion\nis stronger than necessary because the same objects that are usually identified\nwith an IV (Imbens and Angrist, 1994; Heckman and Vytlacil, 2005; Chernozhukov\nand Hansen, 2005) can be identified with several semi-IVs instead, provided\nthere is (at least) one semi-IV excluded from each potential outcome. For\napplied work, tackling endogeneity with semi-IVs instead of IVs should be an\nattractive alternative, since semi-IVs are easier to find: most\nselection-specific costs or benefits can be valid semi-IVs, for example. The\npaper also provides a simple semi-IV GMM estimator for models with homogenous\ntreatment effects and uses it to estimate the returns to education.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.12667v2"
    },
    {
        "title": "Uncertain Short-Run Restrictions and Statistically Identified Structural\n  Vector Autoregressions",
        "authors": [
            "Sascha A. Keweloh"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This study proposes a combination of a statistical identification approach\nwith potentially invalid short-run zero restrictions. The estimator shrinks\ntowards imposed restrictions and stops shrinkage when the data provide evidence\nagainst a restriction. Simulation results demonstrate how incorporating valid\nrestrictions through the shrinkage approach enhances the accuracy of the\nstatistically identified estimator and how the impact of invalid restrictions\ndecreases with the sample size. The estimator is applied to analyze the\ninteraction between the stock and oil market. The results indicate that\nincorporating stock market data into the analysis is crucial, as it enables the\nidentification of information shocks, which are shown to be important drivers\nof the oil price.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.13281v2"
    },
    {
        "title": "Sequential Cauchy Combination Test for Multiple Testing Problems with\n  Financial Applications",
        "authors": [
            "Nabil Bouamara",
            "Sébastien Laurent",
            "Shuping Shi"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We introduce a simple tool to control for false discoveries and identify\nindividual signals in scenarios involving many tests, dependent test\nstatistics, and potentially sparse signals. The tool applies the Cauchy\ncombination test recursively on a sequence of expanding subsets of $p$-values\nand is referred to as the sequential Cauchy combination test. While the\noriginal Cauchy combination test aims to make a global statement about a set of\nnull hypotheses by summing transformed $p$-values, our sequential version\ndetermines which $p$-values trigger the rejection of the global null. The\nsequential test achieves strong familywise error rate control, exhibits less\nconservatism compared to existing controlling procedures when dealing with\ndependent test statistics, and provides a power boost. As illustrations, we\nrevisit two well-known large-scale multiple testing problems in finance for\nwhich the test statistics have either serial dependence or cross-sectional\ndependence, namely monitoring drift bursts in asset prices and searching for\nassets with a nonzero alpha. In both applications, the sequential Cauchy\ncombination test proves to be a preferable alternative. It overcomes many of\nthe drawbacks inherent to inequality-based controlling procedures, extreme\nvalue approaches, resampling and screening methods, and it improves the power\nin simulations, leading to distinct empirical outcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.13406v2"
    },
    {
        "title": "Point Identification of LATE with Two Imperfect Instruments",
        "authors": [
            "Rui Wang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper characterizes point identification results of the local average\ntreatment effect (LATE) using two imperfect instruments. The classical approach\n(Imbens and Angrist (1994)) establishes the identification of LATE via an\ninstrument that satisfies exclusion, monotonicity, and independence. However,\nit may be challenging to find a single instrument that satisfies all these\nassumptions simultaneously. My paper uses two instruments but imposes weaker\nassumptions on both instruments. The first instrument is allowed to violate the\nexclusion restriction and the second instrument does not need to satisfy\nmonotonicity. Therefore, the first instrument can affect the outcome via both\ndirect effects and a shift in the treatment status. The direct effects can be\nidentified via exogenous variation in the second instrument and therefore the\nlocal average treatment effect is identified. An estimator is proposed, and\nusing Monte Carlo simulations, it is shown to perform more robustly than the\ninstrumental variable estimand.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.13795v1"
    },
    {
        "title": "Sensitivity Analysis in Unconditional Quantile Effects",
        "authors": [
            "Julian Martinez-Iriarte"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a framework to analyze the effects of counterfactual\npolicies on the unconditional quantiles of an outcome variable. For a given\ncounterfactual policy, we obtain identified sets for the effect of both\nmarginal and global changes in the proportion of treated individuals. To\nconduct a sensitivity analysis, we introduce the quantile breakdown frontier, a\ncurve that (i) indicates whether a sensitivity analysis is possible or not, and\n(ii) when a sensitivity analysis is possible, quantifies the amount of\nselection bias consistent with a given conclusion of interest across different\nquantiles. To illustrate our method, we perform a sensitivity analysis on the\neffect of unionizing low income workers on the quantiles of the distribution of\n(log) wages.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.14298v3"
    },
    {
        "title": "Under-Identification of Structural Models Based on Timing and\n  Information Set Assumptions",
        "authors": [
            "Daniel Ackerberg",
            "Garth Frazer",
            "Kyoo il Kim",
            "Yao Luo",
            "Yingjun Su"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We revisit identification based on timing and information set assumptions in\nstructural models, which have been used in the context of production functions,\ndemand equations, and hedonic pricing models (e.g. Olley and Pakes (1996),\nBlundell and Bond (2000)). First, we demonstrate a general under-identification\nproblem using these assumptions in a simple version of the Blundell-Bond\ndynamic panel model. In particular, the basic moment conditions can yield\nmultiple discrete solutions: one at the persistence parameter in the main\nequation and another at the persistence parameter governing the regressor. We\nthen show that the problem can persist in a broader set of models but\ndisappears in models under stronger timing assumptions. We then propose\npossible solutions in the simple setting by enforcing an assumed sign\nrestriction and conclude by using lessons from our basic identification\napproach to propose more general practical advice for empirical researchers.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.15170v1"
    },
    {
        "title": "IV Regressions without Exclusion Restrictions",
        "authors": [
            "Wayne Yuan Gao",
            "Rui Wang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We study identification and estimation of endogenous linear and nonlinear\nregression models without excluded instrumental variables, based on the\nstandard mean independence condition and a nonlinear relevance condition. Based\non the identification results, we propose two semiparametric estimators as well\nas a discretization-based estimator that does not require any nonparametric\nregressions. We establish their asymptotic normality and demonstrate via\nsimulations their robust finite-sample performances with respect to exclusion\nrestrictions violations and endogeneity. Our approach is applied to study the\nreturns to education, and to test the direct effects of college proximity\nindicators as well as family background variables on the outcome.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.00626v3"
    },
    {
        "title": "Testing and Identifying Substitution and Complementarity Patterns",
        "authors": [
            "Rui Wang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies semiparametric identification of substitution and\ncomplementarity patterns between two goods using a panel multinomial choice\nmodel with bundles. The model allows the two goods to be either substitutes or\ncomplements and admits heterogeneous complementarity through observed\ncharacteristics. I first provide testable implications for the complementarity\nrelationship between goods. I then characterize the sharp identified set for\nthe model parameters and provide sufficient conditions for point\nidentification. The identification analysis accommodates endogenous covariates\nthrough flexible dependence structures between observed characteristics and\nfixed effects while placing no distributional assumptions on unobserved\npreference shocks. My method is shown to perform more robustly than the\nparametric method through Monte Carlo simulations. As an extension, I allow for\nunobserved heterogeneity in the complementarity, investigate scenarios\ninvolving more than two goods, and study a class of nonseparable utility\nfunctions.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.00678v1"
    },
    {
        "title": "Heterogeneity-robust granular instruments",
        "authors": [
            "Eric Qian"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Granular instrumental variables (GIV) has experienced sharp growth in\nempirical macro-finance. The methodology's rise showcases granularity's\npotential for identification across many economic environments, like the\nestimation of spillovers and demand systems. I propose a new estimator--called\nrobust granular instrumental variables (RGIV)--that enables studying unit-level\nheterogeneity in spillovers. Unlike existing methods that assume heterogeneity\nis a function of observables, RGIV leaves heterogeneity unrestricted. In\ncontrast to the baseline GIV estimator, RGIV allows for unknown shock variances\nand equal-sized units. Applied to the Euro area, I find strong evidence of\ncountry-level heterogeneity in sovereign yield spillovers.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.01273v3"
    },
    {
        "title": "Individual Welfare Analysis: Random Quasilinear Utility, Independence,\n  and Confidence Bounds",
        "authors": [
            "Junlong Feng",
            "Sokbae Lee"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We introduce a novel framework for individual-level welfare analysis. It\nbuilds on a parametric model for continuous demand with a quasilinear utility\nfunction, allowing for heterogeneous coefficients and unobserved\nindividual-good-level preference shocks. We obtain bounds on the\nindividual-level consumer welfare loss at any confidence level due to a\nhypothetical price increase, solving a scalable optimization problem\nconstrained by a novel confidence set under an independence restriction. This\nconfidence set is computationally simple and robust to weak instruments,\nnonlinearity, and partial identification. The validity of the confidence set is\nguaranteed by our new results on the joint limiting distribution of the\nindependence test by Chatterjee (2021). These results together with the\nconfidence set may have applications beyond welfare analysis. Monte Carlo\nsimulations and two empirical applications on gasoline and food demand\ndemonstrate the effectiveness of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.01921v4"
    },
    {
        "title": "Faster estimation of dynamic discrete choice models using index\n  invertibility",
        "authors": [
            "Jackson Bunting",
            "Takuya Ura"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Many estimators of dynamic discrete choice models with persistent unobserved\nheterogeneity have desirable statistical properties but are computationally\nintensive. In this paper we propose a method to quicken estimation for a broad\nclass of dynamic discrete choice problems by exploiting semiparametric index\nrestrictions. Specifically, we propose an estimator for models whose reduced\nform parameters are invertible functions of one or more linear indices (Ahn,\nIchimura, Powell and Ruud 2018), a property we term index invertibility. We\nestablish that index invertibility implies a set of equality constraints on the\nmodel parameters. Our proposed estimator uses the equality constraints to\ndecrease the dimension of the optimization problem, thereby generating\ncomputational gains. Our main result shows that the proposed estimator is\nasymptotically equivalent to the unconstrained, computationally heavy\nestimator. In addition, we provide a series of results on the number of\nindependent index restrictions on the model parameters, providing theoretical\nguidance on the extent of computational gains. Finally, we demonstrate the\nadvantages of our approach via Monte Carlo simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.02171v3"
    },
    {
        "title": "Predictive Incrementality by Experimentation (PIE) for Ad Measurement",
        "authors": [
            "Brett R. Gordon",
            "Robert Moakler",
            "Florian Zettelmeyer"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We present a novel approach to causal measurement for advertising, namely to\nuse exogenous variation in advertising exposure (RCTs) for a subset of ad\ncampaigns to build a model that can predict the causal effect of ad campaigns\nthat were run without RCTs. This approach -- Predictive Incrementality by\nExperimentation (PIE) -- frames the task of estimating the causal effect of an\nad campaign as a prediction problem, with the unit of observation being an RCT\nitself. In contrast, traditional causal inference approaches with observational\ndata seek to adjust covariate imbalance at the user level. A key insight is to\nuse post-campaign features, such as last-click conversion counts, that do not\nrequire an RCT, as features in our predictive model. We find that our PIE model\nrecovers RCT-derived incremental conversions per dollar (ICPD) much better than\nthe program evaluation approaches analyzed in Gordon et al. (forthcoming). The\nprediction errors from the best PIE model are 48%, 42%, and 62% of the\nRCT-based average ICPD for upper-, mid-, and lower-funnel conversion outcomes,\nrespectively. In contrast, across the same data, the average prediction error\nof stratified propensity score matching exceeds 491%, and that of\ndouble/debiased machine learning exceeds 2,904%. Using a decision-making\nframework inspired by industry, we show that PIE leads to different decisions\ncompared to RCTs for only 6% of upper-funnel, 7% of mid-funnel, and 13% of\nlower-funnel outcomes. We conclude that PIE could enable advertising platforms\nto scale causal ad measurement by extrapolating from a limited number of RCTs\nto a large set of non-experimental ad campaigns.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.06828v1"
    },
    {
        "title": "Coarsened Bayesian VARs -- Correcting BVARs for Incorrect Specification",
        "authors": [
            "Florian Huber",
            "Massimiliano Marcellino"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Model mis-specification in multivariate econometric models can strongly\ninfluence quantities of interest such as structural parameters, forecast\ndistributions or responses to structural shocks, even more so if higher-order\nforecasts or responses are considered, due to parameter convolution. We propose\na simple method for addressing these specification issues in the context of\nBayesian VARs. Our method, called coarsened Bayesian VARs (cBVARs), replaces\nthe exact likelihood with a coarsened likelihood that takes into account that\nthe model might be mis-specified along important but unknown dimensions.\nCoupled with a conjugate prior, this results in a computationally simple model.\nAs opposed to more flexible specifications, our approach avoids overfitting, is\nsimple to implement and estimation is fast. The resulting cBVAR performs well\nin simulations for several types of mis-specification. Applied to US data,\ncBVARs improve point and density forecasts compared to standard BVARs, and lead\nto milder but more persistent negative effects of uncertainty shocks on output.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.07856v2"
    },
    {
        "title": "A hybrid model for day-ahead electricity price forecasting: Combining\n  fundamental and stochastic modelling",
        "authors": [
            "Mira Watermeyer",
            "Thomas Möbius",
            "Oliver Grothe",
            "Felix Müsgens"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The accurate prediction of short-term electricity prices is vital for\neffective trading strategies, power plant scheduling, profit maximisation and\nefficient system operation. However, uncertainties in supply and demand make\nsuch predictions challenging. We propose a hybrid model that combines a\ntechno-economic energy system model with stochastic models to address this\nchallenge. The techno-economic model in our hybrid approach provides a deep\nunderstanding of the market. It captures the underlying factors and their\nimpacts on electricity prices, which is impossible with statistical models\nalone. The statistical models incorporate non-techno-economic aspects, such as\nthe expectations and speculative behaviour of market participants, through the\ninterpretation of prices. The hybrid model generates both conventional point\npredictions and probabilistic forecasts, providing a comprehensive\nunderstanding of the market landscape. Probabilistic forecasts are particularly\nvaluable because they account for market uncertainty, facilitating informed\ndecision-making and risk management. Our model delivers state-of-the-art\nresults, helping market participants to make informed decisions and operate\ntheir systems more efficiently.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09336v1"
    },
    {
        "title": "The Impact of Industrial Zone:Evidence from China's National High-tech\n  Zone Policy",
        "authors": [
            "Li Han"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Based on the statistical yearbook data and related patent data of 287 cities\nin China from 2000 to 2020, this study regards the policy of establishing the\nnational high-tech zones as a quasi-natural experiment. Using this experiment,\nthis study firstly estimated the treatment effect of the policy and checked the\nrobustness of the estimation. Then the study examined the heterogeneity in\ndifferent geographic demarcation of China and in different city level of China.\nAfter that, this study explored the possible influence mechanism of the policy.\nIt shows that the possible mechanism of the policy is financial support,\nindustrial agglomeration of secondary industry and the spillovers. In the end,\nthis study examined the spillovers deeply and showed the distribution of\nspillover effect.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09775v1"
    },
    {
        "title": "The Ordinary Least Eigenvalues Estimator",
        "authors": [
            "Yassine Sbai Sassi"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We propose a rate optimal estimator for the linear regression model on\nnetwork data with interacted (unobservable) individual effects. The estimator\nachieves a faster rate of convergence $N$ compared to the standard estimators'\n$\\sqrt{N}$ rate and is efficient in cases that we discuss. We observe that the\nindividual effects alter the eigenvalue distribution of the data's matrix\nrepresentation in significant and distinctive ways. We subsequently offer a\ncorrection for the \\textit{ordinary least squares}' objective function to\nattenuate the statistical noise that arises due to the individual effects, and\nin some cases, completely eliminate it. The new estimator is asymptotically\nnormal and we provide a valid estimator for its asymptotic covariance matrix.\nWhile this paper only considers models accounting for first-order interactions\nbetween individual effects, our estimation procedure is naturally extendable to\nhigher-order interactions and more general specifications of the error terms.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.12554v1"
    },
    {
        "title": "Enhanced multilayer perceptron with feature selection and grid search\n  for travel mode choice prediction",
        "authors": [
            "Li Tang",
            "Chuanli Tang",
            "Qi Fu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Accurate and reliable prediction of individual travel mode choices is crucial\nfor developing multi-mode urban transportation systems, conducting\ntransportation planning and formulating traffic demand management strategies.\nTraditional discrete choice models have dominated the modelling methods for\ndecades yet suffer from strict model assumptions and low prediction accuracy.\nIn recent years, machine learning (ML) models, such as neural networks and\nboosting models, are widely used by researchers for travel mode choice\nprediction and have yielded promising results. However, despite the superior\nprediction performance, a large body of ML methods, especially the branch of\nneural network models, is also limited by overfitting and tedious model\nstructure determination process. To bridge this gap, this study proposes an\nenhanced multilayer perceptron (MLP; a neural network) with two hidden layers\nfor travel mode choice prediction; this MLP is enhanced by XGBoost (a boosting\nmethod) for feature selection and a grid search method for optimal hidden\nneurone determination of each hidden layer. The proposed method was trained and\ntested on a real resident travel diary dataset collected in Chengdu, China.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.12698v2"
    },
    {
        "title": "Common Correlated Effects Estimation of Nonlinear Panel Data Models",
        "authors": [
            "Liang Chen",
            "Minyuan Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper focuses on estimating the coefficients and average partial effects\nof observed regressors in nonlinear panel data models with interactive fixed\neffects, using the common correlated effects (CCE) framework. The proposed\ntwo-step estimation method involves applying principal component analysis to\nestimate latent factors based on cross-sectional averages of the regressors in\nthe first step, and jointly estimating the coefficients of the regressors and\nfactor loadings in the second step. The asymptotic distributions of the\nproposed estimators are derived under general conditions, assuming that the\nnumber of time-series observations is comparable to the number of\ncross-sectional observations. To correct for asymptotic biases of the\nestimators, we introduce both analytical and split-panel jackknife methods, and\nconfirm their good performance in finite samples using Monte Carlo simulations.\nAn empirical application utilizes the proposed method to study the arbitrage\nbehaviour of nonfinancial firms across different security markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.13199v1"
    },
    {
        "title": "Estimation of Characteristics-based Quantile Factor Models",
        "authors": [
            "Liang Chen",
            "Juan Jose Dolado",
            "Jesus Gonzalo",
            "Haozi Pan"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies the estimation of characteristic-based quantile factor\nmodels where the factor loadings are unknown functions of observed individual\ncharacteristics while the idiosyncratic error terms are subject to conditional\nquantile restrictions. We propose a three-stage estimation procedure that is\neasily implementable in practice and has nice properties. The convergence\nrates, the limiting distributions of the estimated factors and loading\nfunctions, and a consistent selection criterion for the number of factors at\neach quantile are derived under general conditions. The proposed estimation\nmethodology is shown to work satisfactorily when: (i) the idiosyncratic errors\nhave heavy tails, (ii) the time dimension of the panel dataset is not large,\nand (iii) the number of factors exceeds the number of characteristics. Finite\nsample simulations and an empirical application aimed at estimating the loading\nfunctions of the daily returns of a large panel of S\\&P500 index securities\nhelp illustrate these properties.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.13206v1"
    },
    {
        "title": "Difference-in-Differences with Compositional Changes",
        "authors": [
            "Pedro H. C. Sant'Anna",
            "Qi Xu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies difference-in-differences (DiD) setups with repeated\ncross-sectional data and potential compositional changes across time periods.\nWe begin our analysis by deriving the efficient influence function and the\nsemiparametric efficiency bound for the average treatment effect on the treated\n(ATT). We introduce nonparametric estimators that attain the semiparametric\nefficiency bound under mild rate conditions on the estimators of the nuisance\nfunctions, exhibiting a type of rate doubly-robust (DR) property. Additionally,\nwe document a trade-off related to compositional changes: We derive the\nasymptotic bias of DR DiD estimators that erroneously exclude compositional\nchanges and the efficiency loss when one fails to correctly rule out\ncompositional changes. We propose a nonparametric Hausman-type test for\ncompositional changes based on these trade-offs. The finite sample performance\nof the proposed DiD tools is evaluated through Monte Carlo experiments and an\nempirical application. As a by-product of our analysis, we present a new\nuniform stochastic expansion of the local polynomial multinomial logit\nestimator, which may be of independent interest.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.13925v1"
    },
    {
        "title": "Assessing Text Mining and Technical Analyses on Forecasting Financial\n  Time Series",
        "authors": [
            "Ali Lashgari"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Forecasting financial time series (FTS) is an essential field in finance and\neconomics that anticipates market movements in financial markets. This paper\ninvestigates the accuracy of text mining and technical analyses in forecasting\nfinancial time series. It focuses on the S&P500 stock market index during the\npandemic, which tracks the performance of the largest publicly traded companies\nin the US. The study compares two methods of forecasting the future price of\nthe S&P500: text mining, which uses NLP techniques to extract meaningful\ninsights from financial news, and technical analysis, which uses historical\nprice and volume data to make predictions. The study examines the advantages\nand limitations of both methods and analyze their performance in predicting the\nS&P500. The FinBERT model outperforms other models in terms of S&P500 price\nprediction, as evidenced by its lower RMSE value, and has the potential to\nrevolutionize financial analysis and prediction using financial news data.\nKeywords: ARIMA, BERT, FinBERT, Forecasting Financial Time Series, GARCH, LSTM,\nTechnical Analysis, Text Mining JEL classifications: G4, C8\n",
        "pdf_link": "http://arxiv.org/pdf/2304.14544v1"
    },
    {
        "title": "Estimation and Inference in Threshold Predictive Regression Models with\n  Locally Explosive Regressors",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this paper, we study the estimation of the threshold predictive regression\nmodel with hybrid stochastic local unit root predictors. We demonstrate the\nestimation procedure and derive the asymptotic distribution of the least square\nestimator and the IV based estimator proposed by Magdalinos and Phillips\n(2009), under the null hypothesis of a diminishing threshold effect. Simulation\nexperiments focus on the finite sample performance of our proposed estimators\nand the corresponding predictability tests as in Gonzalo and Pitarakis (2012),\nunder the presence of threshold effects with stochastic local unit roots. An\nempirical application to stock return equity indices, illustrate the usefulness\nof our framework in uncovering regimes of predictability during certain\nperiods. In particular, we focus on an aspect not previously examined in the\npredictability literature, that is, the effect of economic policy uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.00860v3"
    },
    {
        "title": "Estimating Input Coefficients for Regional Input-Output Tables Using\n  Deep Learning with Mixup",
        "authors": [
            "Shogo Fukui"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  An input-output table is an important data for analyzing the economic\nsituation of a region. Generally, the input-output table for each region\n(regional input-output table) in Japan is not always publicly available, so it\nis necessary to estimate the table. In particular, various methods have been\ndeveloped for estimating input coefficients, which are an important part of the\ninput-output table. Currently, non-survey methods are often used to estimate\ninput coefficients because they require less data and computation, but these\nmethods have some problems, such as discarding information and requiring\nadditional data for estimation.\n  In this study, the input coefficients are estimated by approximating the\ngeneration process with an artificial neural network (ANN) to mitigate the\nproblems of the non-survey methods and to estimate the input coefficients with\nhigher precision. To avoid over-fitting due to the small data used, data\naugmentation, called mixup, is introduced to increase the data size by\ngenerating virtual regions through region composition and scaling.\n  By comparing the estimates of the input coefficients with those of Japan as a\nwhole, it is shown that the accuracy of the method of this research is higher\nand more stable than that of the conventional non-survey methods. In addition,\nthe estimated input coefficients for the three cities in Japan are generally\nclose to the published values for each city.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.01201v3"
    },
    {
        "title": "Transfer Estimates for Causal Effects across Heterogeneous Sites",
        "authors": [
            "Konrad Menzel"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We consider the problem of extrapolating treatment effects across\nheterogeneous populations (``sites\"/``contexts\"). We consider an idealized\nscenario in which the researcher observes cross-sectional data for a large\nnumber of units across several ``experimental\" sites in which an intervention\nhas already been implemented to a new ``target\" site for which a baseline\nsurvey of unit-specific, pre-treatment outcomes and relevant attributes is\navailable. Our approach treats the baseline as functional data, and this choice\nis motivated by the observation that unobserved site-specific confounders\nmanifest themselves not only in average levels of outcomes, but also how these\ninteract with observed unit-specific attributes. We consider the problem of\ndetermining the optimal finite-dimensional feature space in which to solve that\nprediction problem. Our approach is design-based in the sense that the\nperformance of the predictor is evaluated given the specific, finite selection\nof experimental and target sites. Our approach is nonparametric, and our formal\nresults concern the construction of an optimal basis of predictors as well as\nconvergence rates for the estimated conditional average treatment effect\nrelative to the constrained-optimal population predictor for the target site.\nWe quantify the potential gains from adapting experimental estimates to a\ntarget location in an application to conditional cash transfer (CCT) programs\nusing a combined data set from five multi-site randomized controlled trials.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.01435v6"
    },
    {
        "title": "Large Global Volatility Matrix Analysis Based on Observation Structural\n  Information",
        "authors": [
            "Sung Hoon Choi",
            "Donggyu Kim"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this paper, we develop a novel large volatility matrix estimation\nprocedure for analyzing global financial markets. Practitioners often use\nlower-frequency data, such as weekly or monthly returns, to address the issue\nof different trading hours in the international financial market. However, this\napproach can lead to inefficiency due to information loss. To mitigate this\nproblem, our proposed method, called Structured Principal Orthogonal complEment\nThresholding (Structured-POET), incorporates observation structural information\nfor both global and national factor models. We establish the asymptotic\nproperties of the Structured-POET estimator, and also demonstrate the drawbacks\nof conventional covariance matrix estimation procedures when using\nlower-frequency data. Finally, we apply the Structured-POET estimator to an\nout-of-sample portfolio allocation study using international stock market data.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.01464v3"
    },
    {
        "title": "Debiased Inference for Dynamic Nonlinear Panels with Multi-dimensional\n  Heterogeneities",
        "authors": [
            "Xuan Leng",
            "Jiaming Mao",
            "Yutao Sun"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We introduce a generic class of dynamic nonlinear heterogeneous parameter\nmodels that incorporate individual and time effects in both the intercept and\nslope. To address the incidental parameter problem inherent in this class of\nmodels, we develop an analytical bias correction procedure to construct a\nbias-corrected likelihood. The resulting maximum likelihood estimators are\nautomatically bias-corrected. Moreover, likelihood-based tests statistics --\nincluding likelihood-ratio, Lagrange-multiplier, and Wald tests -- follow the\nlimiting chi-square distribution under the null hypothesis. Simulations\ndemonstrate the effectiveness of the proposed correction method, and an\nempirical application on the labor force participation of single mothers\nunderscores its practical importance.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03134v3"
    },
    {
        "title": "Does Principal Component Analysis Preserve the Sparsity in Sparse Weak\n  Factor Models?",
        "authors": [
            "Jie Wei",
            "Yonghui Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies the principal component (PC) method-based estimation of\nweak factor models with sparse loadings. We uncover an intrinsic near-sparsity\npreservation property for the PC estimators of loadings, which comes from the\napproximately upper triangular (block) structure of the rotation matrix. It\nimplies an asymmetric relationship among factors: the rotated loadings for a\nstronger factor can be contaminated by those from a weaker one, but the\nloadings for a weaker factor is almost free of the impact of those from a\nstronger one. More importantly, the finding implies that there is no need to\nuse complicated penalties to sparsify the loading estimators. Instead, we adopt\na simple screening method to recover the sparsity and construct estimators for\nvarious factor strengths. In addition, for sparse weak factor models, we\nprovide a singular value thresholding-based approach to determine the number of\nfactors and establish uniform convergence rates for PC estimators, which\ncomplement Bai and Ng (2023). The accuracy and efficiency of the proposed\nestimators are investigated via Monte Carlo simulations. The application to the\nFRED-QD dataset reveals the underlying factor strengths and loading sparsity as\nwell as their dynamic features.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.05934v2"
    },
    {
        "title": "Efficient Semiparametric Estimation of Average Treatment Effects Under\n  Covariate Adaptive Randomization",
        "authors": [
            "Ahnaf Rafi"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Experiments that use covariate adaptive randomization (CAR) are commonplace\nin applied economics and other fields. In such experiments, the experimenter\nfirst stratifies the sample according to observed baseline covariates and then\nassigns treatment randomly within these strata so as to achieve balance\naccording to pre-specified stratum-specific target assignment proportions. In\nthis paper, we compute the semiparametric efficiency bound for estimating the\naverage treatment effect (ATE) in such experiments with binary treatments\nallowing for the class of CAR procedures considered in Bugni, Canay, and Shaikh\n(2018, 2019). This is a broad class of procedures and is motivated by those\nused in practice. The stratum-specific target proportions play the role of the\npropensity score conditional on all baseline covariates (and not just the\nstrata) in these experiments. Thus, the efficiency bound is a special case of\nthe bound in Hahn (1998), but conditional on all baseline covariates.\nAdditionally, this efficiency bound is shown to be achievable under the same\nconditions as those used to derive the bound by using a cross-fitted\nNadaraya-Watson kernel estimator to form nonparametric regression adjustments.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.08340v1"
    },
    {
        "title": "Hierarchical DCC-HEAVY Model for High-Dimensional Covariance Matrices",
        "authors": [
            "Emilija Dzuverovic",
            "Matteo Barigozzi"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We introduce a HD DCC-HEAVY class of hierarchical-type factor models for\nhigh-dimensional covariance matrices, employing the realized measures built\nfrom higher-frequency data. The modelling approach features straightforward\nestimation and forecasting schemes, independent of the cross-sectional\ndimension of the assets under consideration, and accounts for sophisticated\nasymmetric dynamics in the covariances. Empirical analyses suggest that the HD\nDCC-HEAVY models have a better in-sample fit and deliver statistically and\neconomically significant out-of-sample gains relative to the existing\nhierarchical factor model and standard benchmarks. The results are robust under\ndifferent frequencies and market conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.08488v2"
    },
    {
        "title": "Semiparametrically Optimal Cointegration Test",
        "authors": [
            "Bo Zhou"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper aims to address the issue of semiparametric efficiency for\ncointegration rank testing in finite-order vector autoregressive models, where\nthe innovation distribution is considered an infinite-dimensional nuisance\nparameter. Our asymptotic analysis relies on Le Cam's theory of limit\nexperiment, which in this context takes the form of Locally Asymptotically\nBrownian Functional (LABF). By leveraging the structural version of LABF, an\nOrnstein-Uhlenbeck experiment, we develop the asymptotic power envelopes of\nasymptotically invariant tests for both cases with and without a time trend. We\npropose feasible tests based on a nonparametrically estimated density and\ndemonstrate that their power can achieve the semiparametric power envelopes,\nmaking them semiparametrically optimal. We validate the theoretical results\nthrough large-sample simulations and illustrate satisfactory size control and\nexcellent power performance of our tests under small samples. In both cases\nwith and without time trend, we show that a remarkable amount of additional\npower can be obtained from non-Gaussian distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.08880v1"
    },
    {
        "title": "Identification and Estimation of Production Function with Unobserved\n  Heterogeneity",
        "authors": [
            "Hiroyuki Kasahara",
            "Paul Schrimpf",
            "Michio Suzuki"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper examines the nonparametric identifiability of production\nfunctions, considering firm heterogeneity beyond Hicks-neutral technology\nterms. We propose a finite mixture model to account for unobserved\nheterogeneity in production technology and productivity growth processes. Our\nanalysis demonstrates that the production function for each latent type can be\nnonparametrically identified using four periods of panel data, relying on\nassumptions similar to those employed in existing literature on production\nfunction and panel data identification. By analyzing Japanese plant-level panel\ndata, we uncover significant disparities in estimated input elasticities and\nproductivity growth processes among latent types within narrowly defined\nindustries. We further show that neglecting unobserved heterogeneity in input\nelasticities may lead to substantial and systematic bias in the estimation of\nproductivity growth.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.12067v1"
    },
    {
        "title": "Using Limited Trial Evidence to Credibly Choose Treatment Dosage when\n  Efficacy and Adverse Effects Weakly Increase with Dose",
        "authors": [
            "Charles F. Manski"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In medical treatment and elsewhere, it has become standard to base treatment\nintensity (dosage) on evidence in randomized trials. Yet it has been rare to\nstudy how outcomes vary with dosage. In trials to obtain drug approval, the\nnorm has been to specify some dose of a new drug and compare it with an\nestablished therapy or placebo. Design-based trial analysis views each trial\narm as qualitatively different, but it may be highly credible to assume that\nefficacy and adverse effects (AEs) weakly increase with dosage. Optimization of\npatient care requires joint attention to both, as well as to treatment cost.\nThis paper develops methodology to credibly use limited trial evidence to\nchoose dosage when efficacy and AEs weakly increase with dose. I suppose that\ndosage is an integer choice t in (0, 1, . . . , T), T being a specified maximum\ndose. I study dosage choice when trial evidence on outcomes is available for\nonly K dose levels, where K < T + 1. Then the population distribution of dose\nresponse is partially rather than point identified. The identification region\nis a convex polygon determined by linear equalities and inequalities. I\ncharacterize clinical and public-health decision making using the\nminimax-regret criterion. A simple analytical solution exists when T = 2 and\ncomputation is tractable when T is larger.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.17206v1"
    },
    {
        "title": "Estimating overidentified linear models with heteroskedasticity and\n  outliers",
        "authors": [
            "Lei Bill Wang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  A large degree of overidentification causes severe bias in TSLS. A\nconventional heuristic rule used to motivate new estimators in this context is\napproximate bias. This paper formalizes the definition of approximate bias and\nexpands the applicability of approximate bias to various classes of estimators\nthat bridge OLS, TSLS, and Jackknife IV estimators (JIVEs). By evaluating their\napproximate biases, I propose new approximately unbiased estimators, including\nUOJIVE1 and UOJIVE2. UOJIVE1 can be interpreted as a generalization of an\nexisting estimator UIJIVE1. Both UOJIVEs are proven to be consistent and\nasymptotically normal under a fixed number of instruments and controls. The\nasymptotic proofs for UOJIVE1 in this paper require the absence of high\nleverage points, whereas proofs for UOJIVE2 do not. In addition, UOJIVE2 is\nconsistent under many-instrument asymptotic. The simulation results align with\nthe theorems in this paper: (i) Both UOJIVEs perform well under many instrument\nscenarios with or without heteroskedasticity, (ii) When a high leverage point\ncoincides with a high variance of the error term, an outlier is generated and\nthe performance of UOJIVE1 is much poorer than that of UOJIVE2.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.17615v5"
    },
    {
        "title": "Time-Varying Vector Error-Correction Models: Estimation and Inference",
        "authors": [
            "Jiti Gao",
            "Bin Peng",
            "Yayi Yan"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper considers a time-varying vector error-correction model that allows\nfor different time series behaviours (e.g., unit-root and locally stationary\nprocesses) to interact with each other to co-exist. From practical\nperspectives, this framework can be used to estimate shifts in the\npredictability of non-stationary variables, test whether economic theories hold\nperiodically, etc. We first develop a time-varying Granger Representation\nTheorem, which facilitates the establishment of asymptotic properties for the\nmodel, and then propose estimation and inferential methods and theory for both\nshort-run and long-run coefficients. We also propose an information criterion\nto estimate the lag length, a singular-value ratio test to determine the\ncointegration rank, and a hypothesis test to examine the parameter stability.\nTo validate the theoretical findings, we conduct extensive simulations.\nFinally, we demonstrate the empirical relevance by applying the framework to\ninvestigate the rational expectations hypothesis of the U.S. term structure.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.17829v1"
    },
    {
        "title": "Identifying Dynamic LATEs with a Static Instrument",
        "authors": [
            "Bruno Ferman",
            "Otávio Tecchio"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In many situations, researchers are interested in identifying dynamic effects\nof an irreversible treatment with a static binary instrumental variable (IV).\nFor example, in evaluations of dynamic effects of training programs, with a\nsingle lottery determining eligibility. A common approach in these situations\nis to report per-period IV estimates. Under a dynamic extension of standard IV\nassumptions, we show that such IV estimators identify a weighted sum of\ntreatment effects for different latent groups and treatment exposures. However,\nthere is possibility of negative weights. We consider point and partial\nidentification of dynamic treatment effects in this setting under different\nsets of assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.18114v2"
    },
    {
        "title": "Nonlinear Impulse Response Functions and Local Projections",
        "authors": [
            "Christian Gourieroux",
            "Quinlan Lee"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The goal of this paper is to extend the method of estimating Impluse Response\nFunctions (IRFs) by means of Local Projection (LP) in a nonlinear dynamic\nframework. We discuss the existence of a nonlinear autoregressive\nrepresentation for a Markov process, and explain how their Impulse Response\nFunctions are directly linked to the nonlinear Local Projection, as in the case\nfor the linear setting. We then present a nonparametric LP estimator, and\ncompare its asymptotic properties to that of IRFs obtained through direct\nestimation. We also explore issues of identification for the nonlinear IRF in\nthe multivariate framework, which remarkably differs in comparison to the\nGaussian linear case. In particular, we show that identification is conditional\non the uniqueness of deconvolution. Then, we consider IRF and LP in augmented\nMarkov models.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.18145v1"
    },
    {
        "title": "Impulse Response Analysis of Structural Nonlinear Time Series Models",
        "authors": [
            "Giovanni Ballarin"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a semiparametric sieve approach to estimate impulse\nresponse functions of nonlinear time series within a general class of\nstructural autoregressive models. We prove that a two-step procedure can\nflexibly accommodate nonlinear specifications while avoiding the need to choose\nof fixed parametric forms. Sieve impulse responses are proven to be consistent\nby deriving uniform estimation guarantees, and an iterative algorithm makes it\nstraightforward to compute them in practice. With simulations, we show that the\nproposed semiparametric approach proves effective against misspecification\nwhile suffering only minor efficiency losses. In a US monetary policy\napplication, we find that the pointwise sieve GDP response associated with an\ninterest rate increase is larger than that of a linear model. Finally, in an\nanalysis of interest rate uncertainty shocks, sieve responses imply\nsignificantly more substantial contractionary effects both on production and\ninflation.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.19089v4"
    },
    {
        "title": "A Simple Method for Predicting Covariance Matrices of Financial Returns",
        "authors": [
            "Kasper Johansson",
            "Mehmet Giray Ogut",
            "Markus Pelger",
            "Thomas Schmelzer",
            "Stephen Boyd"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We consider the well-studied problem of predicting the time-varying\ncovariance matrix of a vector of financial returns. Popular methods range from\nsimple predictors like rolling window or exponentially weighted moving average\n(EWMA) to more sophisticated predictors such as generalized autoregressive\nconditional heteroscedastic (GARCH) type methods. Building on a specific\ncovariance estimator suggested by Engle in 2002, we propose a relatively simple\nextension that requires little or no tuning or fitting, is interpretable, and\nproduces results at least as good as MGARCH, a popular extension of GARCH that\nhandles multiple assets. To evaluate predictors we introduce a novel approach,\nevaluating the regret of the log-likelihood over a time period such as a\nquarter. This metric allows us to see not only how well a covariance predictor\ndoes over all, but also how quickly it reacts to changes in market conditions.\nOur simple predictor outperforms MGARCH in terms of regret. We also test\ncovariance predictors on downstream applications such as portfolio optimization\nmethods that depend on the covariance matrix. For these applications our simple\ncovariance predictor and MGARCH perform similarly.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.19484v2"
    },
    {
        "title": "Deep Neural Network Estimation in Panel Data Models",
        "authors": [
            "Ilias Chronopoulos",
            "Katerina Chrysikou",
            "George Kapetanios",
            "James Mitchell",
            "Aristeidis Raftapostolos"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this paper we study neural networks and their approximating power in panel\ndata models. We provide asymptotic guarantees on deep feed-forward neural\nnetwork estimation of the conditional mean, building on the work of Farrell et\nal. (2021), and explore latent patterns in the cross-section. We use the\nproposed estimators to forecast the progression of new COVID-19 cases across\nthe G7 countries during the pandemic. We find significant forecasting gains\nover both linear panel and nonlinear time series models. Containment or\nlockdown policies, as instigated at the national-level by governments, are\nfound to have out-of-sample predictive power for new COVID-19 cases. We\nillustrate how the use of partial derivatives can help open the \"black-box\" of\nneural networks and facilitate semi-structural analysis: school and workplace\nclosures are found to have been effective policies at restricting the\nprogression of the pandemic across the G7 countries. But our methods illustrate\nsignificant heterogeneity and time-variation in the effectiveness of specific\ncontainment policies.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.19921v1"
    },
    {
        "title": "Inference in Predictive Quantile Regressions",
        "authors": [
            "Alex Maynard",
            "Katsumi Shimotsu",
            "Nina Kuriyama"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies inference in predictive quantile regressions when the\npredictive regressor has a near-unit root. We derive asymptotic distributions\nfor the quantile regression estimator and its heteroskedasticity and\nautocorrelation consistent (HAC) t-statistic in terms of functionals of\nOrnstein-Uhlenbeck processes. We then propose a switching-fully modified (FM)\npredictive test for quantile predictability. The proposed test employs an FM\nstyle correction with a Bonferroni bound for the local-to-unity parameter when\nthe predictor has a near unit root. It switches to a standard predictive\nquantile regression test with a slightly conservative critical value when the\nlargest root of the predictor lies in the stationary range. Simulations\nindicate that the test has a reliable size in small samples and good power. We\nemploy this new methodology to test the ability of three commonly employed,\nhighly persistent and endogenous lagged valuation regressors - the dividend\nprice ratio, earnings price ratio, and book-to-market ratio - to predict the\nmedian, shoulders, and tails of the stock return distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.00296v2"
    },
    {
        "title": "Social Interactions with Endogenous Group Formation",
        "authors": [
            "Shuyang Sheng",
            "Xiaoting Sun"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper explores the identification and estimation of social interaction\nmodels with endogenous group formation. We characterize group formation using a\ntwo-sided many-to-one matching model, where individuals select groups based on\ntheir preferences, while groups rank individuals according to their\nqualifications, accepting the most qualified until reaching capacities. The\nselection into groups leads to a bias in standard estimates of peer effects,\nwhich is difficult to correct for due to equilibrium effects. We employ the\nlimiting approximation of a market as the market size grows large to simplify\nthe selection bias. Assuming exchangeable unobservables, we can express the\nselection bias of an individual as a group-invariant nonparametric function of\nher preference and qualification indices. In addition to the selection\ncorrection, we show that the excluded variables in group formation can serve as\ninstruments to tackle the reflection problem. We propose semiparametric\ndistribution-free estimators that are root-n consistent and asymptotically\nnormal.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.01544v1"
    },
    {
        "title": "The Synthetic Control Method with Nonlinear Outcomes: Estimating the\n  Impact of the 2019 Anti-Extradition Law Amendments Bill Protests on Hong\n  Kong's Economy",
        "authors": [
            "Wei Tian"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The synthetic control estimator (Abadie et al., 2010) is asymptotically\nunbiased assuming that the outcome is a linear function of the underlying\npredictors and that the treated unit can be well approximated by the synthetic\ncontrol before the treatment. When the outcome is nonlinear, the bias of the\nsynthetic control estimator can be severe. In this paper, we provide conditions\nfor the synthetic control estimator to be asymptotically unbiased when the\noutcome is nonlinear, and propose a flexible and data-driven method to choose\nthe synthetic control weights. Monte Carlo simulations show that compared with\nthe competing methods, the nonlinear synthetic control method has similar or\nbetter performance when the outcome is linear, and better performance when the\noutcome is nonlinear, and that the confidence intervals have good coverage\nprobabilities across settings. In the empirical application, we illustrate the\nmethod by estimating the impact of the 2019 anti-extradition law amendments\nbill protests on Hong Kong's economy, and find that the year-long protests\nreduced real GDP per capita in Hong Kong by 11.27% in the first quarter of\n2020, which was larger in magnitude than the economic decline during the 1997\nAsian financial crisis or the 2008 global financial crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.01967v1"
    },
    {
        "title": "Individual Causal Inference Using Panel Data With Multiple Outcomes",
        "authors": [
            "Wei Tian"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Policy evaluation in empirical microeconomics has been focusing on estimating\nthe average treatment effect and more recently the heterogeneous treatment\neffects, often relying on the unconfoundedness assumption. We propose a method\nbased on the interactive fixed effects model to estimate treatment effects at\nthe individual level, which allows both the treatment assignment and the\npotential outcomes to be correlated with the unobserved individual\ncharacteristics. This method is suitable for panel datasets where multiple\nrelated outcomes are observed for a large number of individuals over a small\nnumber of time periods. Monte Carlo simulations show that our method\noutperforms related methods. To illustrate our method, we provide an example of\nestimating the effect of health insurance coverage on individual usage of\nhospital emergency departments using the Oregon Health Insurance Experiment\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.01969v1"
    },
    {
        "title": "Improving the accuracy of bubble date estimators under time-varying\n  volatility",
        "authors": [
            "Eiji Kurozumi",
            "Anton Skrobotov"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this study, we consider a four-regime bubble model under the assumption of\ntime-varying volatility and propose the algorithm of estimating the break dates\nwith volatility correction: First, we estimate the emerging date of the\nexplosive bubble, its collapsing date, and the recovering date to the normal\nmarket under assumption of homoskedasticity; second, we collect the residuals\nand then employ the WLS-based estimation of the bubble dates. We demonstrate by\nMonte Carlo simulations that the accuracy of the break dates estimators improve\nsignificantly by this two-step procedure in some cases compared to those based\non the OLS method.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.02977v1"
    },
    {
        "title": "Robust inference for the treatment effect variance in experiments using\n  machine learning",
        "authors": [
            "Alejandro Sanchez-Becerra"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Experimenters often collect baseline data to study heterogeneity. I propose\nthe first valid confidence intervals for the VCATE, the treatment effect\nvariance explained by observables. Conventional approaches yield incorrect\ncoverage when the VCATE is zero. As a result, practitioners could be prone to\ndetect heterogeneity even when none exists. The reason why coverage worsens at\nthe boundary is that all efficient estimators have a locally-degenerate\ninfluence function and may not be asymptotically normal. I solve the problem\nfor a broad class of multistep estimators with a predictive first stage. My\nconfidence intervals account for higher-order terms in the limiting\ndistribution and are fast to compute. I also find new connections between the\nVCATE and the problem of deciding whom to treat. The gains of targeting\ntreatment are (sharply) bounded by half the square root of the VCATE. Finally,\nI document excellent performance in simulation and reanalyze an experiment from\nMalawi.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.03363v1"
    },
    {
        "title": "Semiparametric Discrete Choice Models for Bundles",
        "authors": [
            "Fu Ouyang",
            "Thomas T. Yang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We propose two approaches to estimate semiparametric discrete choice models\nfor bundles. Our first approach is a kernel-weighted rank estimator based on a\nmatching-based identification strategy. We establish its complete asymptotic\nproperties and prove the validity of the nonparametric bootstrap for inference.\nWe then introduce a new multi-index least absolute deviations (LAD) estimator\nas an alternative, of which the main advantage is its capacity to estimate\npreference parameters on both alternative- and agent-specific regressors. Both\nmethods can account for arbitrary correlation in disturbances across choices,\nwith the former also allowing for interpersonal heteroskedasticity. We also\ndemonstrate that the identification strategy underlying these procedures can be\nextended naturally to panel data settings, producing an analogous localized\nmaximum score estimator and a LAD estimator for estimating bundle choice models\nwith fixed effects. We derive the limiting distribution of the former and\nverify the validity of the numerical bootstrap as an inference tool. All our\nproposed methods can be applied to general multi-index models. Monte Carlo\nexperiments show that they perform well in finite samples.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.04135v3"
    },
    {
        "title": "Evaluating the Impact of Regulatory Policies on Social Welfare in\n  Difference-in-Difference Settings",
        "authors": [
            "Dalia Ghanem",
            "Désiré Kédagni",
            "Ismael Mourifié"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Quantifying the impact of regulatory policies on social welfare generally\nrequires the identification of counterfactual distributions. Many of these\npolicies (e.g. minimum wages or minimum working time) generate mass points\nand/or discontinuities in the outcome distribution. Existing approaches in the\ndifference-in-difference literature cannot accommodate these discontinuities\nwhile accounting for selection on unobservables and non-stationary outcome\ndistributions. We provide a unifying partial identification result that can\naccount for these features. Our main identifying assumption is the stability of\nthe dependence (copula) between the distribution of the untreated potential\noutcome and group membership (treatment assignment) across time. Exploiting\nthis copula stability assumption allows us to provide an identification result\nthat is invariant to monotonic transformations. We provide sharp bounds on the\ncounterfactual distribution of the treatment group suitable for any outcome,\nwhether discrete, continuous, or mixed. Our bounds collapse to the\npoint-identification result in Athey and Imbens (2006) for continuous outcomes\nwith strictly increasing distribution functions. We illustrate our approach and\nthe informativeness of our bounds by analyzing the impact of an increase in the\nlegal minimum wage using data from a recent minimum wage study (Cengiz, Dube,\nLindner, and Zipperer, 2019).\n",
        "pdf_link": "http://arxiv.org/pdf/2306.04494v2"
    },
    {
        "title": "Heterogeneous Autoregressions in Short T Panel Data Models",
        "authors": [
            "M. Hashem Pesaran",
            "Liying Yang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper considers a first-order autoregressive panel data model with\nindividual-specific effects and heterogeneous autoregressive coefficients\ndefined on the interval (-1,1], thus allowing for some of the individual\nprocesses to have unit roots. It proposes estimators for the moments of the\ncross-sectional distribution of the autoregressive (AR) coefficients, assuming\na random coefficient model for the autoregressive coefficients without imposing\nany restrictions on the fixed effects. It is shown the standard generalized\nmethod of moments estimators obtained under homogeneous slopes are biased.\nSmall sample properties of the proposed estimators are investigated by Monte\nCarlo experiments and compared with a number of alternatives, both under\nhomogeneous and heterogeneous slopes. It is found that a simple moment\nestimator of the mean of heterogeneous AR coefficients performs very well even\nfor moderate sample sizes, but to reliably estimate the variance of AR\ncoefficients much larger samples are required. It is also required that the\ntrue value of this variance is not too close to zero. The utility of the\nheterogeneous approach is illustrated in the case of earnings dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.05299v3"
    },
    {
        "title": "Localized Neural Network Modelling of Time Series: A Case Study on US\n  Monetary Policy",
        "authors": [
            "Jiti Gao",
            "Fei Liu",
            "Bin Peng",
            "Yanrong Yang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this paper, we investigate a semiparametric regression model under the\ncontext of treatment effects via a localized neural network (LNN) approach. Due\nto a vast number of parameters involved, we reduce the number of effective\nparameters by (i) exploring the use of identification restrictions; and (ii)\nadopting a variable selection method based on the group-LASSO technique.\nSubsequently, we derive the corresponding estimation theory and propose a\ndependent wild bootstrap procedure to construct valid inferences accounting for\nthe dependence of data. Finally, we validate our theoretical findings through\nextensive numerical studies. In an empirical study, we revisit the impacts of a\ntightening monetary policy action on a variety of economic variables, including\nshort-/long-term interest rate, inflation, unemployment rate, industrial price\nand equity return via the newly proposed framework using a monthly dataset of\nthe US.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.05593v2"
    },
    {
        "title": "Instrument-based estimation of full treatment effects with movers",
        "authors": [
            "Didier Nibbering",
            "Matthijs Oosterveen"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The effect of the full treatment is a primary parameter of interest in policy\nevaluation, while often only the effect of a subset of treatment is estimated.\nWe partially identify the local average treatment effect of receiving full\ntreatment (LAFTE) using an instrumental variable that may induce individuals\ninto only a subset of treatment (movers). We show that movers violate the\nstandard exclusion restriction, necessary conditions on the presence of movers\nare testable, and partial identification holds under a double exclusion\nrestriction. We identify movers in four empirical applications and estimate\ninformative bounds on the LAFTE in three of them.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.07018v1"
    },
    {
        "title": "Kernel Choice Matters for Boundary Inference Using Local Polynomial\n  Density: With Application to Manipulation Testing",
        "authors": [
            "Shunsuke Imai",
            "Yuta Okamoto"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The local polynomial density (LPD) estimator has been a useful tool for\ninference concerning boundary points of density functions. While it is commonly\nbelieved that kernel selection is not crucial for the performance of\nkernel-based estimators, this paper argues that this does not hold true for LPD\nestimators at boundary points. We find that the commonly used kernels with\ncompact support lead to larger asymptotic and finite-sample variances.\nFurthermore, we present theoretical and numerical evidence showing that such\nunfavorable variance properties negatively affect the performance of\nmanipulation testing in regression discontinuity designs, which typically\nsuffer from low power. Notably, we demonstrate that these issues of increased\nvariance and reduced power can be significantly improved just by using a kernel\nfunction with unbounded support. We recommend the use of the spline-type kernel\n(the Laplace density) and illustrate its superior performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.07619v2"
    },
    {
        "title": "Machine Learning for Zombie Hunting: Predicting Distress from Firms'\n  Accounts and Missing Values",
        "authors": [
            "Falco J. Bargagli-Stoffi",
            "Fabio Incerti",
            "Massimo Riccaboni",
            "Armando Rungi"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this contribution, we propose machine learning techniques to predict\nzombie firms. First, we derive the risk of failure by training and testing our\nalgorithms on disclosed financial information and non-random missing values of\n304,906 firms active in Italy from 2008 to 2017. Then, we spot the highest\nfinancial distress conditional on predictions that lies above a threshold for\nwhich a combination of false positive rate (false prediction of firm failure)\nand false negative rate (false prediction of active firms) is minimized.\nTherefore, we identify zombies as firms that persist in a state of financial\ndistress, i.e., their forecasts fall into the risk category above the threshold\nfor at least three consecutive years. For our purpose, we implement a gradient\nboosting algorithm (XGBoost) that exploits information about missing values.\nThe inclusion of missing values in our predictive model is crucial because\npatterns of undisclosed accounts are correlated with firm failure. Finally, we\nshow that our preferred machine learning algorithm outperforms (i) proxy models\nsuch as Z-scores and the Distance-to-Default, (ii) traditional econometric\nmethods, and (iii) other widely used machine learning techniques. We provide\nevidence that zombies are on average less productive and smaller, and that they\ntend to increase in times of crisis. Finally, we argue that our application can\nhelp financial institutions and public authorities design evidence-based\npolicies-e.g., optimal bankruptcy laws and information disclosure policies.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.08165v1"
    },
    {
        "title": "Inference in IV models with clustered dependence, many instruments and\n  weak identification",
        "authors": [
            "Johannes W. Ligtenberg"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Data clustering reduces the effective sample size from the number of\nobservations towards the number of clusters. For instrumental variable models I\nshow that this reduced effective sample size makes the instruments more likely\nto be weak, in the sense that they contain little information about the\nendogenous regressor, and many, in the sense that their number is large\ncompared to the sample size. Clustered data therefore increases the need for\nmany and weak instrument robust tests. However, none of the previously\ndeveloped many and weak instrument robust tests can be applied to this type of\ndata as they all require independent observations. I therefore adapt two types\nof such tests to clustered data. First, I derive cluster jackknife\nAnderson-Rubin and score tests by removing clusters rather than individual\nobservations from the statistics. Second, I propose a cluster many instrument\nAnderson-Rubin test which improves on the first type of tests by using a more\noptimal, but more complex, weighting matrix. I show that if the clusters\nsatisfy an invariance assumption the higher complexity poses no problems. By\nrevisiting a study on the effect of queenly reign on war I show the empirical\nrelevance of the new tests.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.08559v2"
    },
    {
        "title": "Modelling and Forecasting Macroeconomic Risk with Time Varying Skewness\n  Stochastic Volatility Models",
        "authors": [
            "Andrea Renzetti"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Monitoring downside risk and upside risk to the key macroeconomic indicators\nis critical for effective policymaking aimed at maintaining economic stability.\nIn this paper I propose a parametric framework for modelling and forecasting\nmacroeconomic risk based on stochastic volatility models with Skew-Normal and\nSkew-t shocks featuring time varying skewness. Exploiting a mixture stochastic\nrepresentation of the Skew-Normal and Skew-t random variables, in the paper I\ndevelop efficient posterior simulation samplers for Bayesian estimation of both\nunivariate and VAR models of this type. In an application, I use the models to\npredict downside risk to GDP growth in the US and I show that these models\nrepresent a competitive alternative to semi-parametric approaches such as\nquantile regression. Finally, estimating a medium scale VAR on US data I show\nthat time varying skewness is a relevant feature of macroeconomic and financial\nshocks.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.09287v2"
    },
    {
        "title": "Testing for Peer Effects without Specifying the Network Structure",
        "authors": [
            "Hyunseok Jung",
            "Xiaodong Liu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes an Anderson-Rubin (AR) test for the presence of peer\neffects in panel data without the need to specify the network structure. The\nunrestricted model of our test is a linear panel data model of social\ninteractions with dyad-specific peer effect coefficients for all potential\npeers. The proposed AR test evaluates if these peer effect coefficients are all\nzero. As the number of peer effect coefficients increases with the sample size,\nso does the number of instrumental variables (IVs) employed to test the\nrestrictions under the null, rendering Bekker's many-IV environment. By\nextending existing many-IV asymptotic results to panel data, we establish the\nasymptotic validity of the proposed AR test. Our Monte Carlo simulations show\nthe robustness and superior performance of the proposed test compared to some\nexisting tests with misspecified networks. We provide two applications to\ndemonstrate its empirical relevance.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.09806v3"
    },
    {
        "title": "Formal Covariate Benchmarking to Bound Omitted Variable Bias",
        "authors": [
            "Deepankar Basu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Covariate benchmarking is an important part of sensitivity analysis about\nomitted variable bias and can be used to bound the strength of the unobserved\nconfounder using information and judgments about observed covariates. It is\ncommon to carry out formal covariate benchmarking after residualizing the\nunobserved confounder on the set of observed covariates. In this paper, I\nexplain the rationale and details of this procedure. I clarify some important\ndetails of the process of formal covariate benchmarking and highlight some of\nthe difficulties of interpretation that researchers face in reasoning about the\nresidualized part of unobserved confounders. I explain all the points with\nseveral empirical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.10562v1"
    },
    {
        "title": "Difference-in-Differences with Interference",
        "authors": [
            "Ruonan Xu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In many scenarios, such as the evaluation of place-based policies, potential\noutcomes are not only dependent upon the unit's own treatment but also its\nneighbors' treatment. Despite this, \"difference-in-differences\" (DID) type\nestimators typically ignore such interference among neighbors. I show in this\npaper that the canonical DID estimators generally fail to identify interesting\ncausal effects in the presence of neighborhood interference. To incorporate\ninterference structure into DID estimation, I propose doubly robust estimators\nfor the direct average treatment effect on the treated as well as the average\nspillover effects under a modified parallel trends assumption. The approach in\nthis paper relaxes common restrictions in the literature, such as partial\ninterference and correctly specified spillover functions. Moreover, robust\ninference is discussed based on the asymptotic distribution of the proposed\nestimators.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12003v5"
    },
    {
        "title": "A Nonparametric Test of $m$th-degree Inverse Stochastic Dominance",
        "authors": [
            "Hongyi Jiang",
            "Zhenting Sun",
            "Shiyun Hu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a nonparametric test for $m$th-degree inverse stochastic\ndominance which is a powerful tool for ranking distribution functions according\nto social welfare. We construct the test based on empirical process theory. The\ntest is shown to be asymptotically size controlled and consistent. The good\nfinite sample properties of the test are illustrated via Monte Carlo\nsimulations. We apply our test to the inequality growth in the United Kingdom\nfrom 1995 to 2010.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12271v3"
    },
    {
        "title": "Price elasticity of electricity demand: Using instrumental variable\n  regressions to address endogeneity and autocorrelation of high-frequency time\n  series",
        "authors": [
            "Silvana Tiedemann",
            "Raffaele Sgarlato",
            "Lion Hirth"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper examines empirical methods for estimating the response of\naggregated electricity demand to high-frequency price signals, the short-term\nelasticity of electricity demand. We investigate how the endogeneity of prices\nand the autocorrelation of the time series, which are particularly pronounced\nat hourly granularity, affect and distort common estimators. After developing a\ncontrolled test environment with synthetic data that replicate key statistical\nproperties of electricity demand, we show that not only the ordinary least\nsquare (OLS) estimator is inconsistent (due to simultaneity), but so is a\nregular instrumental variable (IV) regression (due to autocorrelation). Using\nwind as an instrument, as it is commonly done, may result in an estimate of the\ndemand elasticity that is inflated by an order of magnitude. We visualize the\nreason for the Thams bias using causal graphs and show that its magnitude\ndepends on the autocorrelation of both the instrument, and the dependent\nvariable. We further incorporate and adapt two extensions of the IV estimation,\nconditional IV and nuisance IV, which have recently been proposed by Thams et\nal. (2022). We show that these extensions can identify the true short-term\nelasticity in a synthetic setting and are thus particularly promising for\nfuture empirical research in this field.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12863v1"
    },
    {
        "title": "Factor-augmented sparse MIDAS regressions with an application to\n  nowcasting",
        "authors": [
            "Jad Beyhum",
            "Jonas Striaukas"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This article investigates factor-augmented sparse MIDAS (Mixed Data Sampling)\nregressions for high-dimensional time series data, which may be observed at\ndifferent frequencies. Our novel approach integrates sparse and dense\ndimensionality reduction techniques. We derive the convergence rate of our\nestimator under misspecification, $\\tau$-mixing dependence, and polynomial\ntails. Our method's finite sample performance is assessed via Monte Carlo\nsimulations. We apply the methodology to nowcasting U.S. GDP growth and\ndemonstrate that it outperforms both sparse regression and standard\nfactor-augmented regression during the COVID-19 pandemic. To ensure the\nrobustness of these results, we also implement factor-augmented sparse logistic\nregression, which further confirms the superior accuracy of our nowcast\nprobabilities during recessions. These findings indicate that recessions are\ninfluenced by both idiosyncratic (sparse) and common (dense) shocks.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.13362v3"
    },
    {
        "title": "Hybrid unadjusted Langevin methods for high-dimensional latent variable\n  models",
        "authors": [
            "Ruben Loaiza-Maya",
            "Didier Nibbering",
            "Dan Zhu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The exact estimation of latent variable models with big data is known to be\nchallenging. The latents have to be integrated out numerically, and the\ndimension of the latent variables increases with the sample size. This paper\ndevelops a novel approximate Bayesian method based on the Langevin diffusion\nprocess. The method employs the Fisher identity to integrate out the latent\nvariables, which makes it accurate and computationally feasible when applied to\nbig data. In contrast to other approximate estimation methods, it does not\nrequire the choice of a parametric distribution for the unknowns, which often\nleads to inaccuracies. In an empirical discrete choice example with a million\nobservations, the proposed method accurately estimates the posterior choice\nprobabilities using only 2% of the computation time of exact MCMC.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.14445v1"
    },
    {
        "title": "Optimization of the Generalized Covariance Estimator in Noncausal\n  Processes",
        "authors": [
            "Gianluca Cubadda",
            "Francesco Giancaterini",
            "Alain Hecq",
            "Joann Jasiak"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper investigates the performance of the Generalized Covariance\nestimator (GCov) in estimating and identifying mixed causal and noncausal\nmodels. The GCov estimator is a semi-parametric method that minimizes an\nobjective function without making any assumptions about the error distribution\nand is based on nonlinear autocovariances to identify the causal and noncausal\norders. When the number and type of nonlinear autocovariances included in the\nobjective function of a GCov estimator is insufficient/inadequate, or the error\ndensity is too close to the Gaussian, identification issues can arise. These\nissues result in local minima in the objective function, which correspond to\nparameter values associated with incorrect causal and noncausal orders. Then,\ndepending on the starting point and the optimization algorithm employed, the\nalgorithm can converge to a local minimum. The paper proposes the use of the\nSimulated Annealing (SA) optimization algorithm as an alternative to\nconventional numerical optimization methods. The results demonstrate that SA\nperforms well when applied to mixed causal and noncausal models, successfully\neliminating the effects of local minima. The proposed approach is illustrated\nby an empirical application involving a bivariate commodity price series.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.14653v3"
    },
    {
        "title": "The Yule-Frisch-Waugh-Lovell Theorem",
        "authors": [
            "Deepankar Basu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper traces the historical and analytical development of what is known\nin the econometrics literature as the Frisch-Waugh-Lovell theorem. This theorem\ndemonstrates that the coefficients on any subset of covariates in a multiple\nregression is equal to the coefficients in a regression of the residualized\noutcome variable on the residualized subset of covariates, where\nresidualization uses the complement of the subset of covariates of interest. In\nthis paper, I suggest that the theorem should be renamed as the\nYule-Frisch-Waugh-Lovell (YFWL) theorem to recognize the pioneering\ncontribution of the statistician G. Udny Yule in its development. Second, I\nhighlight recent work by the statistician, P. Ding, which has extended the YFWL\ntheorem to a comparison of estimated covariance matrices of coefficients from\nmultiple and partial, i.e. residualized regressions. Third, I show that, in\ncases where Ding's results do not apply, one can still resort to a\ncomputational method to conduct statistical inference about coefficients in\nmultiple regressions using information from partial regressions.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.00369v1"
    },
    {
        "title": "Does regional variation in wage levels identify the effects of a\n  national minimum wage?",
        "authors": [
            "Daniel Haanwinckel"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper investigates the validity of estimators that exploit regional\ndifferences in wage levels to identify the labor market effects of a national\nminimum wage. Specifically, it examines variations of the ``fraction affected''\nand ``effective minimum wage'' designs. The study finds that these estimators\nare prone to biases from correlated measurement errors and functional form\nmisspecification, even when identification assumptions from previous literature\nare met. Additionally, minor deviations from these assumptions can introduce\nsignificant biases. Through a series of simulation exercises and a detailed\ncase study of Brazil's federal minimum wage increase starting in 1995, the\npaper documents the practical relevance of these biases and evaluates the\neffectiveness of potential solutions and diagnostic tools.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.01284v4"
    },
    {
        "title": "Synthetic Decomposition for Counterfactual Predictions",
        "authors": [
            "Nathan Canen",
            "Kyungchul Song"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Counterfactual predictions are challenging when the policy variable goes\nbeyond its pre-policy support. However, in many cases, information about the\npolicy of interest is available from different (\"source\") regions where a\nsimilar policy has already been implemented. In this paper, we propose a novel\nmethod of using such data from source regions to predict a new policy in a\ntarget region. Instead of relying on extrapolation of a structural relationship\nusing a parametric specification, we formulate a transferability condition and\nconstruct a synthetic outcome-policy relationship such that it is as close as\npossible to meeting the condition. The synthetic relationship weighs both the\nsimilarity in distributions of observables and in structural relationships. We\ndevelop a general procedure to construct asymptotic confidence intervals for\ncounterfactual predictions and prove its asymptotic validity. We then apply our\nproposal to predict average teenage employment in Texas following a\ncounterfactual increase in the minimum wage.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.05122v1"
    },
    {
        "title": "Decentralized Decision-Making in Retail Chains: Evidence from Inventory\n  Management",
        "authors": [
            "Victor Aguirregabiria",
            "Francis Guiton"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper investigates the impact of decentralizing inventory\ndecision-making in multi-establishment firms using data from a large retail\nchain. Analyzing two years of daily data, we find significant heterogeneity\namong the inventory decisions made by 634 store managers. By estimating a\ndynamic structural model, we reveal substantial heterogeneity in managers'\nperceived costs. Moreover, we observe a correlation between the variance of\nthese perceptions and managers' education and experience. Counterfactual\nexperiments show that centralized inventory management reduces costs by\neliminating the impact of managers' skill heterogeneity. However, these\nbenefits are offset by the negative impact of delayed demand information.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.05562v1"
    },
    {
        "title": "What Does it Take to Control Global Temperatures? A toolbox for testing\n  and estimating the impact of economic policies on climate",
        "authors": [
            "Guillaume Chevillon",
            "Takamitsu Kurita"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper tests the feasibility and estimates the cost of climate control\nthrough economic policies. It provides a toolbox for a statistical historical\nassessment of a Stochastic Integrated Model of Climate and the Economy, and its\nuse in (possibly counterfactual) policy analysis. Recognizing that\nstabilization requires supressing a trend, we use an integrated-cointegrated\nVector Autoregressive Model estimated using a newly compiled dataset ranging\nbetween years A.D. 1000-2008, extending previous results on Control Theory in\nnonstationary systems. We test statistically whether, and quantify to what\nextent, carbon abatement policies can effectively stabilize or reduce global\ntemperatures. Our formal test of policy feasibility shows that carbon abatement\ncan have a significant long run impact and policies can render temperatures\nstationary around a chosen long run mean. In a counterfactual empirical\nillustration of the possibilities of our modeling strategy, we study a\nretrospective policy aiming to keep global temperatures close to their 1900\nhistorical level. Achieving this via carbon abatement may cost about 75% of the\nobserved 2008 level of world GDP, a cost equivalent to reverting to levels of\noutput historically observed in the mid 1960s. By contrast, investment in\ncarbon neutral technology could achieve the policy objective and be\nself-sustainable as long as it costs less than 50% of 2008 global GDP and 75%\nof consumption.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.05818v2"
    },
    {
        "title": "Robust Impulse Responses using External Instruments: the Role of\n  Information",
        "authors": [
            "Davide Brignone",
            "Alessandro Franconi",
            "Marco Mazzali"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  External-instrument identification leads to biased responses when the shock\nis not invertible and the measurement error is present. We propose to use this\nidentification strategy in a structural Dynamic Factor Model, which we call\nProxy DFM. In a simulation analysis, we show that the Proxy DFM always\nsuccessfully retrieves the true impulse responses, while the Proxy SVAR\nsystematically fails to do so when the model is either misspecified, does not\ninclude all relevant information, or the measurement error is present. In an\napplication to US monetary policy, the Proxy DFM shows that a tightening shock\nis unequivocally contractionary, with deteriorations in domestic demand, labor,\ncredit, housing, exchange, and financial markets. This holds true for all raw\ninstruments available in the literature. The variance decomposition analysis\nhighlights the importance of monetary policy shocks in explaining economic\nfluctuations, albeit at different horizons.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.06145v1"
    },
    {
        "title": "Identification in Multiple Treatment Models under Discrete Variation",
        "authors": [
            "Vishal Kamat",
            "Samuel Norris",
            "Matthew Pecenco"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We develop a method to learn about treatment effects in multiple treatment\nmodels with discrete-valued instruments. We allow selection into treatment to\nbe governed by a general class of threshold crossing models that permits\nmultidimensional unobserved heterogeneity. Under a semi-parametric restriction\non the distribution of unobserved heterogeneity, we show how a sequence of\nlinear programs can be used to compute sharp bounds for a number of treatment\neffect parameters when the marginal treatment response functions underlying\nthem remain nonparametric or are additionally parameterized.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.06174v1"
    },
    {
        "title": "Risk Preference Types, Limited Consideration, and Welfare",
        "authors": [
            "Levon Barseghyan",
            "Francesca Molinari"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We provide sufficient conditions for semi-nonparametric point identification\nof a mixture model of decision making under risk, when agents make choices in\nmultiple lines of insurance coverage (contexts) by purchasing a bundle. As a\nfirst departure from the related literature, the model allows for two\npreference types. In the first one, agents behave according to standard\nexpected utility theory with CARA Bernoulli utility function, with an\nagent-specific coefficient of absolute risk aversion whose distribution is left\ncompletely unspecified. In the other, agents behave according to the dual\ntheory of choice under risk(Yaari, 1987) combined with a one-parameter family\ndistortion function, where the parameter is agent-specific and is drawn from a\ndistribution that is left completely unspecified. Within each preference type,\nthe model allows for unobserved heterogeneity in consideration sets, where the\nlatter form at the bundle level -- a second departure from the related\nliterature. Our point identification result rests on observing sufficient\nvariation in covariates across contexts, without requiring any independent\nvariation across alternatives within a single context. We estimate the model on\ndata on households' deductible choices in two lines of property insurance, and\nuse the results to assess the welfare implications of a hypothetical market\nintervention where the two lines of insurance are combined into a single one.\nWe study the role of limited consideration in mediating the welfare effects of\nsuch intervention.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.09411v1"
    },
    {
        "title": "Real-Time Detection of Local No-Arbitrage Violations",
        "authors": [
            "Torben G. Andersen",
            "Viktor Todorov",
            "Bo Zhou"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper focuses on the task of detecting local episodes involving\nviolation of the standard It\\^o semimartingale assumption for financial asset\nprices in real time that might induce arbitrage opportunities. Our proposed\ndetectors, defined as stopping rules, are applied sequentially to continually\nincoming high-frequency data. We show that they are asymptotically\nexponentially distributed in the absence of Ito semimartingale violations. On\nthe other hand, when a violation occurs, we can achieve immediate detection\nunder infill asymptotics. A Monte Carlo study demonstrates that the asymptotic\nresults provide a good approximation to the finite-sample behavior of the\nsequential detectors. An empirical application to S&P 500 index futures data\ncorroborates the effectiveness of our detectors in swiftly identifying the\nemergence of an extreme return persistence episode in real time.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.10872v1"
    },
    {
        "title": "Functional Differencing in Networks",
        "authors": [
            "Stéphane Bonhomme",
            "Kevin Dano"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Economic interactions often occur in networks where heterogeneous agents\n(such as workers or firms) sort and produce. However, most existing estimation\napproaches either require the network to be dense, which is at odds with many\nempirical networks, or they require restricting the form of heterogeneity and\nthe network formation process. We show how the functional differencing approach\nintroduced by Bonhomme (2012) in the context of panel data, can be applied in\nnetwork settings to derive moment restrictions on model parameters and average\neffects. Those restrictions are valid irrespective of the form of\nheterogeneity, and they hold in both dense and sparse networks. We illustrate\nthe analysis with linear and nonlinear models of matched employer-employee\ndata, in the spirit of the model introduced by Abowd, Kramarz, and Margolis\n(1999).\n",
        "pdf_link": "http://arxiv.org/pdf/2307.11484v1"
    },
    {
        "title": "Identification Robust Inference for the Risk Premium in Term Structure\n  Models",
        "authors": [
            "Frank Kleibergen",
            "Lingwei Kong"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We propose identification robust statistics for testing hypotheses on the\nrisk premia in dynamic affine term structure models. We do so using the moment\nequation specification proposed for these models in Adrian et al. (2013). We\nextend the subset (factor) Anderson-Rubin test from Guggenberger et al. (2012)\nto models with multiple dynamic factors and time-varying risk prices. Unlike\nprojection-based tests, it provides a computationally tractable manner to\nconduct identification robust tests on a larger number of parameters. We\nanalyze the potential identification issues arising in empirical studies.\nStatistical inference based on the three-stage estimator from Adrian et al.\n(2013) requires knowledge of the factors' quality and is misleading without\nfull-rank beta's or with sampling errors of comparable size as the loadings.\nEmpirical applications show that some factors, though potentially weak, may\ndrive the time variation of risk prices, and weak identification issues are\nmore prominent in multi-factor models.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.12628v1"
    },
    {
        "title": "The Yule-Frisch-Waugh-Lovell Theorem for Linear Instrumental Variables\n  Estimation",
        "authors": [
            "Deepankar Basu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this paper, I discuss three aspects of the Frisch-Waugh-Lovell theorem.\nFirst, I show that the theorem holds for linear instrumental variables\nestimation of a multiple regression model that is either exactly or\noveridentified. I show that with linear instrumental variables estimation: (a)\ncoefficients on endogenous variables are identical in full and partial (or\nresidualized) regressions; (b) residual vectors are identical for full and\npartial regressions; and (c) estimated covariance matrices of the coefficient\nvectors from full and partial regressions are equal (up to a degree of freedom\ncorrection) if the estimator of the error vector is a function only of the\nresidual vectors and does not use any information about the covariate matrix\nother than its dimensions. While estimation of the full model uses the full set\nof instrumental variables, estimation of the partial model uses the\nresidualized version of the same set of instrumental variables, with\nresidualization carried out with respect to the set of exogenous variables.\nSecond, I show that: (a) the theorem applies in large samples to the K-class of\nestimators, including the limited information maximum likelihood (LIML)\nestimator, and (b) the theorem does not apply in general to linear GMM\nestimators, but it does apply to the two step optimal linear GMM estimator.\nThird, I trace the historical and analytical development of the theorem and\nsuggest that it be renamed as the Yule-Frisch-Waugh-Lovell (YFWL) theorem to\nrecognize the pioneering contribution of the statistician G. Udny Yule in its\ndevelopment.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.12731v2"
    },
    {
        "title": "Using Probabilistic Stated Preference Analyses to Understand Actual\n  Choices",
        "authors": [
            "Romuald Meango"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Can stated preferences help in counterfactual analyses of actual choice? This\nresearch proposes a novel approach to researchers who have access to both\nstated choices in hypothetical scenarios and actual choices. The key idea is to\nuse probabilistic stated choices to identify the distribution of individual\nunobserved heterogeneity, even in the presence of measurement error. If this\nunobserved heterogeneity is the source of endogeneity, the researcher can\ncorrect for its influence in a demand function estimation using actual choices,\nand recover causal effects. Estimation is possible with an off-the-shelf Group\nFixed Effects estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.13966v1"
    },
    {
        "title": "Dynamic Regression Discontinuity: A Within-Design Approach",
        "authors": [
            "Francesco Ruggieri"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  I propose a novel argument to point identify economically interpretable\nintertemporal treatment effects in dynamic regression discontinuity designs\n(RDDs). Specifically, I develop a dynamic potential outcomes model and\nspecialize two assumptions of the difference-in-differences literature, the no\nanticipation and common trends restrictions, to point identify cutoff-specific\nimpulse responses. The estimand associated with each target parameter can be\nexpressed as the sum of two static RDD outcome contrasts, thereby allowing for\nestimation via standard local polynomial tools. I leverage a limited path\nindependence assumption to reduce the dimensionality of the problem.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14203v1"
    },
    {
        "title": "Smoothing of numerical series by the triangle method on the example of\n  hungarian gdp data 1992-2022 based on approximation by series of exponents",
        "authors": [
            "Yekimov Sergey"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In practice , quite often there is a need to describe the values set by means\nof a table in the form of some functional dependence . The observed values ,\ndue to certain circumstances , have an error . For approximation, it is\nadvisable to use a functional dependence that would allow smoothing out the\nerrors of the observation results. Approximation allows you to determine\nintermediate values of functions that are not listed among the data in the\nobservation table. The use of exponential series for data approximation allows\nyou to get a result no worse than from approximation by polynomials In the\neconomic scientific literature, approximation in the form of power functions,\nfor example, the Cobb-Douglas function, has become widespread. The advantage of\nthis type of approximation can be called a simple type of approximating\nfunction , and the disadvantage is that in nature not all processes can be\ndescribed by power functions with a given accuracy. An example is the GDP\nindicator for several decades . For this case , it is difficult to find a power\nfunction approximating a numerical series . But in this case, as shown in this\narticle, you can use exponential series to approximate the data. In this paper,\nthe time series of Hungary's GDP in the period from 1992 to 2022 was\napproximated by a series of thirty exponents of a complex variable. The use of\ndata smoothing by the method of triangles allows you to average the data and\nincrease the accuracy of approximation . This is of practical importance if the\nobserved random variable contains outliers that need to be smoothed out.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14378v1"
    },
    {
        "title": "Bootstrapping Nonstationary Autoregressive Processes with Predictive\n  Regression Models",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We establish the asymptotic validity of the bootstrap-based IVX estimator\nproposed by Phillips and Magdalinos (2009) for the predictive regression model\nparameter based on a local-to-unity specification of the autoregressive\ncoefficient which covers both nearly nonstationary and nearly stationary\nprocesses. A mixed Gaussian limit distribution is obtained for the\nbootstrap-based IVX estimator. The statistical validity of the theoretical\nresults are illustrated by Monte Carlo experiments for various statistical\ninference problems.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14463v1"
    },
    {
        "title": "Weak (Proxy) Factors Robust Hansen-Jagannathan Distance For Linear Asset\n  Pricing Models",
        "authors": [
            "Lingwei Kong"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The Hansen-Jagannathan (HJ) distance statistic is one of the most dominant\nmeasures of model misspecification. However, the conventional HJ specification\ntest procedure has poor finite sample performance, and we show that it can be\nsize distorted even in large samples when (proxy) factors exhibit small\ncorrelations with asset returns. In other words, applied researchers are likely\nto falsely reject a model even when it is correctly specified. We provide two\nalternatives for the HJ statistic and two corresponding novel procedures for\nmodel specification tests, which are robust against the presence of weak\n(proxy) factors, and we also offer a novel robust risk premia estimator.\nSimulation exercises support our theory. Our empirical application documents\nthe non-reliability of the traditional HJ test since it may produce\ncounter-intuitive results when comparing nested models by rejecting a\nfour-factor model but not the reduced three-factor model. At the same time, our\nproposed methods are practically more appealing and show support for a\nfour-factor model for Fama French portfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14499v1"
    },
    {
        "title": "Predictability Tests Robust against Parameter Instability",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We consider Wald type statistics designed for joint predictability and\nstructural break testing based on the instrumentation method of Phillips and\nMagdalinos (2009). We show that under the assumption of nonstationary\npredictors: (i) the tests based on the OLS estimators converge to a nonstandard\nlimiting distribution which depends on the nuisance coefficient of persistence;\nand (ii) the tests based on the IVX estimators can filter out the persistence\nunder certain parameter restrictions due to the supremum functional. These\nresults contribute to the literature of joint predictability and parameter\ninstability testing by providing analytical tractable asymptotic theory when\ntaking into account nonstationary regressors. We compare the finite-sample size\nand power performance of the Wald tests under both estimators via extensive\nMonte Carlo experiments. Critical values are computed using standard bootstrap\ninference methodologies. We illustrate the usefulness of the proposed framework\nto test for predictability under the presence of parameter instability by\nexamining the stock market predictability puzzle for the US equity premium.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.15151v1"
    },
    {
        "title": "Group-Heterogeneous Changes-in-Changes and Distributional Synthetic\n  Controls",
        "authors": [
            "Songnian Chen",
            "Junlong Feng"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We develop new methods for changes-in-changes and distributional synthetic\ncontrols when there exists group level heterogeneity. For changes-in-changes,\nwe allow individuals to belong to a large number of heterogeneous groups. The\nnew method extends the changes-in-changes method in Athey and Imbens (2006) by\nfinding appropriate subgroups within the control groups which share similar\ngroup level unobserved characteristics to the treatment groups. For\ndistributional synthetic control, we show that the appropriate synthetic\ncontrol needs to be constructed using units in potentially different time\nperiods in which they have comparable group level heterogeneity to the\ntreatment group, instead of units that are only in the same time period as in\nGunsilius (2023). Implementation and data requirements for these new methods\nare briefly discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.15313v1"
    },
    {
        "title": "Panel Data Models with Time-Varying Latent Group Structures",
        "authors": [
            "Yiren Wang",
            "Peter C B Phillips",
            "Liangjun Su"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper considers a linear panel model with interactive fixed effects and\nunobserved individual and time heterogeneities that are captured by some latent\ngroup structures and an unknown structural break, respectively. To enhance\nrealism the model may have different numbers of groups and/or different group\nmemberships before and after the break. With the preliminary\nnuclear-norm-regularized estimation followed by row- and column-wise linear\nregressions, we estimate the break point based on the idea of binary\nsegmentation and the latent group structures together with the number of groups\nbefore and after the break by sequential testing K-means algorithm\nsimultaneously. It is shown that the break point, the number of groups and the\ngroup memberships can each be estimated correctly with probability approaching\none. Asymptotic distributions of the estimators of the slope coefficients are\nestablished. Monte Carlo simulations demonstrate excellent finite sample\nperformance for the proposed estimation algorithm. An empirical application to\nreal house price data across 377 Metropolitan Statistical Areas in the US from\n1975 to 2014 suggests the presence both of structural breaks and of changes in\ngroup membership.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.15863v1"
    },
    {
        "title": "What's Logs Got to do With it: On the Perils of log Dependent Variables\n  and Difference-in-Differences",
        "authors": [
            "Brendon McConnell"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The log transformation of the dependent variable is not innocuous when using\na difference-in-differences (DD) model. With a dependent variable in logs, the\nDD term captures an approximation of the proportional difference in growth\nrates across groups. As I show with both simulations and two empirical\nexamples, if the baseline outcome distributions are sufficiently different\nacross groups, the DD parameter for a log-specification can be different in\nsign to that of a levels-specification. I provide a condition, based on (i) the\naggregate time effect, and (ii) the difference in relative baseline outcome\nmeans, for when the sign-switch will occur.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.00167v3"
    },
    {
        "title": "Randomization Inference of Heterogeneous Treatment Effects under Network\n  Interference",
        "authors": [
            "Julius Owusu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We design randomization tests of heterogeneous treatment effects when units\ninteract on a single connected network. Our modeling strategy allows network\ninterference into the potential outcomes framework using the concept of\nexposure mapping. We consider a general class of null hypotheses --\nrepresenting different notions of constant and no treatment effects -- that are\nnot sharp due to unknown parameters and multiple potential outcomes. To make\nthe nulls sharp, we propose a conditional randomization method that expands on\nexisting procedures. Our conditioning approach permits the use of functions of\ntreatment as a conditioning variable, widening the scope of application of the\nrandomization method of inference. We show that the resulting testing\nprocedures based on our conditioning approach are valid. We demonstrate the\ntesting methods using a network data set and also present the findings of an\nextensive Monte Carlo study.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.00202v3"
    },
    {
        "title": "Testing for Threshold Effects in Presence of Heteroskedasticity and\n  Measurement Error with an application to Italian Strikes",
        "authors": [
            "Francesco Angelini",
            "Massimiliano Castellani",
            "Simone Giannerini",
            "Greta Goracci"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Many macroeconomic time series are characterised by nonlinearity both in the\nconditional mean and in the conditional variance and, in practice, it is\nimportant to investigate separately these two aspects. Here we address the\nissue of testing for threshold nonlinearity in the conditional mean, in the\npresence of conditional heteroskedasticity. We propose a supremum Lagrange\nMultiplier approach to test a linear ARMA-GARCH model against the alternative\nof a TARMA-GARCH model. We derive the asymptotic null distribution of the test\nstatistic and this requires novel results since the difficulties of working\nwith nuisance parameters, absent under the null hypothesis, are amplified by\nthe non-linear moving average, combined with GARCH-type innovations. We show\nthat tests that do not account for heteroskedasticity fail to achieve the\ncorrect size even for large sample sizes. Moreover, we show that the TARMA\nspecification naturally accounts for the ubiquitous presence of measurement\nerror that affects macroeconomic data. We apply the results to analyse the time\nseries of Italian strikes and we show that the TARMA-GARCH specification is\nconsistent with the relevant macroeconomic theory while capturing the main\nfeatures of the Italian strikes dynamics, such as asymmetric cycles and\nregime-switching.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.00444v1"
    },
    {
        "title": "Limit Theory under Network Dependence and Nonstationarity",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  These lecture notes represent supplementary material for a short course on\ntime series econometrics and network econometrics. We give emphasis on limit\ntheory for time series regression models as well as the use of the\nlocal-to-unity parametrization when modeling time series nonstationarity.\nMoreover, we present various non-asymptotic theory results for moderate\ndeviation principles when considering the eigenvalues of covariance matrices as\nwell as asymptotics for unit root moderate deviations in nonstationary\nautoregressive processes. Although not all applications from the literature are\ncovered we also discuss some open problems in the time series and network\neconometrics literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.01418v4"
    },
    {
        "title": "A Robust Method for Microforecasting and Estimation of Random Effects",
        "authors": [
            "Raffaella Giacomini",
            "Sokbae Lee",
            "Silvia Sarpietro"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We propose a method for forecasting individual outcomes and estimating random\neffects in linear panel data models and value-added models when the panel has a\nshort time dimension. The method is robust, trivial to implement and requires\nminimal assumptions. The idea is to take a weighted average of time series- and\npooled forecasts/estimators, with individual weights that are based on time\nseries information. We show the forecast optimality of individual weights, both\nin terms of minimax-regret and of mean squared forecast error. We then provide\nfeasible weights that ensure good performance under weaker assumptions than\nthose required by existing approaches. Unlike existing shrinkage methods, our\napproach borrows the strength - but avoids the tyranny - of the majority, by\ntargeting individual (instead of group) accuracy and letting the data decide\nhow much strength each individual should borrow. Unlike existing empirical\nBayesian methods, our frequentist approach requires no distributional\nassumptions, and, in fact, it is particularly advantageous in the presence of\nfeatures such as heavy tails that would make a fully nonparametric procedure\nproblematic.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.01596v1"
    },
    {
        "title": "Composite Quantile Factor Model",
        "authors": [
            "Xiao Huang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper introduces the method of composite quantile factor model for\nfactor analysis in high-dimensional panel data. We propose to estimate the\nfactors and factor loadings across multiple quantiles of the data, allowing the\nestimates to better adapt to features of the data at different quantiles while\nstill modeling the mean of the data. We develop the limiting distribution of\nthe estimated factors and factor loadings, and an information criterion for\nconsistent factor number selection is also discussed. Simulations show that the\nproposed estimator and the information criterion have good finite sample\nproperties for several non-normal distributions under consideration. We also\nconsider an empirical study on the factor analysis for 246 quarterly\nmacroeconomic variables. A companion R package cqrfactor is developed.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.02450v2"
    },
    {
        "title": "Treatment Effects in Staggered Adoption Designs with Non-Parallel Trends",
        "authors": [
            "Brantly Callaway",
            "Emmanuel Selorm Tsyawo"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper considers identifying and estimating causal effect parameters in a\nstaggered treatment adoption setting -- that is, where a researcher has access\nto panel data and treatment timing varies across units. We consider the case\nwhere untreated potential outcomes may follow non-parallel trends over time\nacross groups. This implies that the identifying assumptions of leading\napproaches such as difference-in-differences do not hold. We mainly focus on\nthe case where untreated potential outcomes are generated by an interactive\nfixed effects model and show that variation in treatment timing provides\nadditional moment conditions that can be used to recover a large class of\ntarget causal effect parameters. Our approach exploits the variation in\ntreatment timing without requiring either (i) a large number of time periods or\n(ii) requiring any extra exclusion restrictions. This is in contrast to\nessentially all of the literature on interactive fixed effects models which\nrequires at least one of these extra conditions. Rather, our approach directly\napplies in settings where there is variation in treatment timing. Although our\nmain focus is on a model with interactive fixed effects, our idea of using\nvariation in treatment timing to recover causal effect parameters is quite\ngeneral and could be adapted to other settings with non-parallel trends across\ngroups such as dynamic panel data models.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.02899v1"
    },
    {
        "title": "Threshold Regression in Heterogeneous Panel Data with Interactive Fixed\n  Effects",
        "authors": [
            "Marco Barassi",
            "Yiannis Karavias",
            "Chongxian Zhu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper introduces unit-specific heterogeneity in panel data threshold\nregression. Both slope coefficients and threshold parameters are allowed to\nvary by unit. The heterogeneous threshold parameters manifest via a\nunit-specific empirical quantile transformation of a common underlying\nthreshold parameter which is estimated efficiently from the whole panel. In the\nerrors, the unobserved heterogeneity of the panel takes the general form of\ninteractive fixed effects. The newly introduced parameter heterogeneity has\nimplications for model identification, estimation, interpretation, and\nasymptotic inference. The assumption of a shrinking threshold magnitude now\nimplies shrinking heterogeneity and leads to faster estimator rates of\nconvergence than previously encountered. The asymptotic theory for the proposed\nestimators is derived and Monte Carlo simulations demonstrate its usefulness in\nsmall samples. The new model is employed to examine the Feldstein-Horioka\npuzzle and it is found that the trade liberalization policies of the 80's\nsignificantly impacted cross-country capital mobility.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.04057v1"
    },
    {
        "title": "Causal Interpretation of Linear Social Interaction Models with\n  Endogenous Networks",
        "authors": [
            "Tadao Hoshino"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This study investigates the causal interpretation of linear social\ninteraction models in the presence of endogeneity in network formation under a\nheterogeneous treatment effects framework. We consider an experimental setting\nin which individuals are randomly assigned to treatments while no interventions\nare made for the network structure. We show that running a linear regression\nignoring network endogeneity is not problematic for estimating the average\ndirect treatment effect. However, it leads to sample selection bias and\nnegative-weights problem for the estimation of the average spillover effect. To\novercome these problems, we propose using potential peer treatment as an\ninstrumental variable (IV), which is automatically a valid IV for actual\nspillover exposure. Using this IV, we examine two IV-based estimands and\ndemonstrate that they have a local average treatment-effect-type causal\ninterpretation for the spillover effect.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.04276v2"
    },
    {
        "title": "A Guide to Impact Evaluation under Sample Selection and Missing Data:\n  Teacher's Aides and Adolescent Mental Health",
        "authors": [
            "Simon Calmar Andersen",
            "Louise Beuchert",
            "Phillip Heiler",
            "Helena Skyt Nielsen"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper is concerned with identification, estimation, and specification\ntesting in causal evaluation problems when data is selective and/or missing. We\nleverage recent advances in the literature on graphical methods to provide a\nunifying framework for guiding empirical practice. The approach integrates and\nconnects to prominent identification and testing strategies in the literature\non missing data, causal machine learning, panel data analysis, and more. We\ndemonstrate its utility in the context of identification and specification\ntesting in sample selection models and field experiments with attrition. We\nprovide a novel analysis of a large-scale cluster-randomized controlled\nteacher's aide trial in Danish schools at grade 6. Even with detailed\nadministrative data, the handling of missing data crucially affects broader\nconclusions about effects on mental health. Results suggest that teaching\nassistants provide an effective way of improving internalizing behavior for\nlarge parts of the student population.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.04963v1"
    },
    {
        "title": "Interpolation of numerical series by the Fermat-Torricelli point\n  construction method on the example of the numerical series of inflation in\n  the Czech Republic in 2011-2021",
        "authors": [
            "Yekimov Sergey"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The use of regression analysis for processing experimental data is fraught\nwith certain difficulties, which, when models are constructed, are associated\nwith assumptions, and there is a normal law of error distribution and variables\nare statistically independent. In practice , these conditions do not always\ntake place . This may cause the constructed economic and mathematical model to\nhave no practical value. As an alternative approach to the study of numerical\nseries, according to the author, smoothing of numerical series using\nFermat-Torricelli points with subsequent interpolation of these points by\nseries of exponents could be used. The use of exponential series for\ninterpolating numerical series makes it possible to achieve the accuracy of\nmodel construction no worse than regression analysis . At the same time, the\ninterpolation by series of exponents does not require the statistical material\nthat the errors of the numerical series obey the normal distribution law, and\nstatistical independence of variables is also not required. Interpolation of\nnumerical series by exponential series represents a \"black box\" type model,\nthat is, only input parameters and output parameters matter.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.05183v1"
    },
    {
        "title": "Money Growth and Inflation: A Quantile Sensitivity Approach",
        "authors": [
            "Matteo Iacopini",
            "Aubrey Poon",
            "Luca Rossini",
            "Dan Zhu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  An innovative method is proposed to construct a quantile dependence system\nfor inflation and money growth. By considering all quantiles and leveraging a\nnovel notion of quantile sensitivity, the method allows the assessment of\nchanges in the entire distribution of a variable of interest in response to a\nperturbation in another variable's quantile. The construction of this\nrelationship is demonstrated through a system of linear quantile regressions.\nThen, the proposed framework is exploited to examine the distributional effects\nof money growth on the distributions of inflation and its disaggregate measures\nin the United States and the Euro area. The empirical analysis uncovers\nsignificant impacts of the upper quantile of the money growth distribution on\nthe distribution of inflation and its disaggregate measures. Conversely, the\nlower and median quantiles of the money growth distribution are found to have a\nnegligible influence. Finally, this distributional impact exhibits variation\nover time in both the United States and the Euro area.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.05486v3"
    },
    {
        "title": "Amortized neural networks for agent-based model forecasting",
        "authors": [
            "Denis Koshelev",
            "Alexey Ponomarenko",
            "Sergei Seleznev"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this paper, we propose a new procedure for unconditional and conditional\nforecasting in agent-based models. The proposed algorithm is based on the\napplication of amortized neural networks and consists of two steps. The first\nstep simulates artificial datasets from the model. In the second step, a neural\nnetwork is trained to predict the future values of the variables using the\nhistory of observations. The main advantage of the proposed algorithm is its\nspeed. This is due to the fact that, after the training procedure, it can be\nused to yield predictions for almost any data without additional simulations or\nthe re-estimation of the neural network\n",
        "pdf_link": "http://arxiv.org/pdf/2308.05753v1"
    },
    {
        "title": "Quantile Time Series Regression Models Revisited",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This article discusses recent developments in the literature of quantile time\nseries models in the cases of stationary and nonstationary underline stochastic\nprocesses.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.06617v3"
    },
    {
        "title": "Optimizing B2B Product Offers with Machine Learning, Mixed Logit, and\n  Nonlinear Programming",
        "authors": [
            "John V. Colias",
            "Stella Park",
            "Elizabeth Horn"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In B2B markets, value-based pricing and selling has become an important\nalternative to discounting. This study outlines a modeling method that uses\ncustomer data (product offers made to each current or potential customer,\nfeatures, discounts, and customer purchase decisions) to estimate a mixed logit\nchoice model. The model is estimated via hierarchical Bayes and machine\nlearning, delivering customer-level parameter estimates. Customer-level\nestimates are input into a nonlinear programming next-offer maximization\nproblem to select optimal features and discount level for customer segments,\nwhere segments are based on loyalty and discount elasticity. The mixed logit\nmodel is integrated with economic theory (the random utility model), and it\npredicts both customer perceived value for and response to alternative future\nsales offers. The methodology can be implemented to support value-based pricing\nand selling efforts.\n  Contributions to the literature include: (a) the use of customer-level\nparameter estimates from a mixed logit model, delivered via a hierarchical\nBayes estimation procedure, to support value-based pricing decisions; (b)\nvalidation that mixed logit customer-level modeling can deliver strong\npredictive accuracy, not as high as random forest but comparing favorably; and\n(c) a nonlinear programming problem that uses customer-level mixed logit\nestimates to select optimal features and discounts.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.07830v1"
    },
    {
        "title": "Emerging Frontiers: Exploring the Impact of Generative AI Platforms on\n  University Quantitative Finance Examinations",
        "authors": [
            "Rama K. Malladi"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This study evaluated three Artificial Intelligence (AI) large language model\n(LLM) enabled platforms - ChatGPT, BARD, and Bing AI - to answer an\nundergraduate finance exam with 20 quantitative questions across various\ndifficulty levels. ChatGPT scored 30 percent, outperforming Bing AI, which\nscored 20 percent, while Bard lagged behind with a score of 15 percent. These\nmodels faced common challenges, such as inaccurate computations and formula\nselection. While they are currently insufficient for helping students pass the\nfinance exam, they serve as valuable tools for dedicated learners. Future\nadvancements are expected to overcome these limitations, allowing for improved\nformula selection and accurate computations and potentially enabling students\nto score 90 percent or higher.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.07979v2"
    },
    {
        "title": "Testing Partial Instrument Monotonicity",
        "authors": [
            "Hongyi Jiang",
            "Zhenting Sun"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  When multi-dimensional instruments are used to identify and estimate causal\neffects, the monotonicity condition may not hold due to heterogeneity in the\npopulation. Under a partial monotonicity condition, which only requires the\nmonotonicity to hold for each instrument separately holding all the other\ninstruments fixed, the 2SLS estimand can still be a positively weighted average\nof LATEs. In this paper, we provide a simple nonparametric test for partial\ninstrument monotonicity. We demonstrate the good finite sample properties of\nthe test through Monte Carlo simulations. We then apply the test to monetary\nincentives and distance from results centers as instruments for the knowledge\nof HIV status.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.08390v2"
    },
    {
        "title": "Weak Identification with Many Instruments",
        "authors": [
            "Anna Mikusheva",
            "Liyang Sun"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Linear instrumental variable regressions are widely used to estimate causal\neffects. Many instruments arise from the use of ``technical'' instruments and\nmore recently from the empirical strategy of ``judge design''. This paper\nsurveys and summarizes ideas from recent literature on estimation and\nstatistical inferences with many instruments for a single endogenous regressor.\nWe discuss how to assess the strength of the instruments and how to conduct\nweak identification-robust inference under heteroskedasticity. We establish new\nresults for a jack-knifed version of the Lagrange Multiplier (LM) test\nstatistic. Furthermore, we extend the weak-identification-robust tests to\nsettings with both many exogenous regressors and many instruments. We propose a\ntest that properly partials out many exogenous regressors while preserving the\nre-centering property of the jack-knife. The proposed tests have correct size\nand good power properties.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.09535v2"
    },
    {
        "title": "On the Inconsistency of Cluster-Robust Inference and How Subsampling Can\n  Fix It",
        "authors": [
            "Harold D. Chiang",
            "Yuya Sasaki",
            "Yulong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Conventional methods of cluster-robust inference are inconsistent in the\npresence of unignorably large clusters. We formalize this claim by establishing\na necessary and sufficient condition for the consistency of the conventional\nmethods. We find that this condition for the consistency is rejected for a\nmajority of empirical research papers. In this light, we propose a novel score\nsubsampling method that achieves uniform size control over a broad class of\ndata generating processes, covering that fails the conventional method.\nSimulation studies support these claims. With real data used by an empirical\npaper, we showcase that the conventional methods conclude significance while\nour proposed method concludes insignificance.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.10138v5"
    },
    {
        "title": "GARCHX-NoVaS: A Model-free Approach to Incorporate Exogenous Variables",
        "authors": [
            "Kejin Wu",
            "Sayar Karmakar",
            "Rangan Gupta"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this work, we explore the forecasting ability of a recently proposed\nnormalizing and variance-stabilizing (NoVaS) transformation with the possible\ninclusion of exogenous variables. From an applied point-of-view, extra\nknowledge such as fundamentals- and sentiments-based information could be\nbeneficial to improve the prediction accuracy of market volatility if they are\nincorporated into the forecasting process. In the classical approach, these\nmodels including exogenous variables are typically termed GARCHX-type models.\nBeing a Model-free prediction method, NoVaS has generally shown more accurate,\nstable and robust (to misspecifications) performance than that compared to\nclassical GARCH-type methods. This motivates us to extend this framework to the\nGARCHX forecasting as well. We derive the NoVaS transformation needed to\ninclude exogenous covariates and then construct the corresponding prediction\nprocedure. We show through extensive simulation studies that bolster our claim\nthat the NoVaS method outperforms traditional ones, especially for long-term\ntime aggregated predictions. We also provide an interesting data analysis to\nexhibit how our method could possibly shed light on the role of geopolitical\nrisks in forecasting volatility in national stock market indices for three\ndifferent countries in Europe.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.13346v3"
    },
    {
        "title": "Splash! Robustifying Donor Pools for Policy Studies",
        "authors": [
            "Jared Amani Greathouse",
            "Mani Bayani",
            "Jason Coupet"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Policy researchers using synthetic control methods typically choose a donor\npool in part by using policy domain expertise so the untreated units are most\nlike the treated unit in the pre intervention period. This potentially leaves\nestimation open to biases, especially when researchers have many potential\ndonors. We compare how functional principal component analysis synthetic\ncontrol, forward-selection, and the original synthetic control method select\ndonors. To do this, we use Gaussian Process simulations as well as policy case\nstudies from West German Reunification, a hotel moratorium in Barcelona, and a\nsugar-sweetened beverage tax in San Francisco. We then summarize the\nimplications for policy research and provide avenues for future work.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.13688v1"
    },
    {
        "title": "Break-Point Date Estimation for Nonstationary Autoregressive and\n  Predictive Regression Models",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this article, we study the statistical and asymptotic properties of\nbreak-point estimators in nonstationary autoregressive and predictive\nregression models for testing the presence of a single structural break at an\nunknown location in the full sample. Moreover, we investigate aspects such as\nhow the persistence properties of covariates and the location of the\nbreak-point affects the limiting distribution of the proposed break-point\nestimators.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.13915v1"
    },
    {
        "title": "Identification and Estimation of Demand Models with Endogenous Product\n  Entry and Exit",
        "authors": [
            "Victor Aguirregabiria",
            "Alessandro Iaria",
            "Senay Sokullu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper deals with the endogeneity of firms' entry and exit decisions in\ndemand estimation. Product entry decisions lack a single crossing property in\nterms of demand unobservables, which causes the inconsistency of conventional\nmethods dealing with selection. We present a novel and straightforward two-step\napproach to estimate demand while addressing endogenous product entry. In the\nfirst step, our method estimates a finite mixture model of product entry\naccommodating latent market types. In the second step, it estimates demand\ncontrolling for the propensity scores of all latent market types. We apply this\napproach to data from the airline industry.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.14196v1"
    },
    {
        "title": "Bandwidth Selection for Treatment Choice with Binary Outcomes",
        "authors": [
            "Takuya Ishihara"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This study considers the treatment choice problem when outcome variables are\nbinary. We focus on statistical treatment rules that plug in fitted values\nbased on nonparametric kernel regression and show that optimizing two\nparameters enables the calculation of the maximum regret. Using this result, we\npropose a novel bandwidth selection method based on the minimax regret\ncriterion. Finally, we perform a numerical analysis to compare the optimal\nbandwidth choices for the binary and normally distributed outcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.14375v2"
    },
    {
        "title": "Another Look at the Linear Probability Model and Nonlinear Index Models",
        "authors": [
            "Kaicheng Chen",
            "Robert S. Martin",
            "Jeffrey M. Wooldridge"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We reassess the use of linear models to approximate response probabilities of\nbinary outcomes, focusing on average partial effects (APE). We confirm that\nlinear projection parameters coincide with APEs in certain scenarios. Through\nsimulations, we identify other cases where OLS does or does not approximate\nAPEs and find that having large fraction of fitted values in [0, 1] is neither\nnecessary nor sufficient. We also show nonlinear least squares estimation of\nthe ramp model is consistent and asymptotically normal and is equivalent to\nusing OLS on an iteratively trimmed sample to reduce bias. Our findings offer\npractical guidance for empirical research.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.15338v3"
    },
    {
        "title": "Mixed-Effects Methods for Search and Matching Research",
        "authors": [
            "John M. Abowd",
            "Kevin L. McKinney"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We study mixed-effects methods for estimating equations containing person and\nfirm effects. In economics such models are usually estimated using\nfixed-effects methods. Recent enhancements to those fixed-effects methods\ninclude corrections to the bias in estimating the covariance matrix of the\nperson and firm effects, which we also consider.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.15445v1"
    },
    {
        "title": "Can Machine Learning Catch Economic Recessions Using Economic and Market\n  Sentiments?",
        "authors": [
            "Kian Tehranian"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Quantitative models are an important decision-making factor for policy makers\nand investors. Predicting an economic recession with high accuracy and\nreliability would be very beneficial for the society. This paper assesses\nmachine learning technics to predict economic recessions in United States using\nmarket sentiment and economic indicators (seventy-five explanatory variables)\nfrom Jan 1986 - June 2022 on a monthly basis frequency. In order to solve the\nissue of missing time-series data points, Autoregressive Integrated Moving\nAverage (ARIMA) method used to backcast explanatory variables. Analysis started\nwith reduction in high dimensional dataset to only most important characters\nusing Boruta algorithm, correlation matrix and solving multicollinearity issue.\nAfterwards, built various cross-validated models, both probability regression\nmethods and machine learning technics, to predict recession binary outcome. The\nmethods considered are Probit, Logit, Elastic Net, Random Forest, Gradient\nBoosting, and Neural Network. Lastly, discussed different models performance\nbased on confusion matrix, accuracy and F1 score with potential reasons for\ntheir weakness and robustness.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.16200v1"
    },
    {
        "title": "A Trimming Estimator for the Latent-Diffusion-Observed-Adoption Model",
        "authors": [
            "L. S. Sanna Stephan"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Network diffusion models are applicable to many socioeconomic interactions,\nyet network interaction is hard to observe or measure. Whenever the diffusion\nprocess is unobserved, the number of possible realizations of the latent matrix\nthat captures agents' diffusion statuses grows exponentially with the size of\nnetwork. Due to interdependencies, the log likelihood function can not be\nfactorized in individual components. As a consequence, exact estimation of\nlatent diffusion models with more than one round of interaction is\ncomputationally infeasible. In the present paper, I propose a trimming\nestimator that enables me to establish and maximize an approximate log\nlikelihood function that almost exactly identifies the peak of the true log\nlikelihood function whenever no more than one third of eligible agents are\nsubject to trimming.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.01471v1"
    },
    {
        "title": "Moment-Based Estimation of Diffusion and Adoption Parameters in Networks",
        "authors": [
            "L. S. Sanna Stephan"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  According to standard econometric theory, Maximum Likelihood estimation (MLE)\nis the efficient estimation choice, however, it is not always a feasible one.\nIn network diffusion models with unobserved signal propagation, MLE requires\nintegrating out a large number of latent variables, which quickly becomes\ncomputationally infeasible even for moderate network sizes and time horizons.\nLimiting the model time horizon on the other hand entails loss of important\ninformation while approximation techniques entail a (small) error that.\nSearching for a viable alternative is thus potentially highly beneficial. This\npaper proposes two estimators specifically tailored to the network diffusion\nmodel of partially observed adoption and unobserved network diffusion.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.01489v1"
    },
    {
        "title": "The Robust F-Statistic as a Test for Weak Instruments",
        "authors": [
            "Frank Windmeijer"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Montiel Olea and Pflueger (2013) proposed the effective F-statistic as a test\nfor weak instruments in terms of the Nagar bias of the two-stage least squares\n(2SLS) estimator relative to a benchmark worst-case bias. We show that their\nmethodology applies to a class of linear generalized method of moments (GMM)\nestimators with an associated class of generalized effective F-statistics. The\nstandard nonhomoskedasticity robust F-statistic is a member of this class. The\nassociated GMMf estimator, with the extension f for first-stage, is a novel and\nunusual estimator as the weight matrix is based on the first-stage residuals.\nAs the robust F-statistic can also be used as a test for underidentification,\nexpressions for the calculation of the weak-instruments critical values in\nterms of the Nagar bias of the GMMf estimator relative to the benchmark\nsimplify and no simulation methods or Patnaik (1949) distributional\napproximations are needed. In the grouped-data IV designs of Andrews (2018),\nwhere the robust F-statistic is large but the effective F-statistic is small,\nthe GMMf estimator is shown to behave much better in terms of bias than the\n2SLS estimator, as expected by the weak-instruments test results.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.01637v2"
    },
    {
        "title": "Design-Based Multi-Way Clustering",
        "authors": [
            "Luther Yap"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper extends the design-based framework to settings with multi-way\ncluster dependence, and shows how multi-way clustering can be justified when\nclustered assignment and clustered sampling occurs on different dimensions, or\nwhen either sampling or assignment is multi-way clustered. Unlike one-way\nclustering, the plug-in variance estimator in multi-way clustering is no longer\nconservative, so valid inference either requires an assumption on the\ncorrelation of treatment effects or a more conservative variance estimator.\nSimulations suggest that the plug-in variance estimator is usually robust, and\nthe conservative variance estimator is often too conservative.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.01658v1"
    },
    {
        "title": "The Local Projection Residual Bootstrap for AR(1) Models",
        "authors": [
            "Amilcar Velez"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a local projection residual bootstrap method to construct\nconfidence intervals for impulse response coefficients of AR(1) models. Our\nbootstrap method is based on the local projection (LP) approach and involves a\nresidual bootstrap procedure applied to AR(1) models. We present theoretical\nresults for our bootstrap method and proposed confidence intervals. First, we\nprove the uniform consistency of the LP-residual bootstrap over a large class\nof AR(1) models that allow for a unit root. Then, we prove the asymptotic\nvalidity of our confidence intervals over the same class of AR(1) models.\nFinally, we show that the LP-residual bootstrap provides asymptotic refinements\nfor confidence intervals on a restricted class of AR(1) models relative to\nthose required for the uniform consistency of our bootstrap.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.01889v3"
    },
    {
        "title": "On the use of U-statistics for linear dyadic interaction models",
        "authors": [
            "G. M. Szini"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Even though dyadic regressions are widely used in empirical applications, the\n(asymptotic) properties of estimation methods only began to be studied recently\nin the literature. This paper aims to provide in a step-by-step manner how\nU-statistics tools can be applied to obtain the asymptotic properties of\npairwise differences estimators for a two-way fixed effects model of dyadic\ninteractions. More specifically, we first propose an estimator for the model\nthat relies on pairwise differencing such that the fixed effects are\ndifferenced out. As a result, the summands of the influence function will not\nbe independent anymore, showing dependence on the individual level and\ntranslating to the fact that the usual law of large numbers and central limit\ntheorems do not straightforwardly apply. To overcome such obstacles, we show\nhow to generalize tools of U-statistics for single-index variables to the\ndouble-indices context of dyadic datasets. A key result is that there can be\ndifferent ways of defining the Hajek projection for a directed dyadic\nstructure, which will lead to distinct, but equivalent, consistent estimators\nfor the asymptotic variances. The results presented in this paper are easily\nextended to non-linear models.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.02089v1"
    },
    {
        "title": "Instrumental variable estimation of the proportional hazards model by\n  presmoothing",
        "authors": [
            "Lorenzo Tedesco",
            "Jad Beyhum",
            "Ingrid Van Keilegom"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We consider instrumental variable estimation of the proportional hazards\nmodel of Cox (1972). The instrument and the endogenous variable are discrete\nbut there can be (possibly continuous) exogenous covariables. By making a rank\ninvariance assumption, we can reformulate the proportional hazards model into a\nsemiparametric version of the instrumental variable quantile regression model\nof Chernozhukov and Hansen (2005). A na\\\"ive estimation approach based on\nconditional moment conditions generated by the model would lead to a highly\nnonconvex and nonsmooth objective function. To overcome this problem, we\npropose a new presmoothing methodology. First, we estimate the model\nnonparametrically - and show that this nonparametric estimator has a\nclosed-form solution in the leading case of interest of randomized experiments\nwith one-sided noncompliance. Second, we use the nonparametric estimator to\ngenerate ``proxy'' observations for which exogeneity holds. Third, we apply the\nusual partial likelihood estimator to the ``proxy'' data. While the paper\nfocuses on the proportional hazards model, our presmoothing approach could be\napplied to estimate other semiparametric formulations of the instrumental\nvariable quantile regression model. Our estimation procedure allows for random\nright-censoring. We show asymptotic normality of the resulting estimator. The\napproach is illustrated via simulation studies and an empirical application to\nthe Illinois\n",
        "pdf_link": "http://arxiv.org/pdf/2309.02183v1"
    },
    {
        "title": "Identifying spatial interdependence in panel data with large N and small\n  T",
        "authors": [
            "Deborah Gefang",
            "Stephen G. Hall",
            "George S. Tavlas"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper develops a simple two-stage variational Bayesian algorithm to\nestimate panel spatial autoregressive models, where N, the number of\ncross-sectional units, is much larger than T, the number of time periods\nwithout restricting the spatial effects using a predetermined weighting matrix.\nWe use Dirichlet-Laplace priors for variable selection and parameter shrinkage.\nWithout imposing any a priori structures on the spatial linkages between\nvariables, we let the data speak for themselves. Extensive Monte Carlo studies\nshow that our method is super-fast and our estimated spatial weights matrices\nstrongly resemble the true spatial weights matrices. As an illustration, we\ninvestigate the spatial interdependence of European Union regional gross value\nadded growth rates. In addition to a clear pattern of predominant country\nclusters, we have uncovered a number of important between-country spatial\nlinkages which are yet to be documented in the literature. This new procedure\nfor estimating spatial effects is of particular relevance for researchers and\npolicy makers alike.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.03740v1"
    },
    {
        "title": "Interpreting TSLS Estimators in Information Provision Experiments",
        "authors": [
            "Vod Vilfort",
            "Whitney Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  To estimate the causal effects of beliefs on actions, researchers often run\ninformation provision experiments. We consider the causal interpretation of\ntwo-stage least squares (TSLS) estimators in these experiments. We characterize\ncommon TSLS estimators as weighted averages of causal effects, and interpret\nthese weights under general belief updating conditions that nest parametric\nmodels from the literature. Our framework accommodates TSLS estimators for both\npassive and active control designs. Notably, we find that some passive control\nestimators allow for negative weights, which compromises their causal\ninterpretation. We give practical guidance on such issues, and illustrate our\nresults in two empirical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.04793v4"
    },
    {
        "title": "Forecasted Treatment Effects",
        "authors": [
            "Irene Botosaru",
            "Raffaella Giacomini",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We consider estimation and inference of the effects of a policy in the\nabsence of a control group. We obtain unbiased estimators of individual\n(heterogeneous) treatment effects and a consistent and asymptotically normal\nestimator of the average treatment effect. Our estimator averages over unbiased\nforecasts of individual counterfactuals, based on a (short) time series of\npre-treatment data. The paper emphasizes the importance of focusing on forecast\nunbiasedness rather than accuracy when the end goal is estimation of average\ntreatment effects. We show that simple basis function regressions ensure\nforecast unbiasedness for a broad class of data-generating processes for the\ncounterfactuals, even in short panels. In contrast, model-based forecasting\nrequires stronger assumptions and is prone to misspecification and estimation\nbias. We show that our method can replicate the findings of some previous\nempirical studies, but without using a control group.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.05639v3"
    },
    {
        "title": "Stochastic Learning of Semiparametric Monotone Index Models with Large\n  Sample Size",
        "authors": [
            "Qingsong Yao"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  I study the estimation of semiparametric monotone index models in the\nscenario where the number of observation points $n$ is extremely large and\nconventional approaches fail to work due to heavy computational burdens.\nMotivated by the mini-batch gradient descent algorithm (MBGD) that is widely\nused as a stochastic optimization tool in the machine learning field, I\nproposes a novel subsample- and iteration-based estimation procedure. In\nparticular, starting from any initial guess of the true parameter, I\nprogressively update the parameter using a sequence of subsamples randomly\ndrawn from the data set whose sample size is much smaller than $n$. The update\nis based on the gradient of some well-chosen loss function, where the\nnonparametric component is replaced with its Nadaraya-Watson kernel estimator\nbased on subsamples. My proposed algorithm essentially generalizes MBGD\nalgorithm to the semiparametric setup. Compared with full-sample-based method,\nthe new method reduces the computational time by roughly $n$ times if the\nsubsample size and the kernel function are chosen properly, so can be easily\napplied when the sample size $n$ is large. Moreover, I show that if I further\nconduct averages across the estimators produced during iterations, the\ndifference between the average estimator and full-sample-based estimator will\nbe $1/\\sqrt{n}$-trivial. Consequently, the average estimator is\n$1/\\sqrt{n}$-consistent and asymptotically normally distributed. In other\nwords, the new estimator substantially improves the computational speed, while\nat the same time maintains the estimation accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.06693v2"
    },
    {
        "title": "Causal inference in network experiments: regression-based analysis and\n  design-based properties",
        "authors": [
            "Mengsi Gao",
            "Peng Ding"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Investigating interference or spillover effects among units is a central task\nin many social science problems. Network experiments are powerful tools for\nthis task, which avoids endogeneity by randomly assigning treatments to units\nover networks. However, it is non-trivial to analyze network experiments\nproperly without imposing strong modeling assumptions. Previously, many\nresearchers have proposed sophisticated point estimators and standard errors\nfor causal effects under network experiments. We further show that\nregression-based point estimators and standard errors can have strong\ntheoretical guarantees if the regression functions and robust standard errors\nare carefully specified to accommodate the interference patterns under network\nexperiments. We first recall a well-known result that the Hajek estimator is\nnumerically identical to the coefficient from the weighted-least-squares fit\nbased on the inverse probability of the exposure mapping. Moreover, we\ndemonstrate that the regression-based approach offers three notable advantages:\nits ease of implementation, the ability to derive standard errors through the\nsame weighted-least-squares fit, and the capacity to integrate covariates into\nthe analysis, thereby enhancing estimation efficiency. Furthermore, we analyze\nthe asymptotic bias of the regression-based network-robust standard errors.\nRecognizing that the covariance estimator can be anti-conservative, we propose\nan adjusted covariance estimator to improve the empirical coverage rates.\nAlthough we focus on regression-based point estimators and standard errors, our\ntheory holds under the design-based framework, which assumes that the\nrandomness comes solely from the design of network experiments and allows for\narbitrary misspecification of the regression models.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.07476v2"
    },
    {
        "title": "Structural Econometric Estimation of the Basic Reproduction Number for\n  Covid-19 Across U.S. States and Selected Countries",
        "authors": [
            "Ida Johnsson",
            "M. Hashem Pesaran",
            "Cynthia Fan Yang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a structural econometric approach to estimating the basic\nreproduction number ($\\mathcal{R}_{0}$) of Covid-19. This approach identifies\n$\\mathcal{R}_{0}$ in a panel regression model by filtering out the effects of\nmitigating factors on disease diffusion and is easy to implement. We apply the\nmethod to data from 48 contiguous U.S. states and a diverse set of countries.\nOur results reveal a notable concentration of $\\mathcal{R}_{0}$ estimates with\nan average value of 4.5. Through a counterfactual analysis, we highlight a\nsignificant underestimation of the $\\mathcal{R}_{0}$ when mitigating factors\nare not appropriately accounted for.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.08619v1"
    },
    {
        "title": "Fixed-b Asymptotics for Panel Models with Two-Way Clustering",
        "authors": [
            "Kaicheng Chen",
            "Timothy J. Vogelsang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies a cluster robust variance estimator proposed by Chiang,\nHansen and Sasaki (2024) for linear panels. First, we show algebraically that\nthis variance estimator (CHS estimator, hereafter) is a linear combination of\nthree common variance estimators: the one-way unit cluster estimator, the \"HAC\nof averages\" estimator, and the \"average of HACs\" estimator. Based on this\nfinding, we obtain a fixed-$b$ asymptotic result for the CHS estimator and\ncorresponding test statistics as the cross-section and time sample sizes\njointly go to infinity. Furthermore, we propose two simple bias-corrected\nversions of the variance estimator and derive the fixed-$b$ limits. In a\nsimulation study, we find that the two bias-corrected variance estimators along\nwith fixed-$b$ critical values provide improvements in finite sample coverage\nprobabilities. We illustrate the impact of bias-correction and use of the\nfixed-$b$ critical values on inference in an empirical example on the\nrelationship between industry profitability and market concentration.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.08707v4"
    },
    {
        "title": "Ordered Correlation Forest",
        "authors": [
            "Riccardo Di Francesco"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Empirical studies in various social sciences often involve categorical\noutcomes with inherent ordering, such as self-evaluations of subjective\nwell-being and self-assessments in health domains. While ordered choice models,\nsuch as the ordered logit and ordered probit, are popular tools for analyzing\nthese outcomes, they may impose restrictive parametric and distributional\nassumptions. This paper introduces a novel estimator, the ordered correlation\nforest, that can naturally handle non-linearities in the data and does not\nassume a specific error term distribution. The proposed estimator modifies a\nstandard random forest splitting criterion to build a collection of forests,\neach estimating the conditional probability of a single class. Under an\n\"honesty\" condition, predictions are consistent and asymptotically normal. The\nweights induced by each forest are used to obtain standard errors for the\npredicted probabilities and the covariates' marginal effects. Evidence from\nsynthetic data shows that the proposed estimator features a superior prediction\nperformance than alternative forest-based estimators and demonstrates its\nability to construct valid confidence intervals for the covariates' marginal\neffects.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.08755v1"
    },
    {
        "title": "Bounds on Average Effects in Discrete Choice Panel Data Models",
        "authors": [
            "Cavit Pakel",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In discrete choice panel data, the estimation of average effects is crucial\nfor quantifying the effect of covariates, and for policy evaluation and\ncounterfactual analysis. This task is challenging in short panels with\nindividual-specific effects due to partial identification and the incidental\nparameter problem. In particular, estimation of the sharp identified set is\npractically infeasible at realistic sample sizes whenever the number of support\npoints of the observed covariates is large, such as when the covariates are\ncontinuous. In this paper, we therefore propose estimating outer bounds on the\nidentified set of average effects. Our bounds are easy to construct, converge\nat the parametric rate, and are computationally simple to obtain even in\nmoderately large samples, independent of whether the covariates are discrete or\ncontinuous. We also provide asymptotically valid confidence intervals on the\nidentified set.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.09299v3"
    },
    {
        "title": "require: Package dependencies for reproducible research",
        "authors": [
            "Sergio Correia",
            "Matthew P. Seay"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The ability to conduct reproducible research in Stata is often limited by the\nlack of version control for community-contributed packages. This article\nintroduces the require command, a tool designed to ensure Stata package\ndependencies are compatible across users and computer systems. Given a list of\nStata packages, require verifies that each package is installed, checks for a\nminimum or exact version or package release date, and optionally installs the\npackage if prompted by the researcher.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.11058v2"
    },
    {
        "title": "Identifying Causal Effects in Information Provision Experiments",
        "authors": [
            "Dylan Balla-Elliott"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Information provision experiments are a popular way to study causal effects\nof beliefs on behavior. Researchers estimate these effects using TSLS. I show\nthat existing TSLS specifications do not estimate the average partial effect;\nthey have weights proportional to belief updating in the first-stage. If people\nwhose decisions depend on their beliefs gather information before the\nexperiment, the information treatment may shift beliefs more for people with\nweak belief effects. This attenuates TSLS estimates. I propose researchers use\na local-least-squares (LLS) estimator that I show consistently estimates the\naverage partial effect (APE) under Bayesian updating, and apply it to Settele\n(2022).\n",
        "pdf_link": "http://arxiv.org/pdf/2309.11387v2"
    },
    {
        "title": "A detection analysis for temporal memory patterns at different\n  time-scales",
        "authors": [
            "Fabio Vanni",
            "David Lambert"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper introduces a novel methodology that utilizes latency to unveil\ntime-series dependence patterns. A customized statistical test detects memory\ndependence in event sequences by analyzing their inter-event time\ndistributions. Synthetic experiments based on the renewal-aging property assess\nthe impact of observer latency on the renewal property. Our test uncovers\nmemory patterns across diverse time scales, emphasizing the event sequence's\nprobability structure beyond correlations. The time series analysis produces a\nstatistical test and graphical plots which helps to detect dependence patterns\namong events at different time-scales if any. Furthermore, the test evaluates\nthe renewal assumption through aging experiments, offering valuable\napplications in time-series analysis within economics.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.12034v1"
    },
    {
        "title": "Estimating a k-modal nonparametric mixed logit model with market-level\n  data",
        "authors": [
            "Xiyuan Ren",
            "Joseph Y. J. Chow",
            "Prateek Bansal"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We propose a group-level agent-based mixed (GLAM) logit model that is\nestimated using market-level choice share data. The model non-parametrically\nrepresents taste heterogeneity through market-specific parameters by solving a\nmultiagent inverse utility maximization problem, addressing the limitations of\nexisting market-level choice models with parametric taste heterogeneity. A case\nstudy of mode choice in New York State is conducted using synthetic population\ndata of 53.55 million trips made by 19.53 million residents in 2019. These\ntrips are aggregated based on population segments and census block group-level\norigin-destination (OD) pairs, resulting in 120,740 markets/agents. We\nbenchmark in-sample and out-of-sample predictive performance of the GLAM logit\nmodel against multinomial logit, nested logit, inverse product differentiation\nlogit, and random coefficient logit (RCL) models. The results show that GLAM\nlogit outperforms benchmark models, improving the overall in-sample predictive\naccuracy from 78.7% to 96.71% and out-of-sample accuracy from 65.30% to 81.78%.\nThe price elasticities and diversion ratios retrieved from GLAM logit and\nbenchmark models exhibit similar substitution patterns among the six travel\nmodes. GLAM logit is scalable and computationally efficient, taking less than\none-tenth of the time taken to estimate the RCL model. The agent-specific\nparameters in GLAM logit provide additional insights such as value-of-time\n(VOT) across segments and regions, which has been further utilized to\ndemonstrate its application in analyzing NYS travelers' mode choice response to\nthe congestion pricing. The agent-specific parameters in GLAM logit facilitate\ntheir seamless integration into supply-side optimization models for revenue\nmanagement and system design.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.13159v2"
    },
    {
        "title": "Nonparametric estimation of conditional densities by generalized random\n  forests",
        "authors": [
            "Federico Zincenko"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Considering a continuous random variable $Y$ together with a continuous\nrandom vector $X$, I propose a nonparametric estimator $\\hat{f}(\\cdot|x)$ for\nthe conditional density of $Y$ given $X=x$. This estimator takes the form of an\nexponential series whose coefficients $\\hat{\\theta}_{x}=(\\hat{\\theta}_{x,1},\n\\dots,\\hat{\\theta}_{x,J})$ are the solution of a system of nonlinear equations\nthat depends on an estimator of the conditional expectation $E[\\phi (Y)|X=x]$,\nwhere $\\phi$ is a $J$-dimensional vector of basis functions. The distinguishing\nfeature of the proposed estimator is that $E[\\phi(Y)|X=x]$ is estimated by\ngeneralized random forest (Athey, Tibshirani, and Wager, Annals of Statistics,\n2019), targeting the heterogeneity of $\\hat{\\theta}_{x}$ across $x$. I show\nthat $\\hat{f}(\\cdot|x)$ is uniformly consistent and asymptotically normal,\nallowing $J \\rightarrow \\infty$. I also provide a standard error formula to\nconstruct asymptotically valid confidence intervals. Results from Monte Carlo\nexperiments and an empirical illustration are provided.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.13251v3"
    },
    {
        "title": "Unified Inference for Dynamic Quantile Predictive Regression",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper develops unified asymptotic distribution theory for dynamic\nquantile predictive regressions which is useful when examining quantile\npredictability in stock returns under possible presence of nonstationarity.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.14160v2"
    },
    {
        "title": "Sluggish news reactions: A combinatorial approach for synchronizing\n  stock jumps",
        "authors": [
            "Nabil Bouamara",
            "Kris Boudt",
            "Sébastien Laurent",
            "Christopher J. Neely"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Stock prices often react sluggishly to news, producing gradual jumps and jump\ndelays. Econometricians typically treat these sluggish reactions as\nmicrostructure effects and settle for a coarse sampling grid to guard against\nthem. Synchronizing mistimed stock returns on a fine sampling grid allows us to\nautomatically detect noisy jumps and better approximate the true common jumps\nin related stock prices.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.15705v1"
    },
    {
        "title": "Smoothing the Nonsmoothness",
        "authors": [
            "Chaohua Dong",
            "Jiti Gao",
            "Bin Peng",
            "Yundong Tu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  To tackle difficulties for theoretical studies in situations involving\nnonsmooth functions, we propose a sequence of infinitely differentiable\nfunctions to approximate the nonsmooth function under consideration. A rate of\napproximation is established and an illustration of its application is then\nprovided.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.16348v1"
    },
    {
        "title": "Specification testing with grouped fixed effects",
        "authors": [
            "Claudia Pigini",
            "Alessandro Pionati",
            "Francesco Valentini"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We propose a bootstrap generalized Hausman test for the correct specification\nof unobserved heterogeneity in both linear and nonlinear fixed-effects panel\ndata models. We consider as null hypotheses two scenarios in which the\nunobserved heterogeneity is either time-invariant or specified as additive\nindividual and time effects. We contrast the standard fixed-effects estimators\nwith the recently developed two-way grouped fixed-effects estimator, that is\nconsistent in the presence of time-varying heterogeneity under minimal\nspecification and distributional assumptions for the unobserved effects. The\nHausman test exploits the general formulation for the variance of the vector of\ncontrasts and critical values are computed via parametric percentile bootstrap,\nso as to account for the non-centrality of the asymptotic chi-square\ndistribution arising from the incidental parameters and approximation biases.\nMonte Carlo evidence shows that the test has correct size and good power\nproperties. We provide two empirical applications to illustrate the proposed\ntest: the first one is based on a linear model for the determinants of the wage\nof working women and the second analyzes the trade extensive margin.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.01950v1"
    },
    {
        "title": "Sharp and Robust Estimation of Partially Identified Discrete Response\n  Models",
        "authors": [
            "Shakeeb Khan",
            "Tatiana Komarova",
            "Denis Nekipelov"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Semiparametric discrete choice models are widely used in a variety of\npractical applications. While these models are point identified in the presence\nof continuous covariates, they can become partially identified when covariates\nare discrete. In this paper we find that classical estimators, including the\nmaximum score estimator, (Manski (1975)), loose their attractive statistical\nproperties without point identification. First of all, they are not sharp with\nthe estimator converging to an outer region of the identified set, (Komarova\n(2013)), and in many discrete designs it weakly converges to a random set.\nSecond, they are not robust, with their distribution limit discontinuously\nchanging with respect to the parameters of the model. We propose a novel class\nof estimators based on the concept of a quantile of a random set, which we show\nto be both sharp and robust. We demonstrate that our approach extends from\ncross-sectional settings to classical static and dynamic discrete panel data\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.02414v4"
    },
    {
        "title": "Moran's I Lasso for models with spatially correlated data",
        "authors": [
            "Sylvain Barde",
            "Rowan Cherodian",
            "Guy Tchuente"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a Lasso-based estimator which uses information embedded\nin the Moran statistic to develop a selection procedure called Moran's I Lasso\n(Mi-Lasso) to solve the Eigenvector Spatial Filtering (ESF) eigenvector\nselection problem. ESF uses a subset of eigenvectors from a spatial weights\nmatrix to efficiently account for any omitted cross-sectional correlation terms\nin a classical linear regression framework, thus does not require the\nresearcher to explicitly specify the spatial part of the underlying structural\nmodel. We derive performance bounds and show the necessary conditions for\nconsistent eigenvector selection. The key advantages of the proposed estimator\nare that it is intuitive, theoretically grounded, and substantially faster than\nLasso based on cross-validation or any proposed forward stepwise procedure. Our\nmain simulation results show the proposed selection procedure performs well in\nfinite samples. Compared to existing selection procedures, we find Mi-Lasso has\none of the smallest biases and mean squared errors across a range of sample\nsizes and levels of spatial correlation. An application on house prices further\ndemonstrates Mi-Lasso performs well compared to existing procedures.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.02773v1"
    },
    {
        "title": "Challenges in Statistically Rejecting the Perfect Competition Hypothesis\n  Using Imperfect Competition Data",
        "authors": [
            "Yuri Matsumura",
            "Suguru Otani"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We theoretically prove why statistically rejecting the null hypothesis of\nperfect competition is challenging, known as a common problem in the\nliterature. We also assess the finite sample performance of the conduct\nparameter test in homogeneous goods markets, showing that statistical power\nincreases with the number of markets, a larger conduct parameter, and a\nstronger demand rotation instrument. However, even with a moderate number of\nmarkets and five firms, rejecting the null hypothesis of perfect competition\nremains difficult, irrespective of instrument strength or the use of optimal\ninstruments. Our findings suggest that empirical results failing to reject\nperfect competition are due to the limited number of markets rather than\nmethodological shortcomings.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.04576v4"
    },
    {
        "title": "Robust Minimum Distance Inference in Structural Models",
        "authors": [
            "Joan Alegre",
            "Juan Carlos Escanciano"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes minimum distance inference for a structural parameter of\ninterest, which is robust to the lack of identification of other structural\nnuisance parameters. Some choices of the weighting matrix lead to asymptotic\nchi-squared distributions with degrees of freedom that can be consistently\nestimated from the data, even under partial identification. In any case,\nknowledge of the level of under-identification is not required. We study the\npower of our robust test. Several examples show the wide applicability of the\nprocedure and a Monte Carlo investigates its finite sample performance. Our\nidentification-robust inference method can be applied to make inferences on\nboth calibrated (fixed) parameters and any other structural parameter of\ninterest. We illustrate the method's usefulness by applying it to a structural\nmodel on the non-neutrality of monetary policy, as in \\cite{nakamura2018high},\nwhere we empirically evaluate the validity of the calibrated parameters and we\ncarry out robust inference on the slope of the Phillips curve and the\ninformation effect.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.05761v1"
    },
    {
        "title": "Identification and Estimation of a Semiparametric Logit Model using\n  Network Data",
        "authors": [
            "Brice Romuald Gueyap Kounga"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies the identification and estimation of a semiparametric\nbinary network model in which the unobserved social characteristic is\nendogenous, that is, the unobserved individual characteristic influences both\nthe binary outcome of interest and how links are formed within the network. The\nexact functional form of the latent social characteristic is not known. The\nproposed estimators are obtained based on matching pairs of agents whose\nnetwork formation distributions are the same. The consistency and the\nasymptotic distribution of the estimators are proposed. The finite sample\nproperties of the proposed estimators in a Monte-Carlo simulation are assessed.\nWe conclude this study with an empirical application.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.07151v2"
    },
    {
        "title": "Inference for Nonlinear Endogenous Treatment Effects Accounting for\n  High-Dimensional Covariate Complexity",
        "authors": [
            "Qingliang Fan",
            "Zijian Guo",
            "Ziwei Mei",
            "Cun-Hui Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Nonlinearity and endogeneity are prevalent challenges in causal analysis\nusing observational data. This paper proposes an inference procedure for a\nnonlinear and endogenous marginal effect function, defined as the derivative of\nthe nonparametric treatment function, with a primary focus on an additive model\nthat includes high-dimensional covariates. Using the control function approach\nfor identification, we implement a regularized nonparametric estimation to\nobtain an initial estimator of the model. Such an initial estimator suffers\nfrom two biases: the bias in estimating the control function and the\nregularization bias for the high-dimensional outcome model. Our key innovation\nis to devise the double bias correction procedure that corrects these two\nbiases simultaneously. Building on this debiased estimator, we further provide\na confidence band of the marginal effect function. Simulations and an empirical\nstudy of air pollution and migration demonstrate the validity of our\nprocedures.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.08063v3"
    },
    {
        "title": "Structural Vector Autoregressions and Higher Moments: Challenges and\n  Solutions in Small Samples",
        "authors": [
            "Sascha A. Keweloh"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Generalized method of moments estimators based on higher-order moment\nconditions derived from independent shocks can be used to identify and estimate\nthe simultaneous interaction in structural vector autoregressions. This study\nhighlights two problems that arise when using these estimators in small\nsamples. First, imprecise estimates of the asymptotically efficient weighting\nmatrix and the asymptotic variance lead to volatile estimates and inaccurate\ninference. Second, many moment conditions lead to a small sample scaling bias\ntowards innovations with a variance smaller than the normalizing unit variance\nassumption. To address the first problem, I propose utilizing the assumption of\nindependent structural shocks to estimate the efficient weighting matrix and\nthe variance of the estimator. For the second issue, I propose incorporating a\ncontinuously updated scaling term into the weighting matrix, eliminating the\nscaling bias. To demonstrate the effectiveness of these measures, I conducted a\nMonte Carlo simulation which shows a significant improvement in the performance\nof the estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.08173v1"
    },
    {
        "title": "Real-time Prediction of the Great Recession and the Covid-19 Recession",
        "authors": [
            "Seulki Chung"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper uses standard and penalized logistic regression models to predict\nthe Great Recession and the Covid-19 recession in the US in real time. It\nexamines the predictability of various macroeconomic and financial indicators\nwith respect to the NBER recession indicator. The findings strongly support the\nuse of penalized logistic regression models in recession forecasting. These\nmodels, particularly the ridge logistic regression model, outperform the\nstandard logistic regression model in predicting the Great Recession in the US\nacross different forecast horizons. The study also confirms the traditional\nsignificance of the term spread as an important recession indicator. However,\nit acknowledges that the Covid-19 recession remains unpredictable due to the\nunprecedented nature of the pandemic. The results are validated by creating a\nrecession indicator through principal component analysis (PCA) on selected\nvariables, which strongly correlates with the NBER recession indicator and is\nless affected by publication lags.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.08536v5"
    },
    {
        "title": "Estimating Individual Responses when Tomorrow Matters",
        "authors": [
            "Stephane Bonhomme",
            "Angela Denis"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We propose a regression-based approach to estimate how individuals'\nexpectations influence their responses to a counterfactual change. We provide\nconditions under which average partial effects based on regression estimates\nrecover structural effects. We propose a practical three-step estimation method\nthat relies on panel data on subjective expectations. We illustrate our\napproach in a model of consumption and saving, focusing on the impact of an\nincome tax that not only changes current income but also affects beliefs about\nfuture income. Applying our approach to Italian survey data, we find that\nindividuals' beliefs matter for evaluating the impact of tax policies on\nconsumption decisions.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.09105v3"
    },
    {
        "title": "Trimmed Mean Group Estimation of Average Effects in Ultra Short T Panels\n  under Correlated Heterogeneity",
        "authors": [
            "M. Hashem Pesaran",
            "Liying Yang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The commonly used two-way fixed effects estimator is biased under correlated\nheterogeneity and can lead to misleading inference. This paper proposes a new\ntrimmed mean group (TMG) estimator which is consistent at the irregular rate of\nn^{1/3} even if the time dimension of the panel is as small as the number of\nits regressors. Extensions to panels with time effects are provided, and a\nHausman test of correlated heterogeneity is proposed. Small sample properties\nof the TMG estimator (with and without time effects) are investigated by Monte\nCarlo experiments and shown to be satisfactory and perform better than other\ntrimmed estimators proposed in the literature. The proposed test of correlated\nheterogeneity is also shown to have the correct size and satisfactory power.\nThe utility of the TMG approach is illustrated with an empirical application.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.11680v2"
    },
    {
        "title": "Machine Learning for Staggered Difference-in-Differences and Dynamic\n  Treatment Effect Heterogeneity",
        "authors": [
            "Julia Hatamyar",
            "Noemi Kreif",
            "Rudi Rocha",
            "Martin Huber"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We combine two recently proposed nonparametric difference-in-differences\nmethods, extending them to enable the examination of treatment effect\nheterogeneity in the staggered adoption setting using machine learning. The\nproposed method, machine learning difference-in-differences (MLDID), allows for\nestimation of time-varying conditional average treatment effects on the\ntreated, which can be used to conduct detailed inference on drivers of\ntreatment effect heterogeneity. We perform simulations to evaluate the\nperformance of MLDID and find that it accurately identifies the true predictors\nof treatment effect heterogeneity. We then use MLDID to evaluate the\nheterogeneous impacts of Brazil's Family Health Program on infant mortality,\nand find those in poverty and urban locations experienced the impact of the\npolicy more quickly than other subgroups.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.11962v1"
    },
    {
        "title": "Nonparametric Regression with Dyadic Data",
        "authors": [
            "Brice Romuald Gueyap Kounga"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies the identification and estimation of a nonparametric\nnonseparable dyadic model where the structural function and the distribution of\nthe unobservable random terms are assumed to be unknown. The identification and\nthe estimation of the distribution of the unobservable random term are also\nproposed. I assume that the structural function is continuous and strictly\nincreasing in the unobservable heterogeneity. I propose suitable normalization\nfor the identification by allowing the structural function to have some\ndesirable properties such as homogeneity of degree one in the unobservable\nrandom term and some of its observables. The consistency and the asymptotic\ndistribution of the estimators are proposed. The finite sample properties of\nthe proposed estimators in a Monte-Carlo simulation are assessed.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.12825v1"
    },
    {
        "title": "Bayesian Estimation of Panel Models under Potentially Sparse\n  Heterogeneity",
        "authors": [
            "Hyungsik Roger Moon",
            "Frank Schorfheide",
            "Boyuan Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We incorporate a version of a spike and slab prior, comprising a pointmass at\nzero (\"spike\") and a Normal distribution around zero (\"slab\") into a dynamic\npanel data framework to model coefficient heterogeneity. In addition to\nhomogeneity and full heterogeneity, our specification can also capture sparse\nheterogeneity, that is, there is a core group of units that share common\nparameters and a set of deviators with idiosyncratic parameters. We fit a model\nwith unobserved components to income data from the Panel Study of Income\nDynamics. We find evidence for sparse heterogeneity for balanced panels\ncomposed of individuals with long employment histories.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.13785v2"
    },
    {
        "title": "Dynamic Factor Models: a Genealogy",
        "authors": [
            "Matteo Barigozzi",
            "Marc Hallin"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Dynamic factor models have been developed out of the need of analyzing and\nforecasting time series in increasingly high dimensions. While mathematical\nstatisticians faced with inference problems in high-dimensional observation\nspaces were focusing on the so-called spiked-model-asymptotics, econometricians\nadopted an entirely and considerably more effective asymptotic approach, rooted\nin the factor models originally considered in psychometrics. The so-called\ndynamic factor model methods, in two decades, has grown into a wide and\nsuccessful body of techniques that are widely used in central banks, financial\ninstitutions, economic and statistical institutes. The objective of this\nchapter is not an extensive survey of the topic but a sketch of its historical\ngrowth, with emphasis on the various assumptions and interpretations, and a\nfamily tree of its main variants.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.17278v2"
    },
    {
        "title": "Doubly Robust Identification of Causal Effects of a Continuous Treatment\n  using Discrete Instruments",
        "authors": [
            "Yingying Dong",
            "Ying-Ying Lee"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Many empirical applications estimate causal effects of a continuous\nendogenous variable (treatment) using a binary instrument. Estimation is\ntypically done through linear 2SLS. This approach requires a mean treatment\nchange and causal interpretation requires the LATE-type monotonicity in the\nfirst stage. An alternative approach is to explore distributional changes in\nthe treatment, where the first-stage restriction is treatment rank similarity.\nWe propose causal estimands that are doubly robust in that they are valid under\neither of these two restrictions. We apply the doubly robust estimation to\nestimate the impacts of sleep on well-being. Our new estimates corroborate the\nusual 2SLS estimates.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.18504v3"
    },
    {
        "title": "Popularity, face and voice: Predicting and interpreting livestreamers'\n  retail performance using machine learning techniques",
        "authors": [
            "Xiong Xiong",
            "Fan Yang",
            "Li Su"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Livestreaming commerce, a hybrid of e-commerce and self-media, has expanded\nthe broad spectrum of traditional sales performance determinants. To\ninvestigate the factors that contribute to the success of livestreaming\ncommerce, we construct a longitudinal firm-level database with 19,175\nobservations, covering an entire livestreaming subsector. By comparing the\nforecasting accuracy of eight machine learning models, we identify a random\nforest model that provides the best prediction of gross merchandise volume\n(GMV). Furthermore, we utilize explainable artificial intelligence to open the\nblack-box of machine learning model, discovering four new facts: 1) variables\nrepresenting the popularity of livestreaming events are crucial features in\npredicting GMV. And voice attributes are more important than appearance; 2)\npopularity is a major determinant of sales for female hosts, while vocal\naesthetics is more decisive for their male counterparts; 3) merits and\ndrawbacks of the voice are not equally valued in the livestreaming market; 4)\nbased on changes of comments, page views and likes, sales growth can be divided\ninto three stages. Finally, we innovatively propose a 3D-SHAP diagram that\ndemonstrates the relationship between predicting feature importance, target\nvariable, and its predictors. This diagram identifies bottlenecks for both\nbeginner and top livestreamers, providing insights into ways to optimize their\nsales performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.19200v1"
    },
    {
        "title": "Spectral identification and estimation of mixed causal-noncausal\n  invertible-noninvertible models",
        "authors": [
            "Alain Hecq",
            "Daniel Velasquez-Gaviria"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper introduces new techniques for estimating, identifying and\nsimulating mixed causal-noncausal invertible-noninvertible models. We propose a\nframework that integrates high-order cumulants, merging both the spectrum and\nbispectrum into a single estimation function. The model that most adequately\nrepresents the data under the assumption that the error term is i.i.d. is\nselected. Our Monte Carlo study reveals unbiased parameter estimates and a high\nfrequency with which correct models are identified. We illustrate our strategy\nthrough an empirical analysis of returns from 24 Fama-French emerging market\nstock portfolios. The findings suggest that each portfolio displays noncausal\ndynamics, producing white noise residuals devoid of conditional heteroscedastic\neffects.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.19543v1"
    },
    {
        "title": "Semiparametric Discrete Choice Models for Bundles",
        "authors": [
            "Fu Ouyang",
            "Thomas Tao Yang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We propose two approaches to estimate semiparametric discrete choice models\nfor bundles. Our first approach is a kernel-weighted rank estimator based on a\nmatching-based identification strategy. We establish its complete asymptotic\nproperties and prove the validity of the nonparametric bootstrap for inference.\nWe then introduce a new multi-index least absolute deviations (LAD) estimator\nas an alternative, of which the main advantage is its capacity to estimate\npreference parameters on both alternative- and agent-specific regressors. Both\nmethods can account for arbitrary correlation in disturbances across choices,\nwith the former also allowing for interpersonal heteroskedasticity. We also\ndemonstrate that the identification strategy underlying these procedures can be\nextended naturally to panel data settings, producing an analogous localized\nmaximum score estimator and a LAD estimator for estimating bundle choice models\nwith fixed effects. We derive the limiting distribution of the former and\nverify the validity of the numerical bootstrap as an inference tool. All our\nproposed methods can be applied to general multi-index models. Monte Carlo\nexperiments show that they perform well in finite samples.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.00013v3"
    },
    {
        "title": "Robustify and Tighten the Lee Bounds: A Sample Selection Model under\n  Stochastic Monotonicity and Symmetry Assumptions",
        "authors": [
            "Yuta Okamoto"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In the presence of sample selection, Lee's (2009) nonparametric bounds are a\npopular tool for estimating a treatment effect. However, the Lee bounds rely on\nthe monotonicity assumption, whose empirical validity is sometimes unclear.\nFurthermore, the bounds are often regarded to be wide and less informative even\nunder monotonicity. To address these issues, this study introduces a stochastic\nversion of the monotonicity assumption alongside a nonparametric distributional\nshape constraint. The former enhances the robustness of the Lee bounds with\nrespect to monotonicity, while the latter helps tighten these bounds. The\nobtained bounds do not rely on the exclusion restriction and can be root-$n$\nconsistently estimable, making them practically viable. The potential\nusefulness of the proposed methods is illustrated by their application on\nexperimental data from the after-school instruction programme studied by\nMuralidharan, Singh, and Ganimian (2019).\n",
        "pdf_link": "http://arxiv.org/pdf/2311.00439v4"
    },
    {
        "title": "On Gaussian Process Priors in Conditional Moment Restriction Models",
        "authors": [
            "Sid Kankanala"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies quasi Bayesian estimation and uncertainty quantification\nfor an unknown function that is identified by a nonparametric conditional\nmoment restriction. We derive contraction rates for a class of Gaussian process\npriors. Furthermore, we provide conditions under which a Bernstein von Mises\ntheorem holds for the quasi-posterior distribution. As a consequence, we show\nthat optimally weighted quasi-Bayes credible sets have exact asymptotic\nfrequentist coverage.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.00662v2"
    },
    {
        "title": "The learning effects of subsidies to bundled goods: a semiparametric\n  approach",
        "authors": [
            "Luis Alvarez",
            "Ciro Biderman"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Can temporary subsidies to bundles induce long-run changes in demand due to\nlearning about the quality of one of the constituent goods? This paper provides\ntheoretical support and empirical evidence on this mechanism. Theoretically, we\nintroduce a model where an agent learns about the quality of an innovation\nthrough repeated consumption. We then assess the predictions of our theory in a\nrandomised experiment in a ridesharing platform. The experiment subsidised car\ntrips integrating with a train or metro station, which we interpret as a\nbundle. Given the heavy-tailed nature of our data, we propose a semiparametric\nspecification for treatment effects that enables the construction of more\nefficient estimators. We then introduce an efficient estimator for our\nspecification by relying on L-moments. Our results indicate that a ten-weekday\n50\\% discount on integrated trips leads to a large contemporaneous increase in\nthe demand for integration, and, consistent with our model, persistent changes\nin the mean and dispersion of nonintegrated app rides. These effects last for\nover four months. A calibration of our theoretical model suggests that around\n40\\% of the contemporaneous increase in integrated rides may be attributable to\nincreased incentives to learning. Our results have nontrivial policy\nimplications for the design of public transit systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.01217v4"
    },
    {
        "title": "Pooled Bewley Estimator of Long Run Relationships in Dynamic\n  Heterogenous Panels",
        "authors": [
            "Alexander Chudik",
            "M. Hashem Pesaran",
            "Ron P. Smith"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Using a transformation of the autoregressive distributed lag model due to\nBewley, a novel pooled Bewley (PB) estimator of long-run coefficients for\ndynamic panels with heterogeneous short-run dynamics is proposed. The PB\nestimator is directly comparable to the widely used Pooled Mean Group (PMG)\nestimator, and is shown to be consistent and asymptotically normal. Monte Carlo\nsimulations show good small sample performance of PB compared to the existing\nestimators in the literature, namely PMG, panel dynamic OLS (PDOLS), and panel\nfully-modified OLS (FMOLS). Application of two bias-correction methods and a\nbootstrapping of critical values to conduct inference robust to cross-sectional\ndependence of errors are also considered. The utility of the PB estimator is\nillustrated in an empirical application to the aggregate consumption function.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.02196v1"
    },
    {
        "title": "Estimation and Inference for a Class of Generalized Hierarchical Models",
        "authors": [
            "Chaohua Dong",
            "Jiti Gao",
            "Bin Peng",
            "Yayi Yan"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this paper, we consider estimation and inference for the unknown\nparameters and function involved in a class of generalized hierarchical models.\nSuch models are of great interest in the literature of neural networks (such as\nBauer and Kohler, 2019). We propose a rectified linear unit (ReLU) based deep\nneural network (DNN) approach, and contribute to the design of DNN by i)\nproviding more transparency for practical implementation, ii) defining\ndifferent types of sparsity, iii) showing the differentiability, iv) pointing\nout the set of effective parameters, and v) offering a new variant of rectified\nlinear activation function (ReLU), etc. Asymptotic properties are established\naccordingly, and a feasible procedure for the purpose of inference is also\nproposed. We conduct extensive numerical studies to examine the finite-sample\nperformance of the estimation methods, and we also evaluate the empirical\nrelevance and applicability of the proposed models and estimation methods to\nreal data.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.02789v5"
    },
    {
        "title": "Optimal Estimation Methodologies for Panel Data Regression Models",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This survey study discusses main aspects to optimal estimation methodologies\nfor panel data regression models. In particular, we present current\nmethodological developments for modeling stationary panel data as well as\nrobust methods for estimation and inference in nonstationary panel data\nregression models. Some applications from the network econometrics and high\ndimensional statistics literature are also discussed within a stationary time\nseries environment.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.03471v3"
    },
    {
        "title": "Debiased Fixed Effects Estimation of Binary Logit Models with\n  Three-Dimensional Panel Data",
        "authors": [
            "Amrei Stammann"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Naive maximum likelihood estimation of binary logit models with fixed effects\nleads to unreliable inference due to the incidental parameter problem. We study\nthe case of three-dimensional panel data, where the model includes three sets\nof additive and overlapping unobserved effects. This encompasses models for\nnetwork panel data, where senders and receivers maintain bilateral\nrelationships over time, and fixed effects account for unobserved heterogeneity\nat the sender-time, receiver-time, and sender-receiver levels. In an asymptotic\nframework, where all three panel dimensions grow large at constant relative\nrates, we characterize the leading bias of the naive estimator. The inference\nproblem we identify is particularly severe, as it is not possible to balance\nthe order of the bias and the standard deviation. As a consequence, the naive\nestimator has a degenerating asymptotic distribution, which exacerbates the\ninference problem relative to other fixed effects estimators studied in the\nliterature. To resolve the inference problem, we derive explicit expressions to\ndebias the fixed effects estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.04073v1"
    },
    {
        "title": "Design-based Estimation Theory for Complex Experiments",
        "authors": [
            "Haoge Chang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper considers the estimation of treatment effects in randomized\nexperiments with complex experimental designs, including cases with\ninterference between units. We develop a design-based estimation theory for\narbitrary experimental designs. Our theory facilitates the analysis of many\ndesign-estimator pairs that researchers commonly employ in practice and provide\nprocedures to consistently estimate asymptotic variance bounds. We propose new\nclasses of estimators with favorable asymptotic properties from a design-based\npoint of view. In addition, we propose a scalar measure of experimental\ncomplexity which can be linked to the design-based variance of the estimators.\nWe demonstrate the performance of our estimators using simulated datasets based\non an actual network experiment studying the effect of social networks on\ninsurance adoptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.06891v1"
    },
    {
        "title": "High Dimensional Binary Choice Model with Unknown Heteroskedasticity or\n  Instrumental Variables",
        "authors": [
            "Fu Ouyang",
            "Thomas Tao Yang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a new method for estimating high-dimensional binary\nchoice models. The model we consider is semiparametric, placing no\ndistributional assumptions on the error term, allowing for heteroskedastic\nerrors, and permitting endogenous regressors. Our proposed approaches extend\nthe special regressor estimator originally proposed by Lewbel (2000). This\nestimator becomes impractical in high-dimensional settings due to the curse of\ndimensionality associated with high-dimensional conditional density estimation.\nTo overcome this challenge, we introduce an innovative data-driven dimension\nreduction method for nonparametric kernel estimators, which constitutes the\nmain innovation of this work. The method combines distance covariance-based\nscreening with cross-validation (CV) procedures, rendering the special\nregressor estimation feasible in high dimensions. Using the new feasible\nconditional density estimator, we address the variable and moment (instrumental\nvariable) selection problems for these models. We apply penalized least squares\n(LS) and Generalized Method of Moments (GMM) estimators with a smoothly clipped\nabsolute deviation (SCAD) penalty. A comprehensive analysis of the oracle and\nasymptotic properties of these estimators is provided. Monte Carlo simulations\nare employed to demonstrate the effectiveness of our proposed procedures in\nfinite sample scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.07067v1"
    },
    {
        "title": "Estimating Conditional Value-at-Risk with Nonstationary Quantile\n  Predictive Regression Models",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper develops an asymptotic distribution theory for an endogenous\ninstrumentation approach in quantile predictive regressions when both generated\ncovariates and persistent predictors are used. The generated covariates are\nobtained from an auxiliary quantile predictive regression model and the\nstatistical problem of interest is the robust estimation and inference of the\nparameters that correspond to the primary quantile predictive regression in\nwhich this generated covariate is added to the set of nonstationary regressors.\nWe find that the proposed doubly IVX corrected estimator is robust to the\nabstract degree of persistence regardless of the presence of generated\nregressor obtained from the first stage procedure. The asymptotic properties of\nthe two-stage IVX estimator such as mixed Gaussianity are established while the\nasymptotic covariance matrix is adjusted to account for the first-step\nestimation error.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.08218v6"
    },
    {
        "title": "Locally Asymptotically Minimax Statistical Treatment Rules Under Partial\n  Identification",
        "authors": [
            "Daido Kido"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Policymakers often desire a statistical treatment rule (STR) that determines\na treatment assignment rule deployed in a future population from available\ndata. With the true knowledge of the data generating process, the average\ntreatment effect (ATE) is the key quantity characterizing the optimal treatment\nrule. Unfortunately, the ATE is often not point identified but partially\nidentified. Presuming the partial identification of the ATE, this study\nconducts a local asymptotic analysis and develops the locally asymptotically\nminimax (LAM) STR. The analysis does not assume the full differentiability but\nthe directional differentiability of the boundary functions of the\nidentification region of the ATE. Accordingly, the study shows that the LAM STR\ndiffers from the plug-in STR. A simulation study also demonstrates that the LAM\nSTR outperforms the plug-in STR.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.08958v1"
    },
    {
        "title": "Incorporating Preferences Into Treatment Assignment Problems",
        "authors": [
            "Daido Kido"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This study investigates the problem of individualizing treatment allocations\nusing stated preferences for treatments. If individuals know in advance how the\nassignment will be individualized based on their stated preferences, they may\nstate false preferences. We derive an individualized treatment rule (ITR) that\nmaximizes welfare when individuals strategically state their preferences. We\nalso show that the optimal ITR is strategy-proof, that is, individuals do not\nhave a strong incentive to lie even if they know the optimal ITR a priori.\nConstructing the optimal ITR requires information on the distribution of true\npreferences and the average treatment effect conditioned on true preferences.\nIn practice, the information must be identified and estimated from the data. As\ntrue preferences are hidden information, the identification is not\nstraightforward. We discuss two experimental designs that allow the\nidentification: strictly strategy-proof randomized controlled trials and doubly\nrandomized preference trials. Under the presumption that data comes from one of\nthese experiments, we develop data-dependent procedures for determining ITR,\nthat is, statistical treatment rules (STRs). The maximum regret of the proposed\nSTRs converges to zero at a rate of the square root of the sample size. An\nempirical application demonstrates our proposed STRs.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.08963v1"
    },
    {
        "title": "Estimating Functionals of the Joint Distribution of Potential Outcomes\n  with Optimal Transport",
        "authors": [
            "Daniel Ober-Reynolds"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Many causal parameters depend on a moment of the joint distribution of\npotential outcomes. Such parameters are especially relevant in policy\nevaluation settings, where noncompliance is common and accommodated through the\nmodel of Imbens & Angrist (1994). This paper shows that the sharp identified\nset for these parameters is an interval with endpoints characterized by the\nvalue of optimal transport problems. Sample analogue estimators are proposed\nbased on the dual problem of optimal transport. These estimators are root-n\nconsistent and converge in distribution under mild assumptions. Inference\nprocedures based on the bootstrap are straightforward and computationally\nconvenient. The ideas and estimators are demonstrated in an application\nrevisiting the National Supported Work Demonstration job training program. I\nfind suggestive evidence that workers who would see below average earnings\nwithout treatment tend to see above average benefits from treatment.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.09435v1"
    },
    {
        "title": "Inference in Auctions with Many Bidders Using Transaction Prices",
        "authors": [
            "Federico A. Bugni",
            "Yulong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies inference in first- and second-price sealed-bid auctions\nwith many bidders, using an asymptotic framework where the number of bidders\nincreases while the number of auctions remains fixed. Relevant applications\ninclude online, treasury, spectrum, and art auctions. Our approach enables\nasymptotically exact inference on key features such as the winner's expected\nutility, the seller's expected revenue, and the tail of the valuation\ndistribution using only transaction price data. Our simulations demonstrate the\naccuracy of the methods in finite samples. We apply our methods to Hong Kong\nvehicle license auctions, focusing on high-priced, single-letter plates.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.09972v3"
    },
    {
        "title": "Theory coherent shrinkage of Time-Varying Parameters in VARs",
        "authors": [
            "Andrea Renzetti"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper introduces a novel theory-coherent shrinkage prior for\nTime-Varying Parameter VARs (TVP-VARs). The prior centers the time-varying\nparameters on a path implied a priori by an underlying economic theory, chosen\nto describe the dynamics of the macroeconomic variables in the system.\nLeveraging information from conventional economic theory using this prior\nsignificantly improves inference precision and forecast accuracy compared to\nthe standard TVP-VAR. In an application, I use this prior to incorporate\ninformation from a New Keynesian model that includes both the Zero Lower Bound\n(ZLB) and forward guidance into a medium-scale TVP-VAR model. This approach\nleads to more precise estimates of the impulse response functions, revealing a\ndistinct propagation of risk premium shocks inside and outside the ZLB in US\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.11858v2"
    },
    {
        "title": "Large-Sample Properties of the Synthetic Control Method under Selection\n  on Unobservables",
        "authors": [
            "Dmitry Arkhangelsky",
            "David Hirshberg"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We analyze the synthetic control (SC) method in panel data settings with many\nunits. We assume the treatment assignment is based on unobserved heterogeneity\nand pre-treatment information, allowing for both strictly and sequentially\nexogenous assignment processes. We show that the critical property that\ndetermines the behavior of the SC method is the ability of input features to\napproximate the unobserved heterogeneity. Our results imply that the SC method\ndelivers asymptotically normal estimators for a large class of linear panel\ndata models as long as the number of pre-treatment periods is sufficiently\nlarge, making it a natural alternative to the Difference-in-Differences.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.13575v2"
    },
    {
        "title": "Was Javert right to be suspicious? Unpacking treatment effect\n  heterogeneity of alternative sentences on time-to-recidivism in Brazil",
        "authors": [
            "Santiago Acerenza",
            "Vitor Possebom",
            "Pedro H. C. Sant'Anna"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper presents econometric tools to unpack the treatment effect\nheterogeneity of punishing misdemeanor offenses on time-to-recidivism. We show\nhow one can identify, estimate, and make inferences on the distributional,\nquantile, and average marginal treatment effects in setups where the treatment\nselection is endogenous and the outcome of interest, usually a duration\nvariable, is potentially right-censored. We explore our proposed econometric\nmethodology to evaluate the effect of fines and community service sentences as\na form of punishment on time-to-recidivism in the State of S\\~ao Paulo, Brazil,\nbetween 2010 and 2019, leveraging the as-if random assignment of judges to\ncases. Our results highlight substantial treatment effect heterogeneity that\nother tools are not meant to capture. For instance, we find that people whom\nmost judges would punish take longer to recidivate as a consequence of the\npunishment, while people who would be punished only by strict judges recidivate\nat an earlier date than if they were not punished.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.13969v4"
    },
    {
        "title": "Counterfactual Sensitivity in Quantitative Trade and Spatial Models",
        "authors": [
            "Bas Sanders"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Counterfactuals in quantitative trade and spatial models are functions of the\ncurrent state of the world and the model parameters. Common practice treats the\ncurrent state of the world as perfectly observed, but there is good reason to\nbelieve that it is measured with error. This paper provides tools for\nquantifying uncertainty about counterfactuals when the current state of the\nworld is measured with error. I recommend an empirical Bayes approach to\nuncertainty quantification, and show that it is both practical and\ntheoretically justified. I apply the proposed method to the settings in Adao,\nCostinot, and Donaldson (2017) and Allen and Arkolakis (2022) and find\nnon-trivial uncertainty about counterfactuals.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.14032v3"
    },
    {
        "title": "A Review of Cross-Sectional Matrix Exponential Spatial Models",
        "authors": [
            "Ye Yang",
            "Osman Dogan",
            "Suleyman Taspinar",
            "Fei Jin"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The matrix exponential spatial models exhibit similarities to the\nconventional spatial autoregressive model in spatial econometrics but offer\nanalytical, computational, and interpretive advantages. This paper provides a\ncomprehensive review of the literature on the estimation, inference, and model\nselection approaches for the cross-sectional matrix exponential spatial models.\nWe discuss summary measures for the marginal effects of regressors and detail\nthe matrix-vector product method for efficient estimation. Our aim is not only\nto summarize the main findings from the spatial econometric literature but also\nto make them more accessible to applied researchers. Additionally, we\ncontribute to the literature by introducing some new results. We propose an\nM-estimation approach for models with heteroskedastic error terms and\ndemonstrate that the resulting M-estimator is consistent and has an asymptotic\nnormal distribution. We also consider some new results for model selection\nexercises. In a Monte Carlo study, we examine the finite sample properties of\nvarious estimators from the literature alongside the M-estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.14813v1"
    },
    {
        "title": "Causal Models for Longitudinal and Panel Data: A Survey",
        "authors": [
            "Dmitry Arkhangelsky",
            "Guido Imbens"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this survey we discuss the recent causal panel data literature. This\nrecent literature has focused on credibly estimating causal effects of binary\ninterventions in settings with longitudinal data, emphasizing practical advice\nfor empirical researchers. It pays particular attention to heterogeneity in the\ncausal effects, often in situations where few units are treated and with\nparticular structures on the assignment pattern. The literature has extended\nearlier work on difference-in-differences or two-way-fixed-effect estimators.\nIt has more generally incorporated factor models or interactive fixed effects.\nIt has also developed novel methods using synthetic control approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.15458v3"
    },
    {
        "title": "(Frisch-Waugh-Lovell)': On the Estimation of Regression Models by Row",
        "authors": [
            "Damian Clarke",
            "Nicolás Paris",
            "Benjamín Villena-Roldán"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We demonstrate that regression models can be estimated by working\nindependently in a row-wise fashion. We document a simple procedure which\nallows for a wide class of econometric estimators to be implemented\ncumulatively, where, in the limit, estimators can be produced without ever\nstoring more than a single line of data in a computer's memory. This result is\nuseful in understanding the mechanics of many common regression models. These\nprocedures can be used to speed up the computation of estimates computed via\nOLS, IV, Ridge regression, LASSO, Elastic Net, and Non-linear models including\nprobit and logit, with all common modes of inference. This has implications for\nestimation and inference with `big data', where memory constraints may imply\nthat working with all data at once is particularly costly. We additionally show\nthat even with moderately sized datasets, this method can reduce computation\ntime compared with traditional estimation routines.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.15829v1"
    },
    {
        "title": "On Quantile Treatment Effects, Rank Similarity, and Variation of\n  Instrumental Variables",
        "authors": [
            "Sukjin Han",
            "Haiqing Xu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper investigates how certain relationship between observed and\ncounterfactual distributions serves as an identifying condition for treatment\neffects when the treatment is endogenous, and shows that this condition holds\nin a range of nonparametric models for treatment effects. To this end, we first\nprovide a novel characterization of the prevalent assumption restricting\ntreatment heterogeneity in the literature, namely rank similarity. Our\ncharacterization demonstrates the stringency of this assumption and allows us\nto relax it in an economically meaningful way, resulting in our identifying\ncondition. It also justifies the quest of richer exogenous variations in the\ndata (e.g., multi-valued or multiple instrumental variables) in exchange for\nweaker identifying conditions. The primary goal of this investigation is to\nprovide empirical researchers with tools that are robust and easy to implement\nbut still yield tight policy evaluations.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.15871v1"
    },
    {
        "title": "Valid Wald Inference with Many Weak Instruments",
        "authors": [
            "Luther Yap"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes three novel test procedures that yield valid inference in\nan environment with many weak instrumental variables (MWIV). It is observed\nthat the t statistic of the jackknife instrumental variable estimator (JIVE)\nhas an asymptotic distribution that is identical to the two-stage-least squares\n(TSLS) t statistic in the just-identified environment. Consequently, test\nprocedures that were valid for TSLS t are also valid for the JIVE t. Two such\nprocedures, i.e., VtF and conditional Wald, are adapted directly. By exploiting\na feature of MWIV environments, a third, more powerful, one-sided VtF-based\ntest procedure can be obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.15932v1"
    },
    {
        "title": "Robust Conditional Wald Inference for Over-Identified IV",
        "authors": [
            "David S. Lee",
            "Justin McCrary",
            "Marcelo J. Moreira",
            "Jack Porter",
            "Luther Yap"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  For the over-identified linear instrumental variables model, researchers\ncommonly report the 2SLS estimate along with the robust standard error and seek\nto conduct inference with these quantities. If errors are homoskedastic, one\ncan control the degree of inferential distortion using the first-stage F\ncritical values from Stock and Yogo (2005), or use the robust-to-weak\ninstruments Conditional Wald critical values of Moreira (2003). If errors are\nnon-homoskedastic, these methods do not apply. We derive the generalization of\nConditional Wald critical values that is robust to non-homoskedastic errors\n(e.g., heteroskedasticity or clustered variance structures), which can also be\napplied to nonlinear weakly-identified models (e.g. weakly-identified GMM).\n",
        "pdf_link": "http://arxiv.org/pdf/2311.15952v1"
    },
    {
        "title": "Identifying Causal Effects of Discrete, Ordered and ContinuousTreatments\n  using Multiple Instrumental Variables",
        "authors": [
            "Nadja van 't Hoff"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Inferring causal relationships from observational data is often challenging\ndue to endogeneity. This paper provides new identification results for causal\neffects of discrete, ordered and continuous treatments using multiple binary\ninstruments. The key contribution is the identification of a new causal\nparameter that has a straightforward interpretation with a positive weighting\nscheme and is applicable in many settings due to a mild monotonicity\nassumption. This paper further leverages recent advances in causal machine\nlearning for both estimation and the detection of local violations of the\nunderlying monotonicity assumption. The methodology is applied to estimate the\nreturns to education and assess the impact of having an additional child on\nfemale labor market outcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.17575v3"
    },
    {
        "title": "Extrapolating Away from the Cutoff in Regression Discontinuity Designs",
        "authors": [
            "Yiwei Sun"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Canonical RD designs yield credible local estimates of the treatment effect\nat the cutoff under mild continuity assumptions, but they fail to identify\ntreatment effects away from the cutoff without additional assumptions. The\nfundamental challenge of identifying treatment effects away from the cutoff is\nthat the counterfactual outcome under the alternative treatment status is never\nobserved. This paper aims to provide a methodological blueprint to identify\ntreatment effects away from the cutoff in various empirical settings by\noffering a non-exhaustive list of assumptions on the counterfactual outcome.\nInstead of assuming the exact evolution of the counterfactual outcome, this\npaper bounds its variation using the data and sensitivity parameters. The\nproposed assumptions are weaker than those introduced previously in the\nliterature, resulting in partially identified treatment effects that are less\nsusceptible to assumption violations. This approach accommodates both single\ncutoff and multi-cutoff designs. The specific choice of the extrapolation\nassumption depends on the institutional background of each empirical\napplication. Additionally, researchers are recommended to conduct sensitivity\nanalysis on the chosen parameter and assess resulting shifts in conclusions.\nThe paper compares the proposed identification results with results using\nprevious methods via an empirical application and simulated data. It\ndemonstrates that set identification yields a more credible conclusion about\nthe sign of the treatment effect.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.18136v1"
    },
    {
        "title": "Bootstrap Inference on Partially Linear Binary Choice Model",
        "authors": [
            "Wenzheng Gao",
            "Zhenting Sun"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The partially linear binary choice model can be used for estimating\nstructural equations where nonlinearity may appear due to diminishing marginal\nreturns, different life cycle regimes, or hectic physical phenomena. The\ninference procedure for this model based on the analytic asymptotic\napproximation could be unreliable in finite samples if the sample size is not\nsufficiently large. This paper proposes a bootstrap inference approach for the\nmodel. Monte Carlo simulations show that the proposed inference method performs\nwell in finite samples compared to the procedure based on the asymptotic\napproximation.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.18759v1"
    },
    {
        "title": "Stochastic volatility models with skewness selection",
        "authors": [
            "Igor Ferreira Batista Martins",
            "Hedibert Freitas Lopes"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper expands traditional stochastic volatility models by allowing for\ntime-varying skewness without imposing it. While dynamic asymmetry may capture\nthe likely direction of future asset returns, it comes at the risk of leading\nto overparameterization. Our proposed approach mitigates this concern by\nleveraging sparsity-inducing priors to automatically selects the skewness\nparameter as being dynamic, static or zero in a data-driven framework. We\nconsider two empirical applications. First, in a bond yield application,\ndynamic skewness captures interest rate cycles of monetary easing and\ntightening being partially explained by central banks' mandates. In an currency\nmodeling framework, our model indicates no skewness in the carry factor after\naccounting for stochastic volatility which supports the idea of carry crashes\nbeing the result of volatility surges instead of dynamic skewness.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.00282v1"
    },
    {
        "title": "GMM-lev estimation and individual heterogeneity: Monte Carlo evidence\n  and empirical applications",
        "authors": [
            "Maria Elena Bontempi",
            "Jan Ditzen"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We introduce a new estimator, CRE-GMM, which exploits the correlated random\neffects (CRE) approach within the generalised method of moments (GMM),\nspecifically applied to level equations, GMM-lev. It has the advantage of\nestimating the effect of measurable time-invariant covariates using all\navailable information. This is not possible with GMM-dif, applied to the\nequations of each period transformed into first differences, while GMM-sys uses\nlittle information as it adds the equation in levels for only one period. The\nGMM-lev, by implying a two-component error term containing individual\nheterogeneity and shock, exposes the explanatory variables to possible double\nendogeneity. For example, the estimation of actual persistence could suffer\nfrom bias if instruments were correlated with the unit-specific error\ncomponent. The CRE-GMM deals with double endogeneity, captures initial\nconditions and enhance inference. Monte Carlo simulations for different panel\ntypes and under different double endogeneity assumptions show the advantage of\nour approach. The empirical applications on production and R&D contribute to\nclarify the advantages of using CRE-GMM.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.00399v2"
    },
    {
        "title": "A Method of Moments Approach to Asymptotically Unbiased Synthetic\n  Controls",
        "authors": [
            "Joseph Fry"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  A common approach to constructing a Synthetic Control unit is to fit on the\noutcome variable and covariates in pre-treatment time periods, but it has been\nshown by Ferman and Pinto (2019) that this approach does not provide asymptotic\nunbiasedness when the fit is imperfect and the number of controls is fixed.\nMany related panel methods have a similar limitation when the number of units\nis fixed. I introduce and evaluate a new method in which the Synthetic Control\nis constructed using a General Method of Moments approach where units not being\nincluded in the Synthetic Control are used as instruments. I show that a\nSynthetic Control Estimator of this form will be asymptotically unbiased as the\nnumber of pre-treatment time periods goes to infinity, even when pre-treatment\nfit is imperfect and the number of units is fixed. Furthermore, if both the\nnumber of pre-treatment and post-treatment time periods go to infinity, then\naverages of treatment effects can be consistently estimated. I conduct\nsimulations and an empirical application to compare the performance of this\nmethod with existing approaches in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.01209v2"
    },
    {
        "title": "Almost Dominance: Inference and Application",
        "authors": [
            "Xiaojun Song",
            "Zhenting Sun"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper proposes a general framework for inference on three types of\nalmost dominances: Almost Lorenz dominance, almost inverse stochastic\ndominance, and almost stochastic dominance. We first generalize almost Lorenz\ndominance to almost upward and downward Lorenz dominances. We then provide a\nbootstrap inference procedure for the Lorenz dominance coefficients, which\nmeasure the degrees of almost Lorenz dominances. Furthermore, we propose almost\nupward and downward inverse stochastic dominances and provide inference on the\ninverse stochastic dominance coefficients. We also show that our results can\neasily be extended to almost stochastic dominance. Simulation studies\ndemonstrate the finite sample properties of the proposed estimators and the\nbootstrap confidence intervals. We apply our methods to the inequality growth\nin the United Kingdom and find evidence for almost upward inverse stochastic\ndominance.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.02288v1"
    },
    {
        "title": "A Theory Guide to Using Control Functions to Instrument Hazard Models",
        "authors": [
            "William Liu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  I develop the theory around using control functions to instrument hazard\nmodels, allowing the inclusion of endogenous (e.g., mismeasured) regressors.\nSimple discrete-data hazard models can be expressed as binary choice panel data\nmodels, and the widespread Prentice and Gloeckler (1978) discrete-data\nproportional hazards model can specifically be expressed as a complementary\nlog-log model with time fixed effects. This allows me to recast it as GMM\nestimation and its instrumented version as sequential GMM estimation in a\nZ-estimation (non-classical GMM) framework; this framework can then be\nleveraged to establish asymptotic properties and sufficient conditions. Whilst\nthis paper focuses on the Prentice and Gloeckler (1978) model, the methods and\ndiscussion developed here can be applied more generally to other hazard models\nand binary choice models. I also introduce my Stata command for estimating a\ncomplementary log-log model instrumented via control functions (available as\nivcloglog on SSC), which allows practitioners to easily instrument the Prentice\nand Gloeckler (1978) model.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.03165v1"
    },
    {
        "title": "Occasionally Misspecified",
        "authors": [
            "Jean-Jacques Forneron"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  When fitting a particular Economic model on a sample of data, the model may\nturn out to be heavily misspecified for some observations. This can happen\nbecause of unmodelled idiosyncratic events, such as an abrupt but short-lived\nchange in policy. These outliers can significantly alter estimates and\ninferences. A robust estimation is desirable to limit their influence. For\nskewed data, this induces another bias which can also invalidate the estimation\nand inferences. This paper proposes a robust GMM estimator with a simple bias\ncorrection that does not degrade robustness significantly. The paper provides\nfinite-sample robustness bounds, and asymptotic uniform equivalence with an\noracle that discards all outliers. Consistency and asymptotic normality ensue\nfrom that result. An application to the \"Price-Puzzle,\" which finds inflation\nincreases when monetary policy tightens, illustrates the concerns and the\nmethod. The proposed estimator finds the intuitive result: tighter monetary\npolicy leads to a decline in inflation.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.05342v1"
    },
    {
        "title": "GCov-Based Portmanteau Test",
        "authors": [
            "Joann Jasiak",
            "Aryan Manafi Neyazi"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We examine finite sample performance of the Generalized Covariance (GCov)\nresidual-based specification test for semiparametric models with i.i.d. errors.\nThe residual-based multivariate portmanteau test statistic follows\nasymptotically a $\\chi^2$ distribution when the model is estimated by the GCov\nestimator. The test is shown to perform well in application to the univariate\nmixed causal-noncausal MAR, double autoregressive (DAR) and multivariate Vector\nAutoregressive (VAR) models. We also introduce a bootstrap procedure that\nprovides the limiting distribution of the test statistic when the specification\ntest is applied to a model estimated by the maximum likelihood, or the\napproximate or quasi-maximum likelihood under a parametric assumption on the\nerror distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.05373v1"
    },
    {
        "title": "Trends in Temperature Data: Micro-foundations of Their Nature",
        "authors": [
            "Maria Dolores Gadea",
            "Jesus Gonzalo",
            "Andrey Ramos"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Determining whether Global Average Temperature (GAT) is an integrated process\nof order 1, I(1), or is a stationary process around a trend function is crucial\nfor detection, attribution, impact and forecasting studies of climate change.\nIn this paper, we investigate the nature of trends in GAT building on the\nanalysis of individual temperature grids. Our 'micro-founded' evidence suggests\nthat GAT is stationary around a non-linear deterministic trend in the form of a\nlinear function with a one-period structural break. This break can be\nattributed to a combination of individual grid breaks and the standard\naggregation method under acceleration in global warming. We illustrate our\nfindings using simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.06379v1"
    },
    {
        "title": "Structural Analysis of Vector Autoregressive Models",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This set of lecture notes discuss key concepts for the Structural Analysis of\nVector Autoregressive models for the teaching of a course on Applied\nMacroeconometrics with Advanced Topics.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.06402v9"
    },
    {
        "title": "Efficiency of QMLE for dynamic panel data models with interactive\n  effects",
        "authors": [
            "Jushan Bai"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper derives the efficiency bound for estimating the parameters of\ndynamic panel data models in the presence of an increasing number of incidental\nparameters. We study the efficiency problem by formulating the dynamic panel as\na simultaneous equations system, and show that the quasi-maximum likelihood\nestimator (QMLE) applied to the system achieves the efficiency bound.\nComparison of QMLE with fixed effects estimators is made.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.07881v2"
    },
    {
        "title": "Logit-based alternatives to two-stage least squares",
        "authors": [
            "Denis Chetverikov",
            "Jinyong Hahn",
            "Zhipeng Liao",
            "Shuyang Sheng"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We propose logit-based IV and augmented logit-based IV estimators that serve\nas alternatives to the traditionally used 2SLS estimator in the model where\nboth the endogenous treatment variable and the corresponding instrument are\nbinary. Our novel estimators are as easy to compute as the 2SLS estimator but\nhave an advantage over the 2SLS estimator in terms of causal interpretability.\nIn particular, in certain cases where the probability limits of both our\nestimators and the 2SLS estimator take the form of weighted-average treatment\neffects, our estimators are guaranteed to yield non-negative weights whereas\nthe 2SLS estimator is not.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.10333v1"
    },
    {
        "title": "Some Finite-Sample Results on the Hausman Test",
        "authors": [
            "Jinyong Hahn",
            "Zhipeng Liao",
            "Nan Liu",
            "Shuyang Sheng"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper shows that the endogeneity test using the control function\napproach in linear instrumental variable models is a variant of the Hausman\ntest. Moreover, we find that the test statistics used in these tests can be\nnumerically ordered, indicating their relative power properties in finite\nsamples.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.10558v1"
    },
    {
        "title": "Variable Selection in High Dimensional Linear Regressions with Parameter\n  Instability",
        "authors": [
            "Alexander Chudik",
            "M. Hashem Pesaran",
            "Mahrad Sharifvaghefi"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper considers the problem of variable selection allowing for parameter\ninstability. It distinguishes between signal and pseudo-signal variables that\nare correlated with the target variable, and noise variables that are not, and\ninvestigate the asymptotic properties of the One Covariate at a Time Multiple\nTesting (OCMT) method proposed by Chudik et al. (2018) under parameter\ninsatiability. It is established that OCMT continues to asymptotically select\nan approximating model that includes all the signals and none of the noise\nvariables. Properties of post selection regressions are also investigated, and\nin-sample fit of the selected regression is shown to have the oracle property.\nThe theoretical results support the use of unweighted observations at the\nselection stage of OCMT, whilst applying down-weighting of observations only at\nthe forecasting stage. Monte Carlo and empirical applications show that OCMT\nwithout down-weighting at the selection stage yields smaller mean squared\nforecast errors compared to Lasso, Adaptive Lasso, and boosting.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.15494v2"
    },
    {
        "title": "Direct Multi-Step Forecast based Comparison of Nested Models via an\n  Encompassing Test",
        "authors": [
            "Jean-Yves Pitarakis"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We introduce a novel approach for comparing out-of-sample multi-step\nforecasts obtained from a pair of nested models that is based on the forecast\nencompassing principle. Our proposed approach relies on an alternative way of\ntesting the population moment restriction implied by the forecast encompassing\nprinciple and that links the forecast errors from the two competing models in a\nparticular way. Its key advantage is that it is able to bypass the variance\ndegeneracy problem afflicting model based forecast comparisons across nested\nmodels. It results in a test statistic whose limiting distribution is standard\nnormal and which is particularly simple to construct and can accommodate both\nsingle period and longer-horizon prediction comparisons. Inferences are also\nshown to be robust to different predictor types, including stationary,\nhighly-persistent and purely deterministic processes. Finally, we illustrate\nthe use of our proposed approach through an empirical application that explores\nthe role of global inflation in enhancing individual country specific inflation\nforecasts.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.16099v1"
    },
    {
        "title": "Development of Choice Model for Brand Evaluation",
        "authors": [
            "Marina Kholod",
            "Nikita Mokrenko"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Consumer choice modeling takes center stage as we delve into understanding\nhow personal preferences of decision makers (customers) for products influence\ndemand at the level of the individual. The contemporary choice theory is built\nupon the characteristics of the decision maker, alternatives available for the\nchoice of the decision maker, the attributes of the available alternatives and\ndecision rules that the decision maker uses to make a choice. The choice set in\nour research is represented by six major brands (products) of laundry\ndetergents in the Japanese market. We use the panel data of the purchases of 98\nhouseholds to which we apply the hierarchical probit model, facilitated by a\nMarkov Chain Monte Carlo simulation (MCMC) in order to evaluate the brand\nvalues of six brands. The applied model also allows us to evaluate the tangible\nand intangible brand values. These evaluated metrics help us to assess the\nbrands based on their tangible and intangible characteristics. Moreover,\nconsumer choice modeling also provides a framework for assessing the\nenvironmental performance of laundry detergent brands as the model uses the\ninformation on components (physical attributes) of laundry detergents.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.16927v1"
    },
    {
        "title": "Decision Theory for Treatment Choice Problems with Partial\n  Identification",
        "authors": [
            "José Luis Montiel Olea",
            "Chen Qiu",
            "Jörg Stoye"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We apply classical statistical decision theory to a large class of treatment\nchoice problems with partial identification, revealing important theoretical\nand practical challenges but also interesting research opportunities. The\nchallenges are: In a general class of problems with Gaussian likelihood, all\ndecision rules are admissible; it is maximin-welfare optimal to ignore all\ndata; and, for severe enough partial identification, there are infinitely many\nminimax-regret optimal decision rules, all of which sometimes randomize the\npolicy recommendation. The opportunities are: We introduce a profiled regret\ncriterion that can reveal important differences between rules and render some\nof them inadmissible; and we uniquely characterize the minimax-regret optimal\nrule that least frequently randomizes. We apply our results to aggregation of\nexperimental estimates for policy adoption, to extrapolation of Local Average\nTreatment Effects, and to policy making in the presence of omitted variable\nbias.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.17623v2"
    },
    {
        "title": "Forecasting CPI inflation under economic policy and geopolitical\n  uncertainties",
        "authors": [
            "Shovon Sengupta",
            "Tanujit Chakraborty",
            "Sunny Kumar Singh"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Forecasting consumer price index (CPI) inflation is of paramount importance\nfor both academics and policymakers at the central banks. This study introduces\na filtered ensemble wavelet neural network (FEWNet) to forecast CPI inflation,\nwhich is tested on BRIC countries. FEWNet breaks down inflation data into high\nand low-frequency components using wavelets and utilizes them along with other\neconomic factors (economic policy uncertainty and geopolitical risk) to produce\nforecasts. All the wavelet-transformed series and filtered exogenous variables\nare fed into downstream autoregressive neural networks to make the final\nensemble forecast. Theoretically, we show that FEWNet reduces the empirical\nrisk compared to fully connected autoregressive neural networks. FEWNet is more\naccurate than other forecasting methods and can also estimate the uncertainty\nin its predictions due to its capacity to effectively capture non-linearities\nand long-range dependencies in the data through its adaptable architecture.\nThis makes FEWNet a valuable tool for central banks to manage inflation.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.00249v2"
    },
    {
        "title": "Identification of Nonlinear Dynamic Panels under Partial Stationarity",
        "authors": [
            "Wayne Yuan Gao",
            "Rui Wang"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies identification for a wide range of nonlinear panel data\nmodels, including binary choice, ordered response, and other types of limited\ndependent variable models. Our approach accommodates dynamic models with any\nnumber of lagged dependent variables as well as other types of (potentially\ncontemporary) endogeneity. Our identification strategy relies on a partial\nstationarity condition, which not only allows for an unknown distribution of\nerrors but also for temporal dependencies in errors. We derive partial\nidentification results under flexible model specifications and provide\nadditional support conditions for point identification. We demonstrate the\nrobust finite-sample performance of our approach using Monte Carlo simulations,\nand apply the approach to analyze the empirical application of income\ncategories using various ordered choice models.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.00264v3"
    },
    {
        "title": "Changes-in-Changes for Ordered Choice Models: Too Many \"False Zeros\"?",
        "authors": [
            "Daniel Gutknecht",
            "Cenchen Liu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, we develop a Difference-in-Differences model for discrete,\nordered outcomes, building upon elements from a continuous Changes-in-Changes\nmodel. We focus on outcomes derived from self-reported survey data eliciting\nsocially undesirable, illegal, or stigmatized behaviors like tax evasion or\nsubstance abuse, where too many \"false zeros\", or more broadly, underreporting\nare likely. We start by providing a characterization for parallel trends within\na general threshold-crossing model. We then propose a partial and point\nidentification framework for different distributional treatment effects when\nthe outcome is subject to underreporting. Applying our methodology, we\ninvestigate the impact of recreational marijuana legalization for adults in\nseveral U.S. states on the short-term consumption behavior of 8th-grade\nhigh-school students. The results indicate small, but significant increases in\nconsumption probabilities at each level. These effects are further amplified\nupon accounting for misreporting.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.00618v3"
    },
    {
        "title": "Roughness Signature Functions",
        "authors": [
            "Peter Christensen"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Inspired by the activity signature introduced by Todorov and Tauchen (2010),\nwhich was used to measure the activity of a semimartingale, this paper\nintroduces the roughness signature function. The paper illustrates how it can\nbe used to determine whether a discretely observed process is generated by a\ncontinuous process that is rougher than a Brownian motion, a pure-jump process,\nor a combination of the two. Further, if a continuous rough process is present,\nthe function gives an estimate of the roughness index. This is done through an\nextensive simulation study, where we find that the roughness signature function\nworks as expected on rough processes. We further derive some asymptotic\nproperties of this new signature function. The function is applied empirically\nto three different volatility measures for the S&P500 index. The three measures\nare realized volatility, the VIX, and the option-extracted volatility estimator\nof Todorov (2019). The realized volatility and option-extracted volatility show\nsigns of roughness, with the option-extracted volatility appearing smoother\nthan the realized volatility, while the VIX appears to be driven by a\ncontinuous martingale with jumps.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.02819v1"
    },
    {
        "title": "Counterfactuals in factor models",
        "authors": [
            "Jad Beyhum"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We study a new model where the potential outcomes, corresponding to the\nvalues of a (possibly continuous) treatment, are linked through common factors.\nThe factors can be estimated using a panel of regressors. We propose a\nprocedure to estimate time-specific and unit-specific average marginal effects\nin this context. Our approach can be used either with high-dimensional time\nseries or with large panels. It allows for treatment effects heterogenous\nacross time and units and is straightforward to implement since it only relies\non principal components analysis and elementary computations. We derive the\nasymptotic distribution of our estimator of the average marginal effect and\nhighlight its solid finite sample performance through a simulation exercise.\nThe approach can also be used to estimate average counterfactuals or adapted to\nan instrumental variables setting and we discuss these extensions. Finally, we\nillustrate our novel methodology through an empirical application on income\ninequality.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.03293v1"
    },
    {
        "title": "Identification with possibly invalid IVs",
        "authors": [
            "Christophe Bruneel-Zupanc",
            "Jad Beyhum"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper proposes a novel identification strategy relying on\nquasi-instrumental variables (quasi-IVs). A quasi-IV is a relevant but possibly\ninvalid IV because it is not exogenous or not excluded. We show that a variety\nof models with discrete or continuous endogenous treatment which are usually\nidentified with an IV - quantile models with rank invariance, additive models\nwith homogenous treatment effects, and local average treatment effect models -\ncan be identified under the joint relevance of two complementary quasi-IVs\ninstead. To achieve identification, we complement one excluded but possibly\nendogenous quasi-IV (e.g., \"relevant proxies\" such as lagged treatment choice)\nwith one exogenous (conditional on the excluded quasi-IV) but possibly included\nquasi-IV (e.g., random assignment or exogenous market shocks). Our approach\nalso holds if any of the two quasi-IVs turns out to be a valid IV. In practice,\nbeing able to address endogeneity with complementary quasi-IVs instead of IVs\nis convenient since there are many applications where quasi-IVs are more\nreadily available. Difference-in-differences is a notable example: time is an\nexogenous quasi-IV while the group assignment acts as a complementary excluded\nquasi-IV.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.03990v4"
    },
    {
        "title": "Robust Estimation in Network Vector Autoregression with Nonstationary\n  Regressors",
        "authors": [
            "Christis Katsouris"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This article studies identification and estimation for the network vector\nautoregressive model with nonstationary regressors. In particular, network\ndependence is characterized by a nonstochastic adjacency matrix. The\ninformation set includes a stationary regressand and a node-specific vector of\nnonstationary regressors, both observed at the same equally spaced time\nfrequencies. Our proposed econometric specification correponds to the NVAR\nmodel under time series nonstationarity which relies on the local-to-unity\nparametrization for capturing the unknown form of persistence of these\nnode-specific regressors. Robust econometric estimation is achieved using an\nIVX-type estimator and the asymptotic theory analysis for the augmented vector\nof regressors is studied based on a double asymptotic regime where both the\nnetwork size and the time dimension tend to infinity.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.04050v1"
    },
    {
        "title": "Teacher bias or measurement error?",
        "authors": [
            "Thomas van Huizen",
            "Madelon Jacobs",
            "Matthijs Oosterveen"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In many countries, teachers' track recommendations are used to allocate\nstudents to secondary school tracks. Previous studies have shown that students\nfrom families with low socioeconomic status (SES) receive lower track\nrecommendations than their peers from high SES families, conditional on\nstandardized test scores. It is often argued that this indicates teacher bias.\nHowever, this claim is invalid in the presence of measurement error in test\nscores. We discuss how measurement error in test scores generates a biased\ncoefficient of the conditional SES gap, and consider three empirical strategies\nto address this bias. Using administrative data from the Netherlands, we find\nthat measurement error explains 35 to 43% of the conditional SES gap in track\nrecommendations.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.04200v2"
    },
    {
        "title": "Robust Bayesian Method for Refutable Models",
        "authors": [
            "Moyu Liao"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a robust Bayesian method for economic models that can be rejected\nby some data distributions. The econometrician starts with a refutable\nstructural assumption which can be written as the intersection of several\nassumptions. To avoid the assumption refutable, the econometrician first takes\na stance on which assumption $j$ will be relaxed and considers a function $m_j$\nthat measures the deviation from the assumption $j$. She then specifies a set\nof prior beliefs $\\Pi_s$ whose elements share the same marginal distribution\n$\\pi_{m_j}$ which measures the likelihood of deviations from assumption $j$.\nCompared to the standard Bayesian method that specifies a single prior, the\nrobust Bayesian method allows the econometrician to take a stance only on the\nlikeliness of violation of assumption $j$ while leaving other features of the\nmodel unspecified. We show that many frequentist approaches to relax refutable\nassumptions are equivalent to particular choices of robust Bayesian prior sets,\nand thus we give a Bayesian interpretation to the frequentist methods. We use\nthe local average treatment effect ($LATE$) in the potential outcome framework\nas the leading illustrating example.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.04512v3"
    },
    {
        "title": "IV Estimation of Panel Data Tobit Models with Normal Errors",
        "authors": [
            "Bo E. Honore"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Amemiya (1973) proposed a ``consistent initial estimator'' for the parameters\nin a censored regression model with normal errors. This paper demonstrates that\na similar approach can be used to construct moment conditions for\nfixed--effects versions of the model considered by Amemiya. This result\nsuggests estimators for models that have not previously been considered.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.04803v1"
    },
    {
        "title": "Robust Analysis of Short Panels",
        "authors": [
            "Andrew Chesher",
            "Adam M. Rosen",
            "Yuanqi Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Many structural econometric models include latent variables on whose\nprobability distributions one may wish to place minimal restrictions. Leading\nexamples in panel data models are individual-specific variables sometimes\ntreated as \"fixed effects\" and, in dynamic models, initial conditions. This\npaper presents a generally applicable method for characterizing sharp\nidentified sets when models place no restrictions on the probability\ndistribution of certain latent variables and no restrictions on their\ncovariation with other variables. In our analysis latent variables on which\nrestrictions are undesirable are removed, leading to econometric analysis\nrobust to misspecification of restrictions on their distributions which are\ncommonplace in the applied panel data literature. Endogenous explanatory\nvariables are easily accommodated. Examples of application to some static and\ndynamic binary, ordered and multiple discrete choice and censored panel data\nmodels are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.06611v1"
    },
    {
        "title": "A Note on Uncertainty Quantification for Maximum Likelihood Parameters\n  Estimated with Heuristic Based Optimization Algorithms",
        "authors": [
            "Zachary Porreca"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Gradient-based solvers risk convergence to local optima, leading to incorrect\nresearcher inference. Heuristic-based algorithms are able to ``break free\" of\nthese local optima to eventually converge to the true global optimum. However,\ngiven that they do not provide the gradient/Hessian needed to approximate the\ncovariance matrix and that the significantly longer computational time they\nrequire for convergence likely precludes resampling procedures for inference,\nresearchers often are unable to quantify uncertainty in the estimates they\nderive with these methods. This note presents a simple and relatively fast\ntwo-step procedure to estimate the covariance matrix for parameters estimated\nwith these algorithms. This procedure relies on automatic differentiation, a\ncomputational means of calculating derivatives that is popular in machine\nlearning applications. A brief empirical example demonstrates the advantages of\nthis procedure relative to bootstrapping and shows the similarity in standard\nerror estimates between this procedure and that which would normally accompany\nmaximum likelihood estimation with a gradient-based algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.07176v1"
    },
    {
        "title": "Nowcasting economic activity in European regions using a mixed-frequency\n  dynamic factor model",
        "authors": [
            "Luca Barbaglia",
            "Lorenzo Frattarolo",
            "Niko Hauzenberger",
            "Dominik Hirschbuehl",
            "Florian Huber",
            "Luca Onorante",
            "Michael Pfarrhofer",
            "Luca Tiozzo Pezzoli"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Timely information about the state of regional economies can be essential for\nplanning, implementing and evaluating locally targeted economic policies.\nHowever, European regional accounts for output are published at an annual\nfrequency and with a two-year delay. To obtain robust and more timely measures\nin a computationally efficient manner, we propose a mixed-frequency dynamic\nfactor model that accounts for national information to produce high-frequency\nestimates of the regional gross value added (GVA). We show that our model\nproduces reliable nowcasts of GVA in 162 regions across 12 European countries.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.10054v1"
    },
    {
        "title": "A Bracketing Relationship for Long-Term Policy Evaluation with Combined\n  Experimental and Observational Data",
        "authors": [
            "Yechan Park",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Combining short-term experimental data with observational data enables\ncredible long-term policy evaluation. The literature offers two key but\nnon-nested assumptions, namely the latent unconfoundedness (LU; Athey et al.,\n2020) and equi-confounding bias (ECB; Ghassami et al., 2022) conditions, to\ncorrect observational selection. Committing to the wrong assumption leads to\nbiased estimation. To mitigate such risks, we provide a novel bracketing\nrelationship (cf. Angrist and Pischke, 2009) repurposed for the setting with\ndata combination: the LU-based estimand and the ECB-based estimand serve as the\nlower and upper bounds, respectively, with the true causal effect lying in\nbetween if either assumption holds. For researchers further seeking point\nestimates, our Lalonde-style exercise suggests the conservatively more robust\nLU-based lower bounds align closely with the hold-out experimental estimates\nfor educational policy evaluation. We investigate the economic substantives of\nthese findings through the lens of a nonparametric class of selection\nmechanisms and sensitivity analysis. We uncover as key the sub-martingale\nproperty and sufficient-statistics role (Chetty, 2009) of the potential\noutcomes of student test scores (Chetty et al., 2011, 2014).\n",
        "pdf_link": "http://arxiv.org/pdf/2401.12050v1"
    },
    {
        "title": "Inference under partial identification with minimax test statistics",
        "authors": [
            "Isaac Loh"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We provide a means of computing and estimating the asymptotic distributions\nof statistics based on an outer minimization of an inner maximization. Such\ntest statistics, which arise frequently in moment models, are of special\ninterest in providing hypothesis tests under partial identification. Under\ngeneral conditions, we provide an asymptotic characterization of such test\nstatistics using the minimax theorem, and a means of computing critical values\nusing the bootstrap. Making some light regularity assumptions, our results\naugment several asymptotic approximations that have been provided for partially\nidentified hypothesis tests, and extend them by mitigating their dependence on\nlocal linear approximations of the parameter space. These asymptotic results\nare generally simple to state and straightforward to compute (esp.\\\nadversarially).\n",
        "pdf_link": "http://arxiv.org/pdf/2401.13057v2"
    },
    {
        "title": "Realized Stochastic Volatility Model with Skew-t Distributions for\n  Improved Volatility and Quantile Forecasting",
        "authors": [
            "Makoto Takahashi",
            "Yuta Yamauchi",
            "Toshiaki Watanabe",
            "Yasuhiro Omori"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Forecasting volatility and quantiles of financial returns is essential for\naccurately measuring financial tail risks, such as value-at-risk and expected\nshortfall. The critical elements in these forecasts involve understanding the\ndistribution of financial returns and accurately estimating volatility. This\npaper introduces an advancement to the traditional stochastic volatility model,\ntermed the realized stochastic volatility model, which integrates realized\nvolatility as a precise estimator of volatility. To capture the well-known\ncharacteristics of return distribution, namely skewness and heavy tails, we\nincorporate three types of skew-t distributions. Among these, two distributions\ninclude the skew-normal feature, offering enhanced flexibility in modeling the\nreturn distribution. We employ a Bayesian estimation approach using the Markov\nchain Monte Carlo method and apply it to major stock indices. Our empirical\nanalysis, utilizing data from US and Japanese stock indices, indicates that the\ninclusion of both skewness and heavy tails in daily returns significantly\nimproves the accuracy of volatility and quantile forecasts.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.13179v2"
    },
    {
        "title": "New accessibility measures based on unconventional big data sources",
        "authors": [
            "G. Arbia",
            "V. Nardelli",
            "N. Salvini",
            "I. Valentini"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In health econometric studies we are often interested in quantifying aspects\nrelated to the accessibility to medical infrastructures. The increasing\navailability of data automatically collected through unconventional sources\n(such as webscraping, crowdsourcing or internet of things) recently opened\npreviously unconceivable opportunities to researchers interested in measuring\naccessibility and to use it as a tool for real-time monitoring, surveillance\nand health policies definition. This paper contributes to this strand of\nliterature proposing new accessibility measures that can be continuously feeded\nby automatic data collection. We present new measures of accessibility and we\nillustrate their use to study the territorial impact of supply-side shocks of\nhealth facilities. We also illustrate the potential of our proposal with a case\nstudy based on a huge set of data (related to the Emergency Departments in\nMilan, Italy) that have been webscraped for the purpose of this paper every 5\nminutes since November 2021 to March 2022, amounting to approximately 5 million\nobservations.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.13370v1"
    },
    {
        "title": "Identification of Nonseparable Models with Endogenous Control Variables",
        "authors": [
            "Kaicheng Chen",
            "Kyoo il Kim"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We study identification of the treatment effects in a class of nonseparable\nmodels with the presence of potentially endogenous control variables. We show\nthat given the treatment variable and the controls are measurably separated,\nthe usual conditional independence condition or availability of excluded\ninstrument suffices for identification.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.14395v1"
    },
    {
        "title": "Structural Periodic Vector Autoregressions",
        "authors": [
            "Daniel Dzikowski",
            "Carsten Jentsch"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  While seasonality inherent to raw macroeconomic data is commonly removed by\nseasonal adjustment techniques before it is used for structural inference, this\napproach might distort valuable information contained in the data. As an\nalternative method to commonly used structural vector autoregressions (SVAR)\nfor seasonally adjusted macroeconomic data, this paper offers an approach in\nwhich the periodicity of not seasonally adjusted raw data is modeled directly\nby structural periodic vector autoregressions (SPVAR) that are based on\nperiodic vector autoregressions (PVAR) as the reduced form model. In comparison\nto a VAR, the PVAR does allow not only for periodically time-varying\nintercepts, but also for periodic autoregressive parameters and innovations\nvariances, respectively. As this larger flexibility leads also to an increased\nnumber of parameters, we propose linearly constrained estimation techniques.\nOverall, SPVARs allow to capture seasonal effects and enable a direct and more\nrefined analysis of seasonal patterns in macroeconomic data, which can provide\nuseful insights into their dynamics. Moreover, based on such SPVARs, we propose\na general concept for structural impulse response analyses that takes seasonal\npatterns directly into account. We provide asymptotic theory for estimators of\nperiodic reduced form parameters and structural impulse responses under\nflexible linear restrictions. Further, for the construction of confidence\nintervals, we propose residual-based (seasonal) bootstrap methods that allow\nfor general forms of seasonalities in the data and prove its bootstrap\nconsistency. A real data application on industrial production, inflation and\nfederal funds rate is presented, showing that useful information about the data\nstructure can be lost when using common seasonal adjustment methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.14545v1"
    },
    {
        "title": "High-dimensional forecasting with known knowns and known unknowns",
        "authors": [
            "M. Hashem Pesaran",
            "Ron P. Smith"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Forecasts play a central role in decision making under uncertainty. After a\nbrief review of the general issues, this paper considers ways of using\nhigh-dimensional data in forecasting. We consider selecting variables from a\nknown active set, known knowns, using Lasso and OCMT, and approximating\nunobserved latent factors, known unknowns, by various means. This combines both\nsparse and dense approaches. We demonstrate the various issues involved in\nvariable selection in a high-dimensional setting with an application to\nforecasting UK inflation at different horizons over the period 2020q1-2023q1.\nThis application shows both the power of parsimonious models and the importance\nof allowing for global variables.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.14582v2"
    },
    {
        "title": "csranks: An R Package for Estimation and Inference Involving Ranks",
        "authors": [
            "Denis Chetverikov",
            "Magne Mogstad",
            "Pawel Morgen",
            "Joseph Romano",
            "Azeem Shaikh",
            "Daniel Wilhelm"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This article introduces the R package csranks for estimation and inference\ninvolving ranks. First, we review methods for the construction of confidence\nsets for ranks, namely marginal and simultaneous confidence sets as well as\nconfidence sets for the identities of the tau-best. Second, we review methods\nfor estimation and inference in regressions involving ranks. Third, we describe\nthe implementation of these methods in csranks and illustrate their usefulness\nin two examples: one about the quantification of uncertainty in the PISA\nranking of countries and one about the measurement of intergenerational\nmobility using rank-rank regressions.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.15205v1"
    },
    {
        "title": "Graph Neural Networks: Theory for Estimation with Application on Network\n  Heterogeneity",
        "authors": [
            "Yike Wang",
            "Chris Gu",
            "Taisuke Otsu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper presents a novel application of graph neural networks for modeling\nand estimating network heterogeneity. Network heterogeneity is characterized by\nvariations in unit's decisions or outcomes that depend not only on its own\nattributes but also on the conditions of its surrounding neighborhood. We\ndelineate the convergence rate of the graph neural networks estimator, as well\nas its applicability in semiparametric causal inference with heterogeneous\ntreatment effects. The finite-sample performance of our estimator is evaluated\nthrough Monte Carlo simulations. In an empirical setting related to\nmicrofinance program participation, we apply the new estimator to examine the\naverage treatment effects and outcomes of counterfactual policies, and to\npropose an enhanced strategy for selecting the initial recipients of program\ninformation in social networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.16275v1"
    },
    {
        "title": "Partial Identification of Binary Choice Models with Misreported Outcomes",
        "authors": [
            "Orville Mondal",
            "Rui Wang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper provides partial identification of various binary choice models\nwith misreported dependent variables. We propose two distinct approaches by\nexploiting different instrumental variables respectively. In the first\napproach, the instrument is assumed to only affect the true dependent variable\nbut not misreporting probabilities. The second approach uses an instrument that\ninfluences misreporting probabilities monotonically while having no effect on\nthe true dependent variable. Moreover, we derive identification results under\nadditional restrictions on misreporting, including bounded/monotone\nmisreporting probabilities. We use simulations to demonstrate the robust\nperformance of our approaches, and apply the method to study educational\nattainment.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.17137v1"
    },
    {
        "title": "Marginal treatment effects in the absence of instrumental variables",
        "authors": [
            "Zhewen Pan",
            "Zhengxin Wang",
            "Junsen Zhang",
            "Yahong Zhou"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a method for defining, identifying, and estimating the marginal\ntreatment effect (MTE) without imposing the instrumental variable (IV)\nassumptions of independence, exclusion, and separability (or monotonicity).\nUnder a new definition of the MTE based on reduced-form treatment error that is\nstatistically independent of the covariates, we find that the relationship\nbetween the MTE and standard treatment parameters holds in the absence of IVs.\nWe provide a set of sufficient conditions ensuring the identification of the\ndefined MTE in an environment of essential heterogeneity. The key conditions\ninclude a linear restriction on potential outcome regression functions, a\nnonlinear restriction on the propensity score, and a conditional mean\nindependence restriction that will lead to additive separability. We prove this\nidentification using the notion of semiparametric identification based on\nfunctional form. And we provide an empirical application for the Head Start\nprogram to illustrate the usefulness of the proposed method in analyzing\nheterogenous causal effects when IVs are elusive.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.17595v2"
    },
    {
        "title": "The Heterogeneous Aggregate Valence Analysis (HAVAN) Model: A Flexible\n  Approach to Modeling Unobserved Heterogeneity in Discrete Choice Analysis",
        "authors": [
            "Connor R. Forsythe",
            "Cristian Arteaga",
            "John P. Helveston"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper introduces the Heterogeneous Aggregate Valence Analysis (HAVAN)\nmodel, a novel class of discrete choice models. We adopt the term \"valence'' to\nencompass any latent quantity used to model consumer decision-making (e.g.,\nutility, regret, etc.). Diverging from traditional models that parameterize\nheterogeneous preferences across various product attributes, HAVAN models\n(pronounced \"haven\") instead directly characterize alternative-specific\nheterogeneous preferences. This innovative perspective on consumer\nheterogeneity affords unprecedented flexibility and significantly reduces\nsimulation burdens commonly associated with mixed logit models. In a simulation\nexperiment, the HAVAN model demonstrates superior predictive performance\ncompared to state-of-the-art artificial neural networks. This finding\nunderscores the potential for HAVAN models to improve discrete choice modeling\ncapabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00184v1"
    },
    {
        "title": "Finite- and Large-Sample Inference for Ranks using Multinomial Data with\n  an Application to Ranking Political Parties",
        "authors": [
            "Sergei Bazylik",
            "Magne Mogstad",
            "Joseph Romano",
            "Azeem Shaikh",
            "Daniel Wilhelm"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  It is common to rank different categories by means of preferences that are\nrevealed through data on choices. A prominent example is the ranking of\npolitical candidates or parties using the estimated share of support each one\nreceives in surveys or polls about political attitudes. Since these rankings\nare computed using estimates of the share of support rather than the true share\nof support, there may be considerable uncertainty concerning the true ranking\nof the political candidates or parties. In this paper, we consider the problem\nof accounting for such uncertainty by constructing confidence sets for the rank\nof each category. We consider both the problem of constructing marginal\nconfidence sets for the rank of a particular category as well as simultaneous\nconfidence sets for the ranks of all categories. A distinguishing feature of\nour analysis is that we exploit the multinomial structure of the data to\ndevelop confidence sets that are valid in finite samples. We additionally\ndevelop confidence sets using the bootstrap that are valid only approximately\nin large samples. We use our methodology to rank political parties in Australia\nusing data from the 2019 Australian Election Survey. We find that our\nfinite-sample confidence sets are informative across the entire ranking of\npolitical parties, even in Australian territories with few survey respondents\nand/or with parties that are chosen by only a small share of the survey\nrespondents. In contrast, the bootstrap-based confidence sets may sometimes be\nconsiderably less informative. These findings motivate us to compare these\nmethods in an empirically-driven simulation study, in which we conclude that\nour finite-sample confidence sets often perform better than their large-sample,\nbootstrap-based counterparts, especially in settings that resemble our\nempirical application.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00192v1"
    },
    {
        "title": "Stochastic convergence in per capita CO$_2$ emissions. An approach from\n  nonlinear stationarity analysis",
        "authors": [
            "María José Presno",
            "Manuel Landajo",
            "Paula Fernández González"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper studies stochastic convergence of per capita CO$_2$ emissions in\n28 OECD countries for the 1901-2009 period. The analysis is carried out at two\naggregation levels, first for the whole set of countries (joint analysis) and\nthen separately for developed and developing states (group analysis). A\npowerful time series methodology, adapted to a nonlinear framework that allows\nfor quadratic trends with possibly smooth transitions between regimes, is\napplied. This approach provides more robust conclusions in convergence path\nanalysis, enabling (a) robust detection of the presence, and if so, the number\nof changes in the level and/or slope of the trend of the series, (b) inferences\non stationarity of relative per capita CO$_2$ emissions, conditionally on the\npresence of breaks and smooth transitions between regimes, and (c) estimation\nof change locations in the convergence paths. Finally, as stochastic\nconvergence is attained when both stationarity around a trend and\n$\\beta$-convergence hold, the linear approach proposed by Tomljanovich and\nVogelsang (2002) is extended in order to allow for more general quadratic\nmodels. Overall, joint analysis finds some evidence of stochastic convergence\nin per capita CO$_2$ emissions. Some dispersion in terms of $\\beta$-convergence\nis detected by group analysis, particularly among developed countries. This is\nin accordance with per capita GDP not being the sole determinant of convergence\nin emissions, with factors like search for more efficient technologies, fossil\nfuel substitution, innovation, and possibly outsources of industries, also\nhaving a crucial role.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00567v1"
    },
    {
        "title": "Arellano-Bond LASSO Estimator for Dynamic Linear Panel Models",
        "authors": [
            "Victor Chernozhukov",
            "Iván Fernández-Val",
            "Chen Huang",
            "Weining Wang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The Arellano-Bond estimator is a fundamental method for dynamic panel data\nmodels, widely used in practice. However, the estimator is severely biased when\nthe data's time series dimension $T$ is long due to the large degree of\noveridentification. We show that weak dependence along the panel's time series\ndimension naturally implies approximate sparsity of the most informative moment\nconditions, motivating the following approach to remove the bias: First, apply\nLASSO to the cross-section data at each time period to construct most\ninformative (and cross-fitted) instruments, using lagged values of suitable\ncovariates. This step relies on approximate sparsity to select the most\ninformative instruments. Second, apply a linear instrumental variable estimator\nafter first differencing the dynamic structural equation using the constructed\ninstruments. Under weak time series dependence, we show the new estimator is\nconsistent and asymptotically normal under much weaker conditions on $T$'s\ngrowth than the Arellano-Bond estimator. Our theory covers models with high\ndimensional covariates, including multiple lags of the dependent variable,\ncommon in modern applications. We illustrate our approach by applying it to\nweekly county-level panel data from the United States to study opening K-12\nschools and other mitigation policies' short and long-term effects on\nCOVID-19's spread.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00584v4"
    },
    {
        "title": "EU-28's progress towards the 2020 renewable energy share. A club\n  convergence analysis",
        "authors": [
            "María José Presno",
            "Manuel Landajo"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper assesses the convergence of the EU-28 countries towards their\ncommon goal of 20% in the renewable energy share indicator by year 2020. The\npotential presence of clubs of convergence towards different steady state\nequilibria is also analyzed from both the standpoints of global convergence to\nthe 20% goal and specific convergence to the various targets assigned to Member\nStates. Two clubs of convergence are detected in the former case, each\ncorresponding to different RES targets. A probit model is also fitted with the\naim of better understanding the determinants of club membership, that seemingly\ninclude real GDP per capita, expenditure on environmental protection, energy\ndependence, and nuclear capacity, with all of them having statistically\nsignificant effects. Finally, convergence is also analyzed separately for the\ntransport, heating and cooling, and electricity sectors.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00788v1"
    },
    {
        "title": "The prices of renewable commodities: A robust stationarity analysis",
        "authors": [
            "Manuel Landajo",
            "María José Presno"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper addresses the problem of testing for persistence in the effects of\nthe shocks affecting the prices of renewable commodities, which have potential\nimplications on stabilization policies and economic forecasting, among other\nareas. A robust methodology is employed that enables the determination of the\npotential presence and number of instant/gradual structural changes in the\nseries, stationarity testing conditional on the number of changes detected, and\nthe detection of change points. This procedure is applied to the annual real\nprices of eighteen renewable commodities over the period of 1900-2018. Results\nindicate that most of the series display non-linear features, including\nquadratic patterns and regime transitions that often coincide with well-known\npolitical and economic episodes. The conclusions of stationarity testing\nsuggest that roughly half of the series are integrated. Stationarity fails to\nbe rejected for grains, whereas most livestock and textile commodities do\nreject stationarity. Evidence is mixed in all soft commodities and tropical\ncrops, where stationarity can be rejected in approximately half of the cases.\nThe implication would be that for these commodities, stabilization schemes\nwould not be recommended.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.01005v1"
    },
    {
        "title": "Data-driven model selection within the matrix completion method for\n  causal panel data models",
        "authors": [
            "Sandro Heiniger"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Matrix completion estimators are employed in causal panel data models to\nregulate the rank of the underlying factor model using nuclear norm\nminimization. This convex optimization problem enables concurrent\nregularization of a potentially high-dimensional set of covariates to shrink\nthe model size. For valid finite sample inference, we adopt a permutation-based\napproach and prove its validity for any treatment assignment mechanism.\nSimulations illustrate the consistency of the proposed estimator in parameter\nestimation and variable selection. An application to public health policies in\nGermany demonstrates the data-driven model selection feature on empirical data\nand finds no effect of travel restrictions on the containment of severe\nCovid-19 infections.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.01069v1"
    },
    {
        "title": "The general solution to an autoregressive law of motion",
        "authors": [
            "Brendan K. Beare",
            "Massimo Franchi",
            "Phil Howlett"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We provide a complete description of the set of all solutions to an\nautoregressive law of motion in a finite-dimensional complex vector space.\nEvery solution is shown to be the sum of three parts, each corresponding to a\ndirected flow of time. One part flows forward from the arbitrarily distant\npast; one flows backward from the arbitrarily distant future; and one flows\noutward from time zero. The three parts are obtained by applying three\ncomplementary spectral projections to the solution, these corresponding to a\nseparation of the eigenvalues of the autoregressive operator according to\nwhether they are inside, outside or on the unit circle. We provide a\nfinite-dimensional parametrization of the set of all solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.01966v2"
    },
    {
        "title": "One-inflated zero-truncated count regression models",
        "authors": [
            "Ryan T. Godwin"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We find that in zero-truncated count data (y=1,2,...), individuals often gain\ninformation at first observation (y=1), leading to a common but unaddressed\nphenomenon of \"one-inflation\". The current standard, the zero-truncated\nnegative binomial (ZTNB) model, is misspecified under one-inflation, causing\nbias and inconsistency. To address this, we introduce the one-inflated\nzero-truncated negative binomial (OIZTNB) regression model. The importance of\nour model is highlighted through simulation studies, and through the discovery\nof one-inflation in four datasets that have traditionally championed ZTNB. We\nrecommended OIZTNB over ZTNB for most data, and provide estimation, marginal\neffects, and testing in the accompanying R package oneinfl.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.02272v1"
    },
    {
        "title": "Data-driven Policy Learning for Continuous Treatments",
        "authors": [
            "Chunrong Ai",
            "Yue Fang",
            "Haitian Xie"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper studies policy learning for continuous treatments from\nobservational data. Continuous treatments present more significant challenges\nthan discrete ones because population welfare may need nonparametric\nestimation, and policy space may be infinite-dimensional and may satisfy shape\nrestrictions. We propose to approximate the policy space with a sequence of\nfinite-dimensional spaces and, for any given policy, obtain the empirical\nwelfare by applying the kernel method. We consider two cases: known and unknown\npropensity scores. In the latter case, we allow for machine learning of the\npropensity score and modify the empirical welfare to account for the effect of\nmachine learning. The learned policy maximizes the empirical welfare or the\nmodified empirical welfare over the approximating space. In both cases, we\nmodify the penalty algorithm proposed in \\cite{mbakop2021model} to\ndata-automate the tuning parameters (i.e., bandwidth and dimension of the\napproximating space) and establish an oracle inequality for the welfare regret.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.02535v2"
    },
    {
        "title": "Monthly GDP nowcasting with Machine Learning and Unstructured Data",
        "authors": [
            "Juan Tenorio",
            "Wilder Perez"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In the dynamic landscape of continuous change, Machine Learning (ML)\n\"nowcasting\" models offer a distinct advantage for informed decision-making in\nboth public and private sectors. This study introduces ML-based GDP growth\nprojection models for monthly rates in Peru, integrating structured\nmacroeconomic indicators with high-frequency unstructured sentiment variables.\nAnalyzing data from January 2007 to May 2023, encompassing 91 leading economic\nindicators, the study evaluates six ML algorithms to identify optimal\npredictors. Findings highlight the superior predictive capability of ML models\nusing unstructured data, particularly Gradient Boosting Machine, LASSO, and\nElastic Net, exhibiting a 20% to 25% reduction in prediction errors compared to\ntraditional AR and Dynamic Factor Models (DFM). This enhanced performance is\nattributed to better handling of data of ML models in high-uncertainty periods,\nsuch as economic crises.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.04165v1"
    },
    {
        "title": "Inference for Two-Stage Extremum Estimators",
        "authors": [
            "Aristide Houndetoungan",
            "Abdoul Haki Maoude"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We present a simulation-based inference approach for two-stage estimators,\nfocusing on extremum estimators in the second stage. We accommodate a broad\nrange of first-stage estimators, including extremum estimators,\nhigh-dimensional estimators, and other types of estimators such as Bayesian\nestimators. The key contribution of our approach lies in its ability to\nestimate the asymptotic distribution of two-stage estimators, even when the\ndistributions of both the first- and second-stage estimators are non-normal and\nwhen the second-stage estimator's bias, scaled by the square root of the sample\nsize, does not vanish asymptotically. This enables reliable inference in\nsituations where standard methods fail. Additionally, we propose a debiased\nestimator, based on the mean of the estimated distribution function, which\nexhibits improved finite sample properties. Unlike resampling methods, our\napproach avoids the need for multiple calculations of the two-stage estimator.\nWe illustrate the effectiveness of our method in an empirical application on\npeer effects in adolescent fast-food consumption, where we address the issue of\nbiased instrumental variable estimates resulting from many weak instruments.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.05030v2"
    },
    {
        "title": "Selective linear segmentation for detecting relevant parameter changes",
        "authors": [
            "Arnaud Dufays",
            "Aristide Houndetoungan",
            "Alain Coën"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Change-point processes are one flexible approach to model long time series.\nWe propose a method to uncover which model parameter truly vary when a\nchange-point is detected. Given a set of breakpoints, we use a penalized\nlikelihood approach to select the best set of parameters that changes over time\nand we prove that the penalty function leads to a consistent selection of the\ntrue model. Estimation is carried out via the deterministic annealing\nexpectation-maximization algorithm. Our method accounts for model selection\nuncertainty and associates a probability to all the possible time-varying\nparameter specifications. Monte Carlo simulations highlight that the method\nworks well for many time series models including heteroskedastic processes. For\na sample of 14 Hedge funds (HF) strategies, using an asset based style pricing\nmodel, we shed light on the promising ability of our method to detect the\ntime-varying dynamics of risk exposures as well as to forecast HF returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.05329v1"
    },
    {
        "title": "Difference-in-Differences Estimators with Continuous Treatments and no\n  Stayers",
        "authors": [
            "Clément de Chaisemartin",
            "Xavier D'Haultfœuille",
            "Gonzalo Vazquez-Bare"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Many treatments or policy interventions are continuous in nature. Examples\ninclude prices, taxes or temperatures. Empirical researchers have usually\nrelied on two-way fixed effect regressions to estimate treatment effects in\nsuch cases. However, such estimators are not robust to heterogeneous treatment\neffects in general; they also rely on the linearity of treatment effects. We\npropose estimators for continuous treatments that do not impose those\nrestrictions, and that can be used when there are no stayers: the treatment of\nall units changes from one period to the next. We start by extending the\nnonparametric results of de Chaisemartin et al. (2023) to cases without\nstayers. We also present a parametric estimator, and use it to revisit\nDesch\\^enes and Greenstone (2012).\n",
        "pdf_link": "http://arxiv.org/pdf/2402.05432v1"
    },
    {
        "title": "Local Projections Inference with High-Dimensional Covariates without\n  Sparsity",
        "authors": [
            "Jooyoung Cha"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper presents a comprehensive local projections (LP) framework for\nestimating future responses to current shocks, robust to high-dimensional\ncontrols without relying on sparsity assumptions. The approach is applicable to\nvarious settings, including impulse response analysis and\ndifference-in-differences (DiD) estimation. While methods like LASSO exist,\nthey often assume most parameters are exactly zero, limiting their\neffectiveness in dense data generation processes. I propose a novel technique\nincorporating high-dimensional covariates in local projections using the\nOrthogonal Greedy Algorithm with a high-dimensional AIC (OGA+HDAIC) model\nselection method. This approach offers robustness in both sparse and dense\nscenarios, improved interpretability, and more reliable causal inference in\nlocal projections. Simulation studies show superior performance in dense and\npersistent scenarios compared to conventional LP and LASSO-based approaches. In\nan empirical application to Acemoglu, Naidu, Restrepo, and Robinson (2019), I\ndemonstrate efficiency gains and robustness to a large set of controls.\nAdditionally, I examine the effect of subjective beliefs on economic\naggregates, demonstrating robustness to various model specifications. A novel\nstate-dependent analysis reveals that inflation behaves more in line with\nrational expectations in good states, but exhibits more subjective, pessimistic\ndynamics in bad states.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.07743v2"
    },
    {
        "title": "On Bayesian Filtering for Markov Regime Switching Models",
        "authors": [
            "Nigar Hashimzade",
            "Oleg Kirsanov",
            "Tatiana Kirsanova",
            "Junior Maih"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper presents a framework for empirical analysis of dynamic\nmacroeconomic models using Bayesian filtering, with a specific focus on the\nstate-space formulation of Dynamic Stochastic General Equilibrium (DSGE) models\nwith multiple regimes. We outline the theoretical foundations of model\nestimation, provide the details of two families of powerful multiple-regime\nfilters, IMM and GPB, and construct corresponding multiple-regime smoothers. A\nsimulation exercise, based on a prototypical New Keynesian DSGE model, is used\nto demonstrate the computational robustness of the proposed filters and\nsmoothers and evaluate their accuracy and speed for a selection of filters from\neach family. We show that the canonical IMM filter is faster and is no less,\nand often more, accurate than its competitors within IMM and GPB families, the\nlatter including the commonly used Kim and Nelson (1999) filter. Using it with\nthe matching smoother improves the precision in recovering unobserved variables\nby about 25 percent. Furthermore, applying it to the U.S. 1947-2023\nmacroeconomic time series, we successfully identify significant past policy\nshifts including those related to the post-Covid-19 period. Our results\ndemonstrate the practical applicability and potential of the proposed routines\nin macroeconomic analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.08051v1"
    },
    {
        "title": "Heterogeneity, Uncertainty and Learning: Semiparametric Identification\n  and Estimation",
        "authors": [
            "Jackson Bunting",
            "Paul Diegert",
            "Arnaud Maurel"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We provide semiparametric identification results for a broad class of\nlearning models in which continuous outcomes depend on three types of\nunobservables: i) known heterogeneity, ii) initially unknown heterogeneity that\nmay be revealed over time, and iii) transitory uncertainty. We consider a\ncommon environment where the researcher only has access to a short panel on\nchoices and realized outcomes. We establish identification of the outcome\nequation parameters and the distribution of the three types of unobservables,\nunder the standard assumption that unknown heterogeneity and uncertainty are\nnormally distributed. We also show that, absent known heterogeneity, the model\nis identified without making any distributional assumption. We then derive the\nasymptotic properties of a sieve MLE estimator for the model parameters, and\ndevise a tractable profile likelihood based estimation procedure. Monte Carlo\nsimulation results indicate that our estimator exhibits good finite-sample\nproperties.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.08575v1"
    },
    {
        "title": "Quantile Granger Causality in the Presence of Instability",
        "authors": [
            "Alexander Mayer",
            "Dominik Wied",
            "Victor Troster"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a new framework for assessing Granger causality in quantiles in\nunstable environments, for a fixed quantile or over a continuum of quantile\nlevels. Our proposed test statistics are consistent against fixed alternatives,\nthey have nontrivial power against local alternatives, and they are pivotal in\ncertain important special cases. In addition, we show the validity of a\nbootstrap procedure when asymptotic distributions depend on nuisance\nparameters. Monte Carlo simulations reveal that the proposed test statistics\nhave correct empirical size and high power, even in absence of structural\nbreaks. Moreover, a procedure providing additional insight into the timing of\nGranger causal regimes based on our new tests is proposed. Finally, an\nempirical application in energy economics highlights the applicability of our\nmethod as the new tests provide stronger evidence of Granger causality.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09744v2"
    },
    {
        "title": "Identification with Posterior-Separable Information Costs",
        "authors": [
            "Martin Bustos"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  I provide a model of rational inattention with heterogeneity and prove it is\nobservationally equivalent to a state-dependent stochastic choice model subject\nto attention costs. I demonstrate that additive separability of unobservable\nheterogeneity, together with an independence assumption, suffice for the\nempirical model to admit a representative agent. Using conditional\nprobabilities, I show how to identify: how covariates affect the desirability\nof goods, (a measure of) welfare, factual changes in welfare, and bounds on\ncounterfactual market shares.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09789v1"
    },
    {
        "title": "Spatial Data Analysis",
        "authors": [
            "Tobias Rüttenauer"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This handbook chapter provides an essential introduction to the field of\nspatial econometrics, offering a comprehensive overview of techniques and\nmethodologies for analysing spatial data in the social sciences. Spatial\neconometrics addresses the unique challenges posed by spatially dependent\nobservations, where spatial relationships among data points can significantly\nimpact statistical analyses. The chapter begins by exploring the fundamental\nconcepts of spatial dependence and spatial autocorrelation, and highlighting\ntheir implications for traditional econometric models. It then introduces a\nrange of spatial econometric models, particularly spatial lag, spatial error,\nand spatial lag of X models, illustrating how these models accommodate spatial\nrelationships and yield accurate and insightful results about the underlying\nspatial processes. The chapter provides an intuitive understanding of these\nmodels compare to each other. A practical example on London house prices\ndemonstrates the application of spatial econometrics, emphasising its relevance\nin uncovering hidden spatial patterns, addressing endogeneity, and providing\nrobust estimates in the presence of spatial dependence.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09895v1"
    },
    {
        "title": "When Can We Use Two-Way Fixed-Effects (TWFE): A Comparison of TWFE and\n  Novel Dynamic Difference-in-Differences Estimators",
        "authors": [
            "Tobias Rüttenauer",
            "Ozan Aksoy"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The conventional Two-Way Fixed-Effects (TWFE) estimator has come under\nscrutiny lately. Recent literature has revealed potential shortcomings of TWFE\nwhen the treatment effects are heterogeneous. Scholars have developed new\nadvanced dynamic Difference-in-Differences (DiD) estimators to tackle these\npotential shortcomings. However, confusion remains in applied research as to\nwhen the conventional TWFE is biased and what issues the novel estimators can\nand cannot address. In this study, we first provide an intuitive explanation of\nthe problems of TWFE and elucidate the key features of the novel alternative\nDiD estimators. We then systematically demonstrate the conditions under which\nthe conventional TWFE is inconsistent. We employ Monte Carlo simulations to\nassess the performance of dynamic DiD estimators under violations of key\nassumptions, which likely happens in applied cases. While the new dynamic DiD\nestimators offer notable advantages in capturing heterogeneous treatment\neffects, we show that the conventional TWFE performs generally well if the\nmodel specifies an event-time function. All estimators are equally sensitive to\nviolations of the parallel trends assumption, anticipation effects or\nviolations of time-varying exogeneity. Despite their advantages, the new\ndynamic DiD estimators tackle a very specific problem and they do not serve as\na universal remedy for violations of the most critical assumptions. We finally\nderive, based on our simulations, recommendations for how and when to use TWFE\nand the new DiD estimators in applied research.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09928v2"
    },
    {
        "title": "Manipulation Test for Multidimensional RDD",
        "authors": [
            "Federico Crippa"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The causal inference model proposed by Lee (2008) for the regression\ndiscontinuity design (RDD) relies on assumptions that imply the continuity of\nthe density of the assignment (running) variable. The test for this implication\nis commonly referred to as the manipulation test and is regularly reported in\napplied research to strengthen the design's validity. The multidimensional RDD\n(MRDD) extends the RDD to contexts where treatment assignment depends on\nseveral running variables. This paper introduces a manipulation test for the\nMRDD. First, it develops a theoretical model for causal inference with the\nMRDD, used to derive a testable implication on the conditional marginal\ndensities of the running variables. Then, it constructs the test for the\nimplication based on a quadratic form of a vector of statistics separately\ncomputed for each marginal density. Finally, the proposed test is compared with\nalternative procedures commonly employed in applied research.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.10836v2"
    },
    {
        "title": "Inference on LATEs with covariates",
        "authors": [
            "Tom Boot",
            "Didier Nibbering"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In theory, two-stage least squares (TSLS) identifies a weighted average of\ncovariate-specific local average treatment effects (LATEs) from a saturated\nspecification, without making parametric assumptions on how available\ncovariates enter the model. In practice, TSLS is severely biased as saturation\nleads to a large number of control dummies and an equally large number of,\narguably weak, instruments. This paper derives asymptotically valid tests and\nconfidence intervals for the weighted average of LATEs that is targeted, yet\nmissed by saturated TSLS. The proposed inference procedure is robust to\nunobserved treatment effect heterogeneity, covariates with rich support, and\nweak identification. We find LATEs statistically significantly different from\nzero in applications in criminology, finance, health, and education.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.12607v2"
    },
    {
        "title": "Extending the Scope of Inference About Predictive Ability to Machine\n  Learning Methods",
        "authors": [
            "Juan Carlos Escanciano",
            "Ricardo Parra"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Though out-of-sample forecast evaluation is systematically employed with\nmodern machine learning methods and there exists a well-established classic\ninference theory for predictive ability, see, e.g., West (1996, Asymptotic\nInference About Predictive Ability, Econometrica, 64, 1067-1084), such theory\nis not directly applicable to modern machine learners such as the Lasso in the\nhigh dimensional setting. We investigate under which conditions such extensions\nare possible. Two key properties for standard out-of-sample asymptotic\ninference to be valid with machine learning are (i) a zero-mean condition for\nthe score of the prediction loss function; and (ii) a fast rate of convergence\nfor the machine learner. Monte Carlo simulations confirm our theoretical\nfindings. We recommend a small out-of-sample vs in-sample size ratio for\naccurate finite sample inferences with machine learning. We illustrate the wide\napplicability of our results with a new out-of-sample test for the Martingale\nDifference Hypothesis (MDH). We obtain the asymptotic null distribution of our\ntest and use it to evaluate the MDH of some major exchange rates at daily and\nhigher frequencies.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.12838v2"
    },
    {
        "title": "Bridging Methodologies: Angrist and Imbens' Contributions to Causal\n  Identification",
        "authors": [
            "Lucas Girard",
            "Yannick Guyonvarch"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In the 1990s, Joshua Angrist and Guido Imbens studied the causal\ninterpretation of Instrumental Variable estimates (a widespread methodology in\neconomics) through the lens of potential outcomes (a classical framework to\nformalize causality in statistics). Bridging a gap between those two strands of\nliterature, they stress the importance of treatment effect heterogeneity and\nshow that, under defendable assumptions in various applications, this method\nrecovers an average causal effect for a specific subpopulation of individuals\nwhose treatment is affected by the instrument. They were awarded the Nobel\nPrize primarily for this Local Average Treatment Effect (LATE). The first part\nof this article presents that methodological contribution in-depth: the\norigination in earlier applied articles, the different identification results\nand extensions, and related debates on the relevance of LATEs for public policy\ndecisions. The second part reviews the main contributions of the authors beyond\nthe LATE. J. Angrist has pursued the search for informative and varied\nempirical research designs in several fields, particularly in education. G.\nImbens has complemented the toolbox for treatment effect estimation in many\nways, notably through propensity score reweighting, matching, and, more\nrecently, adapting machine learning procedures.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.13023v1"
    },
    {
        "title": "Vulnerability Webs: Systemic Risk in Software Networks",
        "authors": [
            "Cornelius Fritz",
            "Co-Pierre Georg",
            "Angelo Mele",
            "Michael Schweinberger"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Modern software development is a collaborative effort that re-uses existing\ncode to reduce development and maintenance costs. This practice exposes\nsoftware to vulnerabilities in the form of undetected bugs in direct and\nindirect dependencies, as demonstrated by the Crowdstrike and HeartBleed bugs.\nThe economic costs resulting from such vulnerabilities can be staggering. We\nstudy a directed network of 52,897 software dependencies across 16,102 Python\nrepositories, guided by a strategic model of network formation that\nincorporates both observable and unobservable heterogeneity. Using a scalable\nvariational approximation of the conditional distribution of unobserved\nheterogeneity, we show that outsourcing code to other software packages by\ncreating dependencies generates negative externalities. Modeling the\npropagation of risk in networks of software packages as an epidemiological\nprocess, we show that increasing protection of dependencies based on popular\nheuristics is ineffective at reducing systemic risk. By contrast, AI-assisted\ncoding enables developers to replace dependencies with in-house code and\nreduces systemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.13375v2"
    },
    {
        "title": "Enhancing Rolling Horizon Production Planning Through Stochastic\n  Optimization Evaluated by Means of Simulation",
        "authors": [
            "Manuel Schlenkrich",
            "Wolfgang Seiringer",
            "Klaus Altendorfer",
            "Sophie N. Parragh"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Production planning must account for uncertainty in a production system,\narising from fluctuating demand forecasts. Therefore, this article focuses on\nthe integration of updated customer demand into the rolling horizon planning\ncycle. We use scenario-based stochastic programming to solve capacitated lot\nsizing problems under stochastic demand in a rolling horizon environment. This\nenvironment is replicated using a discrete event simulation-optimization\nframework, where the optimization problem is periodically solved, leveraging\nthe latest demand information to continually adjust the production plan. We\nevaluate the stochastic optimization approach and compare its performance to\nsolving a deterministic lot sizing model, using expected demand figures as\ninput, as well as to standard Material Requirements Planning (MRP). In the\nsimulation study, we analyze three different customer behaviors related to\nforecasting, along with four levels of shop load, within a multi-item and\nmulti-stage production system. We test a range of significant parameter values\nfor the three planning methods and compute the overall costs to benchmark them.\nThe results show that the production plans obtained by MRP are outperformed by\ndeterministic and stochastic optimization. Particularly, when facing tight\nresource restrictions and rising uncertainty in customer demand, the use of\nstochastic optimization becomes preferable compared to deterministic\noptimization.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.14506v2"
    },
    {
        "title": "Functional Spatial Autoregressive Models",
        "authors": [
            "Tadao Hoshino"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This study introduces a novel spatial autoregressive model in which the\ndependent variable is a function that may exhibit functional autocorrelation\nwith the outcome functions of nearby units. This model can be characterized as\na simultaneous integral equation system, which, in general, does not\nnecessarily have a unique solution. For this issue, we provide a simple\ncondition on the magnitude of the spatial interaction to ensure the uniqueness\nin data realization. For estimation, to account for the endogeneity caused by\nthe spatial interaction, we propose a regularized two-stage least squares\nestimator based on a basis approximation for the functional parameter. The\nasymptotic properties of the estimator including the consistency and asymptotic\nnormality are investigated under certain conditions. Additionally, we propose a\nsimple Wald-type test for detecting the presence of spatial effects. As an\nempirical illustration, we apply the proposed model and method to analyze age\ndistributions in Japanese cities.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.14763v3"
    },
    {
        "title": "Estimating Stochastic Block Models in the Presence of Covariates",
        "authors": [
            "Yuichi Kitamura",
            "Louise Laage"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In the standard stochastic block model for networks, the probability of a\nconnection between two nodes, often referred to as the edge probability,\ndepends on the unobserved communities each of these nodes belongs to. We\nconsider a flexible framework in which each edge probability, together with the\nprobability of community assignment, are also impacted by observed covariates.\nWe propose a computationally tractable two-step procedure to estimate the\nconditional edge probabilities as well as the community assignment\nprobabilities. The first step relies on a spectral clustering algorithm applied\nto a localized adjacency matrix of the network. In the second step, k-nearest\nneighbor regression estimates are computed on the extracted communities. We\nstudy the statistical properties of these estimators by providing\nnon-asymptotic bounds.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.16322v1"
    },
    {
        "title": "Fast Algorithms for Quantile Regression with Selection",
        "authors": [
            "Santiago Pereda-Fernández"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper addresses computational challenges in estimating Quantile\nRegression with Selection (QRS). The estimation of the parameters that model\nself-selection requires the estimation of the entire quantile process several\ntimes. Moreover, closed-form expressions of the asymptotic variance are too\ncumbersome, making the bootstrap more convenient to perform inference. Taking\nadvantage of recent advancements in the estimation of quantile regression,\nalong with some specific characteristics of the QRS estimation problem, I\npropose streamlined algorithms for the QRS estimator. These algorithms\nsignificantly reduce computation time through preprocessing techniques and\nquantile grid reduction for the estimation of the copula and slope parameters.\nI show the optimization enhancements with some simulations. Lastly, I show how\npreprocessing methods can improve the precision of the estimates without\nsacrificing computational efficiency. Hence, they constitute a practical\nsolutions for estimators with non-differentiable and non-convex criterion\nfunctions such as those based on copulas.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.16693v1"
    },
    {
        "title": "Treatment effects without multicollinearity? Temporal order and the\n  Gram-Schmidt process in causal inference",
        "authors": [
            "Robin M. Cross",
            "Steven T. Buccola"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper incorporates information about the temporal order of regressors to\nestimate orthogonal and economically interpretable regression coefficients. We\nestablish new finite sample properties for the Gram-Schmidt orthogonalization\nprocess. Coefficients are unbiased and stable with lower standard errors than\nthose from Ordinary Least Squares. We provide conditions under which\ncoefficients represent average total treatment effects on the treated and\nextend the model to groups of ordered and simultaneous regressors. Finally, we\nreanalyze two studies that controlled for temporally ordered and collinear\ncharacteristics, including race, education, and income. The new approach\nexpands Bohren et al.'s decomposition of systemic discrimination into\nchannel-specific effects and improves significance levels.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.17103v2"
    },
    {
        "title": "Testing Information Ordering for Strategic Agents",
        "authors": [
            "Sukjin Han",
            "Hiroaki Kaido",
            "Lorenzo Magnolfi"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  A key primitive of a strategic environment is the information available to\nplayers. Specifying a priori an information structure is often difficult for\nempirical researchers. We develop a test of information ordering that allows\nresearchers to examine if the true information structure is at least as\ninformative as a proposed baseline. We construct a computationally tractable\ntest statistic by utilizing the notion of Bayes Correlated Equilibrium (BCE) to\ntranslate the ordering of information structures into an ordering of functions.\nWe apply our test to examine whether hubs provide informational advantages to\ncertain airlines in addition to market power.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.19425v1"
    },
    {
        "title": "Set-Valued Control Functions",
        "authors": [
            "Sukjin Han",
            "Hiroaki Kaido"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The control function approach allows the researcher to identify various\ncausal effects of interest. While powerful, it requires a strong invertibility\nassumption, which limits its applicability. This paper expands the scope of the\nnonparametric control function approach by allowing the control function to be\nset-valued and derive sharp bounds on structural parameters. The proposed\ngeneralization accommodates a wide range of selection processes involving\ndiscrete endogenous variables, random coefficients, treatment selections with\ninterference, and dynamic treatment selections.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00347v2"
    },
    {
        "title": "Inference for Interval-Identified Parameters Selected from an Estimated\n  Set",
        "authors": [
            "Sukjin Han",
            "Adam McCloskey"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Interval identification of parameters such as average treatment effects,\naverage partial effects and welfare is particularly common when using\nobservational data and experimental data with imperfect compliance due to the\nendogeneity of individuals' treatment uptake. In this setting, a treatment or\npolicy will typically become an object of interest to the researcher when it is\neither selected from the estimated set of best-performers or arises from a\ndata-dependent selection rule. In this paper, we develop new inference tools\nfor interval-identified parameters chosen via these forms of selection. We\ndevelop three types of confidence intervals for data-dependent and\ninterval-identified parameters, discuss how they apply to several examples of\ninterest and prove their uniform asymptotic validity under weak assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00422v1"
    },
    {
        "title": "Prices and preferences in the electric vehicle market",
        "authors": [
            "Chung Yi See",
            "Vasco Rato Santos",
            "Lucas Woodley",
            "Megan Yeo",
            "Daniel Palmer",
            "Shuheng Zhang",
            "and Ashley Nunes"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Although electric vehicles are less polluting than gasoline powered vehicles,\nadoption is challenged by higher procurement prices. Existing discourse\nemphasizes EV battery costs as being principally responsible for this price\ndifferential and widespread adoption is routinely conditioned upon battery\ncosts declining. We scrutinize such reasoning by sourcing data on EV attributes\nand market conditions between 2011 and 2023. Our findings are fourfold. First,\nEV prices are influenced principally by the number of amenities, additional\nfeatures, and dealer-installed accessories sold as standard on an EV, and to a\nlesser extent, by EV horsepower. Second, EV range is negatively correlated with\nEV price implying that range anxiety concerns may be less consequential than\nexisting discourse suggests. Third, battery capacity is positively correlated\nwith EV price, due to more capacity being synonymous with the delivery of more\nhorsepower. Collectively, this suggests that higher procurement prices for EVs\nreflects consumer preference for vehicles that are feature dense and more\npowerful. Fourth and finally, accommodating these preferences have produced\nvehicles with lower fuel economy, a shift that reduces envisioned lifecycle\nemissions benefits by at least 3.26 percent, subject to the battery pack\nchemistry leveraged and the carbon intensity of the electrical grid. These\nfindings warrant attention as decarbonization efforts increasingly emphasize\nelectrification as a pathway for complying with domestic and international\nclimate agreements.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00458v1"
    },
    {
        "title": "Improved Tests for Mediation",
        "authors": [
            "Grant Hillier",
            "Kees Jan van Garderen",
            "Noud van Giersbergen"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Testing for a mediation effect is important in many disciplines, but is made\ndifficult - even asymptotically - by the influence of nuisance parameters.\nClassical tests such as likelihood ratio (LR) and Wald (Sobel) tests have very\npoor power properties in parts of the parameter space, and many attempts have\nbeen made to produce improved tests, with limited success. In this paper we\nshow that augmenting the critical region of the LR test can produce a test with\nmuch improved behavior everywhere. In fact, we first show that there exists a\ntest of this type that is (asymptotically) exact for certain test levels\n$\\alpha $, including the common choices $\\alpha =.01,.05,.10.$ The critical\nregion of this exact test has some undesirable properties. We go on to show\nthat there is a very simple class of augmented LR critical regions which\nprovides tests that are nearly exact, and avoid the issues inherent in the\nexact test. We suggest an optimal and coherent member of this class, provide\nthe table needed to implement the test and to report p-values if desired.\nSimulation confirms validity with non-Gaussian disturbances, under\nheteroskedasticity, and in a nonlinear (logit) model. A short application of\nthe method to an entrepreneurial attitudes study is included for illustration.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.02144v1"
    },
    {
        "title": "Matrix-based Prediction Approach for Intraday Instantaneous Volatility\n  Vector",
        "authors": [
            "Sung Hoon Choi",
            "Donggyu Kim"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, we introduce a novel method for predicting intraday\ninstantaneous volatility based on Ito semimartingale models using\nhigh-frequency financial data. Several studies have highlighted stylized\nvolatility time series features, such as interday auto-regressive dynamics and\nthe intraday U-shaped pattern. To accommodate these volatility features, we\npropose an interday-by-intraday instantaneous volatility matrix process that\ncan be decomposed into low-rank conditional expected instantaneous volatility\nand noise matrices. To predict the low-rank conditional expected instantaneous\nvolatility matrix, we propose the Two-sIde Projected-PCA (TIP-PCA) procedure.\nWe establish asymptotic properties of the proposed estimators and conduct a\nsimulation study to assess the finite sample performance of the proposed\nprediction method. Finally, we apply the TIP-PCA method to an out-of-sample\ninstantaneous volatility vector prediction study using high-frequency data from\nthe S&P 500 index and 11 sector index funds.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.02591v2"
    },
    {
        "title": "A Logarithmic Mean Divisia Index Decomposition of CO$_2$ Emissions from\n  Energy Use in Romania",
        "authors": [
            "Mariana Carmelia Balanica-Dragomir",
            "Gabriel Murariu",
            "Lucian Puiu Georgescu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Carbon emissions have become a specific alarming indicators and intricate\nchallenges that lead an extended argue about climate change. The growing trend\nin the utilization of fossil fuels for the economic progress and simultaneously\nreducing the carbon quantity has turn into a substantial and global challenge.\nThe aim of this paper is to examine the driving factors of CO$_2$ emissions\nfrom energy sector in Romania during the period 2008-2022 emissions using the\nlog mean Divisia index (LMDI) method and takes into account five items: CO$_2$\nemissions, primary energy resources, energy consumption, gross domestic product\nand population, the driving forces of CO$_2$ emissions, based on which it was\ncalculated the contribution of carbon intensity, energy mixes, generating\nefficiency, economy, and population. The results indicate that generating\nefficiency effect -90968.57 is the largest inhibiting index while economic\neffect is the largest positive index 69084.04 having the role of increasing\nCO$_2$ emissions.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.04354v1"
    },
    {
        "title": "Partially identified heteroskedastic SVARs",
        "authors": [
            "Emanuele Bacchiocchi",
            "Andrea Bastianin",
            "Toru Kitagawa",
            "Elisabetta Mirto"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper studies the identification of Structural Vector Autoregressions\n(SVARs) exploiting a break in the variances of the structural shocks.\nPoint-identification for this class of models relies on an eigen-decomposition\ninvolving the covariance matrices of reduced-form errors and requires that all\nthe eigenvalues are distinct. This point-identification, however, fails in the\npresence of multiplicity of eigenvalues. This occurs in an empirically relevant\nscenario where, for instance, only a subset of structural shocks had the break\nin their variances, or where a group of variables shows a variance shift of the\nsame amount. Together with zero or sign restrictions on the structural\nparameters and impulse responses, we derive the identified sets for impulse\nresponses and show how to compute them. We perform inference on the impulse\nresponse functions, building on the robust Bayesian approach developed for set\nidentified SVARs. To illustrate our proposal, we present an empirical example\nbased on the literature on the global crude oil market where the identification\nis expected to fail due to multiplicity of eigenvalues.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.06879v2"
    },
    {
        "title": "Imputation of Counterfactual Outcomes when the Errors are Predictable",
        "authors": [
            "Silvia Goncalves",
            "Serena Ng"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  A crucial input into causal inference is the imputed counterfactual outcome.\nImputation error can arise because of sampling uncertainty from estimating the\nprediction model using the untreated observations, or from out-of-sample\ninformation not captured by the model. While the literature has focused on\nsampling uncertainty, it vanishes with the sample size. Often overlooked is the\npossibility that the out-of-sample error can be informative about the missing\ncounterfactual outcome if it is mutually or serially correlated. Motivated by\nthe best linear unbiased predictor (\\blup) of \\citet{goldberger:62} in a time\nseries setting, we propose an improved predictor of potential outcome when the\nerrors are correlated. The proposed \\pup\\; is practical as it is not restricted\nto linear models, can be used with consistent estimators already developed, and\nimproves mean-squared error for a large class of strong mixing error processes.\nIgnoring predictability in the errors can distort conditional inference.\nHowever, the precise impact will depend on the choice of estimator as well as\nthe realized values of the residuals.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.08130v2"
    },
    {
        "title": "Invalid proxies and volatility changes",
        "authors": [
            "Giovanni Angelini",
            "Luca Fanelli",
            "Luca Neri"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  When in proxy-SVARs the covariance matrix of VAR disturbances is subject to\nexogenous, permanent, nonrecurring breaks that generate target impulse response\nfunctions (IRFs) that change across volatility regimes, even strong, exogenous\nexternal instruments can result in inconsistent estimates of the dynamic causal\neffects of interest if the breaks are not properly accounted for. In such\ncases, it is essential to explicitly incorporate the shifts in unconditional\nvolatility in order to point-identify the target structural shocks and possibly\nrestore consistency. We demonstrate that, under a necessary and sufficient rank\ncondition that leverages moments implied by changes in volatility, the target\nIRFs can be point-identified and consistently estimated. Importantly, standard\nasymptotic inference remains valid in this context despite (i) the covariance\nbetween the proxies and the instrumented structural shocks being local-to-zero,\nas in Staiger and Stock (1997), and (ii) the potential failure of instrument\nexogeneity. We introduce a novel identification strategy that appropriately\ncombines external instruments with \"informative\" changes in volatility, thus\nobviating the need to assume proxy relevance and exogeneity in estimation. We\nillustrate the effectiveness of the suggested method by revisiting a fiscal\nproxy-SVAR previously estimated in the literature, complementing the fiscal\ninstruments with information derived from the massive reduction in volatility\nobserved in the transition from the Great Inflation to the Great Moderation\nregimes.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.08753v1"
    },
    {
        "title": "Macroeconomic Spillovers of Weather Shocks across U.S. States",
        "authors": [
            "Emanuele Bacchiocchi",
            "Andrea Bastianin",
            "Graziano Moramarco"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We estimate the short-run effects of severe weather shocks on local economic\nactivity and cross-border spillovers operating through economic linkages\nbetween U.S. states. We measure weather shocks using emergency declarations\ntriggered by natural disasters and estimate their impacts with a monthly Global\nVector Autoregressive (GVAR) model for U.S. states. Impulse responses highlight\ncountry-wide effects of weather shocks hitting individual regions. Taking into\naccount economic interconnections between states allows capturing much stronger\nspillovers than those associated with mere spatial adjacency. Also,\ngeographical heterogeneity is critical for assessing country-wide effects of\nweather shocks, and network effects amplify the local impacts of shocks.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.10907v2"
    },
    {
        "title": "Comprehensive OOS Evaluation of Predictive Algorithms with Statistical\n  Decision Theory",
        "authors": [
            "Jeff Dominitz",
            "Charles F. Manski"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We argue that comprehensive out-of-sample (OOS) evaluation using statistical\ndecision theory (SDT) should replace the current practice of K-fold and Common\nTask Framework validation in machine learning (ML) research. SDT provides a\nformal framework for performing comprehensive OOS evaluation across all\npossible (1) training samples, (2) populations that may generate training data,\nand (3) populations of prediction interest. Regarding feature (3), we emphasize\nthat SDT requires the practitioner to directly confront the possibility that\nthe future may not look like the past and to account for a possible need to\nextrapolate from one population to another when building a predictive\nalgorithm. SDT is simple in abstraction, but it is often computationally\ndemanding to implement. We discuss progress in tractable implementation of SDT\nwhen prediction accuracy is measured by mean square error or by\nmisclassification rate. We summarize research studying settings in which the\ntraining data will be generated from a subpopulation of the population of\nprediction interest. We consider conditional prediction with alternative\nrestrictions on the state space of possible populations that may generate\ntraining data. We present an illustrative application of the methodology to the\nproblem of predicting patient illness to inform clinical decision making. We\nconclude by calling on ML researchers to join with econometricians and\nstatisticians in expanding the domain within which implementation of SDT is\ntractable.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.11016v2"
    },
    {
        "title": "Nonparametric Identification and Estimation with Non-Classical\n  Errors-in-Variables",
        "authors": [
            "Kirill S. Evdokimov",
            "Andrei Zeleneev"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper considers nonparametric identification and estimation of the\nregression function when a covariate is mismeasured. The measurement error need\nnot be classical. Employing the small measurement error approximation, we\nestablish nonparametric identification under weak and easy-to-interpret\nconditions on the instrumental variable. The paper also provides nonparametric\nestimators of the regression function and derives their rates of convergence.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.11309v1"
    },
    {
        "title": "Robust Inference in Locally Misspecified Bipartite Networks",
        "authors": [
            "Luis E. Candelaria",
            "Yichong Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper introduces a methodology to conduct robust inference in bipartite\nnetworks under local misspecification. We focus on a class of dyadic network\nmodels with misspecified conditional moment restrictions. The framework of\nmisspecification is local, as the effect of misspecification varies with the\nsample size. We utilize this local asymptotic approach to construct a robust\nestimator that is minimax optimal for the mean square error within a\nneighborhood of misspecification. Additionally, we introduce bias-aware\nconfidence intervals that account for the effect of the local misspecification.\nThese confidence intervals have the correct asymptotic coverage for the true\nparameter of interest under sparse network asymptotics. Monte Carlo experiments\ndemonstrate that the robust estimator performs well in finite samples and\nsparse networks. As an empirical illustration, we study the formation of a\nscientific collaboration network among economists.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.13725v1"
    },
    {
        "title": "Policy Relevant Treatment Effects with Multidimensional Unobserved\n  Heterogeneity",
        "authors": [
            "Takuya Ura",
            "Lina Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper provides a framework for the policy relevant treatment effects\nusing instrumental variables. In the framework, a treatment selection may or\nmay not satisfy the classical monotonicity condition and can accommodate\nmultidimensional unobserved heterogeneity. We can bound the target parameter by\nextracting information from identifiable estimands. We also provide a more\nconservative yet computationally simpler bound by applying a convex relaxation\nmethod. Linear shape restrictions can be easily incorporated to further improve\nthe bounds. Numerical and simulation results illustrate the informativeness of\nour convex-relaxation bounds, i.e., that our bounds are sufficiently tight.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.13738v1"
    },
    {
        "title": "Fused LASSO as Non-Crossing Quantile Regression",
        "authors": [
            "Tibor Szendrei",
            "Arnab Bhattacharjee",
            "Mark E. Schaffer"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Quantile crossing has been an ever-present thorn in the side of quantile\nregression. This has spurred research into obtaining densities and coefficients\nthat obey the quantile monotonicity property. While important contributions,\nthese papers do not provide insight into how exactly these constraints\ninfluence the estimated coefficients. This paper extends non-crossing\nconstraints and shows that by varying a single hyperparameter ($\\alpha$) one\ncan obtain commonly used quantile estimators. Namely, we obtain the quantile\nregression estimator of Koenker and Bassett (1978) when $\\alpha=0$, the non\ncrossing quantile regression estimator of Bondell et al. (2010) when\n$\\alpha=1$, and the composite quantile regression estimator of Koenker (1984)\nand Zou and Yuan (2008) when $\\alpha\\rightarrow\\infty$. As such, we show that\nnon-crossing constraints are simply a special type of fused-shrinkage.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.14036v2"
    },
    {
        "title": "A Gaussian smooth transition vector autoregressive model: An application\n  to the macroeconomic effects of severe weather shocks",
        "authors": [
            "Markku Lanne",
            "Savi Virolainen"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We introduce a new smooth transition vector autoregressive model with a\nGaussian conditional distribution and transition weights that, for a $p$th\norder model, depend on the full distribution of the preceding $p$ observations.\nSpecifically, the transition weight of each regime increases in its relative\nweighted likelihood. This data-driven approach facilitates capturing complex\nswitching dynamics, enhancing the identification of gradual regime shifts. In\nan empirical application to the macroeconomic effects of a severe weather\nshock, we find that in monthly U.S. data from 1961:1 to 2022:3, the impacts of\nthe shock are stronger in the regime prevailing in the early part of the sample\nand in certain crisis periods than in the regime dominating the latter part of\nthe sample. This suggests overall adaptation of the U.S. economy to increased\nsevere weather over time.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.14216v2"
    },
    {
        "title": "Fast TTC Computation",
        "authors": [
            "Irene Aldridge"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper proposes a fast Markov Matrix-based methodology for computing Top\nTrading Cycles (TTC) that delivers O(1) computational speed, that is speed\nindependent of the number of agents and objects in the system. The proposed\nmethodology is well suited for complex large-dimensional problems like housing\nchoice. The methodology retains all the properties of TTC, namely,\nPareto-efficiency, individual rationality and strategy-proofness.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.15111v1"
    },
    {
        "title": "Difference-in-Differences with Unpoolable Data",
        "authors": [
            "Sunny Karim",
            "Matthew D. Webb",
            "Nichole Austin",
            "Erin Strumpf"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Difference-in-differences (DID) is commonly used to estimate treatment\neffects but is infeasible in settings where data are unpoolable due to privacy\nconcerns or legal restrictions on data sharing, particularly across\njurisdictions. In this study, we identify and relax the assumption of data\npoolability in DID estimation. We propose an innovative approach to estimate\nDID with unpoolable data (UN-DID) which can accommodate covariates, multiple\ngroups, and staggered adoption. Through analytical proofs and Monte Carlo\nsimulations, we show that UN-DID and conventional DID estimates of the average\ntreatment effect and standard errors are equal and unbiased in settings without\ncovariates. With covariates, both methods produce estimates that are unbiased,\nequivalent, and converge to the true value. The estimates differ slightly but\nthe statistical inference and substantive conclusions remain the same. Two\nempirical examples with real-world data further underscore UN-DID's utility.\nThe UN-DID method allows the estimation of cross-jurisdictional treatment\neffects with unpoolable data, enabling better counterfactuals to be used and\nnew research questions to be answered.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.15910v2"
    },
    {
        "title": "Debiased Machine Learning when Nuisance Parameters Appear in Indicator\n  Functions",
        "authors": [
            "Gyungbae Park"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper studies debiased machine learning when nuisance parameters appear\nin indicator functions. An important example is maximized average welfare under\noptimal treatment assignment rules. For asymptotically valid inference for a\nparameter of interest, the current literature on debiased machine learning\nrelies on Gateaux differentiability of the functions inside moment conditions,\nwhich does not hold when nuisance parameters appear in indicator functions. In\nthis paper, we propose smoothing the indicator functions, and develop an\nasymptotic distribution theory for this class of models. The asymptotic\nbehavior of the proposed estimator exhibits a trade-off between bias and\nvariance due to smoothing. We study how a parameter which controls the degree\nof smoothing can be chosen optimally to minimize an upper bound of the\nasymptotic mean squared error. A Monte Carlo simulation supports the asymptotic\ndistribution theory, and an empirical example illustrates the implementation of\nthe method.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.15934v1"
    },
    {
        "title": "The Informativeness of Combined Experimental and Observational Data\n  under Dynamic Selection",
        "authors": [
            "Yechan Park",
            "Yuya Sasaki"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper addresses the challenge of estimating the Average Treatment Effect\non the Treated Survivors (ATETS; Vikstrom et al., 2018) in the absence of\nlong-term experimental data, utilizing available long-term observational data\ninstead. We establish two theoretical results. First, it is impossible to\nobtain informative bounds for the ATETS with no model restriction and no\nauxiliary data. Second, to overturn this negative result, we explore as a\npromising avenue the recent econometric developments in combining experimental\nand observational data (e.g., Athey et al., 2020, 2019); we indeed find that\nexploiting short-term experimental data can be informative without imposing\nclassical model restrictions. Furthermore, building on Chesher and Rosen\n(2017), we explore how to systematically derive sharp identification bounds,\nexploiting both the novel data-combination principles and classical model\nrestrictions. Applying the proposed method, we explore what can be learned\nabout the long-run effects of job training programs on employment without\nlong-term experimental data.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.16177v1"
    },
    {
        "title": "The inclusive Synthetic Control Method",
        "authors": [
            "Roberta Di Stefano",
            "Giovanni Mellace"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We introduce the inclusive synthetic control method (iSCM), a modification of\nsynthetic control methods that includes units in the donor pool potentially\naffected, directly or indirectly, by an intervention. This method is ideal for\nsituations where including treated units in the donor pool is essential or\nwhere donor units may experience spillover effects. The iSCM is straightforward\nto implement with most synthetic control estimators. As an empirical\nillustration, we re-estimate the causal effect of German reunification on GDP\nper capita, accounting for spillover effects from West Germany to Austria.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.17624v2"
    },
    {
        "title": "Deconvolution from two order statistics",
        "authors": [
            "JoonHwan Cho",
            "Yao Luo",
            "Ruli Xiao"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Economic data are often contaminated by measurement errors and truncated by\nranking. This paper shows that the classical measurement error model with\nindependent and additive measurement errors is identified nonparametrically\nusing only two order statistics of repeated measurements. The identification\nresult confirms a hypothesis by Athey and Haile (2002) for a symmetric\nascending auction model with unobserved heterogeneity. Extensions allow for\nheterogeneous measurement errors, broadening the applicability to additional\nempirical settings, including asymmetric auctions and wage offer models. We\nadapt an existing simulated sieve estimator and illustrate its performance in\nfinite samples.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.17777v1"
    },
    {
        "title": "Distributional Treatment Effect with Latent Rank Invariance",
        "authors": [
            "Myungkou Shin"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Treatment effect heterogeneity is of a great concern when evaluating the\ntreatment. However, even with a simple case of a binary random treatment, the\ndistribution of treatment effect is difficult to identify due to the\nfundamental limitation that we cannot observe both treated potential outcome\nand untreated potential outcome for a given individual. This paper assumes a\nconditional independence assumption that the two potential outcomes are\nindependent of each other given a scalar latent variable. Using two proxy\nvariables, we identify conditional distribution of the potential outcomes given\nthe latent variable. To pin down the location of the latent variable, we assume\nstrict monotonicty on some functional of the conditional distribution; with\nspecific example of strictly increasing conditional expectation, we label the\nlatent variable as 'latent rank' and motivate the identifying assumption as\n'latent rank invariance.'\n",
        "pdf_link": "http://arxiv.org/pdf/2403.18503v2"
    },
    {
        "title": "Dynamic Correlation of Market Connectivity, Risk Spillover and Abnormal\n  Volatility in Stock Price",
        "authors": [
            "Muzi Chen",
            "Nan Li",
            "Lifen Zheng",
            "Difang Huang",
            "Boyao Wu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The connectivity of stock markets reflects the information efficiency of\ncapital markets and contributes to interior risk contagion and spillover\neffects. We compare Shanghai Stock Exchange A-shares (SSE A-shares) during\ntranquil periods, with high leverage periods associated with the 2015 subprime\nmortgage crisis. We use Pearson correlations of returns, the maximum strongly\nconnected subgraph, and $3\\sigma$ principle to iteratively determine the\nthreshold value for building a dynamic correlation network of SSE A-shares.\nAnalyses are carried out based on the networking structure, intra-sector\nconnectivity, and node status, identifying several contributions. First,\ncompared with tranquil periods, the SSE A-shares network experiences a more\nsignificant small-world and connective effect during the subprime mortgage\ncrisis and the high leverage period in 2015. Second, the finance, energy and\nutilities sectors have a stronger intra-industry connectivity than other\nsectors. Third, HUB nodes drive the growth of the SSE A-shares market during\nbull periods, while stocks have a think-tail degree distribution in bear\nperiods and show distinct characteristics in terms of market value and finance.\nGranger linear and non-linear causality networks are also considered for the\ncomparison purpose. Studies on the evolution of inter-cycle connectivity in the\nSSE A-share market may help investors improve portfolios and develop more\nrobust risk management policies.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.19363v1"
    },
    {
        "title": "Dynamic Analyses of Contagion Risk and Module Evolution on the SSE\n  A-Shares Market Based on Minimum Information Entropy",
        "authors": [
            "Muzi Chen",
            "Yuhang Wang",
            "Boyao Wu",
            "Difang Huang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The interactive effect is significant in the Chinese stock market,\nexacerbating the abnormal market volatilities and risk contagion. Based on\ndaily stock returns in the Shanghai Stock Exchange (SSE) A-shares, this paper\ndivides the period between 2005 and 2018 into eight bull and bear market stages\nto investigate interactive patterns in the Chinese financial market. We employ\nthe LASSO method to construct the stock network and further use the Map\nEquation method to analyze the evolution of modules in the SSE A-shares market.\nEmpirical results show: (1) The connected effect is more significant in bear\nmarkets than bull markets; (2) A system module can be found in the network\nduring the first four stages, and the industry aggregation effect leads to\nmodule differentiation in the last four stages; (3) Some stocks have leading\neffects on others throughout eight periods, and medium- and small-cap stocks\nwith poor financial conditions are more likely to become risk sources,\nespecially in bear markets. Our conclusions are beneficial to improving\ninvestment strategies and making regulatory policies.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.19439v1"
    },
    {
        "title": "Sequential Synthetic Difference in Differences",
        "authors": [
            "Dmitry Arkhangelsky",
            "Aleksei Samkov"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We study the estimation of treatment effects of a binary policy in\nenvironments with a staggered treatment rollout. We propose a new estimator --\nSequential Synthetic Difference in Difference (Sequential SDiD) -- and\nestablish its theoretical properties in a linear model with interactive fixed\neffects. Our estimator is based on sequentially applying the original SDiD\nestimator proposed in Arkhangelsky et al. (2021) to appropriately aggregated\ndata. To establish the theoretical properties of our method, we compare it to\nan infeasible OLS estimator based on the knowledge of the subspaces spanned by\nthe interactive fixed effects. We show that this OLS estimator has a sequential\nrepresentation and use this result to show that it is asymptotically equivalent\nto the Sequential SDiD estimator. This result implies the asymptotic normality\nof our estimator along with corresponding efficiency guarantees. The method\ndeveloped in this paper presents a natural alternative to the conventional DiD\nstrategies in staggered adoption designs.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.00164v1"
    },
    {
        "title": "Estimating Heterogeneous Effects: Applications to Labor Economics",
        "authors": [
            "Stephane Bonhomme",
            "Angela Denis"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  A growing number of applications involve settings where, in order to infer\nheterogeneous effects, a researcher compares various units. Examples of\nresearch designs include children moving between different neighborhoods,\nworkers moving between firms, patients migrating from one city to another, and\nbanks offering loans to different firms. We present a unified framework for\nthese settings, based on a linear model with normal random coefficients and\nnormal errors. Using the model, we discuss how to recover the mean and\ndispersion of effects, other features of their distribution, and to construct\npredictors of the effects. We provide moment conditions on the model's\nparameters, and outline various estimation strategies. A main objective of the\npaper is to clarify some of the underlying assumptions by highlighting their\neconomic content, and to discuss and inform some of the key practical choices.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.01495v1"
    },
    {
        "title": "The impact of geopolitical risk on the international agricultural\n  market: Empirical analysis based on the GJR-GARCH-MIDAS model",
        "authors": [
            "Yun-Shi Dai",
            "Peng-Fei Dai",
            "Wei-Xing Zhou"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The current international landscape is turbulent and unstable, with frequent\noutbreaks of geopolitical conflicts worldwide. Geopolitical risk has emerged as\na significant threat to regional and global peace, stability, and economic\nprosperity, causing serious disruptions to the global food system and food\nsecurity. Focusing on the international food market, this paper builds\ndifferent dimensions of geopolitical risk measures based on the random matrix\ntheory and constructs single- and two-factor GJR-GARCH-MIDAS models with fixed\ntime span and rolling window, respectively, to investigate the impact of\ngeopolitical risk on food market volatility. The findings indicate that\nmodeling based on rolling window performs better in describing the overall\nvolatility of the wheat, maize, soybean, and rice markets, and the two-factor\nmodels generally exhibit stronger explanatory power in most cases. In terms of\nshort-term fluctuations, all four staple food markets demonstrate obvious\nvolatility clustering and high volatility persistence, without significant\nasymmetry. Regarding long-term volatility, the realized volatility of wheat,\nmaize, and soybean significantly exacerbates their long-run market volatility.\nAdditionally, geopolitical risks of different dimensions show varying\ndirections and degrees of effects in explaining the long-term market volatility\nof the four staple food commodities. This study contributes to the\nunderstanding of the macro-drivers of food market fluctuations, provides useful\ninformation for investment using agricultural futures, and offers valuable\ninsights into maintaining the stable operation of food markets and safeguarding\nglobal food security.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.01641v1"
    },
    {
        "title": "Moran's I 2-Stage Lasso: for Models with Spatial Correlation and\n  Endogenous Variables",
        "authors": [
            "Sylvain Barde",
            "Rowan Cherodian",
            "Guy Tchuente"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a novel estimation procedure for models with endogenous variables\nin the presence of spatial correlation based on Eigenvector Spatial Filtering.\nThe procedure, called Moran's $I$ 2-Stage Lasso (Mi-2SL), uses a two-stage\nLasso estimator where the Standardised Moran's I is used to set the Lasso\ntuning parameter. Unlike existing spatial econometric methods, this has the key\nbenefit of not requiring the researcher to explicitly model the spatial\ncorrelation process, which is of interest in cases where they are only\ninterested in removing the resulting bias when estimating the direct effect of\ncovariates. We show the conditions necessary for consistent and asymptotically\nnormal parameter estimation assuming the support (relevant) set of eigenvectors\nis known. Our Monte Carlo simulation results also show that Mi-2SL performs\nwell against common alternatives in the presence of spatial correlation. Our\nempirical application replicates Cadena and Kovak (2016) instrumental variables\nestimates using Mi-2SL and shows that in that case, Mi-2SL can boost the\nperformance of the first stage.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.02584v1"
    },
    {
        "title": "Bayesian Bi-level Sparse Group Regressions for Macroeconomic Density\n  Forecasting",
        "authors": [
            "Matteo Mogliani",
            "Anna Simoni"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a Machine Learning approach for optimal macroeconomic density\nforecasting in a high-dimensional setting where the underlying model exhibits a\nknown group structure. Our approach is general enough to encompass specific\nforecasting models featuring either many covariates, or unknown nonlinearities,\nor series sampled at different frequencies. By relying on the novel concept of\nbi-level sparsity in time-series econometrics, we construct density forecasts\nbased on a prior that induces sparsity both at the group level and within\ngroups. We demonstrate the consistency of both posterior and predictive\ndistributions. We show that the posterior distribution contracts at the\nminimax-optimal rate and, asymptotically, puts mass on a set that includes the\nsupport of the model. Our theory allows for correlation between groups, while\npredictors in the same group can be characterized by strong covariation as well\nas common characteristics and patterns. Finite sample performance is\nillustrated through comprehensive Monte Carlo experiments and a real-data\nnowcasting exercise of the US GDP growth rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.02671v3"
    },
    {
        "title": "Marginal Treatment Effects and Monotonicity",
        "authors": [
            "Henrik Sigstad"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  How robust are analyses based on marginal treatment effects (MTE) to\nviolations of Imbens and Angrist (1994) monotonicity? In this note, I present\nweaker forms of monotonicity under which popular MTE-based estimands still\nidentify the parameters of interest.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.03235v1"
    },
    {
        "title": "Forecasting with Neuro-Dynamic Programming",
        "authors": [
            "Pedro Afonso Fernandes"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Economic forecasting is concerned with the estimation of some variable like\ngross domestic product (GDP) in the next period given a set of variables that\ndescribes the current situation or state of the economy, including industrial\nproduction, retail trade turnover or economic confidence. Neuro-dynamic\nprogramming (NDP) provides tools to deal with forecasting and other sequential\nproblems with such high-dimensional states spaces. Whereas conventional\nforecasting methods penalises the difference (or loss) between predicted and\nactual outcomes, NDP favours the difference between temporally successive\npredictions, following an interactive and trial-and-error approach. Past data\nprovides a guidance to train the models, but in a different way from ordinary\nleast squares (OLS) and other supervised learning methods, signalling the\nadjustment costs between sequential states. We found that it is possible to\ntrain a GDP forecasting model with data concerned with other countries that\nperforms better than models trained with past data from the tested country\n(Portugal). In addition, we found that non-linear architectures to approximate\nthe value function of a sequential problem, namely, neural networks can perform\nbetter than a simple linear architecture, lowering the out-of-sample mean\nabsolute forecast error (MAE) by 32% from an OLS model.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.03737v1"
    },
    {
        "title": "Fast and simple inner-loop algorithms of static / dynamic BLP\n  estimations",
        "authors": [
            "Takeshi Fukasawa"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This study investigates computationally efficient inner-loop algorithms for\nestimating static/dynamic BLP models. It provides the following ideas to reduce\nthe number of inner-loop iterations: (1). Add a term concerning the outside\noption share in the BLP contraction mapping; (2). Analytically represent mean\nproduct utilities as a function of value functions and solve for the value\nfunctions (for dynamic BLP); (3). Combine an acceleration method of fixed point\niterations, especially Anderson acceleration. They are independent and easy to\nimplement. This study shows good performance of these methods by numerical\nexperiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.04494v4"
    },
    {
        "title": "Absolute Technical Efficiency Indices",
        "authors": [
            "Montacer Ben Cheikh Larbi",
            "Sina Belkhiria"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Technical efficiency indices (TEIs) can be estimated using the traditional\nstochastic frontier analysis approach, which yields relative indices that do\nnot allow self-interpretations. In this paper, we introduce a single-step\nestimation procedure for TEIs that eliminates the need to identify best\npractices and avoids imposing restrictive hypotheses on the error term. The\nresulting indices are absolute and allow for individual interpretation. In our\nmodel, we estimate a distance function using the inverse coefficient of\nresource utilization, rather than treating it as unobservable. We employ a\nTobit model with a translog distance function as our econometric framework.\nApplying this model to a sample of 19 airline companies from 2012 to 2021, we\nfind that: (1) Absolute technical efficiency varies considerably between\ncompanies with medium-haul European airlines being technically the most\nefficient, while Asian airlines are the least efficient; (2) Our estimated TEIs\nare consistent with the observed data with a decline in efficiency especially\nduring the Covid-19 crisis and Brexit period; (3) All airlines contained in our\nsample would be able to increase their average technical efficiency by 0.209%\nif they reduced their average kerosene consumption by 1%; (4) Total factor\nproductivity (TFP) growth slowed between 2013 and 2019 due to a decrease in\nDisembodied Technical Change (DTC) and a small effect from Scale Economies\n(SE). Toward the end of our study period, TFP growth seemed increasingly driven\nby the SE effect, with a sharp decline in 2020 followed by an equally sharp\nrecovery in 2021 for most airlines.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.04590v1"
    },
    {
        "title": "Stratifying on Treatment Status",
        "authors": [
            "Jinyong Hahn",
            "John Ham",
            "Geert Ridder",
            "Shuyang Sheng"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We investigate the estimation of treatment effects from a sample that is\nstratified on the binary treatment status. In the case of unconfounded\nassignment where the potential outcomes are independent of the treatment given\ncovariates, we show that standard estimators of the average treatment effect\nare inconsistent. In the case of an endogenous treatment and a binary\ninstrument, we show that the IV estimator is inconsistent for the local average\ntreatment effect. In both cases, we propose simple alternative estimators that\nare consistent in stratified samples, assuming that the fraction treated in the\npopulation is known or can be estimated.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.04700v1"
    },
    {
        "title": "Towards a generalized accessibility measure for transportation equity\n  and efficiency",
        "authors": [
            "Rajat Verma",
            "Mithun Debnath",
            "Shagun Mittal",
            "Satish V. Ukkusuri"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Locational measures of accessibility are widely used in urban and\ntransportation planning to understand the impact of the transportation system\non influencing people's access to places. However, there is a considerable lack\nof measurement standards and publicly available data. We propose a generalized\nmeasure of locational accessibility that has a comprehensible form for\ntransportation planning analysis. This metric combines the cumulative\nopportunities approach with gravity-based measures and is capable of catering\nto multiple trip purposes, travel modes, cost thresholds, and scales of\nanalysis. Using data from multiple publicly available datasets, this metric is\ncomputed by trip purpose and travel time threshold for all block groups in the\nUnited States, and the data is made publicly accessible. Further, case studies\nof three large metropolitan areas reveal substantial inefficiencies in\ntransportation infrastructure, with the most inefficiency observed in sprawling\nand non-core urban areas, especially for bicycling. Subsequently, it is shown\nthat targeted investment in facilities can contribute to a more equitable\ndistribution of accessibility to essential shopping and service facilities. By\nassigning greater weights to socioeconomically disadvantaged neighborhoods, the\nproposed metric formally incorporates equity considerations into transportation\nplanning, contributing to a more equitable distribution of accessibility to\nessential services and facilities.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.04985v1"
    },
    {
        "title": "Context-dependent Causality (the Non-Nonotonic Case)",
        "authors": [
            "Nir Billfeld",
            "Moshe Kim"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We develop a novel identification strategy as well as a new estimator for\ncontext-dependent causal inference in non-parametric triangular models with\nnon-separable disturbances. Departing from the common practice, our analysis\ndoes not rely on the strict monotonicity assumption. Our key contribution lies\nin leveraging on diffusion models to formulate the structural equations as a\nsystem evolving from noise accumulation to account for the influence of the\nlatent context (confounder) on the outcome. Our identifiability strategy\ninvolves a system of Fredholm integral equations expressing the distributional\nrelationship between a latent context variable and a vector of observables.\nThese integral equations involve an unknown kernel and are governed by a set of\nstructural form functions, inducing a non-monotonic inverse problem. We prove\nthat if the kernel density can be represented as an infinite mixture of\nGaussians, then there exists a unique solution for the unknown function. This\nis a significant result, as it shows that it is possible to solve a\nnon-monotonic inverse problem even when the kernel is unknown. On the\nmethodological front we leverage on a novel and enriched Contaminated\nGenerative Adversarial (Neural) Networks (CONGAN) which we provide as a\nsolution to the non-monotonic inverse problem.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.05021v1"
    },
    {
        "title": "Estimating granular house price distributions in the Australian market\n  using Gaussian mixtures",
        "authors": [
            "Willem P Sijp",
            "Anastasios Panagiotelis"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  A new methodology is proposed to approximate the time-dependent house price\ndistribution at a fine regional scale using Gaussian mixtures. The means,\nvariances and weights of the mixture components are related to time, location\nand dwelling type through a non linear function trained by a deep functional\napproximator. Price indices are derived as means, medians, quantiles or other\nfunctions of the estimated distributions. Price densities for larger regions,\nsuch as a city, are calculated via a weighted sum of the component density\nfunctions. The method is applied to a data set covering all of Australia at a\nfine spatial and temporal resolution. In addition to enabling a detailed\nexploration of the data, the proposed index yields lower prediction errors in\nthe practical task of individual dwelling price projection from previous sales\nvalues within the three major Australian cities. The estimated quantiles are\nalso found to be well calibrated empirically, capturing the complexity of house\nprice distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.05178v1"
    },
    {
        "title": "Regression Discontinuity Design with Spillovers",
        "authors": [
            "Eric Auerbach",
            "Yong Cai",
            "Ahnaf Rafi"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Researchers who estimate treatment effects using a regression discontinuity\ndesign (RDD) typically assume that there are no spillovers between the treated\nand control units. This may be unrealistic. We characterize the estimand of RDD\nin a setting where spillovers occur between units that are close in their\nvalues of the running variable. Under the assumption that spillovers are\nlinear-in-means, we show that the estimand depends on the ratio of two terms:\n(1) the radius over which spillovers occur and (2) the choice of bandwidth used\nfor the local linear regression. Specifically, RDD estimates direct treatment\neffect when radius is of larger order than the bandwidth, and total treatment\neffect when radius is of smaller order than the bandwidth. In the more\nrealistic regime where radius is of similar order as the bandwidth, the RDD\nestimand is a mix of the above effects. To recover direct and spillover\neffects, we propose incorporating estimated spillover terms into local linear\nregression -- the local analog of peer effects regression. We also clarify the\nsettings under which the donut-hole RD is able to eliminate the effects of\nspillovers.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.06471v1"
    },
    {
        "title": "Merger Analysis with Latent Price",
        "authors": [
            "Paul S. Koh"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Standard empirical tools for merger analysis assume price data, which may not\nbe readily available. This paper characterizes sufficient conditions for\nidentifying the unilateral effects of mergers without price data. I show that\ndata on merging firms' revenues, margins, and revenue diversion ratios are\nsufficient for identifying the gross upward pricing pressure indices,\ncompensating marginal cost reductions, and first-order welfare effects. Revenue\ndiversion ratios can be identified from consumer expenditure data via the\nHotz-Miller inversion under the standard discrete-continuous demand assumption.\nMerger simulations are also feasible with CES demand if data on all firms'\nmargins and revenues are available. I use the proposed framework to evaluate\nthe Albertsons/Safeway merger (2015) and the Staples/Office Depot merger\n(2016).\n",
        "pdf_link": "http://arxiv.org/pdf/2404.07684v3"
    },
    {
        "title": "Uniform Inference in High-Dimensional Threshold Regression Models",
        "authors": [
            "Jiatong Li",
            "Hongqiang Yan"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We develop uniform inference for high-dimensional threshold regression\nparameters, allowing for either cross-sectional or time series data. We first\nestablish Oracle inequalities for prediction errors and $\\ell_1$ estimation\nerrors for the Lasso estimator of the slope parameters and the threshold\nparameter, accommodating heteroskedastic non-subgaussian error terms and\nnon-subgaussian covariates. Next, we derive the asymptotic distribution of\ntests involving an increasing number of slope parameters by debiasing (or\ndesparsifying) the Lasso estimator in cases with no threshold effect and with a\nfixed threshold effect. We show that the asymptotic distributions in both cases\nare the same, allowing us to perform uniform inference without specifying\nwhether the true model is a linear or threshold regression. Finally, we\ndemonstrate the consistent performance of our estimator in both cases through\nsimulation studies, and we apply the proposed estimator to analyze two\nempirical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.08105v2"
    },
    {
        "title": "Estimation and Inference for Three-Dimensional Panel Data Models",
        "authors": [
            "Guohua Feng",
            "Jiti Gao",
            "Fei Liu",
            "Bin Peng"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Hierarchical panel data models have recently garnered significant attention.\nThis study contributes to the relevant literature by introducing a novel\nthree-dimensional (3D) hierarchical panel data model, which integrates panel\nregression with three sets of latent factor structures: one set of global\nfactors and two sets of local factors. Instead of aggregating latent factors\nfrom various nodes, as seen in the literature of distributed principal\ncomponent analysis (PCA), we propose an estimation approach capable of\nrecovering the parameters of interest and disentangling latent factors at\ndifferent levels and across different dimensions. We establish an asymptotic\ntheory and provide a bootstrap procedure to obtain inference for the parameters\nof interest while accommodating various types of cross-sectional dependence and\ntime series autocorrelation. Finally, we demonstrate the applicability of our\nframework by examining productivity convergence in manufacturing industries\nworldwide.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.08365v2"
    },
    {
        "title": "Identifying Causal Effects under Kink Setting: Theory and Evidence",
        "authors": [
            "Yi Lu",
            "Jianguo Wang",
            "Huihua Xie"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper develops a generalized framework for identifying causal impacts in\na reduced-form manner under kinked settings when agents can manipulate their\nchoices around the threshold. The causal estimation using a bunching framework\nwas initially developed by Diamond and Persson (2017) under notched settings.\nMany empirical applications of bunching designs involve kinked settings. We\npropose a model-free causal estimator in kinked settings with sharp bunching\nand then extend to the scenarios with diffuse bunching, misreporting,\noptimization frictions, and heterogeneity. The estimation method is mostly\nnon-parametric and accounts for the interior response under kinked settings.\nApplying the proposed approach, we estimate how medical subsidies affect\noutpatient behaviors in China.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.09117v1"
    },
    {
        "title": "Julia as a universal platform for statistical software development",
        "authors": [
            "David Roodman"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The julia package integrates the Julia programming language into Stata. Users\ncan transfer data between Stata and Julia, issue Julia commands to analyze and\nplot, and pass results back to Stata. Julia's econometric ecosystem is not as\nmature as Stata's or R's or Python's. But Julia is an excellent environment for\ndeveloping high-performance numerical applications, which can then be called\nfrom many platforms. For example, the boottest program for wild bootstrap-based\ninference (Roodman et al. 2019) and fwildclusterboot for R (Fischer and Roodman\n2021) can both call the same Julia back end. And the program reghdfejl mimics\nreghdfe (Correia 2016) in fitting linear models with high-dimensional fixed\neffects but calls a Julia package for tenfold acceleration on hard problems.\nreghdfejl also supports nonlinear fixed-effect models that cannot otherwise be\nfit in Stata--though preliminarily, as the Julia package for that purpose is\nimmature.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.09309v3"
    },
    {
        "title": "The Role of Carbon Pricing in Food Inflation: Evidence from Canadian\n  Provinces",
        "authors": [
            "Jiansong Xu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In the search for political-economic tools for greenhouse gas mitigation,\ncarbon pricing, which includes carbon tax and cap-and-trade, is implemented by\nmany governments. However, the inflating food prices in carbon-pricing\ncountries, such as Canada, have led many to believe such policies harm food\naffordability. This study aims to identify changes in food prices induced by\ncarbon pricing using the case of Canadian provinces. Using the staggered\ndifference-in-difference (DiD) approach, we find an overall deflationary effect\nof carbon pricing on food prices (measured by monthly provincial food CPI). The\naverage reductions in food CPI compared to before carbon pricing are $2\\%$ and\n$4\\%$ within and beyond two years of implementation. We further find that the\ndeflationary effects are partially driven by lower consumption with no\nsignificant change via farm input costs. Evidence in this paper suggests no\ninflationary effect of carbon pricing in Canadian provinces, thus giving no\nsupport to the growing voices against carbon pricing policies.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.09467v5"
    },
    {
        "title": "From Predictive Algorithms to Automatic Generation of Anomalies",
        "authors": [
            "Sendhil Mullainathan",
            "Ashesh Rambachan"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Machine learning algorithms can find predictive signals that researchers fail\nto notice; yet they are notoriously hard-to-interpret. How can we extract\ntheoretical insights from these black boxes? History provides a clue. Facing a\nsimilar problem -- how to extract theoretical insights from their intuitions --\nresearchers often turned to ``anomalies:'' constructed examples that highlight\nflaws in an existing theory and spur the development of new ones. Canonical\nexamples include the Allais paradox and the Kahneman-Tversky choice experiments\nfor expected utility theory. We suggest anomalies can extract theoretical\ninsights from black box predictive algorithms. We develop procedures to\nautomatically generate anomalies for an existing theory when given a predictive\nalgorithm. We cast anomaly generation as an adversarial game between a theory\nand a falsifier, the solutions to which are anomalies: instances where the\nblack box algorithm predicts - were we to collect data - we would likely\nobserve violations of the theory. As an illustration, we generate anomalies for\nexpected utility theory using a large, publicly available dataset on real\nlottery choices. Based on an estimated neural network that predicts lottery\nchoices, our procedures recover known anomalies and discover new ones for\nexpected utility theory. In incentivized experiments, subjects violate expected\nutility theory on these algorithmically generated anomalies; moreover, the\nviolation rates are similar to observed rates for the Allais paradox and Common\nratio effect.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.10111v1"
    },
    {
        "title": "Forecasting with panel data: Estimation uncertainty versus parameter\n  heterogeneity",
        "authors": [
            "M. Hashem Pesaran",
            "Andreas Pick",
            "Allan Timmermann"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We provide a comprehensive examination of the predictive accuracy of panel\nforecasting methods based on individual, pooling, fixed effects, and Bayesian\nestimation, and propose optimal weights for forecast combination schemes. We\nconsider linear panel data models, allowing for weakly exogenous regressors and\ncorrelated heterogeneity. We quantify the gains from exploiting panel data and\ndemonstrate how forecasting performance depends on the degree of parameter\nheterogeneity, whether such heterogeneity is correlated with the regressors,\nthe goodness of fit of the model, and the cross-sectional ($N$) and time ($T$)\ndimensions. Monte Carlo simulations and empirical applications to house prices\nand CPI inflation show that forecast combination and Bayesian forecasting\nmethods perform best overall and rarely produce the least accurate forecasts\nfor individual series.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.11198v1"
    },
    {
        "title": "Bayesian Markov-Switching Vector Autoregressive Process",
        "authors": [
            "Battulga Gankhuu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This study introduces marginal density functions of the general Bayesian\nMarkov-Switching Vector Autoregressive (MS-VAR) process. In the case of the\nBayesian MS-VAR process, we provide closed-form density functions and\nMonte-Carlo simulation algorithms, including the importance sampling method.\nThe Monte-Carlo simulation method departs from the previous simulation methods\nbecause it removes the duplication in a regime vector.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.11235v3"
    },
    {
        "title": "Weighted-Average Least Squares for Negative Binomial Regression",
        "authors": [
            "Kevin Huynh"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Model averaging methods have become an increasingly popular tool for\nimproving predictions and dealing with model uncertainty, especially in\nBayesian settings. Recently, frequentist model averaging methods such as\ninformation theoretic and least squares model averaging have emerged. This work\nfocuses on the issue of covariate uncertainty where managing the computational\nresources is key: The model space grows exponentially with the number of\ncovariates such that averaged models must often be approximated.\nWeighted-average least squares (WALS), first introduced for (generalized)\nlinear models in the econometric literature, combines Bayesian and frequentist\naspects and additionally employs a semiorthogonal transformation of the\nregressors to reduce the computational burden. This paper extends WALS for\ngeneralized linear models to the negative binomial (NB) regression model for\noverdispersed count data. A simulation experiment and an empirical application\nusing data on doctor visits were conducted to compare the predictive power of\nWALS for NB regression to traditional estimators. The results show that WALS\nfor NB improves on the maximum likelihood estimator in sparse situations and is\ncompetitive with lasso while being computationally more efficient.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.11324v1"
    },
    {
        "title": "Regret Analysis in Threshold Policy Design",
        "authors": [
            "Federico Crippa"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Threshold policies are targeting mechanisms that assign treatments based on\nwhether an observable characteristic exceeds a certain threshold. They are\nwidespread across multiple domains, such as welfare programs, taxation, and\nclinical medicine. This paper addresses the problem of designing threshold\npolicies using experimental data, when the goal is to maximize the population\nwelfare. First, I characterize the regret (a measure of policy optimality) of\nthe Empirical Welfare Maximizer (EWM) policy, popular in the literature. Next,\nI introduce the Smoothed Welfare Maximizer (SWM) policy, which improves the\nEWM's regret convergence rate under an additional smoothness condition. The two\npolicies are compared studying how differently their regrets depend on the\npopulation distribution, and investigating their finite sample performances\nthrough Monte Carlo simulations. In many contexts, the welfare guaranteed by\nthe novel SWM policy is larger than with the EWM. An empirical illustration\ndemonstrates how the treatment recommendation of the two policies may in\npractice notably differ.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.11767v1"
    },
    {
        "title": "Axiomatic modeling of fixed proportion technologies",
        "authors": [
            "Xun Zhou",
            "Timo Kuosmanen"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Understanding input substitution and output transformation possibilities is\ncritical for efficient resource allocation and firm strategy. There are\nimportant examples of fixed proportion technologies where certain inputs are\nnon-substitutable and/or certain outputs are non-transformable. However, there\nis widespread confusion about the appropriate modeling of fixed proportion\ntechnologies in data envelopment analysis. We point out and rectify several\nmisconceptions in the existing literature, and show how fixed proportion\ntechnologies can be correctly incorporated into the axiomatic framework. A\nMonte Carlo study is performed to demonstrate the proposed solution.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.12462v1"
    },
    {
        "title": "Two-step Estimation of Network Formation Models with Unobserved\n  Heterogeneities and Strategic Interactions",
        "authors": [
            "Shaomin Wu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, I characterize the network formation process as a static game\nof incomplete information, where the latent payoff of forming a link between\ntwo individuals depends on the structure of the network, as well as private\ninformation on agents' attributes. I allow agents' private unobserved\nattributes to be correlated with observed attributes through individual fixed\neffects. Using data from a single large network, I propose a two-step estimator\nfor the model primitives. In the first step, I estimate agents' equilibrium\nbeliefs of other people's choice probabilities. In the second step, I plug in\nthe first-step estimator to the conditional choice probability expression and\nestimate the model parameters and the unobserved individual fixed effects\ntogether using Joint MLE. Assuming that the observed attributes are discrete, I\nshowed that the first step estimator is uniformly consistent with rate\n$N^{-1/4}$, where $N$ is the total number of linking proposals. I also show\nthat the second-step estimator converges asymptotically to a normal\ndistribution at the same rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.12581v1"
    },
    {
        "title": "The modified conditional sum-of-squares estimator for fractionally\n  integrated models",
        "authors": [
            "Mustafa R. Kılınç",
            "Michael Massmann"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, we analyse the influence of estimating a constant term on the\nbias of the conditional sum-of-squares (CSS) estimator in a stationary or\nnon-stationary type-II ARFIMA ($p_1$,$d$,$p_2$) model. We derive expressions\nfor the estimator's bias and show that the leading term can be easily removed\nby a simple modification of the CSS objective function. We call this new\nestimator the modified conditional sum-of-squares (MCSS) estimator. We show\ntheoretically and by means of Monte Carlo simulations that its performance\nrelative to that of the CSS estimator is markedly improved even for small\nsample sizes. Finally, we revisit three classical short datasets that have in\nthe past been described by ARFIMA($p_1$,$d$,$p_2$) models with constant term,\nnamely the post-second World War real GNP data, the extended Nelson-Plosser\ndata, and the Nile data.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.12882v1"
    },
    {
        "title": "On the Asymmetric Volatility Connectedness",
        "authors": [
            "Abdulnasser Hatemi-J"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Connectedness measures the degree at which a time-series variable spills over\nvolatility to other variables compared to the rate that it is receiving. The\nidea is based on the percentage of variance decomposition from one variable to\nthe others, which is estimated by making use of a VAR model. Diebold and Yilmaz\n(2012, 2014) suggested estimating this simple and useful measure of percentage\nrisk spillover impact. Their method is symmetric by nature, however. The\ncurrent paper offers an alternative asymmetric approach for measuring the\nvolatility spillover direction, which is based on estimating the asymmetric\nvariance decompositions introduced by Hatemi-J (2011, 2014). This approach\naccounts explicitly for the asymmetric property in the estimations, which\naccords better with reality. An application is provided to capture the\npotential asymmetric volatility spillover impacts between the three largest\nfinancial markets in the world.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.12997v2"
    },
    {
        "title": "How do applied researchers use the Causal Forest? A methodological\n  review of a method",
        "authors": [
            "Patrick Rehill"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This methodological review examines the use of the causal forest method by\napplied researchers across 133 peer-reviewed papers. It shows that the emerging\nbest practice relies heavily on the approach and tools created by the original\nauthors of the causal forest such as their grf package and the approaches given\nby them in examples. Generally researchers use the causal forest on a\nrelatively low-dimensional dataset relying on observed controls or in some\ncases experiments to identify effects. There are several common ways to then\ncommunicate results -- by mapping out the univariate distribution of\nindividual-level treatment effect estimates, displaying variable importance\nresults for the forest and graphing the distribution of treatment effects\nacross covariates that are important either for theoretical reasons or because\nthey have high variable importance. Some deviations from this common practice\nare interesting and deserve further development and use. Others are unnecessary\nor even harmful. The paper concludes by reflecting on the emerging best\npractice for causal forest use and paths for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.13356v2"
    },
    {
        "title": "Identification and Estimation of Nonseparable Triangular Equations with\n  Mismeasured Instruments",
        "authors": [
            "Shaomin Wu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, I study the nonparametric identification and estimation of the\nmarginal effect of an endogenous variable $X$ on the outcome variable $Y$,\ngiven a potentially mismeasured instrument variable $W^*$, without assuming\nlinearity or separability of the functions governing the relationship between\nobservables and unobservables. To address the challenges arising from the\nco-existence of measurement error and nonseparability, I first employ the\ndeconvolution technique from the measurement error literature to identify the\njoint distribution of $Y, X, W^*$ using two error-laden measurements of $W^*$.\nI then recover the structural derivative of the function of interest and the\n\"Local Average Response\" (LAR) from the joint distribution via the \"unobserved\ninstrument\" approach in Matzkin (2016). I also propose nonparametric estimators\nfor these parameters and derive their uniform rates of convergence. Monte Carlo\nexercises show evidence that the estimators I propose have good finite sample\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.13735v1"
    },
    {
        "title": "A joint test of unconfoundedness and common trends",
        "authors": [
            "Martin Huber",
            "Eva-Maria Oeß"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper introduces an overidentification test of two alternative\nassumptions to identify the average treatment effect on the treated in a\ntwo-period panel data setting: unconfoundedness and common trends. Under the\nunconfoundedness assumption, treatment assignment and post-treatment outcomes\nare independent, conditional on control variables and pre-treatment outcomes,\nwhich motivates including pre-treatment outcomes in the set of controls.\nConversely, under the common trends assumption, the trend and the treatment\nassignment are independent, conditional on control variables. This motivates\nemploying a Difference-in-Differences (DiD) approach by comparing the\ndifferences between pre- and post-treatment outcomes of the treatment and\ncontrol group. Given the non-nested nature of these assumptions and their often\nambiguous plausibility in empirical settings, we propose a joint test using a\ndoubly robust statistic that can be combined with machine learning to control\nfor observed confounders in a data-driven manner. We discuss various causal\nmodels that imply the satisfaction of either common trends, unconfoundedness,\nor both assumptions jointly, and we investigate the finite sample properties of\nour test through a simulation study. Additionally, we apply the proposed method\nto five empirical examples using publicly available datasets and find the test\nto reject the null hypothesis in two cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.16961v3"
    },
    {
        "title": "Overidentification in Shift-Share Designs",
        "authors": [
            "Jinyong Hahn",
            "Guido Kuersteiner",
            "Andres Santos",
            "Wavid Willigrod"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper studies the testability of identifying restrictions commonly\nemployed to assign a causal interpretation to two stage least squares (TSLS)\nestimators based on Bartik instruments. For homogeneous effects models applied\nto short panels, our analysis yields testable implications previously noted in\nthe literature for the two major available identification strategies. We\npropose overidentification tests for these restrictions that remain valid in\nhigh dimensional regimes and are robust to heteroskedasticity and clustering.\nWe further show that homogeneous effect models in short panels, and their\ncorresponding overidentification tests, are of central importance by\nestablishing that: (i) In heterogenous effects models, interpreting TSLS as a\npositively weighted average of treatment effects can impose implausible\nassumptions on the distribution of the data; and (ii) Alternative identifying\nstrategies relying on long panels can prove uninformative in short panel\napplications. We highlight the empirical relevance of our results by examining\nthe viability of Bartik instruments for identifying the effect of rising\nChinese import competition on US local labor markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.17049v1"
    },
    {
        "title": "A Survey Selection Correction using Nonrandom Followup with an\n  Application to the Gender Entrepreneurship Gap",
        "authors": [
            "Clint Harris",
            "Jon Eckhardt",
            "Brent Goldfarb"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Selection into samples undermines efforts to describe populations and to\nestimate relationships between variables. We develop a simple method for\ncorrecting for sample selection that explains differences in survey responses\nbetween early and late respondents with correlation between potential responses\nand preference for survey response. Our method relies on researchers observing\nthe number of data collection attempts prior to each individual's survey\nresponse rather than covariates that affect response rates without affecting\npotential responses. Applying our method to a survey of entrepreneurial\naspirations among undergraduates at University of Wisconsin-Madison, we find\nsuggestive evidence that the entrepreneurial aspiration rate is larger among\nsurvey respondents than the population, as well as the male-female gender gap\nin the entrepreneurial aspiration rate, which we estimate as 21 percentage\npoints in the sample and 19 percentage points in the population. Our results\nsuggest that the male-female gap in entrepreneurial aspirations arises prior to\ndirect exposure to the labor market.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.17693v1"
    },
    {
        "title": "Testing for Asymmetric Information in Insurance with Deep Learning",
        "authors": [
            "Serguei Maliar",
            "Bernard Salanie"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The positive correlation test for asymmetric information developed by\nChiappori and Salanie (2000) has been applied in many insurance markets. Most\nof the literature focuses on the special case of constant correlation; it also\nrelies on restrictive parametric specifications for the choice of coverage and\nthe occurrence of claims. We relax these restrictions by estimating conditional\ncovariances and correlations using deep learning methods. We test the positive\ncorrelation property by using the intersection test of Chernozhukov, Lee, and\nRosen (2013) and the \"sorted groups\" test of Chernozhukov, Demirer, Duflo, and\nFernandez-Val (2023). Our results confirm earlier findings that the correlation\nbetween risk and coverage is small. Random forests and gradient boosting trees\nproduce similar results to neural networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.18207v1"
    },
    {
        "title": "Optimal Treatment Allocation under Constraints",
        "authors": [
            "Torben S. D. Johansen"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In optimal policy problems where treatment effects vary at the individual\nlevel, optimally allocating treatments to recipients is complex even when\npotential outcomes are known. We present an algorithm for multi-arm treatment\nallocation problems that is guaranteed to find the optimal allocation in\nstrongly polynomial time, and which is able to handle arbitrary potential\noutcomes as well as constraints on treatment requirement and capacity. Further,\nstarting from an arbitrary allocation, we show how to optimally re-allocate\ntreatments in a Pareto-improving manner. To showcase our results, we use data\nfrom Danish nurse home visiting for infants. We estimate nurse specific\ntreatment effects for children born 1959-1967 in Copenhagen, comparing nurses\nagainst each other. We exploit random assignment of newborn children to nurses\nwithin a district to obtain causal estimates of nurse-specific treatment\neffects using causal machine learning. Using these estimates, and treating the\nDanish nurse home visiting program as a case of an optimal treatment allocation\nproblem (where a treatment is a nurse), we document room for significant\nproductivity improvements by optimally re-allocating nurses to children. Our\nestimates suggest that optimal allocation of nurses to children could have\nimproved average yearly earnings by USD 1,815 and length of education by around\ntwo months.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.18268v1"
    },
    {
        "title": "A Locally Robust Semiparametric Approach to Examiner IV Designs",
        "authors": [
            "Lonjezo Sithole"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  I propose a locally robust semiparametric framework for estimating causal\neffects using the popular examiner IV design, in the presence of many examiners\nand possibly many covariates relative to the sample size. The key ingredient of\nthis approach is an orthogonal moment function that is robust to biases and\nlocal misspecification from the first step estimation of the examiner IV. I\nderive the orthogonal moment function and show that it delivers multiple\nrobustness where the outcome model or at least one of the first step components\nis misspecified but the estimating equation remains valid. The proposed\nframework not only allows for estimation of the examiner IV in the presence of\nmany examiners and many covariates relative to sample size, using a wide range\nof nonparametric and machine learning techniques including LASSO, Dantzig,\nneural networks and random forests, but also delivers root-n consistent\nestimation of the parameter of interest under mild assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.19144v1"
    },
    {
        "title": "Asymptotic Properties of the Distributional Synthetic Controls",
        "authors": [
            "Lu Zhang",
            "Xiaomeng Zhang",
            "Xinyu Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  As an alternative to synthetic control, the distributional Synthetic Control\n(DSC) proposed by Gunsilius (2023) provides estimates for quantile treatment\neffect and thus enabling researchers to comprehensively understand the impact\nof interventions in causal inference. But the asymptotic properties of DSC have\nnot been built. In this paper, we first establish the DSC estimator's\nasymptotic optimality in the essence that the treatment effect estimator given\nby DSC achieves the lowest possible squared prediction error among all\npotential estimators from averaging quantiles of control units. We then\nestablish the convergence rate of the DSC weights. A significant aspect of our\nresearch is that we find the DSC synthesis forms an optimal weighted average,\nparticularly in situations where it is impractical to perfectly fit the treated\nunit's quantiles through the weighted average of the control units' quantiles.\nSimulation results verify our theoretical insights.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.00953v2"
    },
    {
        "title": "Synthetic Controls with spillover effects: A comparative study",
        "authors": [
            "Andrii Melnychuk"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Iterative Synthetic Control Method is introduced in this study, a\nmodification of the Synthetic Control Method (SCM) designed to improve its\npredictive performance by utilizing control units affected by the treatment in\nquestion. This method is then compared to other SCM modifications: SCM without\nany modifications, SCM after removing all spillover-affected units, Inclusive\nSCM, and the SP SCM model. For the comparison, Monte Carlo simulations are\nutilized, generating artificial datasets with known counterfactuals and\ncomparing the predictive performance of the methods. Generally, the Inclusive\nSCM performed best in all settings and is relatively simple to implement. The\nIterative SCM, introduced in this paper, was in close seconds, with a small\ndifference in performance and a simpler implementation.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.01645v1"
    },
    {
        "title": "Unleashing the Power of AI: Transforming Marketing Decision-Making in\n  Heavy Machinery with Machine Learning, Radar Chart Simulation, and Markov\n  Chain Analysis",
        "authors": [
            "Tian Tian",
            "Jiahao Deng"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This pioneering research introduces a novel approach for decision-makers in\nthe heavy machinery industry, specifically focusing on production management.\nThe study integrates machine learning techniques like Ridge Regression, Markov\nchain analysis, and radar charts to optimize North American Crawler Cranes\nmarket production processes. Ridge Regression enables growth pattern\nidentification and performance assessment, facilitating comparisons and\naddressing industry challenges. Markov chain analysis evaluates risk factors,\naiding in informed decision-making and risk management. Radar charts simulate\nbenchmark product designs, enabling data-driven decisions for production\noptimization. This interdisciplinary approach equips decision-makers with\ntransformative insights, enhancing competitiveness in the heavy machinery\nindustry and beyond. By leveraging these techniques, companies can\nrevolutionize their production management strategies, driving success in\ndiverse markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.01913v1"
    },
    {
        "title": "Identifying and exploiting alpha in linear asset pricing models with\n  strong, semi-strong, and latent factors",
        "authors": [
            "M. Hashem Pesaran",
            "Ron P. Smith"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The risk premia of traded factors are the sum of factor means and a parameter\nvector we denote by {\\phi} which is identified from the cross section\nregression of alpha of individual securities on the vector of factor loadings.\nIf phi is non-zero one can construct \"phi-portfolios\" which exploit the\nsystematic components of non-zero alpha. We show that for known values of betas\nand when phi is non-zero there exist phi-portfolios that dominate mean-variance\nportfolios. The paper then proposes a two-step bias corrected estimator of phi\nand derives its asymptotic distribution allowing for idiosyncratic pricing\nerrors, weak missing factors, and weak error cross-sectional dependence. Small\nsample results from extensive Monte Carlo experiments show that the proposed\nestimator has the correct size with good power properties. The paper also\nprovides an empirical application to a large number of U.S. securities with\nrisk factors selected from a large number of potential risk factors according\nto their strength and constructs phi-portfolios and compares their Sharpe\nratios to mean variance and S&P 500 portfolio.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.02217v4"
    },
    {
        "title": "A quantile-based nonadditive fixed effects model",
        "authors": [
            "Xin Liu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  I propose a quantile-based nonadditive fixed effects panel model to study\nheterogeneous causal effects. Similar to standard fixed effects (FE) model, my\nmodel allows arbitrary dependence between regressors and unobserved\nheterogeneity, but it generalizes the additive separability of standard FE to\nallow the unobserved heterogeneity to enter nonseparably. Similar to structural\nquantile models, my model's random coefficient vector depends on an unobserved,\nscalar ''rank'' variable, in which outcomes (excluding an additive noise term)\nare monotonic at a particular value of the regressor vector, which is much\nweaker than the conventional monotonicity assumption that must hold at all\npossible values. This rank is assumed to be stable over time, which is often\nmore economically plausible than the panel quantile studies that assume\nindividual rank is iid over time. It uncovers the heterogeneous causal effects\nas functions of the rank variable. I provide identification and estimation\nresults, establishing uniform consistency and uniform asymptotic normality of\nthe heterogeneous causal effect function estimator. Simulations show reasonable\nfinite-sample performance and show my model complements fixed effects quantile\nregression. Finally, I illustrate the proposed methods by examining the causal\neffect of a country's oil wealth on its military defense spending.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.03826v1"
    },
    {
        "title": "Detailed Gender Wage Gap Decompositions: Controlling for Worker\n  Unobserved Heterogeneity Using Network Theory",
        "authors": [
            "Jamie Fogel",
            "Bernardo Modenesi"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Recent advances in the literature of decomposition methods in economics have\nallowed for the identification and estimation of detailed wage gap\ndecompositions. In this context, building reliable counterfactuals requires\nusing tighter controls to ensure that similar workers are correctly identified\nby making sure that important unobserved variables such as skills are\ncontrolled for, as well as comparing only workers with similar observable\ncharacteristics. This paper contributes to the wage decomposition literature in\ntwo main ways: (i) developing an economic principled network based approach to\ncontrol for unobserved worker skills heterogeneity in the presence of potential\ndiscrimination; and (ii) extending existing generic decomposition tools to\naccommodate for potential lack of overlapping supports in covariates between\ngroups being compared, which is likely to be the norm in more detailed\ndecompositions. We illustrate the methodology by decomposing the gender wage\ngap in Brazil.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.04365v1"
    },
    {
        "title": "Two-way Fixed Effects and Differences-in-Differences Estimators in\n  Heterogeneous Adoption Designs",
        "authors": [
            "Clément de Chaisemartin",
            "Diego Ciccia Xavier D'Haultfœuille",
            "Felix Knau"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We consider treatment-effect estimation under a parallel trends assumption,\nin designs where no unit is treated at period one, all units receive a strictly\npositive dose at period two, and the dose varies across units. There are\ntherefore no true control groups in such cases. First, we develop a test of the\nassumption that the treatment effect is mean independent of the treatment,\nunder which the commonly-used two-way-fixed-effects estimator is consistent.\nWhen this test is rejected or lacks power, we propose alternative estimators,\nrobust to heterogeneous effects. If there are units with a period-two treatment\narbitrarily close to zero, the robust estimator is a difference-in-difference\nusing units with a period-two treatment below a bandwidth as controls. Without\nsuch units, we propose non-parametric bounds, and an estimator relying on a\nparametric specification of treatment-effect heterogeneity. We use our results\nto revisit Pierce and Schott (2016) and Enikolopov et al. (2011).\n",
        "pdf_link": "http://arxiv.org/pdf/2405.04465v4"
    },
    {
        "title": "SVARs with breaks: Identification and inference",
        "authors": [
            "Emanuele Bacchiocchi",
            "Toru Kitagawa"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper we propose a class of structural vector autoregressions (SVARs)\ncharacterized by structural breaks (SVAR-WB). Together with standard\nrestrictions on the parameters and on functions of them, we also consider\nconstraints across the different regimes. Such constraints can be either (a) in\nthe form of stability restrictions, indicating that not all the parameters or\nimpulse responses are subject to structural changes, or (b) in terms of\ninequalities regarding particular characteristics of the SVAR-WB across the\nregimes. We show that all these kinds of restrictions provide benefits in terms\nof identification. We derive conditions for point and set identification of the\nstructural parameters of the SVAR-WB, mixing equality, sign, rank and stability\nrestrictions, as well as constraints on forecast error variances (FEVs). As\npoint identification, when achieved, holds locally but not globally, there will\nbe a set of isolated structural parameters that are observationally equivalent\nin the parametric space. In this respect, both common frequentist and Bayesian\napproaches produce unreliable inference as the former focuses on just one of\nthese observationally equivalent points, while for the latter on a\nnon-vanishing sensitivity to the prior. To overcome these issues, we propose\nalternative approaches for estimation and inference that account for all\nadmissible observationally equivalent structural parameters. Moreover, we\ndevelop a pure Bayesian and a robust Bayesian approach for doing inference in\nset-identified SVAR-WBs. Both the theory of identification and inference are\nillustrated through a set of examples and an empirical application on the\ntransmission of US monetary policy over the great inflation and great\nmoderation regimes.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.04973v1"
    },
    {
        "title": "Causal Duration Analysis with Diff-in-Diff",
        "authors": [
            "Ben Deaner",
            "Hyejin Ku"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In economic program evaluation, it is common to obtain panel data in which\noutcomes are indicators that an individual has reached an absorbing state. For\nexample, they may indicate whether an individual has exited a period of\nunemployment, passed an exam, left a marriage, or had their parole revoked. The\nparallel trends assumption that underpins difference-in-differences generally\nfails in such settings. We suggest identifying conditions that are analogous to\nthose of difference-in-differences but apply to hazard rates rather than mean\noutcomes. These alternative assumptions motivate estimators that retain the\nsimplicity and transparency of standard diff-in-diff, and we suggest analogous\nspecification tests. Our approach can be adapted to general linear restrictions\nbetween the hazard rates of different groups, motivating duration analogues of\nthe triple differences and synthetic control methods. We apply our procedures\nto examine the impact of a policy that increased the generosity of unemployment\nbenefits, using a cross-cohort comparison.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.05220v1"
    },
    {
        "title": "Sequential Validation of Treatment Heterogeneity",
        "authors": [
            "Stefan Wager"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We use the martingale construction of Luedtke and van der Laan (2016) to\ndevelop tests for the presence of treatment heterogeneity. The resulting\nsequential validation approach can be instantiated using various validation\nmetrics, such as BLPs, GATES, QINI curves, etc., and provides an alternative to\ncross-validation-like cross-fold application of these metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.05534v1"
    },
    {
        "title": "A Sharp Test for the Judge Leniency Design",
        "authors": [
            "Mohamed Coulibaly",
            "Yu-Chin Hsu",
            "Ismael Mourifié",
            "Yuanyuan Wan"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a new specification test to assess the validity of the judge\nleniency design. We characterize a set of sharp testable implications, which\nexploit all the relevant information in the observed data distribution to\ndetect violations of the judge leniency design assumptions. The proposed sharp\ntest is asymptotically valid and consistent and will not make discordant\nrecommendations. When the judge's leniency design assumptions are rejected, we\npropose a way to salvage the model using partial monotonicity and exclusion\nassumptions, under which a variant of the Local Instrumental Variable (LIV)\nestimand can recover the Marginal Treatment Effect. Simulation studies show our\ntest outperforms existing non-sharp tests by significant margins. We apply our\ntest to assess the validity of the judge leniency design using data from\nStevenson (2018), and it rejects the validity for three crime categories:\nrobbery, drug selling, and drug possession.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.06156v1"
    },
    {
        "title": "Identifying Peer Effects in Networks with Unobserved Effort and Isolated\n  Students",
        "authors": [
            "Aristide Houndetoungan",
            "Cristelle Kouame",
            "Michael Vlassopoulos"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Peer influence on effort devoted to some activity is often studied using\nproxy variables when actual effort is unobserved. For instance, in education,\nacademic effort is often proxied by GPA. We propose an alternative approach\nthat circumvents this approximation. Our framework distinguishes unobserved\nshocks to GPA that do not affect effort from preference shocks that do affect\neffort levels. We show that peer effects estimates obtained using our approach\ncan differ significantly from classical estimates (where effort is\napproximated) if the network includes isolated students. Applying our approach\nto data on high school students in the United States, we find that peer effect\nestimates relying on GPA as a proxy for effort are 40% lower than those\nobtained using our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.06850v1"
    },
    {
        "title": "On the Ollivier-Ricci curvature as fragility indicator of the stock\n  markets",
        "authors": [
            "Joaquín Sánchez García",
            "Sebastian Gherghe"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Recently, an indicator for stock market fragility and crash size in terms of\nthe Ollivier-Ricci curvature has been proposed. We study analytical and\nempirical properties of such indicator, test its elasticity with respect to\ndifferent parameters and provide heuristics for the parameters involved. We\nshow when and how the indicator accurately describes a financial crisis. We\nalso propose an alternate method for calculating the indicator using a specific\nsub-graph with special curvature properties.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.07134v1"
    },
    {
        "title": "Robust Inference for High-Dimensional Panel Data Models",
        "authors": [
            "Jiti Gao",
            "Bin Peng",
            "Yayi Yan"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, we propose a robust estimation and inferential method for\nhigh-dimensional panel data models. Specifically, (1) we investigate the case\nwhere the number of regressors can grow faster than the sample size, (2) we pay\nparticular attention to non-Gaussian, serially and cross-sectionally correlated\nand heteroskedastic error processes, and (3) we develop an estimation method\nfor high-dimensional long-run covariance matrix using a thresholded estimator.\n  Methodologically and technically, we develop two Nagaev-types of\nconcentration inequalities: one for a partial sum and the other for a quadratic\nform, subject to a set of easily verifiable conditions. Leveraging these two\ninequalities, we also derive a non-asymptotic bound for the LASSO estimator,\nachieve asymptotic normality via the node-wise LASSO regression, and establish\na sharp convergence rate for the thresholded heteroskedasticity and\nautocorrelation consistent (HAC) estimator.\n  Our study thus provides the relevant literature with a complete toolkit for\nconducting inference about the parameters of interest involved in a\nhigh-dimensional panel data framework. We also demonstrate the practical\nrelevance of these theoretical results by investigating a high-dimensional\npanel data model with interactive fixed effects. Moreover, we conduct extensive\nnumerical studies using simulated and real data examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.07420v2"
    },
    {
        "title": "Random Utility Models with Skewed Random Components: the Smallest versus\n  Largest Extreme Value Distribution",
        "authors": [
            "Richard T. Carson",
            "Derrick H. Sun",
            "Yixiao Sun"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  At the core of most random utility models (RUMs) is an individual agent with\na random utility component following a largest extreme value Type I (LEVI)\ndistribution. What if, instead, the random component follows its mirror image\n-- the smallest extreme value Type I (SEVI) distribution? Differences between\nthese specifications, closely tied to the random component's skewness, can be\nquite profound. For the same preference parameters, the two RUMs, equivalent\nwith only two choice alternatives, diverge progressively as the number of\nalternatives increases, resulting in substantially different estimates and\npredictions for key measures, such as elasticities and market shares.\n  The LEVI model imposes the well-known independence-of-irrelevant-alternatives\nproperty, while SEVI does not. Instead, the SEVI choice probability for a\nparticular option involves enumerating all subsets that contain this option.\nThe SEVI model, though more complex to estimate, is shown to have\ncomputationally tractable closed-form choice probabilities. Much of the paper\ndelves into explicating the properties of the SEVI model and exploring\nimplications of the random component's skewness.\n  Conceptually, the difference between the LEVI and SEVI models centers on\nwhether information, known only to the agent, is more likely to increase or\ndecrease the systematic utility parameterized using observed attributes. LEVI\ndoes the former; SEVI the latter. An immediate implication is that if choice is\ncharacterized by SEVI random components, then the observed choice is more\nlikely to correspond to the systematic-utility-maximizing choice than if\ncharacterized by LEVI. Examining standard empirical examples from different\napplied areas, we find that the SEVI model outperforms the LEVI model,\nsuggesting the relevance of its inclusion in applied researchers' toolkits.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.08222v2"
    },
    {
        "title": "Latent group structure in linear panel data models with endogenous\n  regressors",
        "authors": [
            "Junho Choi",
            "Ryo Okui"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper concerns the estimation of linear panel data models with\nendogenous regressors and a latent group structure in the coefficients. We\nconsider instrumental variables estimation of the group-specific coefficient\nvector. We show that direct application of the Kmeans algorithm to the\ngeneralized method of moments objective function does not yield unique\nestimates. We newly develop and theoretically justify two-stage estimation\nmethods that apply the Kmeans algorithm to a regression of the dependent\nvariable on predicted values of the endogenous regressors. The results of Monte\nCarlo simulations demonstrate that two-stage estimation with the first stage\nmodeled using a latent group structure achieves good classification accuracy,\neven if the true first-stage regression is fully heterogeneous. We apply our\nestimation methods to revisiting the relationship between income and democracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.08687v1"
    },
    {
        "title": "Double Robustness of Local Projections and Some Unpleasant VARithmetic",
        "authors": [
            "José Luis Montiel Olea",
            "Mikkel Plagborg-Møller",
            "Eric Qian",
            "Christian K. Wolf"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We consider impulse response inference in a locally misspecified vector\nautoregression (VAR) model. The conventional local projection (LP) confidence\ninterval has correct coverage even when the misspecification is so large that\nit can be detected with probability approaching 1. This result follows from a\n\"double robustness\" property analogous to that of popular partially linear\nregression estimators. In contrast, the conventional VAR confidence interval\nwith short-to-moderate lag length can severely undercover, even for\nmisspecification that is small, economically plausible, and difficult to detect\nstatistically. There is no free lunch: the VAR confidence interval has robust\ncoverage only if the lag length is so large that the interval is as wide as the\nLP interval.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.09509v2"
    },
    {
        "title": "Comprehensive Causal Machine Learning",
        "authors": [
            "Michael Lechner",
            "Jana Mareckova"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Uncovering causal effects at various levels of granularity provides\nsubstantial value to decision makers. Comprehensive machine learning approaches\nto causal effect estimation allow to use a single causal machine learning\napproach for estimation and inference of causal mean effects for all levels of\ngranularity. Focusing on selection-on-observables, this paper compares three\nsuch approaches, the modified causal forest (mcf), the generalized random\nforest (grf), and double machine learning (dml). It also provides proven\ntheoretical guarantees for the mcf and compares the theoretical properties of\nthe approaches. The findings indicate that dml-based methods excel for average\ntreatment effects at the population level (ATE) and group level (GATE) with few\ngroups, when selection into treatment is not too strong. However, for finer\ncausal heterogeneity, explicitly outcome-centred forest-based approaches are\nsuperior. The mcf has three additional benefits: (i) It is the most robust\nestimator in cases when dml-based approaches underperform because of\nsubstantial selectivity; (ii) it is the best estimator for GATEs when the\nnumber of groups gets larger; and (iii), it is the only estimator that is\ninternally consistent, in the sense that low-dimensional causal ATEs and GATEs\nare obtained as aggregates of finer-grained causal parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.10198v1"
    },
    {
        "title": "Macroeconomic Factors, Industrial Indexes and Bank Spread in Brazil",
        "authors": [
            "Carlos Alberto Durigan Junior",
            "André Taue Saito",
            "Daniel Reed Bergmann",
            "Nuno Manoel Martins Dias Fouto"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The main objective of this paper is to Identify which macroe conomic factors\nand industrial indexes influenced the total Brazilian banking spread between\nMarch 2011 and March 2015. This paper considers subclassification of industrial\nactivities in Brazil. Monthly time series data were used in multivariate linear\nregression models using Eviews (7.0). Eighteen variables were considered as\ncandidates to be determinants. Variables which positively influenced bank\nspread are; Default, IPIs (Industrial Production Indexes) for capital goods,\nintermediate goods, du rable consumer goods, semi-durable and non-durable\ngoods, the Selic, GDP, unemployment rate and EMBI +. Variables which influence\nnegatively are; Consumer and general consumer goods IPIs, IPCA, the balance of\nthe loan portfolio and the retail sales index. A p-value of 05% was considered.\nThe main conclusion of this work is that the progress of industry, job creation\nand consumption can reduce bank spread. Keywords: Credit. Bank spread.\nMacroeconomics. Industrial Production Indexes. Finance.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.10655v1"
    },
    {
        "title": "Testing Sign Congruence Between Two Parameters",
        "authors": [
            "Douglas L. Miller",
            "Francesca Molinari",
            "Jörg Stoye"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We test the null hypothesis that two parameters $(\\mu_1,\\mu_2)$ have the same\nsign, assuming that (asymptotically) normal estimators\n$(\\hat{\\mu}_1,\\hat{\\mu}_2)$ are available. Examples of this problem include the\nanalysis of heterogeneous treatment effects, causal interpretation of\nreduced-form estimands, meta-studies, and mediation analysis. A number of tests\nwere recently proposed. We recommend a test that is simple and rejects more\noften than many of these recent proposals. Like all other tests in the\nliterature, it is conservative if the truth is near $(0,0)$ and therefore also\nbiased. To clarify whether these features are avoidable, we also provide a test\nthat is unbiased and has exact size control on the boundary of the null\nhypothesis, but which has counterintuitive properties and hence we do not\nrecommend. We use the test to improve p-values in Kowlaksi (2022) from\ninformation contained in that paper's main text and to establish statistical\nsignificance of some key estimates in Dippel et al. (2021).\n",
        "pdf_link": "http://arxiv.org/pdf/2405.11759v3"
    },
    {
        "title": "Instrumented Difference-in-Differences with Heterogeneous Treatment\n  Effects",
        "authors": [
            "Sho Miyaji"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Many studies exploit variation in policy adoption timing across units as an\ninstrument for treatment. This paper formalizes the underlying identification\nstrategy as an instrumented difference-in-differences (DID-IV). In this design,\na Wald-DID estimand, which scales the DID estimand of the outcome by the DID\nestimand of the treatment, captures the local average treatment effect on the\ntreated (LATET). We extend the canonical DID-IV design to multiple period\nsettings with the staggered adoption of the instrument across units. Moreover,\nwe propose a credible estimation method in this design that is robust to\ntreatment effect heterogeneity. We illustrate the empirical relevance of our\nfindings, estimating returns to schooling in the United Kingdom. In this\napplication, the two-way fixed effects instrumental variable regression, the\nconventional approach to implement DID-IV designs, yields a negative estimate.\nBy contrast, our estimation method indicates a substantial gain from schooling.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12083v5"
    },
    {
        "title": "Conditional Choice Probability Estimation of Dynamic Discrete Choice\n  Models with 2-period Finite Dependence",
        "authors": [
            "Yu Hao",
            "Hiroyuki Kasahara"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper extends the work of Arcidiacono and Miller (2011, 2019) by\nintroducing a novel characterization of finite dependence within dynamic\ndiscrete choice models, demonstrating that numerous models display 2-period\nfinite dependence. We recast finite dependence as a problem of sequentially\nsearching for weights and introduce a computationally efficient method for\ndetermining these weights by utilizing the Kronecker product structure embedded\nin state transitions. With the estimated weights, we develop a computationally\nattractive Conditional Choice Probability estimator with 2-period finite\ndependence. The computational efficacy of our proposed estimator is\ndemonstrated through Monte Carlo simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12467v1"
    },
    {
        "title": "Exogenous Consideration and Extended Random Utility",
        "authors": [
            "Roy Allen"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In a consideration set model, an individual maximizes utility among the\nconsidered alternatives. I relate a consideration set additive random utility\nmodel to classic discrete choice and the extended additive random utility\nmodel, in which utility can be $-\\infty$ for infeasible alternatives. When\nobservable utility shifters are bounded, all three models are observationally\nequivalent. Moreover, they have the same counterfactual bounds and welfare\nformulas for changes in utility shifters like price. For attention\ninterventions, welfare cannot change in the full consideration model but is\ncompletely unbounded in the limited consideration model. The identified set for\nconsideration set probabilities has a minimal width for any bounded support of\nshifters, but with unbounded support it is a point: identification \"towards\"\ninfinity does not resemble identification \"at\" infinity.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.13945v1"
    },
    {
        "title": "Modularity, Higher-Order Recombination, and New Venture Success",
        "authors": [
            "Likun Cao",
            "Ziwen Chen",
            "James Evans"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Modularity is critical for the emergence and evolution of complex social,\nnatural, and technological systems robust to exploratory failure. We consider\nthis in the context of emerging business organizations, which can be understood\nas complex systems. We build a theory of organizational emergence as\nhigher-order, modular recombination wherein successful start-ups assemble novel\ncombinations of successful modular components, rather than engage in the\nlower-order combination of disparate, singular components. Lower-order\ncombinations are critical for long-term socio-economic transformation, but\nmanifest diffuse benefits requiring support as public goods. Higher-order\ncombinations facilitate rapid experimentation and attract private funding. We\nevaluate this with U.S. venture-funded start-ups over 45 years using company\ndescriptions. We build a dynamic semantic space with word embedding models\nconstructed from evolving business discourse, which allow us to measure the\nmodularity of and distance between new venture components. Using event history\nmodels, we demonstrate how ventures more likely achieve successful IPOs and\nhigh-priced acquisitions when they combine diverse modules of clustered\ncomponents. We demonstrate how higher-order combination enables venture success\nby accelerating firm development and diversifying investment, and we reflect on\nits implications for social innovation.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.15042v1"
    },
    {
        "title": "Empirical Crypto Asset Pricing",
        "authors": [
            "Adam Baybutt"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We motivate the study of the crypto asset class with eleven empirical facts,\nand study the drivers of crypto asset returns through the lens of univariate\nfactors. We argue crypto assets are a new, attractive, and independent asset\nclass. In a novel and rigorously built panel of crypto assets, we examine\npricing ability of sixty three asset characteristics to find rich signal\ncontent across the characteristics and at several future horizons. Only\nunivariate financial factors (i.e., functions of previous returns) were\nassociated with statistically significant long-short strategies, suggestive of\nspeculatively driven returns as opposed to more fundamental pricing factors.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.15716v1"
    },
    {
        "title": "Two-way fixed effects instrumental variable regressions in staggered\n  DID-IV designs",
        "authors": [
            "Sho Miyaji"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Many studies run two-way fixed effects instrumental variable (TWFEIV)\nregressions, leveraging variation in the timing of policy adoption across units\nas an instrument for treatment. This paper studies the properties of the TWFEIV\nestimator in staggered instrumented difference-in-differences (DID-IV) designs.\nWe show that in settings with the staggered adoption of the instrument across\nunits, the TWFEIV estimator can be decomposed into a weighted average of all\npossible two-group/two-period Wald-DID estimators. Under staggered DID-IV\ndesigns, a causal interpretation of the TWFEIV estimand hinges on the stable\neffects of the instrument on the treatment and the outcome over time. We\nillustrate the use of our decomposition theorem for the TWFEIV estimator\nthrough an empirical application.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.16467v1"
    },
    {
        "title": "Cross-border cannibalization: Spillover effects of wind and solar energy\n  on interconnected European electricity markets",
        "authors": [
            "Clemens Stiewe",
            "Alice Lixuan Xu",
            "Anselm Eicke",
            "Lion Hirth"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The average revenue, or market value, of wind and solar energy tends to fall\nwith increasing market shares, as is now evident across European electricity\nmarkets. At the same time, these markets have become more interconnected. In\nthis paper, we empirically study the multiple cross-border effects on the value\nof renewable energy: on one hand, interconnection is a flexibility resource\nthat allows to export energy when it is locally abundant, benefitting\nrenewables. On the other hand, wind and solar radiation are correlated across\nspace, so neighboring supply adds to the local one to depress domestic prices.\nWe estimate both effects, using spatial panel regression on electricity market\ndata from 2015 to 2023 from 30 European bidding zones. We find that domestic\nwind and solar value is not only depressed by domestic, but also by neighboring\nrenewables expansion. The better interconnected a market is, the smaller the\neffect of domestic but the larger the effect of neighboring renewables. While\nwind value is stabilized by interconnection, solar value is not. If wind market\nshare increases both at home and in neighboring markets by one percentage\npoint, the value factor of wind energy is reduced by just above 1 percentage\npoints. For solar, this number is almost 4 percentage points.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.17166v1"
    },
    {
        "title": "Quantifying the Reliance of Black-Box Decision-Makers on Variables of\n  Interest",
        "authors": [
            "Daniel Vebman"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper introduces a framework for measuring how much black-box\ndecision-makers rely on variables of interest. The framework adapts a\npermutation-based measure of variable importance from the explainable machine\nlearning literature. With an emphasis on applicability, I present some of the\nframework's theoretical and computational properties, explain how reliance\ncomputations have policy implications, and work through an illustrative\nexample. In the empirical application to interruptions by Supreme Court\nJustices during oral argument, I find that the effect of gender is more muted\ncompared to the existing literature's estimate; I then use this paper's\nframework to compare Justices' reliance on gender and alignment to their\nreliance on experience, which are incomparable using regression coefficients.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.17225v1"
    },
    {
        "title": "Mixing it up: Inflation at risk",
        "authors": [
            "Maximilian Schröder"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Assessing the contribution of various risk factors to future inflation risks\nwas crucial for guiding monetary policy during the recent high inflation\nperiod. However, existing methodologies often provide limited insights by\nfocusing solely on specific percentiles of the forecast distribution. In\ncontrast, this paper introduces a comprehensive framework that examines how\neconomic indicators impact the entire forecast distribution of macroeconomic\nvariables, facilitating the decomposition of the overall risk outlook into its\nunderlying drivers. Additionally, the framework allows for the construction of\nrisk measures that align with central bank preferences, serving as valuable\nsummary statistics. Applied to the recent inflation surge, the framework\nreveals that U.S. inflation risk was primarily influenced by the recovery of\nthe U.S. business cycle and surging commodity prices, partially mitigated by\nadjustments in monetary policy and credit spreads.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.17237v2"
    },
    {
        "title": "Estimating treatment-effect heterogeneity across sites, in multi-site\n  randomized experiments with few units per site",
        "authors": [
            "Clément de Chaisemartin",
            "Antoine Deeb"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In multi-site randomized trials with many sites and few randomization units\nper site, an Empirical-Bayes estimator can be used to estimate the variance of\nthe treatment effect across sites. When this estimator indicates that treatment\neffects do vary, we propose estimators of the coefficients from regressions of\nsite-level effects on site-level characteristics that are unobserved but can be\nunbiasedly estimated, such as sites' average outcome without treatment, or\nsite-specific treatment effects on mediator variables. In experiments with\nimperfect compliance, we show that the sign of the correlation between local\naverage treatment effects (LATEs) and site-level characteristics is identified,\nand we propose a partly testable assumption under which the variance of LATEs\nis identified. We use our results to revisit Behaghel et al (2014), who study\nthe effect of counseling programs on job seekers' job-finding rate, in 200 job\nplacement agencies in France. We find considerable treatment-effect\nheterogeneity, both for intention to treat and LATE effects, and the treatment\neffect is negatively correlated with sites' job-finding rate without treatment.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.17254v3"
    },
    {
        "title": "Count Data Models with Heterogeneous Peer Effects under Rational\n  Expectations",
        "authors": [
            "Aristide Houndetoungan"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper develops a micro-founded peer effect model for count responses\nusing a game of incomplete information. The model incorporates heterogeneity in\npeer effects through agents' groups based on observed characteristics.\nParameter identification is established using the identification condition of\nlinear models, which relies on the presence of friends' friends who are not\ndirect friends in the network. I show that this condition extends to a large\nclass of nonlinear models. The model parameters are estimated using the nested\npseudo-likelihood approach, controlling for network endogeneity. I present an\nempirical application on students' participation in extracurricular activities.\nI find that females are more responsive to their peers than males, whereas male\npeers do not influence male students. An easy-to-use R packag--named\nCDatanet--is available for implementing the model.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.17290v1"
    },
    {
        "title": "Dyadic Regression with Sample Selection",
        "authors": [
            "Kensuke Sakamoto"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper addresses the sample selection problem in panel dyadic regression\nanalysis. Dyadic data often include many zeros in the main outcomes due to the\nunderlying network formation process. This not only contaminates popular\nestimators used in practice but also complicates the inference due to the\ndyadic dependence structure. We extend Kyriazidou (1997)'s approach to dyadic\ndata and characterize the asymptotic distribution of our proposed estimator.\nThe convergence rates are $\\sqrt{n}$ or $\\sqrt{n^{2}h_{n}}$, depending on the\ndegeneracy of the H\\'{a}jek projection part of the estimator, where $n$ is the\nnumber of nodes and $h_{n}$ is a bandwidth. We propose a bias-corrected\nconfidence interval and a variance estimator that adapts to the degeneracy. A\nMonte Carlo simulation shows the good finite sample performance of our\nestimator and highlights the importance of bias correction in both asymptotic\nregimes when the fraction of zeros in outcomes varies. We illustrate our\nprocedure using data from Moretti and Wilson (2017)'s paper on migration.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.17787v2"
    },
    {
        "title": "Semi-nonparametric models of multidimensional matching: an optimal\n  transport approach",
        "authors": [
            "Dongwoo Kim",
            "Young Jun Lee"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper proposes empirically tractable multidimensional matching models,\nfocusing on worker-job matching. We generalize the parametric model proposed by\nLindenlaub (2017), which relies on the assumption of joint normality of\nobserved characteristics of workers and jobs. In our paper, we allow\nunrestricted distributions of characteristics and show identification of the\nproduction technology, and equilibrium wage and matching functions using tools\nfrom optimal transport theory. Given identification, we propose efficient,\nconsistent, asymptotically normal sieve estimators. We revisit Lindenlaub's\nempirical application and show that, between 1990 and 2010, the U.S. economy\nexperienced much larger technological progress favoring cognitive abilities\nthan the original findings suggest. Furthermore, our flexible model\nspecifications provide a significantly better fit for patterns in the evolution\nof wage inequality.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.18089v1"
    },
    {
        "title": "Modelling and Forecasting Energy Market Volatility Using GARCH and\n  Machine Learning Approach",
        "authors": [
            "Seulki Chung"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper presents a comparative analysis of univariate and multivariate\nGARCH-family models and machine learning algorithms in modeling and forecasting\nthe volatility of major energy commodities: crude oil, gasoline, heating oil,\nand natural gas. It uses a comprehensive dataset incorporating financial,\nmacroeconomic, and environmental variables to assess predictive performance and\ndiscusses volatility persistence and transmission across these commodities.\nAspects of volatility persistence and transmission, traditionally examined by\nGARCH-class models, are jointly explored using the SHAP (Shapley Additive\nexPlanations) method. The findings reveal that machine learning models\ndemonstrate superior out-of-sample forecasting performance compared to\ntraditional GARCH models. Machine learning models tend to underpredict, while\nGARCH models tend to overpredict energy market volatility, suggesting a hybrid\nuse of both types of models. There is volatility transmission from crude oil to\nthe gasoline and heating oil markets. The volatility transmission in the\nnatural gas market is less prevalent.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.19849v1"
    },
    {
        "title": "Optimizing hydrogen and e-methanol production through Power-to-X\n  integration in biogas plants",
        "authors": [
            "Alberto Alamia",
            "Behzad Partoon",
            "Eoghan Rattigan",
            "Gorm Brunn Andresen"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The European Union strategy for net zero emissions relies on developing\nhydrogen and electro fuels infrastructure. These fuels will be crucial as\nenergy carriers and balancing agents for renewable energy variability. Large\nscale production requires more renewable capacity, and various Power to X (PtX)\nconcepts are emerging in renewable rich countries. However, sourcing renewable\ncarbon to scale carbon based electro fuels is a significant challenge. This\nstudy explores a PtX hub that sources renewable CO2 from biogas plants,\nintegrating renewable energy, hydrogen production, and methanol synthesis on\nsite. This concept creates an internal market for energy and materials,\ninterfacing with the external energy system. The size and operation of the PtX\nhub were optimized, considering integration with local energy systems and a\npotential hydrogen grid. The levelized costs of hydrogen and methanol were\nestimated for a 2030 start, considering new legislation on renewable fuels of\nnon biological origin (RFNBOs). Our results show the PtX hub can rely mainly on\non site renewable energy, selling excess electricity to the grid. A local\nhydrogen grid connection improves operations, and the behind the meter market\nlowers energy prices, buffering against market variability. We found methanol\ncosts could be below 650 euros per ton and hydrogen production costs below 3\neuros per kg, with standalone methanol plants costing 23 per cent more. The CO2\nrecovery to methanol production ratio is crucial, with over 90 per cent\nrecovery requiring significant investment in CO2 and H2 storage. Overall, our\nfindings support planning PtX infrastructures integrated with the agricultural\nsector as a cost effective way to access renewable carbon.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.00442v1"
    },
    {
        "title": "Financial Deepening and Economic Growth in Select Emerging Markets with\n  Currency Board Systems: Theory and Evidence",
        "authors": [
            "Yujuan Qiu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper investigates some indicators of financial development in select\ncountries with currency board systems and raises some questions about the\nconnection between financial development and growth in currency board systems.\nMost of those cases are long past episodes of what we would now call emerging\nmarkets. However, the paper also looks at Hong Kong, the currency board system\nthat is one of the world's largest and most advanced financial markets. The\nglobal financial crisis of 2008 09 created doubts about the efficiency of\nfinancial markets in advanced economies, including in Hong Kong, and unsettled\nthe previous consensus that a large financial sector would be more stable than\na smaller one.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.00472v1"
    },
    {
        "title": "Cluster-robust jackknife and bootstrap inference for binary response\n  models",
        "authors": [
            "James G. MacKinnon",
            "Morten Ørregaard Nielsen",
            "Matthew D. Webb"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We study cluster-robust inference for binary response models. Inference based\non the most commonly-used cluster-robust variance matrix estimator (CRVE) can\nbe very unreliable. We study several alternatives. Conceptually the simplest of\nthese, but also the most computationally demanding, involves jackknifing at the\ncluster level. We also propose a linearized version of the cluster-jackknife\nvariance matrix estimator as well as linearized versions of the wild cluster\nbootstrap. The linearizations are based on empirical scores and are\ncomputationally efficient. Throughout we use the logit model as a leading\nexample. We also discuss a new Stata software package called logitjack which\nimplements these procedures. Simulation results strongly favor the new methods,\nand two empirical examples suggest that it can be important to use them in\npractice.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.00650v1"
    },
    {
        "title": "A Robust Residual-Based Test for Structural Changes in Factor Models",
        "authors": [
            "Bin Peng",
            "Liangjun Su",
            "Yayi Yan"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, we propose an easy-to-implement residual-based specification\ntesting procedure for detecting structural changes in factor models, which is\npowerful against both smooth and abrupt structural changes with unknown break\ndates. The proposed test is robust against the over-specified number of\nfactors, and serially and cross-sectionally correlated error processes. A new\ncentral limit theorem is given for the quadratic forms of panel data with\ndependence over both dimensions, thereby filling a gap in the literature. We\nestablish the asymptotic properties of the proposed test statistic, and\naccordingly develop a simulation-based scheme to select critical value in order\nto improve finite sample performance. Through extensive simulations and a\nreal-world application, we confirm our theoretical results and demonstrate that\nthe proposed test exhibits desirable size and power in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.00941v1"
    },
    {
        "title": "Random Subspace Local Projections",
        "authors": [
            "Viet Hoang Dinh",
            "Didier Nibbering",
            "Benjamin Wong"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We show how random subspace methods can be adapted to estimating local\nprojections with many controls. Random subspace methods have their roots in the\nmachine learning literature and are implemented by averaging over regressions\nestimated over different combinations of subsets of these controls. We document\nthree key results: (i) Our approach can successfully recover the impulse\nresponse functions across Monte Carlo experiments representative of different\nmacroeconomic settings and identification schemes. (ii) Our results suggest\nthat random subspace methods are more accurate than other dimension reduction\nmethods if the underlying large dataset has a factor structure similar to\ntypical macroeconomic datasets such as FRED-MD. (iii) Our approach leads to\ndifferences in the estimated impulse response functions relative to benchmark\nmethods when applied to two widely studied empirical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.01002v1"
    },
    {
        "title": "Enabling Decision-Making with the Modified Causal Forest: Policy Trees\n  for Treatment Assignment",
        "authors": [
            "Hugo Bodory",
            "Federica Mascolo",
            "Michael Lechner"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Decision-making plays a pivotal role in shaping outcomes in various\ndisciplines, such as medicine, economics, and business. This paper provides\nguidance to practitioners on how to implement a decision tree designed to\naddress treatment assignment policies using an interpretable and non-parametric\nalgorithm. Our Policy Tree is motivated on the method proposed by Zhou, Athey,\nand Wager (2023), distinguishing itself for the policy score calculation,\nincorporating constraints, and handling categorical and continuous variables.\nWe demonstrate the usage of the Policy Tree for multiple, discrete treatments\non data sets from different fields. The Policy Tree is available in Python's\nopen-source package mcf (Modified Causal Forest).\n",
        "pdf_link": "http://arxiv.org/pdf/2406.02241v1"
    },
    {
        "title": "The Impact of Acquisition on Product Quality in the Console Gaming\n  Industry",
        "authors": [
            "Shivam Somani"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The console gaming industry, a dominant force in the global entertainment\nsector, has witnessed a wave of consolidation in recent years, epitomized by\nMicrosoft's high-profile acquisitions of Activision Blizzard and Zenimax. This\nstudy investigates the repercussions of such mergers on consumer welfare and\ninnovation within the gaming landscape, focusing on product quality as a key\nmetric. Through a comprehensive analysis employing a difference-in-difference\nmodel, the research evaluates the effects of acquisition on game review\nratings, drawing from a dataset comprising over 16,000 console games released\nbetween 2000 and 2023. The research addresses key assumptions underlying the\ndifference-in-difference methodology, including parallel trends and spillover\neffects, to ensure the robustness of the findings. The DID results suggest a\npositive and statistically significant impact of acquisition on game review\nratings, when controlling for genre and release year. The study contributes to\nthe literature by offering empirical evidence on the direct consequences of\nindustry consolidation on consumer welfare and competition dynamics within the\ngaming sector.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.02525v1"
    },
    {
        "title": "When does IV identification not restrict outcomes?",
        "authors": [
            "Leonard Goff"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Many identification results in instrumental variables (IV) models hold\nwithout requiring any restrictions on the distribution of potential outcomes,\nor how those outcomes are correlated with selection behavior. This enables IV\nmodels to allow for arbitrary heterogeneity in treatment effects and the\npossibility of selection on gains in the outcome. I provide a necessary and\nsufficient condition for treatment effects to be point identified in a manner\nthat does not restrict outcomes when the instruments take a finite number of\nvalues. The condition generalizes the well-known LATE monotonicity assumption,\nand unifies a wide variety of other known IV identification results. The result\nalso yields a brute-force approach to reveal all selection models that allow\nfor point identification of treatment effects without restricting outcomes, and\nthen enumerate all of the identified parameters within each such selection\nmodel. The search uncovers new selection models that yield identification,\nprovides impossibility results for others, and offers opportunities to relax\nassumptions on selection used in existing literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.02835v3"
    },
    {
        "title": "Is local opposition taking the wind out of the energy transition?",
        "authors": [
            "Federica Daniele",
            "Guido de Blasio",
            "Alessandra Pasquini"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Local opposition to the installation of renewable energy sources is a\npotential threat to the energy transition. Local communities tend to oppose the\nconstruction of energy plants due to the associated negative externalities (the\nso-called 'not in my backyard' or NIMBY phenomenon) according to widespread\nbelief, mostly based on anecdotal evidence. Using administrative data on wind\nturbine installation and electoral outcomes across municipalities located in\nthe South of Italy during 2000-19, we estimate the impact of wind turbines'\ninstallation on incumbent regional governments' electoral support during the\nnext elections. Our main findings, derived by a wind-speed based instrumental\nvariable strategy, point in the direction of a mild and not statistically\nsignificant electoral backlash for right-wing regional administrations and of a\nstrong and statistically significant positive reinforcement for left-wing\nregional administrations. Based on our analysis, the hypothesis of an electoral\neffect of NIMBY type of behavior in connection with the development of wind\nturbines appears not to be supported by the data.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.03022v1"
    },
    {
        "title": "Identification of structural shocks in Bayesian VEC models with\n  two-state Markov-switching heteroskedasticity",
        "authors": [
            "Justyna Wróblewska",
            "Łukasz Kwiatkowski"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We develop a Bayesian framework for cointegrated structural VAR models\nidentified by two-state Markovian breaks in conditional covariances. The\nresulting structural VEC specification with Markov-switching heteroskedasticity\n(SVEC-MSH) is formulated in the so-called B-parameterization, in which the\nprior distribution is specified directly for the matrix of the instantaneous\nreactions of the endogenous variables to structural innovations. We discuss\nsome caveats pertaining to the identification conditions presented earlier in\nthe literature on stationary structural VAR-MSH models, and revise the\nrestrictions to actually ensure the unique global identification through the\ntwo-state heteroskedasticity. To enable the posterior inference in the proposed\nmodel, we design an MCMC procedure, combining the Gibbs sampler and the\nMetropolis-Hastings algorithm. The methodology is illustrated both with a\nsimulated as well as real-world data examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.03053v2"
    },
    {
        "title": "GLOBUS: Global building renovation potential by 2070",
        "authors": [
            "Shufan Zhang",
            "Minda Ma",
            "Nan Zhou",
            "Jinyue Yan"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Surpassing the two large emission sectors of transportation and industry, the\nbuilding sector accounted for 34% and 37% of global energy consumption and\ncarbon emissions in 2021, respectively. The building sector, the final piece to\nbe addressed in the transition to net-zero carbon emissions, requires a\ncomprehensive, multisectoral strategy for reducing emissions. Until now, the\nabsence of data on global building floorspace has impeded the measurement of\nbuilding carbon intensity (carbon emissions per floorspace) and the\nidentification of ways to achieve carbon neutrality for buildings. For this\nstudy, we develop a global building stock model (GLOBUS) to fill that data gap.\nOur study's primary contribution lies in providing a dataset of global building\nstock turnover using scenarios that incorporate various levels of building\nrenovation. By unifying the evaluation indicators, the dataset empowers\nbuilding science researchers to perform comparative analyses based on\nfloorspace. Specifically, the building stock dataset establishes a reference\nfor measuring carbon emission intensity and decarbonization intensity of\nbuildings within different countries. Further, we emphasize the sufficiency of\nexisting buildings by incorporating building renovation into the model.\nRenovation can minimize the need to expand the building stock, thereby\nbolstering decarbonization of the building sector.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.04133v1"
    },
    {
        "title": "Data-Driven Real-time Coupon Allocation in the Online Platform",
        "authors": [
            "Jinglong Dai",
            "Hanwei Li",
            "Weiming Zhu",
            "Jianfeng Lin",
            "Binqiang Huang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Traditionally, firms have offered coupons to customer groups at predetermined\ndiscount rates. However, advancements in machine learning and the availability\nof abundant customer data now enable platforms to provide real-time customized\ncoupons to individuals. In this study, we partner with Meituan, a leading\nshopping platform, to develop a real-time, end-to-end coupon allocation system\nthat is fast and effective in stimulating demand while adhering to marketing\nbudgets when faced with uncertain traffic from a diverse customer base.\nLeveraging comprehensive customer and product features, we estimate Conversion\nRates (CVR) under various coupon values and employ isotonic regression to\nensure the monotonicity of predicted CVRs with respect to coupon value. Using\ncalibrated CVR predictions as input, we propose a Lagrangian Dual-based\nalgorithm that efficiently determines optimal coupon values for each arriving\ncustomer within 50 milliseconds. We theoretically and numerically investigate\nthe model performance under parameter misspecifications and apply a control\nloop to adapt to real-time updated information, thereby better adhering to the\nmarketing budget. Finally, we demonstrate through large-scale field experiments\nand observational data that our proposed coupon allocation algorithm\noutperforms traditional approaches in terms of both higher conversion rates and\nincreased revenue. As of May 2024, Meituan has implemented our framework to\ndistribute coupons to over 100 million users across more than 110 major cities\nin China, resulting in an additional CNY 8 million in annual profit. We\ndemonstrate how to integrate a machine learning prediction model for estimating\ncustomer CVR, a Lagrangian Dual-based coupon value optimizer, and a control\nsystem to achieve real-time coupon delivery while dynamically adapting to\nrandom customer arrival patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.05987v3"
    },
    {
        "title": "Robustness to Missing Data: Breakdown Point Analysis",
        "authors": [
            "Daniel Ober-Reynolds"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Missing data is pervasive in econometric applications, and rarely is it\nplausible that the data are missing (completely) at random. This paper proposes\na methodology for studying the robustness of results drawn from incomplete\ndatasets. Selection is measured as the squared Hellinger divergence between the\ndistributions of complete and incomplete observations, which has a natural\ninterpretation. The breakdown point is defined as the minimal amount of\nselection needed to overturn a given result. Reporting point estimates and\nlower confidence intervals of the breakdown point is a simple, concise way to\ncommunicate the robustness of a result. An estimator of the breakdown point of\na result drawn from a generalized method of moments model is proposed and shown\nroot-n consistent and asymptotically normal under mild assumptions. Lower\nconfidence intervals of the breakdown point are simple to construct. The paper\nconcludes with a simulation study illustrating the finite sample performance of\nthe estimators in several common models.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.06804v1"
    },
    {
        "title": "Did Harold Zuercher Have Time-Separable Preferences?",
        "authors": [
            "Jay Lu",
            "Yao Luo",
            "Kota Saito",
            "Yi Xin"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper proposes an empirical model of dynamic discrete choice to allow\nfor non-separable time preferences, generalizing the well-known Rust (1987)\nmodel. Under weak conditions, we show the existence of value functions and\nhence well-defined optimal choices. We construct a contraction mapping of the\nvalue function and propose an estimation method similar to Rust's nested fixed\npoint algorithm. Finally, we apply the framework to the bus engine replacement\ndata. We improve the fit of the data with our general model and reject the null\nhypothesis that Harold Zuercher has separable time preferences. Misspecifying\nan agent's preference as time-separable when it is not leads to biased\ninferences about structure parameters (such as the agent's risk attitudes) and\nmisleading policy recommendations.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.07809v1"
    },
    {
        "title": "Positive and negative word of mouth in the United States",
        "authors": [
            "Shawn Berry"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Word of mouth is a process by which consumers transmit positive or negative\nsentiment to other consumers about a business. While this process has long been\nrecognized as a type of promotion for businesses, the value of word of mouth is\nquestionable. This study will examine the various correlates of word of mouth\nto demographic variables, including the role of the trust of business owners.\nEducation level, region of residence, and income level were found to be\nsignificant predictors of positive word of mouth. Although the results\ngenerally suggest that the majority of respondents do not engage in word of\nmouth, there are valuable insights to be learned.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.08279v1"
    },
    {
        "title": "Identification and Inference on Treatment Effects under\n  Covariate-Adaptive Randomization and Imperfect Compliance",
        "authors": [
            "Federico A. Bugni",
            "Mengsi Gao",
            "Filip Obradovic",
            "Amilcar Velez"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Randomized controlled trials (RCTs) frequently utilize covariate-adaptive\nrandomization (CAR) (e.g., stratified block randomization) and commonly suffer\nfrom imperfect compliance. This paper studies the identification and inference\nfor the average treatment effect (ATE) and the average treatment effect on the\ntreated (ATT) in such RCTs with a binary treatment.\n  We first develop characterizations of the identified sets for both estimands.\nSince data are generally not i.i.d. under CAR, these characterizations do not\nfollow from existing results. We then provide consistent estimators of the\nidentified sets and asymptotically valid confidence intervals for the\nparameters. Our asymptotic analysis leads to concrete practical recommendations\nregarding how to estimate the treatment assignment probabilities that enter in\nestimated bounds. In the case of the ATE, using sample analog assignment\nfrequencies is more efficient than using the true assignment probabilities. On\nthe contrary, using the true assignment probabilities is preferable for the\nATT.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.08419v2"
    },
    {
        "title": "Jackknife inference with two-way clustering",
        "authors": [
            "James G. MacKinnon",
            "Morten Ørregaard Nielsen",
            "Matthew D. Webb"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  For linear regression models with cross-section or panel data, it is natural\nto assume that the disturbances are clustered in two dimensions. However, the\nfinite-sample properties of two-way cluster-robust tests and confidence\nintervals are often poor. We discuss several ways to improve inference with\ntwo-way clustering. Two of these are existing methods for avoiding, or at least\nameliorating, the problem of undefined standard errors when a cluster-robust\nvariance matrix estimator (CRVE) is not positive definite. One is a new method\nthat always avoids the problem. More importantly, we propose a family of new\ntwo-way CRVEs based on the cluster jackknife. Simulations for models with\ntwo-way fixed effects suggest that, in many cases, the cluster-jackknife CRVE\ncombined with our new method yields surprisingly accurate inferences. We\nprovide a simple software package, twowayjack for Stata, that implements our\nrecommended variance estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.08880v1"
    },
    {
        "title": "Multidimensional clustering in judge designs",
        "authors": [
            "Johannes W. Ligtenberg",
            "Tiemen Woutersen"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Estimates in judge designs run the risk of being biased due to the many judge\nidentities that are implicitly or explicitly used as instrumental variables.\nThe usual method to analyse judge designs, via a leave-out mean instrument,\neliminates this many instrument bias only in case the data are clustered in at\nmost one dimension. What is left out in the mean defines this clustering\ndimension. How most judge designs cluster their standard errors, however,\nimplies that there are additional clustering dimensions, which makes that a\nmany instrument bias remains. We propose two estimators that are many\ninstrument bias free, also in multidimensional clustered judge designs. The\nfirst generalises the one dimensional cluster jackknife instrumental variable\nestimator, by removing from this estimator the additional bias terms due to the\nextra dependence in the data. The second models all but one clustering\ndimensions by fixed effects and we show how these numerous fixed effects can be\nremoved without introducing extra bias. A Monte-Carlo experiment and the\nrevisitation of two judge designs show the empirical relevance of properly\naccounting for multidimensional clustering in estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.09473v1"
    },
    {
        "title": "EM Estimation of Conditional Matrix Variate $t$ Distributions",
        "authors": [
            "Battulga Gankhuu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Conditional matrix variate student $t$ distribution was introduced by\nBattulga (2024a). In this paper, we propose a new version of the conditional\nmatrix variate student $t$ distribution. The paper provides EM algorithms,\nwhich estimate parameters of the conditional matrix variate student $t$\ndistributions, including general cases and special cases with Minnesota prior.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.10837v3"
    },
    {
        "title": "Resilience of international oil trade networks under extreme event\n  shock-recovery simulations",
        "authors": [
            "Na Wei",
            "Wen-Jie Xie",
            "Wei-Xing Zhou"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  With the frequent occurrence of black swan events, global energy security\nsituation has become increasingly complex and severe. Assessing the resilience\nof the international oil trade network (iOTN) is crucial for evaluating its\nability to withstand extreme shocks and recover thereafter, ensuring energy\nsecurity. We overcomes the limitations of discrete historical data by\ndeveloping a simulation model for extreme event shock-recovery in the iOTNs. We\nintroduce network efficiency indicator to measure oil resource allocation\nefficiency and evaluate network performance. Then, construct a resilience index\nto explore the resilience of the iOTNs from dimensions of resistance and\nrecoverability. Our findings indicate that extreme events can lead to sharp\ndeclines in performance of the iOTNs, especially when economies with\nsignificant trading positions and relations suffer shocks. The upward trend in\nrecoverability and resilience reflects the self-organizing nature of the iOTNs,\ndemonstrating its capacity for optimizing its own structure and functionality.\nUnlike traditional energy security research based solely on discrete historical\ndata or resistance indicators, our model evaluates resilience from multiple\ndimensions, offering insights for global energy governance systems while\nproviding diverse perspectives for various economies to mitigate risks and\nuphold energy security.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.11467v1"
    },
    {
        "title": "Testing for Underpowered Literatures",
        "authors": [
            "Stefan Faridani"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  How many experimental studies would have come to different conclusions had\nthey been run on larger samples? I show how to estimate the expected number of\nstatistically significant results that a set of experiments would have reported\nhad their sample sizes all been counterfactually increased by a chosen factor.\nThe estimator is consistent and asymptotically normal. Unlike existing methods,\nmy approach requires no assumptions about the distribution of true effects of\nthe interventions being studied other than continuity. This method includes an\nadjustment for publication bias in the reported t-scores. An application to\nrandomized controlled trials (RCTs) published in top economics journals finds\nthat doubling every experiment's sample size would only increase the power of\ntwo-sided t-tests by 7.2 percentage points on average. This effect is small and\nis comparable to the effect for systematic replication projects in laboratory\npsychology where previous studies enabled accurate power calculations ex ante.\nThese effects are both smaller than for non-RCTs. This comparison suggests that\nRCTs are on average relatively insensitive to sample size increases. The policy\nimplication is that grant givers should generally fund more experiments rather\nthan fewer, larger ones.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.13122v1"
    },
    {
        "title": "Bayesian Inference for Multidimensional Welfare Comparisons",
        "authors": [
            "David Gunawan",
            "William Griffiths",
            "Duangkamon Chotikapanich"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Using both single-index measures and stochastic dominance concepts, we show\nhow Bayesian inference can be used to make multivariate welfare comparisons. A\nfour-dimensional distribution for the well-being attributes income, mental\nhealth, education, and happiness are estimated via Bayesian Markov chain Monte\nCarlo using unit-record data taken from the Household, Income and Labour\nDynamics in Australia survey. Marginal distributions of beta and gamma mixtures\nand discrete ordinal distributions are combined using a copula. Improvements in\nboth well-being generally and poverty magnitude are assessed using posterior\nmeans of single-index measures and posterior probabilities of stochastic\ndominance. The conditions for stochastic dominance depend on the class of\nutility functions that is assumed to define a social welfare function and the\nnumber of attributes in the utility function. Three classes of utility\nfunctions are considered, and posterior probabilities of dominance are computed\nfor one, two, and four-attribute utility functions for three time intervals\nwithin the period 2001 to 2019.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.13395v1"
    },
    {
        "title": "Estimating Time-Varying Parameters of Various Smoothness in Linear\n  Models via Kernel Regression",
        "authors": [
            "Mikihito Nishi"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We consider estimating nonparametric time-varying parameters in linear models\nusing kernel regression. Our contributions are twofold. First, we consider a\nbroad class of time-varying parameters including deterministic smooth\nfunctions, the rescaled random walk, structural breaks, the threshold model and\ntheir mixtures. We show that those time-varying parameters can be consistently\nestimated by kernel regression. Our analysis exploits the smoothness of the\ntime-varying parameter quantified by a single parameter. The second\ncontribution is to reveal that the bandwidth used in kernel regression\ndetermines the trade-off between the rate of convergence and the size of the\nclass of time-varying parameters that can be estimated. We demonstrate that an\nimproper choice of the bandwidth yields biased estimation, and argue that the\nbandwidth should be selected according to the smoothness of the time-varying\nparameters. An empirical application shows that the kernel-based estimator with\na particular bandwidth choice can capture the random-walk dynamics in\ntime-varying parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.14046v3"
    },
    {
        "title": "MIDAS-QR with 2-Dimensional Structure",
        "authors": [
            "Tibor Szendrei",
            "Arnab Bhattacharjee",
            "Mark E. Schaffer"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Mixed frequency data has been shown to improve the performance of\ngrowth-at-risk models in the literature. Most of the research has focused on\nimposing structure on the high-frequency lags when estimating MIDAS-QR models\nakin to what is done in mean models. However, only imposing structure on the\nlag-dimension can potentially induce quantile variation that would otherwise\nnot be there. In this paper we extend the framework by introducing structure on\nboth the lag dimension and the quantile dimension. In this way we are able to\nshrink unnecessary quantile variation in the high-frequency variables. This\nleads to more gradual lag profiles in both dimensions compared to the MIDAS-QR\nand UMIDAS-QR. We show that this proposed method leads to further gains in\nnowcasting and forecasting on a pseudo-out-of-sample exercise on US data.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.15157v1"
    },
    {
        "title": "Difference-in-Differences when Parallel Trends Holds Conditional on\n  Covariates",
        "authors": [
            "Carolina Caetano",
            "Brantly Callaway"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, we study difference-in-differences identification and\nestimation strategies when the parallel trends assumption holds after\nconditioning on covariates. We consider empirically relevant settings where the\ncovariates can be time-varying, time-invariant, or both. We uncover a number of\nweaknesses of commonly used two-way fixed effects (TWFE) regressions in this\ncontext, even in applications with only two time periods. In addition to some\nweaknesses due to estimating linear regression models that are similar to cases\nwith cross-sectional data, we also point out a collection of additional issues\nthat we refer to as \\textit{hidden linearity bias} that arise because the\ntransformations used to eliminate the unit fixed effect also transform the\ncovariates (e.g., taking first differences can result in the estimating\nequation only including the change in covariates over time, not their level,\nand also drop time-invariant covariates altogether). We provide simple\ndiagnostics for assessing how susceptible a TWFE regression is to hidden\nlinearity bias based on reformulating the TWFE regression as a weighting\nestimator. Finally, we propose simple alternative estimation strategies that\ncan circumvent these issues.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.15288v2"
    },
    {
        "title": "Identification and Estimation of Causal Effects in High-Frequency Event\n  Studies",
        "authors": [
            "Alessandro Casini",
            "Adam McCloskey"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We provide precise conditions for nonparametric identification of causal\neffects by high-frequency event study regressions, which have been used widely\nin the recent macroeconomics, financial economics and political economy\nliteratures. The high-frequency event study method regresses changes in an\noutcome variable on a measure of unexpected changes in a policy variable in a\nnarrow time window around an event or a policy announcement (e.g., a 30-minute\nwindow around an FOMC announcement). We show that, contrary to popular belief,\nthe narrow size of the window is not sufficient for identification. Rather, the\npopulation regression coefficient identifies a causal estimand when (i) the\neffect of the policy shock on the outcome does not depend on the other shocks\n(separability) and (ii) the surprise component of the news or event dominates\nall other shocks that are present in the event window (relative exogeneity).\nTechnically, the latter condition requires the policy shock to have infinite\nvariance in the event window. Under these conditions, we establish the causal\nmeaning of the event study estimand corresponding to the regression coefficient\nand the consistency and asymptotic normality of the event study estimator.\nNotably, this standard linear regression estimator is robust to general forms\nof nonlinearity. We apply our results to Nakamura and Steinsson's (2018a)\nanalysis of the real economic effects of monetary policy, providing a simple\nempirical procedure to analyze the extent to which the standard event study\nestimator adequately estimates causal effects of interest.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.15667v4"
    },
    {
        "title": "Testing for Restricted Stochastic Dominance under Survey Nonresponse\n  with Panel Data: Theory and an Evaluation of Poverty in Australia",
        "authors": [
            "Rami V. Tabri",
            "Mathew J. Elias"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper lays the groundwork for a unifying approach to stochastic\ndominance testing under survey nonresponse that integrates the partial\nidentification approach to incomplete data and design-based inference for\ncomplex survey data. We propose a novel inference procedure for restricted\n$s$th-order stochastic dominance, tailored to accommodate a broad spectrum of\nnonresponse assumptions. The method uses pseudo-empirical likelihood to\nformulate the test statistic and compares it to a critical value from the\nchi-squared distribution with one degree of freedom. We detail the procedure's\nasymptotic properties under both null and alternative hypotheses, establishing\nits uniform validity under the null and consistency against various\nalternatives. Using the Household, Income and Labour Dynamics in Australia\nsurvey, we demonstrate the procedure's utility in a sensitivity analysis of\ntemporal poverty comparisons among Australian households.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.15702v1"
    },
    {
        "title": "Efficient two-sample instrumental variable estimators with change points\n  and near-weak identification",
        "authors": [
            "Bertille Antoine",
            "Otilia Boldea",
            "Niccolo Zaccaria"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We consider estimation and inference in a linear model with endogenous\nregressors where the parameters of interest change across two samples. If the\nfirst-stage is common, we show how to use this information to obtain more\nefficient two-sample GMM estimators than the standard split-sample GMM, even in\nthe presence of near-weak instruments. We also propose two tests to detect\nchange points in the parameters of interest, depending on whether the\nfirst-stage is common or not. We derive the limiting distribution of these\ntests and show that they have non-trivial power even under weaker and possibly\ntime-varying identification patterns. The finite sample properties of our\nproposed estimators and testing procedures are illustrated in a series of\nMonte-Carlo experiments, and in an application to the open-economy New\nKeynesian Phillips curve. Our empirical analysis using US data provides strong\nsupport for a New Keynesian Phillips curve with incomplete pass-through and\nreveals important time variation in the relationship between inflation and\nexchange rate pass-through.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.17056v1"
    },
    {
        "title": "Forecast Relative Error Decomposition",
        "authors": [
            "Christian Gourieroux",
            "Quinlan Lee"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We introduce a class of relative error decomposition measures that are\nwell-suited for the analysis of shocks in nonlinear dynamic models. They\ninclude the Forecast Relative Error Decomposition (FRED), Forecast Error\nKullback Decomposition (FEKD) and Forecast Error Laplace Decomposition (FELD).\nThese measures are favourable over the traditional Forecast Error Variance\nDecomposition (FEVD) because they account for nonlinear dependence in both a\nserial and cross-sectional sense. This is illustrated by applications to\ndynamic models for qualitative data, count data, stochastic volatility and\ncyberrisk.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.17708v1"
    },
    {
        "title": "A Note on Identification of Match Fixed Effects as Interpretable\n  Unobserved Match Affinity",
        "authors": [
            "Suguru Otani",
            "Tohya Sugano"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We highlight that match fixed effects, represented by the coefficients of\ninteraction terms involving dummy variables for two elements, lack\nidentification without specific restrictions on parameters. Consequently, the\ncoefficients typically reported as relative match fixed effects by statistical\nsoftware are not interpretable. To address this, we establish normalization\nconditions that enable identification of match fixed effect parameters as\ninterpretable indicators of unobserved match affinity, facilitating comparisons\namong observed matches. Using data from middle school students in the 2007\nTrends in International Mathematics and Science Study (TIMSS), we highlight the\ndistribution of comparable match fixed effects within a specific school.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.18913v3"
    },
    {
        "title": "Factor multivariate stochastic volatility models of high dimension",
        "authors": [
            "Benjamin Poignard",
            "Manabu Asai"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Building upon the pertinence of the factor decomposition to break the curse\nof dimensionality inherent to multivariate volatility processes, we develop a\nfactor model-based multivariate stochastic volatility (fMSV) framework that\nrelies on two viewpoints: sparse approximate factor model and sparse factor\nloading matrix. We propose a two-stage estimation procedure for the fMSV model:\nthe first stage obtains the estimators of the factor model, and the second\nstage estimates the MSV part using the estimated common factor variables. We\nderive the asymptotic properties of the estimators. Simulated experiments are\nperformed to assess the forecasting performances of the covariance matrices.\nThe empirical analysis based on vectors of asset returns illustrates that the\nforecasting performances of the fMSV models outperforms competing conditional\ncovariance models.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19033v1"
    },
    {
        "title": "Three Scores and 15 Years (1948-2023) of Rao's Score Test: A Brief\n  History",
        "authors": [
            "Anil K. Bera",
            "Yannis Bilias"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Rao (1948) introduced the score test statistic as an alternative to the\nlikelihood ratio and Wald test statistics. In spite of the optimality\nproperties of the score statistic shown in Rao and Poti (1946), the Rao score\n(RS) test remained unnoticed for almost 20 years. Today, the RS test is part of\nthe ``Holy Trinity'' of hypothesis testing and has found its place in the\nStatistics and Econometrics textbooks and related software. Reviewing the\nhistory of the RS test we note that remarkable test statistics proposed in the\nliterature earlier or around the time of Rao (1948) mostly from intuition, such\nas Pearson (1900) goodness-fit-test, Moran (1948) I test for spatial dependence\nand Durbin and Watson (1950) test for serial correlation, can be given RS test\nstatistic interpretation. At the same time, recent developments in the robust\nhypothesis testing under certain forms of misspecification, make the RS test an\nactive area of research in Statistics and Econometrics. From our brief account\nof the history the RS test we conclude that its impact in science goes far\nbeyond its calendar starting point with promising future research activities\nfor many years to come.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19956v3"
    },
    {
        "title": "How do financial variables impact public debt growth in China? An\n  empirical study based on Markov regime-switching model",
        "authors": [
            "Tianbao Zhou",
            "Zhixin Liu",
            "Yingying Xu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The deep financial turmoil in China caused by the COVID-19 pandemic has\nexacerbated fiscal shocks and soaring public debt levels, which raises concerns\nabout the stability and sustainability of China's public debt growth in the\nfuture. This paper employs the Markov regime-switching model with time-varying\ntransition probability (TVTP-MS) to investigate the growth pattern of China's\npublic debt and the impact of financial variables such as credit, house prices\nand stock prices on the growth of public debt. We identify two distinct regimes\nof China's public debt, i.e., the surge regime with high growth rate and high\nvolatility and the steady regime with low growth rate and low volatility. The\nmain results are twofold. On the one hand, an increase in the growth rate of\nthe financial variables helps to moderate the growth rate of public debt,\nwhereas the effects differ between the two regimes. More specifically, the\nimpacts of credit and house prices are significant in the surge regime, whereas\nstock prices affect public debt growth significantly in the steady regime. On\nthe other hand, a higher growth rate of financial variables also increases the\nprobability of public debt either staying in or switching to the steady regime.\nThese findings highlight the necessity of aligning financial adjustments with\nthe prevailing public debt regime when developing sustainable fiscal policies.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.02183v1"
    },
    {
        "title": "Conditional Forecasts in Large Bayesian VARs with Multiple Equality and\n  Inequality Constraints",
        "authors": [
            "Joshua C. C. Chan",
            "Davide Pettenuzzo",
            "Aubrey Poon",
            "Dan Zhu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Conditional forecasts, i.e. projections of a set of variables of interest on\nthe future paths of some other variables, are used routinely by empirical\nmacroeconomists in a number of applied settings. In spite of this, the existing\nalgorithms used to generate conditional forecasts tend to be very\ncomputationally intensive, especially when working with large Vector\nAutoregressions or when multiple linear equality and inequality constraints are\nimposed at once. We introduce a novel precision-based sampler that is fast,\nscales well, and yields conditional forecasts from linear equality and\ninequality constraints. We show in a simulation study that the proposed method\nproduces forecasts that are identical to those from the existing algorithms but\nin a fraction of the time. We then illustrate the performance of our method in\na large Bayesian Vector Autoregression where we simultaneously impose a mix of\nlinear equality and inequality constraints on the future trajectories of key US\nmacroeconomic indicators over the 2020--2022 period.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.02262v1"
    },
    {
        "title": "Wild inference for wild SVARs with application to\n  heteroscedasticity-based IV",
        "authors": [
            "Bulat Gafarov",
            "Madina Karamysheva",
            "Andrey Polbin",
            "Anton Skrobotov"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Structural vector autoregressions are used to compute impulse response\nfunctions (IRF) for persistent data. Existing multiple-parameter inference\nrequires cumbersome pretesting for unit roots, cointegration, and trends with\nsubsequent stationarization. To avoid pretesting, we propose a novel\n\\emph{dependent wild bootstrap} procedure for simultaneous inference on IRF\nusing local projections (LP) estimated in levels in possibly\n\\emph{nonstationary} and \\emph{heteroscedastic} SVARs. The bootstrap also\nallows efficient smoothing of LP estimates.\n  We study IRF to US monetary policy identified using FOMC meetings count as an\ninstrument for heteroscedasticity of monetary shocks. We validate our method\nusing DSGE model simulations and alternative SVAR methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.03265v2"
    },
    {
        "title": "Overeducation under different macroeconomic conditions: The case of\n  Spanish university graduates",
        "authors": [
            "Maite Blázquez Cuesta",
            "Marco A. Pérez Navarro",
            "Rocío Sánchez-Mangas"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper examines the incidence and persistence of overeducation in the\nearly careers of Spanish university graduates. We investigate the role played\nby the business cycle and field of study and their interaction in shaping both\nphenomena. We also analyse the relevance of specific types of knowledge and\nskills as driving factors in reducing overeducation risk. We use data from the\nSurvey on the Labour Insertion of University Graduates (EILU) conducted by the\nSpanish National Statistics Institute in 2014 and 2019. The survey collects\nrich information on cohorts that graduated in the 2009/2010 and 2014/2015\nacademic years during the Great Recession and the subsequent economic recovery,\nrespectively. Our results show, first, the relevance of the economic scenario\nwhen graduates enter the labour market. Graduation during a recession increased\novereducation risk and persistence. Second, a clear heterogeneous pattern\noccurs across fields of study, with health sciences graduates displaying better\nperformance in terms of both overeducation incidence and persistence and less\nimpact of the business cycle. Third, we find evidence that some transversal\nskills (language, IT, management) can help to reduce overeducation risk in the\nabsence of specific knowledge required for the job, thus indicating some kind\nof compensatory role. Finally, our findings have important policy implications.\nOvereducation, and more importantly overeducation persistence, imply a\nnon-neglectable misallocation of resources. Therefore, policymakers need to\naddress this issue in the design of education and labour market policies.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.04437v1"
    },
    {
        "title": "Learning control variables and instruments for causal analysis in\n  observational data",
        "authors": [
            "Nicolas Apfel",
            "Julia Hatamyar",
            "Martin Huber",
            "Jannis Kueck"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This study introduces a data-driven, machine learning-based method to detect\nsuitable control variables and instruments for assessing the causal effect of a\ntreatment on an outcome in observational data, if they exist. Our approach\ntests the joint existence of instruments, which are associated with the\ntreatment but not directly with the outcome (at least conditional on\nobservables), and suitable control variables, conditional on which the\ntreatment is exogenous, and learns the partition of instruments and control\nvariables from the observed data. The detection of sets of instruments and\ncontrol variables relies on the condition that proper instruments are\nconditionally independent of the outcome given the treatment and suitable\ncontrol variables. We establish the consistency of our method for detecting\ncontrol variables and instruments under certain regularity conditions,\ninvestigate the finite sample performance through a simulation study, and\nprovide an empirical application to labor market data from the Job Corps study.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.04448v1"
    },
    {
        "title": "Methodology for Calculating CO2 Absorption by Tree Planting for Greening\n  Projects",
        "authors": [
            "Kento Ichii",
            "Toshiki Muraoka",
            "Nobumichi Shinohara",
            "Shunsuke Managi",
            "Shutaro Takeda"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In order to explore the possibility of carbon credits for greening projects,\nwhich play an important role in climate change mitigation, this paper examines\na formula for estimating the amount of carbon fixation for greening activities\nin urban areas through tree planting. The usefulness of the formula studied was\nexamined by conducting calculations based on actual data through measurements\nmade by on-site surveys of a greening companie. A series of calculation results\nsuggest that this formula may be useful. Recognizing carbon credits for green\nbusinesses for the carbon sequestration of their projects is an important\nincentive not only as part of environmental improvement and climate change\naction, but also to improve the health and well-being of local communities and\nto generate economic benefits. This study is a pioneering exploration of the\nmethodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.05596v1"
    },
    {
        "title": "Femicide Laws, Unilateral Divorce, and Abortion Decriminalization Fail\n  to Stop Women's Killings in Mexico",
        "authors": [
            "Roxana Gutiérrez-Romero"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper evaluates the effectiveness of femicide laws in combating\ngender-based killings of women, a major cause of premature female mortality\nglobally. Focusing on Mexico, a pioneer in adopting such legislation, the paper\nleverages variations in the enactment of femicide laws and associated prison\nsentences across states. Using the difference-in-difference estimator, the\nanalysis reveals that these laws have not significantly affected the incidence\nof femicides, homicides of women, or reports of women who have disappeared.\nThese findings remain robust even when accounting for differences in prison\nsentencing, whether states also implemented unilateral divorce laws, or\ndecriminalized abortion alongside femicide legislation. The results suggest\nthat legislative measures are insufficient to address violence against women in\nsettings where impunity prevails.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.06722v1"
    },
    {
        "title": "Causes and Electoral Consequences of Political Assassinations: The Role\n  of Organized Crime in Mexico",
        "authors": [
            "Roxana Gutiérrez-Romero",
            "Nayely Iturbe"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Mexico has experienced a notable surge in assassinations of political\ncandidates and mayors. This article argues that these killings are largely\ndriven by organized crime, aiming to influence candidate selection, control\nlocal governments for rent-seeking, and retaliate against government\ncrackdowns. Using a new dataset of political assassinations in Mexico from 2000\nto 2021 and instrumental variables, we address endogeneity concerns in the\nlocation and timing of government crackdowns. Our instruments include\nhistorical Chinese immigration patterns linked to opium cultivation in Mexico,\nlocal corn prices, and U.S. illicit drug prices. The findings reveal that\ncandidates in municipalities near oil pipelines face an increased risk of\nassassination due to drug trafficking organizations expanding into oil theft,\nparticularly during elections and fuel price hikes. Government arrests or\nkillings of organized crime members trigger retaliatory violence, further\nendangering incumbent mayors. This political violence has a negligible impact\non voter turnout, as it targets politicians rather than voters. However, voter\nturnout increases in areas where authorities disrupt drug smuggling, raising\nthe chances of the local party being re-elected. These results offer new\ninsights into how criminal groups attempt to capture local governments and the\nimplications for democracy under criminal governance.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.06733v1"
    },
    {
        "title": "Dealing with idiosyncratic cross-correlation when constructing\n  confidence regions for PC factors",
        "authors": [
            "Diego Fresoli",
            "Pilar Poncela",
            "Esther Ruiz"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, we propose a computationally simple estimator of the\nasymptotic covariance matrix of the Principal Components (PC) factors valid in\nthe presence of cross-correlated idiosyncratic components. The proposed\nestimator of the asymptotic Mean Square Error (MSE) of PC factors is based on\nadaptive thresholding the sample covariances of the id iosyncratic residuals\nwith the threshold based on their individual variances. We compare the nite\nsample performance of condence regions for the PC factors obtained using the\nproposed asymptotic MSE with those of available extant asymptotic and bootstrap\nregions and show that the former beats all alternative procedures for a wide\nvariety of idiosyncratic cross-correlation structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.06883v1"
    },
    {
        "title": "The Hidden Subsidy of the Affordable Care Act",
        "authors": [
            "Liam Sigaud",
            "Markus Bjoerkheim",
            "Vitor Melo"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Under the ACA, the federal government paid a substantially larger share of\nmedical costs of newly eligible Medicaid enrollees than previously eligible\nones. States could save up to 100% of their per-enrollee costs by reclassifying\noriginal enrollees into the newly eligible group. We examine whether this\nfiscal incentive changed states' enrollment practices. We find that Medicaid\nexpansion caused large declines in the number of beneficiaries enrolled in the\noriginal Medicaid population, suggesting widespread reclassifications. In 2019\nalone, this phenomenon affected 4.4 million Medicaid enrollees at a federal\ncost of $8.3 billion. Our results imply that reclassifications inflated the\nfederal cost of Medicaid expansion by 18.2%.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.07217v1"
    },
    {
        "title": "R. A. Fisher's Exact Test Revisited",
        "authors": [
            "Martin Mugnier"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This note provides a conceptual clarification of Ronald Aylmer Fisher's\n(1935) pioneering exact test in the context of the Lady Testing Tea experiment.\nIt unveils a critical implicit assumption in Fisher's calibration: the taster\nminimizes expected misclassification given fixed probabilistic information.\nWithout similar assumptions or an explicit alternative hypothesis, the\nrationale behind Fisher's specification of the rejection region remains\nunclear.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.07251v1"
    },
    {
        "title": "Production function estimation using subjective expectations data",
        "authors": [
            "Agnes Norris Keiller",
            "Aureo de Paula",
            "John Van Reenen"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Standard methods for estimating production functions in the Olley and Pakes\n(1996) tradition require assumptions on input choices. We introduce a new\nmethod that exploits (increasingly available) data on a firm's expectations of\nits future output and inputs that allows us to obtain consistent production\nfunction parameter estimates while relaxing these input demand assumptions. In\ncontrast to dynamic panel methods, our proposed estimator can be implemented on\nvery short panels (including a single cross-section), and Monte Carlo\nsimulations show it outperforms alternative estimators when firms' material\ninput choices are subject to optimization error. Implementing a range of\nproduction function estimators on UK data, we find our proposed estimator\nyields results that are either similar to or more credible than commonly-used\nalternatives. These differences are larger in industries where material inputs\nappear harder to optimize. We show that TFP implied by our proposed estimator\nis more strongly associated with future jobs growth than existing methods,\nsuggesting that failing to adequately account for input endogeneity may\nunderestimate the degree of dynamic reallocation in the economy.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.07988v1"
    },
    {
        "title": "Comparative analysis of Mixed-Data Sampling (MIDAS) model compared to\n  Lag-Llama model for inflation nowcasting",
        "authors": [
            "Adam Bahelka",
            "Harmen de Weerd"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Inflation is one of the most important economic indicators closely watched by\nboth public institutions and private agents. This study compares the\nperformance of a traditional econometric model, Mixed Data Sampling regression,\nwith one of the newest developments from the field of Artificial Intelligence,\na foundational time series forecasting model based on a Long short-term memory\nneural network called Lag-Llama, in their ability to nowcast the Harmonized\nIndex of Consumer Prices in the Euro area. Two models were compared and\nassessed whether the Lag-Llama can outperform the MIDAS regression, ensuring\nthat the MIDAS regression is evaluated under the best-case scenario using a\ndataset spanning from 2010 to 2022. The following metrics were used to evaluate\nthe models: Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE),\nMean Squared Error (MSE), correlation with the target, R-squared and adjusted\nR-squared. The results show better performance of the pre-trained Lag-Llama\nacross all metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.08510v1"
    },
    {
        "title": "An Introduction to Causal Discovery",
        "authors": [
            "Martin Huber"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In social sciences and economics, causal inference traditionally focuses on\nassessing the impact of predefined treatments (or interventions) on predefined\noutcomes, such as the effect of education programs on earnings. Causal\ndiscovery, in contrast, aims to uncover causal relationships among multiple\nvariables in a data-driven manner, by investigating statistical associations\nrather than relying on predefined causal structures. This approach, more common\nin computer science, seeks to understand causality in an entire system of\nvariables, which can be visualized by causal graphs. This survey provides an\nintroduction to key concepts, algorithms, and applications of causal discovery\nfrom the perspectives of economics and social sciences. It covers fundamental\nconcepts like d-separation, causal faithfulness, and Markov equivalence,\nsketches various algorithms for causal discovery, and discusses the back-door\nand front-door criteria for identifying causal effects. The survey concludes\nwith more specific examples of causal discovery, e.g. for learning all\nvariables that directly affect an outcome of interest and/or testing\nidentification of causal effects in observational data.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.08602v1"
    },
    {
        "title": "A Short Note on Event-Study Synthetic Difference-in-Differences\n  Estimators",
        "authors": [
            "Diego Ciccia"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  I propose an event study extension of Synthetic Difference-in-Differences\n(SDID) estimators. I show that, in simple and staggered adoption designs,\nestimators from Arkhangelsky et al. (2021) can be disaggregated into dynamic\ntreatment effect estimators, comparing the lagged outcome differentials of\ntreated and synthetic controls to their pre-treatment average. Estimators\npresented in this note can be computed using the sdid_event Stata package.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.09565v2"
    },
    {
        "title": "Regularizing stock return covariance matrices via multiple testing of\n  correlations",
        "authors": [
            "Richard Luger"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper develops a large-scale inference approach for the regularization\nof stock return covariance matrices. The framework allows for the presence of\nheavy tails and multivariate GARCH-type effects of unknown form among the stock\nreturns. The approach involves simultaneous testing of all pairwise\ncorrelations, followed by setting non-statistically significant elements to\nzero. This adaptive thresholding is achieved through sign-based Monte Carlo\nresampling within multiple testing procedures, controlling either the\ntraditional familywise error rate, a generalized familywise error rate, or the\nfalse discovery proportion. Subsequent shrinkage ensures that the final\ncovariance matrix estimate is positive definite and well-conditioned while\npreserving the achieved sparsity. Compared to alternative estimators, this new\nregularization method demonstrates strong performance in simulation experiments\nand real portfolio optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.09696v1"
    },
    {
        "title": "The Dynamic, the Static, and the Weak factor models and the analysis of\n  high-dimensional time series",
        "authors": [
            "Matteo Barigozzi",
            "Marc Hallin"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Several fundamental and closely interconnected issues related to factor\nmodels are reviewed and discussed: dynamic versus static loadings, rate-strong\nversus rate-weak factors, the concept of weakly common component recently\nintroduced by Gersing et al. (2023), the irrelevance of cross-sectional\nordering and the assumption of cross-sectional exchangeability, and the problem\nof undetected strong factors.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.10653v1"
    },
    {
        "title": "Nowcasting R&D Expenditures: A Machine Learning Approach",
        "authors": [
            "Atin Aboutorabi",
            "Gaétan de Rassenfosse"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Macroeconomic data are crucial for monitoring countries' performance and\ndriving policy. However, traditional data acquisition processes are slow,\nsubject to delays, and performed at a low frequency. We address this\n'ragged-edge' problem with a two-step framework. The first step is a supervised\nlearning model predicting observed low-frequency figures. We propose a\nneural-network-based nowcasting model that exploits mixed-frequency,\nhigh-dimensional data. The second step uses the elasticities derived from the\nprevious step to interpolate unobserved high-frequency figures. We apply our\nmethod to nowcast countries' yearly research and development (R&D) expenditure\nseries. These series are collected through infrequent surveys, making them\nideal candidates for this task. We exploit a range of predictors, chiefly\nInternet search volume data, and document the relevance of these data in\nimproving out-of-sample predictions. Furthermore, we leverage the high\nfrequency of our data to derive monthly estimates of R&D expenditures, which\nare currently unobserved. We compare our results with those obtained from the\nclassical regression-based and the sparse temporal disaggregation methods.\nFinally, we validate our results by reporting a strong correlation with monthly\nR&D employment data.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.11765v1"
    },
    {
        "title": "Conduct Parameter Estimation in Homogeneous Goods Markets with\n  Equilibrium Existence and Uniqueness Conditions: The Case of Log-linear\n  Specification",
        "authors": [
            "Yuri Matsumura",
            "Suguru Otani"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a constrained generalized method of moments estimator (GMM)\nincorporating theoretical conditions for the unique existence of equilibrium\nprices for estimating conduct parameters in a log-linear model with homogeneous\ngoods markets. First, we derive such conditions. Second, Monte Carlo\nsimulations confirm that in a log-linear model, incorporating the conditions\nresolves the problems of implausibly low or negative values of conduct\nparameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.12422v1"
    },
    {
        "title": "Revisiting Randomization with the Cube Method",
        "authors": [
            "Laurent Davezies",
            "Guillaume Hollard",
            "Pedro Vergara Merino"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a novel randomization approach for randomized controlled trials\n(RCTs), named the cube method. The cube method allows for the selection of\nbalanced samples across various covariate types, ensuring consistent adherence\nto balance tests and, whence, substantial precision gains when estimating\ntreatment effects. We establish several statistical properties for the\npopulation and sample average treatment effects (PATE and SATE, respectively)\nunder randomization using the cube method. The relevance of the cube method is\nparticularly striking when comparing the behavior of prevailing methods\nemployed for treatment allocation when the number of covariates to balance is\nincreasing. We formally derive and compare bounds of balancing adjustments\ndepending on the number of units $n$ and the number of covariates $p$ and show\nthat our randomization approach outperforms methods proposed in the literature\nwhen $p$ is large and $p/n$ tends to 0. We run simulation studies to illustrate\nthe substantial gains from the cube method for a large set of covariates.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.13613v1"
    },
    {
        "title": "Predicting the Distribution of Treatment Effects: A Covariate-Adjustment\n  Approach",
        "authors": [
            "Bruno Fava"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Important questions for impact evaluation require knowledge not only of\naverage effects, but of the distribution of treatment effects. What proportion\nof people are harmed? Does a policy help many by a little? Or a few by a lot?\nThe inability to observe individual counterfactuals makes these empirical\nquestions challenging. I propose an approach to inference on points of the\ndistribution of treatment effects by incorporating predicted counterfactuals\nthrough covariate adjustment. I show that finite-sample inference is valid\nunder weak assumptions, for example, when data come from a Randomized\nControlled Trial (RCT), and that large-sample inference is asymptotically exact\nunder suitable conditions. Finally, I revisit five RCTs in microcredit where\naverage effects are not statistically significant and find evidence of both\npositive and negative treatment effects in household income. On average across\nstudies, at least 13.6% of households benefited, and 12.5% were negatively\naffected.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.14635v2"
    },
    {
        "title": "Leveraging Uniformization and Sparsity for Computation of Continuous\n  Time Dynamic Discrete Choice Games",
        "authors": [
            "Jason R. Blevins"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Continuous-time formulations of dynamic discrete choice games offer notable\ncomputational advantages, particularly in modeling strategic interactions in\noligopolistic markets. This paper extends these benefits by addressing\ncomputational challenges in order to improve model solution and estimation. We\nfirst establish new results on the rates of convergence of the value iteration,\npolicy evaluation, and relative value iteration operators in the model, holding\nfixed player beliefs. Next, we introduce a new representation of the value\nfunction in the model based on uniformization -- a technique used in the\nanalysis of continuous time Markov chains -- which allows us to draw a direct\nanalogy to discrete time models. Furthermore, we show that uniformization also\nleads to a stable method to compute the matrix exponential, an operator\nappearing in the model's log likelihood function when only discrete time\n\"snapshot\" data are available. We also develop a new algorithm that\nconcurrently computes the matrix exponential and its derivatives with respect\nto model parameters, enhancing computational efficiency. By leveraging the\ninherent sparsity of the model's intensity matrix, combined with sparse matrix\ntechniques and precomputed addresses, we show how to significantly speed up\ncomputations. These strategies allow researchers to estimate more sophisticated\nand realistic models of strategic interactions and policy impacts in empirical\nindustrial organization.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.14914v1"
    },
    {
        "title": "Big Data Analytics-Enabled Dynamic Capabilities and Market Performance:\n  Examining the Roles of Marketing Ambidexterity and Competitor Pressure",
        "authors": [
            "Gulfam Haider",
            "Laiba Zubair",
            "Aman Saleem"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This study, rooted in dynamic capability theory and the developing era of Big\nData Analytics, explores the transformative effect of BDA EDCs on marketing.\nAmbidexterity and firms market performance in the textile sector of Pakistans\ncities. Specifically, focusing on the firms who directly deal with customers,\ninvestigates the nuanced role of BDA EDCs in textile retail firms potential to\nnavigate market dynamics. Emphasizing the exploitation component of marketing\nambidexterity, the study investigated the mediating function of marketing\nambidexterity and the moderating influence of competitive pressure. Using a\nsurvey questionnaire, the study targets key choice makers in textile firms of\nFaisalabad, Chiniot and Lahore, Pakistan. The PLS-SEM model was employed as an\nanalytical technique, allows for a full examination of the complicated\nrelations between BDA EDCs, marketing ambidexterity, rival pressure, and market\nperformance. The study Predicting a positive impact of Big Data on marketing\nambidexterity, with a specific emphasis on exploitation. The study expects this\nexploitation-orientated marketing ambidexterity to significantly enhance the\nfirms market performance. This research contributes to the existing literature\non dynamic capabilities-based frameworks from the perspective of the retail\nsegment of textile industry. The study emphasizes the role of BDA-EDCs in the\nretail sector, imparting insights into the direct and indirect results of BDA\nEDCs on market performance inside the retail area. The study s novelty lies in\nits contextualization of BDA-EDCs in the textile zone of Faisalabad, Lahore and\nChiniot, providing a unique perspective on the effect of BDA on marketing\nambidexterity and market performance in firms. Methodologically, the study uses\nnumerous samples of retail sectors to make sure broader universality,\ncontributing realistic insights.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.15522v1"
    },
    {
        "title": "Bayesian modelling of VAR precision matrices using stochastic block\n  networks",
        "authors": [
            "Florian Huber",
            "Gary Koop",
            "Massimiliano Marcellino",
            "Tobias Scheckel"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Commonly used priors for Vector Autoregressions (VARs) induce shrinkage on\nthe autoregressive coefficients. Introducing shrinkage on the error covariance\nmatrix is sometimes done but, in the vast majority of cases, without\nconsidering the network structure of the shocks and by placing the prior on the\nlower Cholesky factor of the precision matrix. In this paper, we propose a\nprior on the VAR error precision matrix directly. Our prior, which resembles a\nstandard spike and slab prior, models variable inclusion probabilities through\na stochastic block model that clusters shocks into groups. Within groups, the\nprobability of having relations across group members is higher (inducing less\nsparsity) whereas relations across groups imply a lower probability that\nmembers of each group are conditionally related. We show in simulations that\nour approach recovers the true network structure well. Using a US macroeconomic\ndata set, we illustrate how our approach can be used to cluster shocks together\nand that this feature leads to improved density forecasts.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.16349v1"
    },
    {
        "title": "Identification and inference of outcome conditioned partial effects of\n  general interventions",
        "authors": [
            "Zhengyu Zhang",
            "Zequn Jin",
            "Lihua Lin"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper proposes a new class of distributional causal quantities, referred\nto as the \\textit{outcome conditioned partial policy effects} (OCPPEs), to\nmeasure the \\textit{average} effect of a general counterfactual intervention of\na target covariate on the individuals in different quantile ranges of the\noutcome distribution.\n  The OCPPE approach is valuable in several aspects: (i) Unlike the\nunconditional quantile partial effect (UQPE) that is not $\\sqrt{n}$-estimable,\nan OCPPE is $\\sqrt{n}$-estimable. Analysts can use it to capture heterogeneity\nacross the unconditional distribution of $Y$ as well as obtain accurate\nestimation of the aggregated effect at the upper and lower tails of $Y$. (ii)\nThe semiparametric efficiency bound for an OCPPE is explicitly derived. (iii)\nWe propose an efficient debiased estimator for OCPPE, and provide feasible\nuniform inference procedures for the OCPPE process. (iv) The efficient doubly\nrobust score for an OCPPE can be used to optimize infinitesimal nudges to a\ncontinuous treatment by maximizing a quantile specific Empirical Welfare\nfunction. We illustrate the method by analyzing how anti-smoking policies\nimpact low percentiles of live infants' birthweights.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.16950v1"
    },
    {
        "title": "Starting Small: Prioritizing Safety over Efficacy in Randomized\n  Experiments Using the Exact Finite Sample Likelihood",
        "authors": [
            "Neil Christy",
            "A. E. Kowalski"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We use the exact finite sample likelihood and statistical decision theory to\nanswer questions of ``why?'' and ``what should you have done?'' using data from\nrandomized experiments and a utility function that prioritizes safety over\nefficacy. We propose a finite sample Bayesian decision rule and a finite sample\nmaximum likelihood decision rule. We show that in finite samples from 2 to 50,\nit is possible for these rules to achieve better performance according to\nestablished maximin and maximum regret criteria than a rule based on the\nBoole-Frechet-Hoeffding bounds. We also propose a finite sample maximum\nlikelihood criterion. We apply our rules and criterion to an actual clinical\ntrial that yielded a promising estimate of efficacy, and our results point to\nsafety as a reason for why results were mixed in subsequent trials.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.18206v1"
    },
    {
        "title": "Using Total Margin of Error to Account for Non-Sampling Error in\n  Election Polls: The Case of Nonresponse",
        "authors": [
            "Jeff Dominitz",
            "Charles F. Manski"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The potential impact of non-sampling errors on election polls is well known,\nbut measurement has focused on the margin of sampling error. Survey\nstatisticians have long recommended measurement of total survey error by mean\nsquare error (MSE), which jointly measures sampling and non-sampling errors. We\nthink it reasonable to use the square root of maximum MSE to measure the total\nmargin of error (TME). Measurement of TME should encompass both sampling error\nand all forms of non-sampling error. We suggest that measurement of TME should\nbe a standard feature in the reporting of polls. To provide a clear\nillustration, and because we believe the exceedingly low response rates\ncommonly obtained by election polls to be a particularly worrisome source of\npotential error, we demonstrate how to measure the potential impact of\nnonresponse using the concept of TME. We first show how to measure TME when a\npollster lacks any knowledge of the candidate preferences of nonrespondents. We\nthen extend the analysis to settings where the pollster has partial knowledge\nthat bounds the preferences of non-respondents. In each setting, we derive a\nsimple poll estimate that approximately minimizes TME, a midpoint estimate, and\ncompare it to a conventional poll estimate.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19339v3"
    },
    {
        "title": "Heterogeneous Grouping Structures in Panel Data",
        "authors": [
            "Katerina Chrysikou",
            "George Kapetanios"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper we examine the existence of heterogeneity within a group, in\npanels with latent grouping structure. The assumption of within group\nhomogeneity is prevalent in this literature, implying that the formation of\ngroups alleviates cross-sectional heterogeneity, regardless of the prior\nknowledge of groups. While the latter hypothesis makes inference powerful, it\ncan be often restrictive. We allow for models with richer heterogeneity that\ncan be found both in the cross-section and within a group, without imposing the\nsimple assumption that all groups must be heterogeneous. We further contribute\nto the method proposed by \\cite{su2016identifying}, by showing that the model\nparameters can be consistently estimated and the groups, while unknown, can be\nidentifiable in the presence of different types of heterogeneity. Within the\nsame framework we consider the validity of assuming both cross-sectional and\nwithin group homogeneity, using testing procedures. Simulations demonstrate\ngood finite-sample performance of the approach in both classification and\nestimation, while empirical applications across several datasets provide\nevidence of multiple clusters, as well as reject the hypothesis of within group\nhomogeneity.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19509v1"
    },
    {
        "title": "On the power properties of inference for parameters with interval\n  identified sets",
        "authors": [
            "Federico A. Bugni",
            "Mengsi Gao",
            "Filip Obradovic",
            "Amilcar Velez"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper studies the power properties of confidence intervals (CIs) for a\npartially-identified parameter of interest with an interval identified set. We\nassume the researcher has bounds estimators to construct the CIs proposed by\nStoye (2009), referred to as CI1, CI2, and CI3. We also assume that these\nestimators are \"ordered\": the lower bound estimator is less than or equal to\nthe upper bound estimator.\n  Under these conditions, we establish two results. First, we show that CI1 and\nCI2 are equally powerful, and both dominate CI3. Second, we consider a\nfavorable situation in which there are two possible bounds estimators to\nconstruct these CIs, and one is more efficient than the other. One would expect\nthat the more efficient bounds estimator yields more powerful inference. We\nprove that this desirable result holds for CI1 and CI2, but not necessarily for\nCI3.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.20386v2"
    },
    {
        "title": "Methodological Foundations of Modern Causal Inference in Social Science\n  Research",
        "authors": [
            "Guanghui Pan"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper serves as a literature review of methodology concerning the\n(modern) causal inference methods to address the causal estimand with\nobservational/survey data that have been or will be used in social science\nresearch. Mainly, this paper is divided into two parts: inference from\nstatistical estimand for the causal estimand, in which we reviewed the\nassumptions for causal identification and the methodological strategies\naddressing the problems if some of the assumptions are violated. We also\ndiscuss the asymptotical analysis concerning the measure from the observational\ndata to the theoretical measure and replicate the deduction of the\nefficient/doubly robust average treatment effect estimator, which is commonly\nused in current social science analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.00032v1"
    },
    {
        "title": "Identification and Inference for Synthetic Control Methods with\n  Spillover Effects: Estimating the Economic Cost of the Sudan Split",
        "authors": [
            "Shosei Sakaguchi",
            "Hayato Tagawa"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The synthetic control method (SCM) is widely used for causal inference with\npanel data, particularly when there are few treated units. SCM assumes the\nstable unit treatment value assumption (SUTVA), which posits that potential\noutcomes are unaffected by the treatment status of other units. However,\ninterventions often impact not only treated units but also untreated units,\nknown as spillover effects. This study introduces a novel panel data method\nthat extends SCM to allow for spillover effects and estimate both treatment and\nspillover effects. This method leverages a spatial autoregressive panel data\nmodel to account for spillover effects. We also propose Bayesian inference\nmethods using Bayesian horseshoe priors for regularization. We apply the\nproposed method to two empirical studies: evaluating the effect of the\nCalifornia tobacco tax on consumption and estimating the economic impact of the\n2011 division of Sudan on GDP per capita.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.00291v2"
    },
    {
        "title": "Distributional Difference-in-Differences Models with Multiple Time\n  Periods: A Monte Carlo Analysis",
        "authors": [
            "Andrea Ciaccio"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Researchers are often interested in evaluating the impact of a policy on the\nentire (or specific parts of the) distribution of the outcome of interest. In\nthis paper, I provide a practical toolkit to recover the whole counterfactual\ndistribution of the untreated potential outcome for the treated group in\nnon-experimental settings with staggered treatment adoption by generalizing the\nexisting quantile treatment effects on the treated (QTT) estimator proposed by\nCallaway and Li (2019). Besides the QTT, I consider different approaches that\nanonymously summarize the quantiles of the distribution of the outcome of\ninterest (such as tests for stochastic dominance rankings) without relying on\nrank invariance assumptions. The finite-sample properties of the estimator\nproposed are analyzed via different Monte Carlo simulations. Despite being\nslightly biased for relatively small sample sizes, the proposed method's\nperformance increases substantially when the sample size increases.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.01208v1"
    },
    {
        "title": "Analysis of Factors Affecting the Entry of Foreign Direct Investment\n  into Indonesia (Case Study of Three Industrial Sectors in Indonesia)",
        "authors": [
            "Tracy Patricia Nindry Abigail Rolnmuch",
            "Yuhana Astuti"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The realization of FDI and DDI from January to December 2022 reached\nRp1,207.2 trillion. The largest FDI investment realization by sector was led by\nthe Basic Metal, Metal Goods, Non-Machinery, and Equipment Industry sector,\nfollowed by the Mining sector and the Electricity, Gas, and Water sector. The\nuneven amount of FDI investment realization in each industry and the impact of\nthe COVID-19 pandemic in Indonesia are the main issues addressed in this study.\nThis study aims to identify the factors that influence the entry of FDI into\nindustries in Indonesia and measure the extent of these factors' influence on\nthe entry of FDI. In this study, classical assumption tests and hypothesis\ntests are conducted to investigate whether the research model is robust enough\nto provide strategic options nationally. Moreover, this study uses the ordinary\nleast squares (OLS) method. The results show that the electricity factor does\nnot influence FDI inflows in the three industries. The Human Development Index\n(HDI) factor has a significant negative effect on FDI in the Mining Industry\nand a significant positive effect on FDI in the Basic Metal, Metal Goods,\nNon-Machinery, and Equipment Industries. However, HDI does not influence FDI in\nthe Electricity, Gas, and Water Industries in Indonesia.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.01985v1"
    },
    {
        "title": "Testing identifying assumptions in Tobit Models",
        "authors": [
            "Santiago Acerenza",
            "Otávio Bartalotti",
            "Federico Veneri"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper develops sharp testable implications for Tobit and IV-Tobit\nmodels' identifying assumptions: linear index specification, (joint) normality\nof latent errors, and treatment (instrument) exogeneity and relevance. The new\nsharp testable equalities can detect all possible observable violations of the\nidentifying conditions. We propose a testing procedure for the model's validity\nusing existing inference methods for intersection bounds. Simulation results\nsuggests proper size for large samples and that the test is powerful to detect\nlarge violation of the exogeneity assumption and violations in the error\nstructure. Finally, we review and propose new alternative paths to partially\nidentify the parameters of interest under less restrictive assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.02573v1"
    },
    {
        "title": "Robust Identification in Randomized Experiments with Noncompliance",
        "authors": [
            "Yi Cui",
            "Désiré Kédagni",
            "Huan Wu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper considers a robust identification of causal parameters in a\nrandomized experiment setting with noncompliance where the standard local\naverage treatment effect assumptions could be violated. Following Li,\nK\\'edagni, and Mourifi\\'e (2024), we propose a misspecification robust bound\nfor a real-valued vector of various causal parameters. We discuss\nidentification under two sets of weaker assumptions: random assignment and\nexclusion restriction (without monotonicity), and random assignment and\nmonotonicity (without exclusion restriction). We introduce two causal\nparameters: the local average treatment-controlled direct effect (LATCDE), and\nthe local average instrument-controlled direct effect (LAICDE). Under the\nrandom assignment and monotonicity assumptions, we derive sharp bounds on the\nlocal average treatment-controlled direct effects for the always-takers and\nnever-takers, respectively, and the total average controlled direct effect for\nthe compliers. Additionally, we show that the intent-to-treat effect can be\nexpressed as a convex weighted average of these three effects. Finally, we\napply our method on the proximity to college instrument and find that growing\nup near a four-year college increases the wage of never-takers (who represent\nmore than 70% of the population) by a range of 4.15% to 27.07%.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.03530v3"
    },
    {
        "title": "Robust Estimation of Regression Models with Potentially Endogenous\n  Outliers via a Modern Optimization Lens",
        "authors": [
            "Zhan Gao",
            "Hyungsik Roger Moon"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper addresses the robust estimation of linear regression models in the\npresence of potentially endogenous outliers. Through Monte Carlo simulations,\nwe demonstrate that existing $L_1$-regularized estimation methods, including\nthe Huber estimator and the least absolute deviation (LAD) estimator, exhibit\nsignificant bias when outliers are endogenous. Motivated by this finding, we\ninvestigate $L_0$-regularized estimation methods. We propose systematic\nheuristic algorithms, notably an iterative hard-thresholding algorithm and a\nlocal combinatorial search refinement, to solve the combinatorial optimization\nproblem of the \\(L_0\\)-regularized estimation efficiently. Our Monte Carlo\nsimulations yield two key results: (i) The local combinatorial search algorithm\nsubstantially improves solution quality compared to the initial\nprojection-based hard-thresholding algorithm while offering greater\ncomputational efficiency than directly solving the mixed integer optimization\nproblem. (ii) The $L_0$-regularized estimator demonstrates superior performance\nin terms of bias reduction, estimation accuracy, and out-of-sample prediction\nerrors compared to $L_1$-regularized alternatives. We illustrate the practical\nvalue of our method through an empirical application to stock return\nforecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.03930v1"
    },
    {
        "title": "Semiparametric Estimation of Individual Coefficients in a Dyadic Link\n  Formation Model Lacking Observable Characteristics",
        "authors": [
            "L. Sanna Stephan"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Dyadic network formation models have wide applicability in economic research,\nyet are difficult to estimate in the presence of individual specific effects\nand in the absence of distributional assumptions regarding the model noise\ncomponent. The availability of (continuously distributed) individual or link\ncharacteristics generally facilitates estimation. Yet, while data on social\nnetworks has recently become more abundant, the characteristics of the entities\ninvolved in the link may not be measured. Adapting the procedure of \\citet{KS},\nI propose to use network data alone in a semiparametric estimation of the\nindividual fixed effect coefficients, which carry the interpretation of the\nindividual relative popularity. This entails the possibility to anticipate how\na new-coming individual will connect in a pre-existing group. The estimator,\nneeded for its fast convergence, fails to implement the monotonicity assumption\nregarding the model noise component, thereby potentially reversing the order if\nthe fixed effect coefficients. This and other numerical issues can be\nconveniently tackled by my novel, data-driven way of normalising the fixed\neffects, which proves to outperform a conventional standardisation in many\ncases. I demonstrate that the normalised coefficients converge both at the same\nrate and to the same limiting distribution as if the true error distribution\nwas known. The cost of semiparametric estimation is thus purely computational,\nwhile the potential benefits are large whenever the errors have a strongly\nconvex or strongly concave distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04552v1"
    },
    {
        "title": "Vela: A Data-Driven Proposal for Joint Collaboration in Space\n  Exploration",
        "authors": [
            "Holly M. Dinkel",
            "Jason K. Cornelius"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The UN Office of Outer Space Affairs identifies synergy of space development\nactivities and international cooperation through data and infrastructure\nsharing in their Sustainable Development Goal 17 (SDG17). Current multilateral\nspace exploration paradigms, however, are divided between the Artemis and the\nRoscosmos-CNSA programs to return to the moon and establish permanent human\nsettlements. As space agencies work to expand human presence in space, economic\nresource consolidation in pursuit of technologically ambitious space\nexpeditions is the most sensible path to accomplish SDG17. This paper compiles\na budget dataset for the top five federally-funded space agencies: CNSA, ESA,\nJAXA, NASA, and Roscosmos. Using time-series econometric anslysis methods in\nSTATA, this work analyzes each agency's economic contributions toward space\nexploration. The dataset results are used to propose a multinational space\nmission, Vela, for the development of an orbiting space station around Mars in\nthe late 2030s. Distribution of economic resources and technological\ncapabilities by the respective space programs are proposed to ensure\nprogrammatic redundancy and increase the odds of success on the given timeline.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04730v1"
    },
    {
        "title": "ARMA-Design: Optimal Treatment Allocation Strategies for A/B Testing in\n  Partially Observable Time Series Experiments",
        "authors": [
            "Ke Sun",
            "Linglong Kong",
            "Hongtu Zhu",
            "Chengchun Shi"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Online experiments %in which experimental units receive a sequence of\ntreatments over time are frequently employed in many technological companies to\nevaluate the performance of a newly developed policy, product, or treatment\nrelative to a baseline control. In many applications, the experimental units\nreceive a sequence of treatments over time. To handle these time-dependent\nsettings, existing A/B testing solutions typically assume a fully observable\nexperimental environment that satisfies the Markov condition. However, this\nassumption often does not hold in practice.\n  This paper studies the optimal design for A/B testing in partially observable\nonline experiments. We introduce a controlled (vector) autoregressive moving\naverage model to capture partial observability. We introduce a small signal\nasymptotic framework to simplify the calculation of asymptotic mean squared\nerrors of average treatment effect estimators under various designs. We develop\ntwo algorithms to estimate the optimal design: one utilizing constrained\noptimization and the other employing reinforcement learning. We demonstrate the\nsuperior performance of our designs using two dispatch simulators that\nrealistically mimic the behaviors of drivers and passengers to create virtual\nenvironments, along with two real datasets from a ride-sharing company. A\nPython implementation of our proposal is available at\nhttps://github.com/datake/ARMADesign.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.05342v4"
    },
    {
        "title": "Change-Point Detection in Time Series Using Mixed Integer Programming",
        "authors": [
            "Artem Prokhorov",
            "Peter Radchenko",
            "Alexander Semenov",
            "Anton Skrobotov"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We use cutting-edge mixed integer optimization (MIO) methods to develop a\nframework for detection and estimation of structural breaks in time series\nregression models. The framework is constructed based on the least squares\nproblem subject to a penalty on the number of breakpoints. We restate the\n$l_0$-penalized regression problem as a quadratic programming problem with\ninteger- and real-valued arguments and show that MIO is capable of finding\nprovably optimal solutions using a well-known optimization solver. Compared to\nthe popular $l_1$-penalized regression (LASSO) and other classical methods, the\nMIO framework permits simultaneous estimation of the number and location of\nstructural breaks as well as regression coefficients, while accommodating the\noption of specifying a given or minimal number of breaks. We derive the\nasymptotic properties of the estimator and demonstrate its effectiveness\nthrough extensive numerical experiments, confirming a more accurate estimation\nof multiple breaks as compared to popular non-MIO alternatives. Two empirical\nexamples demonstrate usefulness of the framework in applications from business\nand economic statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.05665v1"
    },
    {
        "title": "Correcting invalid regression discontinuity designs with multiple time\n  period data",
        "authors": [
            "Dor Leventer",
            "Daniel Nevo"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  A common approach to Regression Discontinuity (RD) designs relies on a\ncontinuity assumption of the mean potential outcomes at the cutoff defining the\nRD design. In practice, this assumption is often implausible when changes other\nthan the intervention of interest occur at the cutoff (e.g., other policies are\nimplemented at the same cutoff). When the continuity assumption is implausible,\nresearchers often retreat to ad-hoc analyses that are not supported by any\ntheory and yield results with unclear causal interpretation. These analyses\nseek to exploit additional data where either all units are treated or all units\nare untreated (regardless of their running variable value). For example, when\ndata from multiple time periods are available. We first derive the bias of RD\ndesigns when the continuity assumption does not hold. We then present a\ntheoretical foundation for analyses using multiple time periods by the means of\na general identification framework incorporating data from additional time\nperiods to overcome the bias. We discuss this framework under various RD\ndesigns, and also extend our work to carry-over effects and time-varying\nrunning variables. We develop local linear regression estimators, bias\ncorrection procedures, and standard errors that are robust to bias-correction\nfor the multiple period setup. The approach is illustrated using an application\nthat studied the effect of new fiscal laws on debt of Italian municipalities.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.05847v1"
    },
    {
        "title": "Estimation and Inference of Average Treatment Effect in Percentage\n  Points under Heterogeneity",
        "authors": [
            "Ying Zeng"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In semi-log regression models with heterogeneous treatment effects, the\naverage treatment effect (ATE) in log points and its exponential transformation\nminus one underestimate the ATE in percentage points. I propose new estimation\nand inference methods for the ATE in percentage points, with inference\nutilizing the Fenton-Wilkinson approximation. These methods are particularly\nrelevant for staggered difference-in-differences designs, where treatment\neffects often vary across groups and periods. I prove the methods' large-sample\nproperties and demonstrate their finite-sample performance through simulations,\nrevealing substantial discrepancies between conventional and proposed measures.\nTwo empirical applications further underscore the practical importance of these\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.06624v1"
    },
    {
        "title": "Endogeneity Corrections in Binary Outcome Models with Nonlinear\n  Transformations: Identification and Inference",
        "authors": [
            "Alexander Mayer",
            "Dominik Wied"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  For binary outcome models, an endogeneity correction based on nonlinear\nrank-based transformations is proposed. Identification without external\ninstruments is achieved under one of two assumptions: either the endogenous\nregressor is a nonlinear function of one component of the error term,\nconditional on the exogenous regressors, or the dependence between the\nendogenous and exogenous regressors is nonlinear. Under these conditions, we\nprove consistency and asymptotic normality. Monte Carlo simulations and an\napplication on German insolvency data illustrate the usefulness of the method.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.06977v3"
    },
    {
        "title": "A Sparse Grid Approach for the Nonparametric Estimation of\n  High-Dimensional Random Coefficient Models",
        "authors": [
            "Maximilian Osterhaus"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  A severe limitation of many nonparametric estimators for random coefficient\nmodels is the exponential increase of the number of parameters in the number of\nrandom coefficients included into the model. This property, known as the curse\nof dimensionality, restricts the application of such estimators to models with\nmoderately few random coefficients. This paper proposes a scalable\nnonparametric estimator for high-dimensional random coefficient models. The\nestimator uses a truncated tensor product of one-dimensional hierarchical basis\nfunctions to approximate the underlying random coefficients' distribution. Due\nto the truncation, the number of parameters increases at a much slower rate\nthan in the regular tensor product basis, rendering the nonparametric\nestimation of high-dimensional random coefficient models feasible. The derived\nestimator allows estimating the underlying distribution with constrained least\nsquares, making the approach computationally simple and fast. Monte Carlo\nexperiments and an application to data on the regulation of air pollution\nillustrate the good performance of the estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.07185v1"
    },
    {
        "title": "Quantile and Distribution Treatment Effects on the Treated with Possibly\n  Non-Continuous Outcomes",
        "authors": [
            "Nelly K. Djuazon",
            "Emmanuel Selorm Tsyawo"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Quantile and Distribution Treatment effects on the Treated (QTT/DTT) for\nnon-continuous outcomes are either not identified or inference thereon is\ninfeasible using existing methods. By introducing functional index parallel\ntrends and no anticipation assumptions, this paper identifies and provides\nuniform inference procedures for QTT/DTT. The inference procedure applies under\nboth the canonical two-group and staggered treatment designs with balanced\npanels, unbalanced panels, or repeated cross-sections. Monte Carlo experiments\ndemonstrate the proposed method's robust and competitive performance, while an\nempirical application illustrates its practical utility.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.07842v1"
    },
    {
        "title": "Revisiting the Many Instruments Problem using Random Matrix Theory",
        "authors": [
            "Helmut Farbmacher",
            "Rebecca Groh",
            "Michael Mühlegger",
            "Gabriel Vollert"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We use recent results from the theory of random matrices to improve\ninstrumental variables estimation with many instruments. In settings where the\nfirst-stage parameters are dense, we show that Ridge lowers the implicit price\nof a bias adjustment. This comes along with improved (finite-sample) properties\nin the second stage regression. Our theoretical results nest existing results\non bias approximation and bias adjustment. Moreover, it extends them to\nsettings with more instruments than observations.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.08580v1"
    },
    {
        "title": "Panel Data Unit Root testing: Overview",
        "authors": [
            "Anton Skrobotov"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This review discusses methods of testing for a panel unit root. Modern\napproaches to testing in cross-sectionally correlated panels are discussed,\npreceding the analysis with an analysis of independent panels. In addition,\nmethods for testing in the case of non-linearity in the data (for example, in\nthe case of structural breaks) are presented, as well as methods for testing in\nshort panels, when the time dimension is small and finite. In conclusion, links\nto existing packages that allow implementing some of the described methods are\nprovided.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.08908v1"
    },
    {
        "title": "Externally Valid Selection of Experimental Sites via the k-Median\n  Problem",
        "authors": [
            "José Luis Montiel Olea",
            "Brenda Prallon",
            "Chen Qiu",
            "Jörg Stoye",
            "Yiwei Sun"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We present a decision-theoretic justification for viewing the question of how\nto best choose where to experiment in order to optimize external validity as a\nk-median (clustering) problem, a popular problem in computer science and\noperations research. We present conditions under which minimizing the\nworst-case, welfare-based regret among all nonrandom schemes that select k\nsites to experiment is approximately equal - and sometimes exactly equal - to\nfinding the k most central vectors of baseline site-level covariates. The\nk-median problem can be formulated as a linear integer program. Two empirical\napplications illustrate the theoretical and computational benefits of the\nsuggested procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.09187v1"
    },
    {
        "title": "Counterfactual and Synthetic Control Method: Causal Inference with\n  Instrumented Principal Component Analysis",
        "authors": [
            "Cong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, we propose a novel method for causal inference within the\nframework of counterfactual and synthetic control. Matching forward the\ngeneralized synthetic control method, our instrumented principal component\nanalysis method instruments factor loadings with predictive covariates rather\nthan including them as regressors. These instrumented factor loadings exhibit\ntime-varying dynamics, offering a better economic interpretation. Covariates\nare instrumented through a transformation matrix, $\\Gamma$, when we have a\nlarge number of covariates it can be easily reduced in accordance with a small\nnumber of latent factors helping us to effectively handle high-dimensional\ndatasets and making the model parsimonious. Moreover, the novel way of handling\ncovariates is less exposed to model misspecification and achieved better\nprediction accuracy. Our simulations show that this method is less biased in\nthe presence of unobserved covariates compared to other mainstream approaches.\nIn the empirical application, we use the proposed method to evaluate the effect\nof Brexit on foreign direct investment to the UK.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.09271v2"
    },
    {
        "title": "Deep Learning for the Estimation of Heterogeneous Parameters in Discrete\n  Choice Models",
        "authors": [
            "Stephan Hetzenecker",
            "Maximilian Osterhaus"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper studies the finite sample performance of the flexible estimation\napproach of Farrell, Liang, and Misra (2021a), who propose to use deep learning\nfor the estimation of heterogeneous parameters in economic models, in the\ncontext of discrete choice models. The approach combines the structure imposed\nby economic models with the flexibility of deep learning, which assures the\ninterpretebility of results on the one hand, and allows estimating flexible\nfunctional forms of observed heterogeneity on the other hand. For inference\nafter the estimation with deep learning, Farrell et al. (2021a) derive an\ninfluence function that can be applied to many quantities of interest. We\nconduct a series of Monte Carlo experiments that investigate the impact of\nregularization on the proposed estimation and inference procedure in the\ncontext of discrete choice models. The results show that the deep learning\napproach generally leads to precise estimates of the true average parameters\nand that regular robust standard errors lead to invalid inference results,\nshowing the need for the influence function approach for inference. Without\nregularization, the influence function approach can lead to substantial bias\nand large estimated standard errors caused by extreme outliers. Regularization\nreduces this property and stabilizes the estimation procedure, but at the\nexpense of inducing an additional bias. The bias in combination with decreasing\nvariance associated with increasing regularization leads to the construction of\ninvalid inferential statements in our experiments. Repeated sample splitting,\nunlike regularization, stabilizes the estimation approach without introducing\nan additional bias, thereby allowing for the construction of valid inferential\nstatements.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.09560v1"
    },
    {
        "title": "Continuous difference-in-differences with double/debiased machine\n  learning",
        "authors": [
            "Lucas Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper extends difference-in-differences to settings involving continuous\ntreatments. Specifically, the average treatment effect on the treated (ATT) at\nany level of continuous treatment intensity is identified using a conditional\nparallel trends assumption. In this framework, estimating the ATTs requires\nfirst estimating infinite-dimensional nuisance parameters, especially the\nconditional density of the continuous treatment, which can introduce\nsignificant biases. To address this challenge, estimators for the causal\nparameters are proposed under the double/debiased machine learning framework.\nWe show that these estimators are asymptotically normal and provide consistent\nvariance estimators. To illustrate the effectiveness of our methods, we\nre-examine the study by Acemoglu and Finkelstein (2008), which assessed the\neffects of the 1983 Medicare Prospective Payment System (PPS) reform. By\nreinterpreting their research design using a difference-in-differences approach\nwith continuous treatment, we nonparametrically estimate the treatment effects\nof the 1983 PPS reform, thereby providing a more detailed understanding of its\nimpact.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.10509v1"
    },
    {
        "title": "Gradient Wild Bootstrap for Instrumental Variable Quantile Regressions\n  with Weak and Few Clusters",
        "authors": [
            "Wenjie Wang",
            "Yichong Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We study the gradient wild bootstrap-based inference for instrumental\nvariable quantile regressions in the framework of a small number of large\nclusters in which the number of clusters is viewed as fixed, and the number of\nobservations for each cluster diverges to infinity. For the Wald inference, we\nshow that our wild bootstrap Wald test, with or without studentization using\nthe cluster-robust covariance estimator (CRVE), controls size asymptotically up\nto a small error as long as the parameter of endogenous variable is strongly\nidentified in at least one of the clusters. We further show that the wild\nbootstrap Wald test with CRVE studentization is more powerful for distant local\nalternatives than that without. Last, we develop a wild bootstrap\nAnderson-Rubin (AR) test for the weak-identification-robust inference. We show\nit controls size asymptotically up to a small error, even under weak or partial\nidentification for all clusters. We illustrate the good finite-sample\nperformance of the new inference methods using simulations and provide an\nempirical application to a well-known dataset about US local labor markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.10686v1"
    },
    {
        "title": "Inference with Many Weak Instruments and Heterogeneity",
        "authors": [
            "Luther Yap"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper considers inference in a linear instrumental variable regression\nmodel with many potentially weak instruments and heterogeneous treatment\neffects. I first show that existing test procedures, including those that are\nrobust to only either weak instruments or heterogeneous treatment effects, can\nbe arbitrarily oversized in this setup. Then, I propose a valid inference\nprocedure based on a score statistic and a leave-three-out variance estimator.\nTo establish this procedure's validity, this paper proves that the score\nstatistic is asymptotically normal and the variance estimator is consistent.\nThe power of the score test is also close to a power envelope in an empirical\napplication.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.11193v2"
    },
    {
        "title": "Towards an Inclusive Approach to Corporate Social Responsibility (CSR)\n  in Morocco: CGEM's Commitment",
        "authors": [
            "Gnaoui Imane",
            "Moutahaddib Aziz"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Corporate social responsibility encourages companies to integrate social and\nenvironmental concerns into their activities and their relations with\nstakeholders. It encompasses all actions aimed at the social good, above and\nbeyond corporate interests and legal requirements. Various international\norganizations, authors and researchers have explored the notion of CSR and\nproposed a range of definitions reflecting their perspectives on the concept.\nIn Morocco, although Moroccan companies are not overwhelmingly embracing CSR,\nseveral factors are encouraging them to integrate the CSR approach not only\ninto their discourse, but also into their strategies. The CGEM is actively\ninvolved in promoting CSR within Moroccan companies, awarding the \"CGEM Label\nfor CSR\" to companies that meet the criteria set out in the CSR Charter. The\nprocess of labeling Moroccan companies is in full expansion. The graphs\npresented in this article are broken down according to several criteria, such\nas company size, sector of activity and listing on the Casablanca Stock\nExchange, in order to provide an overview of CSR-labeled companies in Morocco.\nThe approach adopted for this article is a qualitative one aimed at presenting,\nfirstly, the different definitions of the CSR concept and its evolution over\ntime. In this way, the study focuses on the Moroccan context to dissect and\nanalyze the state of progress of CSR integration in Morocco and the various\nefforts made by the CGEM to implement it. According to the data, 124 Moroccan\ncompanies have been awarded the CSR label. For a label in existence since 2006,\nthis figure reflects a certain reluctance on the part of Moroccan companies to\nfully implement the CSR approach in their strategies. Nevertheless, Morocco is\nin a transitional phase, marked by the gradual adoption of various socially\nresponsible practices.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.11519v1"
    },
    {
        "title": "Robust Bayes Treatment Choice with Partial Identification",
        "authors": [
            "Andrés Aradillas Fernández",
            "José Luis Montiel Olea",
            "Chen Qiu",
            "Jörg Stoye",
            "Serdil Tinda"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We study a class of binary treatment choice problems with partial\nidentification, through the lens of robust (multiple prior) Bayesian analysis.\nWe use a convenient set of prior distributions to derive ex-ante and ex-post\nrobust Bayes decision rules, both for decision makers who can randomize and for\ndecision makers who cannot.\n  Our main messages are as follows: First, ex-ante and ex-post robust Bayes\ndecision rules do not tend to agree in general, whether or not randomized rules\nare allowed. Second, randomized treatment assignment for some data realizations\ncan be optimal in both ex-ante and, perhaps more surprisingly, ex-post\nproblems. Therefore, it is usually with loss of generality to exclude\nrandomized rules from consideration, even when regret is evaluated ex-post.\n  We apply our results to a stylized problem where a policy maker uses\nexperimental data to choose whether to implement a new policy in a population\nof interest, but is concerned about the external validity of the experiment at\nhand (Stoye, 2012); and to the aggregation of data generated by multiple\nrandomized control trials in different sites to make a policy choice in a\npopulation for which no experimental data are available (Manski, 2020; Ishihara\nand Kitagawa, 2021).\n",
        "pdf_link": "http://arxiv.org/pdf/2408.11621v1"
    },
    {
        "title": "Actually, There is No Rotational Indeterminacy in the Approximate Factor\n  Model",
        "authors": [
            "Philipp Gersing"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We show that in the approximate factor model the population normalised\nprincipal components converge in mean square (up to sign) under the standard\nassumptions for $n\\to \\infty$. Consequently, we have a generic interpretation\nof what the principal components estimator is actually identifying and existing\nresults on factor identification are reinforced and refined. Based on this\nresult, we provide a new asymptotic theory for the approximate factor model\nentirely without rotation matrices. We show that the factors space is\nconsistently estimated with finite $T$ for $n\\to \\infty$ while consistency of\nthe factors a.k.a the $L^2$ limit of the normalised principal components\nrequires that both $(n, T)\\to \\infty$.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.11676v2"
    },
    {
        "title": "SPORTSCausal: Spill-Over Time Series Causal Inference",
        "authors": [
            "Carol Liu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Randomized controlled trials (RCTs) have long been the gold standard for\ncausal inference across various fields, including business analysis, economic\nstudies, sociology, clinical research, and network learning. The primary\nadvantage of RCTs over observational studies lies in their ability to\nsignificantly reduce noise from individual variance. However, RCTs depend on\nstrong assumptions, such as group independence, time independence, and group\nrandomness, which are not always feasible in real-world applications.\nTraditional inferential methods, including analysis of covariance (ANCOVA),\noften fail when these assumptions do not hold. In this paper, we propose a\nnovel approach named \\textbf{Sp}ill\\textbf{o}ve\\textbf{r} \\textbf{T}ime\n\\textbf{S}eries \\textbf{Causal} (\\verb+SPORTSCausal+), which enables the\nestimation of treatment effects without relying on these stringent assumptions.\nWe demonstrate the practical applicability of \\verb+SPORTSCausal+ through a\nreal-world budget-control experiment. In this experiment, data was collected\nfrom both a 5\\% live experiment and a 50\\% live experiment using the same\ntreatment. Due to the spillover effect, the vanilla estimation of the treatment\neffect was not robust across different treatment sizes, whereas\n\\verb+SPORTSCausal+ provided a robust estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.11951v1"
    },
    {
        "title": "Momentum Informed Inflation-at-Risk",
        "authors": [
            "Tibor Szendrei",
            "Arnab Bhattacharjee"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Growth-at-Risk has recently become a key measure of macroeconomic tail-risk,\nwhich has seen it be researched extensively. Surprisingly, the same cannot be\nsaid for Inflation-at-Risk where both tails, deflation and high inflation, are\nof key concern to policymakers, which has seen comparatively much less\nresearch. This paper will tackle this gap and provide estimates for\nInflation-at-Risk. The key insight of the paper is that inflation is best\ncharacterised by a combination of two types of nonlinearities: quantile\nvariation, and conditioning on the momentum of inflation.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.12286v1"
    },
    {
        "title": "Integrating an agent-based behavioral model in microtransit forecasting\n  and revenue management",
        "authors": [
            "Xiyuan Ren",
            "Joseph Y. J. Chow",
            "Venktesh Pandey",
            "Linfei Yuan"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  As an IT-enabled multi-passenger mobility service, microtransit has the\npotential to improve accessibility, reduce congestion, and enhance flexibility\nin transportation options. However, due to its heterogeneous impacts on\ndifferent communities and population segments, there is a need for better tools\nin microtransit forecast and revenue management, especially when actual usage\ndata are limited. We propose a novel framework based on an agent-based mixed\nlogit model estimated with microtransit usage data and synthetic trip data. The\nframework involves estimating a lower-branch mode choice model with synthetic\ntrip data, combining lower-branch parameters with microtransit data to estimate\nan upper-branch ride pass subscription model, and applying the nested model to\nevaluate microtransit pricing and subsidy policies. The framework enables\nfurther decision-support analysis to consider diverse travel patterns and\nheterogeneous tastes of the total population. We test the framework in a case\nstudy with synthetic trip data from Replica Inc. and microtransit data from\nArlington Via. The lower-branch model result in a rho-square value of 0.603 on\nweekdays and 0.576 on weekends. Predictions made by the upper-branch model\nclosely match the marginal subscription data. In a ride pass pricing policy\nscenario, we show that a discount in weekly pass (from $25 to $18.9) and\nmonthly pass (from $80 to $71.5) would surprisingly increase total revenue by\n$102/day. In an event- or place-based subsidy policy scenario, we show that a\n100% fare discount would reduce 80 car trips during peak hours at AT&T Stadium,\nrequiring a subsidy of $32,068/year.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.12577v1"
    },
    {
        "title": "Difference-in-differences with as few as two cross-sectional units -- A\n  new perspective to the democracy-growth debate",
        "authors": [
            "Gilles Koumou",
            "Emmanuel Selorm Tsyawo"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Pooled panel analyses often mask heterogeneity in unit-specific treatment\neffects. This challenge, for example, crops up in studies of the impact of\ndemocracy on economic growth, where findings vary substantially due to\ndifferences in country composition. To address this challenge, this paper\nintroduces a Difference-in-Differences (DiD) estimator that leverages the\ntemporal dimension of the data to estimate unit-specific average treatment\neffects on the treated (ATT) with as few as two cross-sectional units. Under\nweak identification and temporal dependence conditions, the proposed DiD\nestimator is shown to be asymptotically normal. The method is further\ncomplemented with an identification test that, unlike pre-trends tests, is more\npowerful and can detect violations of parallel trends in post-treatment\nperiods. Empirical results using the DiD estimator suggest Benin's economy\nwould have been 6.3% smaller on average over the 1993-2018 period had she not\ndemocratised.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.13047v3"
    },
    {
        "title": "Endogenous Treatment Models with Social Interactions: An Application to\n  the Impact of Exercise on Self-Esteem",
        "authors": [
            "Zhongjian Lin",
            "Francis Vella"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We address the estimation of endogenous treatment models with social\ninteractions in both the treatment and outcome equations. We model the\ninteractions between individuals in an internally consistent manner via a game\ntheoretic approach based on discrete Bayesian games. This introduces a\nsubstantial computational burden in estimation which we address through a\nsequential version of the nested fixed point algorithm. We also provide some\nrelevant treatment effects, and procedures for their estimation, which capture\nthe impact on both the individual and the total sample. Our empirical\napplication examines the impact of an individual's exercise frequency on her\nlevel of self-esteem. We find that an individual's exercise frequency is\ninfluenced by her expectation of her friends'. We also find that an\nindividual's level of self-esteem is affected by her level of exercise and, at\nrelatively lower levels of self-esteem, by the expectation of her friends'\nself-esteem.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.13971v1"
    },
    {
        "title": "Modeling the Dynamics of Growth in Master-Planned Communities",
        "authors": [
            "Christopher K. Allsup",
            "Irene S. Gabashvili"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper describes how a time-varying Markov model was used to forecast\nhousing development at a master-planned community during a transition from high\nto low growth. Our approach draws on detailed historical data to model the\ndynamics of the market participants, producing results that are entirely\ndata-driven and free of bias. While traditional time series forecasting methods\noften struggle to account for nonlinear regime changes in growth, our approach\nsuccessfully captures the onset of buildout as well as external economic\nshocks, such as the 1990 and 2008-2011 recessions and the 2021 post-pandemic\nboom.\n  This research serves as a valuable tool for urban planners, homeowner\nassociations, and property stakeholders aiming to navigate the complexities of\ngrowth at master-planned communities during periods of both system stability\nand instability.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.14214v2"
    },
    {
        "title": "The effects of data preprocessing on probability of default model\n  fairness",
        "authors": [
            "Di Wu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In the context of financial credit risk evaluation, the fairness of machine\nlearning models has become a critical concern, especially given the potential\nfor biased predictions that disproportionately affect certain demographic\ngroups. This study investigates the impact of data preprocessing, with a\nspecific focus on Truncated Singular Value Decomposition (SVD), on the fairness\nand performance of probability of default models. Using a comprehensive dataset\nsourced from Kaggle, various preprocessing techniques, including SVD, were\napplied to assess their effect on model accuracy, discriminatory power, and\nfairness.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.15452v1"
    },
    {
        "title": "BayesSRW: Bayesian Sampling and Re-weighting approach for variance\n  reduction",
        "authors": [
            "Carol Liu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, we address the challenge of sampling in scenarios where\nlimited resources prevent exhaustive measurement across all subjects. We\nconsider a setting where samples are drawn from multiple groups, each following\na distribution with unknown mean and variance parameters. We introduce a novel\nsampling strategy, motivated simply by Cauchy-Schwarz inequality, which\nminimizes the variance of the population mean estimator by allocating samples\nproportionally to both the group size and the standard deviation. This approach\nimproves the efficiency of sampling by focusing resources on groups with\ngreater variability, thereby enhancing the precision of the overall estimate.\nAdditionally, we extend our method to a two-stage sampling procedure in a Bayes\napproach, named BayesSRW, where a preliminary stage is used to estimate the\nvariance, which then informs the optimal allocation of the remaining sampling\nbudget. Through simulation examples, we demonstrate the effectiveness of our\napproach in reducing estimation uncertainty and providing more reliable\ninsights in applications ranging from user experience surveys to\nhigh-dimensional peptide array studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.15454v1"
    },
    {
        "title": "Marginal homogeneity tests with panel data",
        "authors": [
            "Federico Bugni",
            "Jackson Bunting",
            "Muyang Ren"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  A panel dataset satisfies marginal homogeneity if the time-specific marginal\ndistributions are homogeneous or time-invariant. Marginal homogeneity is\nrelevant in economic settings such as dynamic discrete games. In this paper, we\npropose several tests for the hypothesis of marginal homogeneity and\ninvestigate their properties. We consider an asymptotic framework in which the\nnumber of individuals n in the panel diverges, and the number of periods T is\nfixed. We implement our tests by comparing a studentized or non-studentized\nT-sample version of the Cramer-von Mises statistic with a suitable critical\nvalue. We propose three methods to construct the critical value: asymptotic\napproximations, the bootstrap, and time permutations. We show that the first\ntwo methods result in asymptotically exact hypothesis tests. The permutation\ntest based on a non-studentized statistic is asymptotically exact when T=2, but\nis asymptotically invalid when T>2. In contrast, the permutation test based on\na studentized statistic is always asymptotically exact. Finally, under a\ntime-exchangeability assumption, the permutation test is exact in finite\nsamples, both with and without studentization.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.15862v1"
    },
    {
        "title": "Sensitivity Analysis for Dynamic Discrete Choice Models",
        "authors": [
            "Chun Pong Lau"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In dynamic discrete choice models, some parameters, such as the discount\nfactor, are being fixed instead of being estimated. This paper proposes two\nsensitivity analysis procedures for dynamic discrete choice models with respect\nto the fixed parameters. First, I develop a local sensitivity measure that\nestimates the change in the target parameter for a unit change in the fixed\nparameter. This measure is fast to compute as it does not require model\nre-estimation. Second, I propose a global sensitivity analysis procedure that\nuses model primitives to study the relationship between target parameters and\nfixed parameters. I show how to apply the sensitivity analysis procedures of\nthis paper through two empirical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.16330v1"
    },
    {
        "title": "Bandit Algorithms for Policy Learning: Methods, Implementation, and\n  Welfare-performance",
        "authors": [
            "Toru Kitagawa",
            "Jeff Rowley"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Static supervised learning-in which experimental data serves as a training\nsample for the estimation of an optimal treatment assignment policy-is a\ncommonly assumed framework of policy learning. An arguably more realistic but\nchallenging scenario is a dynamic setting in which the planner performs\nexperimentation and exploitation simultaneously with subjects that arrive\nsequentially. This paper studies bandit algorithms for learning an optimal\nindividualised treatment assignment policy. Specifically, we study\napplicability of the EXP4.P (Exponential weighting for Exploration and\nExploitation with Experts) algorithm developed by Beygelzimer et al. (2011) to\npolicy learning. Assuming that the class of policies has a finite\nVapnik-Chervonenkis dimension and that the number of subjects to be allocated\nis known, we present a high probability welfare-regret bound of the algorithm.\nTo implement the algorithm, we use an incremental enumeration algorithm for\nhyperplane arrangements. We perform extensive numerical analysis to assess the\nalgorithm's sensitivity to its tuning parameters and its welfare-regret\nperformance. Further simulation exercises are calibrated to the National Job\nTraining Partnership Act (JTPA) Study sample to determine how the algorithm\nperforms when applied to economic data. Our findings highlight various\ncomputational challenges and suggest that the limited welfare gain from the\nalgorithm is due to substantial heterogeneity in causal effects in the JTPA\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.00379v1"
    },
    {
        "title": "The Application of Green GDP and Its Impact on Global Economy and\n  Environment: Analysis of GGDP based on SEEA model",
        "authors": [
            "Mingpu Ma"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper presents an analysis of Green Gross Domestic Product (GGDP) using\nthe System of Environmental-Economic Accounting (SEEA) model to evaluate its\nimpact on global climate mitigation and economic health. GGDP is proposed as a\nsuperior measure to tradi-tional GDP by incorporating natural resource\nconsumption, environmental pollution control, and degradation factors. The\nstudy develops a GGDP model and employs grey correlation analysis and grey\nprediction models to assess its relationship with these factors. Key findings\ndemonstrate that replacing GDP with GGDP can positively influence climate\nchange, partic-ularly in reducing CO2 emissions and stabilizing global\ntemperatures. The analysis further explores the implications of GGDP adoption\nacross developed and developing countries, with specific predictions for China\nand the United States. The results indicate a potential increase in economic\nlevels for developing countries, while developed nations may experi-ence a\ndecrease. Additionally, the shift to GGDP is shown to significantly reduce\nnatural re-source depletion and population growth rates in the United States,\nsuggesting broader envi-ronmental and economic benefits. This paper highlights\nthe universal applicability of the GGDP model and its potential to enhance\nenvironmental and economic policies globally.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.02642v1"
    },
    {
        "title": "The Impact of Data Elements on Narrowing the Urban-Rural Consumption Gap\n  in China: Mechanisms and Policy Analysis",
        "authors": [
            "Mingpu Ma"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The urban-rural consumption gap, as one of the important indicators in social\ndevelopment, directly reflects the imbalance in urban and rural economic and\nsocial development. Data elements, as an important component of New Quality\nProductivity, are of significant importance in promoting economic development\nand improving people's living standards in the information age. This study,\nthrough the analysis of fixed-effects regression models, system GMM regression\nmodels, and the intermediate effect model, found that the development level of\ndata elements to some extent promotes the narrowing of the urban-rural\nconsumption gap. At the same time, the intermediate variable of urban-rural\nincome gap plays an important role between data elements and consumption gap,\nwith a significant intermediate effect. The results of the study indicate that\nthe advancement of data elements can promote the balance of urban and rural\nresidents' consumption levels by reducing the urban-rural income gap, providing\ntheoretical support and policy recommendations for achieving common prosperity\nand promoting coordinated urban-rural development. Building upon this, this\npaper emphasizes the complex correlation between the development of data\nelements and the urban-rural consumption gap, and puts forward policy\nsuggestions such as promoting the development of the data element market,\nstrengthening the construction of the digital economy and e-commerce, and\npromoting integrated urban-rural development. Overall, the development of data\nelements is not only an important path to reducing the urban-rural consumption\ngap but also one of the key drivers for promoting the balanced development of\nChina's economic and social development. This study has a certain theoretical\nand practical significance for understanding the mechanism of the urban-rural\nconsumption gap and improving policies for urban-rural economic development.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.02662v1"
    },
    {
        "title": "Performance of Empirical Risk Minimization For Principal Component\n  Regression",
        "authors": [
            "Christian Brownlees",
            "Guðmundur Stefán Guðmundsson",
            "Yaping Wang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper establishes bounds on the predictive performance of empirical risk\nminimization for principal component regression. Our analysis is nonparametric,\nin the sense that the relation between the prediction target and the predictors\nis not specified. In particular, we do not rely on the assumption that the\nprediction target is generated by a factor model. In our analysis we consider\nthe cases in which the largest eigenvalues of the covariance matrix of the\npredictors grow linearly in the number of predictors (strong signal regime) or\nsublinearly (weak signal regime). The main result of this paper shows that\nempirical risk minimization for principal component regression is consistent\nfor prediction and, under appropriate conditions, it achieves near-optimal\nperformance in both the strong and weak signal regimes.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.03606v2"
    },
    {
        "title": "An MPEC Estimator for the Sequential Search Model",
        "authors": [
            "Shinji Koiso",
            "Suguru Otani"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper proposes a constrained maximum likelihood estimator for sequential\nsearch models, using the MPEC (Mathematical Programming with Equilibrium\nConstraints) approach. This method enhances numerical accuracy while avoiding\nad hoc components and errors related to equilibrium conditions. Monte Carlo\nsimulations show that the estimator performs better in small samples, with\nlower bias and root-mean-squared error, though less effectively in large\nsamples. Despite these mixed results, the MPEC approach remains valuable for\nidentifying candidate parameters comparable to the benchmark, without relying\non ad hoc look-up tables, as it generates the table through solved equilibrium\nconstraints.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.04378v1"
    },
    {
        "title": "Lee Bounds with Multilayered Sample Selection",
        "authors": [
            "Kory Kroft",
            "Ismael Mourifié",
            "Atom Vayalinkal"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper investigates the causal effect of job training on wage rates in\nthe presence of firm heterogeneity. When training affects worker sorting to\nfirms, sample selection is no longer binary but is \"multilayered\". This paper\nextends the canonical Heckman (1979) sample selection model - which assumes\nselection is binary - to a setting where it is multilayered, and shows that in\nthis setting Lee bounds set identifies a total effect that combines a\nweighted-average of the causal effect of job training on wage rates across\nfirms with a weighted-average of the contrast in wages between different firms\nfor a fixed level of training. Thus, Lee bounds set identifies a\npolicy-relevant estimand only when firms pay homogeneous wages and/or when job\ntraining does not affect worker sorting across firms. We derive sharp\nclosed-form bounds for the causal effect of job training on wage rates at each\nfirm which leverage information on firm-specific wages. We illustrate our\npartial identification approach with an empirical application to the Job Corps\nStudy. Results show that while conventional Lee bounds are strictly positive,\nour within-firm bounds include 0 showing that canonical Lee bounds may be\ncapturing a pure sorting effect of job training.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.04589v1"
    },
    {
        "title": "Difference-in-Differences with Multiple Events",
        "authors": [
            "Lin-Tung Tsai"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper introduces a two-stage DiD design to address bias from confounding\nevents correlated with the target event. The first DiD estimates the combined\neffects of both treatments with a control group not yet treated nor confounded.\nThe second DiD isolates the target treatment effects using a parallel treatment\neffect assumption and a control group simultaneously treated but not yet\nconfounded. I revisit the effect of minimum wage hikes on teen employment. I\nfind that the short-term effect reduces by two thirds after controlling for the\nMedicaid expansion.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.05184v2"
    },
    {
        "title": "The Surprising Robustness of Partial Least Squares",
        "authors": [
            "João B. Assunção",
            "Pedro Afonso Fernandes"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Partial least squares (PLS) is a simple factorisation method that works well\nwith high dimensional problems in which the number of observations is limited\ngiven the number of independent variables. In this article, we show that PLS\ncan perform better than ordinary least squares (OLS), least absolute shrinkage\nand selection operator (LASSO) and ridge regression in forecasting quarterly\ngross domestic product (GDP) growth, covering the period from 2000 to 2023. In\nfact, through dimension reduction, PLS proved to be effective in lowering the\nout-of-sample forecasting error, specially since 2020. For the period\n2000-2019, the four methods produce similar results, suggesting that PLS is a\nvalid regularisation technique like LASSO or ridge.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.05713v1"
    },
    {
        "title": "Trends and biases in the social cost of carbon",
        "authors": [
            "Richard S. J. Tol"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  An updated and extended meta-analysis confirms that the central estimate of\nthe social cost of carbon is around $200/tC with a large, right-skewed\nuncertainty and trending up. The pure rate of time preference and the inverse\nof the elasticity of intertemporal substitution are key assumptions, the total\nimpact of 2.5K warming less so. The social cost of carbon is much higher if\nclimate change is assumed to affect economic growth rather than the level of\noutput and welfare. The literature is dominated by a relatively small network\nof authors, based in a few countries. Publication and citation bias have pushed\nthe social cost of carbon up.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08158v1"
    },
    {
        "title": "Substitution in the perturbed utility route choice model",
        "authors": [
            "Mogens Fosgerau",
            "Nikolaj Nielsen",
            "Mads Paulsen",
            "Thomas Kjær Rasmussen",
            "Rui Yao"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper considers substitution patterns in the perturbed utility route\nchoice model. We provide a general result that determines the marginal change\nin link flows following a marginal change in link costs across the network. We\ngive a general condition on the network structure under which all paths are\nnecessarily substitutes and an example in which some paths are complements. The\npresence of complementarity contradicts a result in a previous paper in this\njournal; we point out and correct the error.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08347v1"
    },
    {
        "title": "Bayesian Dynamic Factor Models for High-dimensional Matrix-valued Time\n  Series",
        "authors": [
            "Wei Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  High-dimensional matrix-valued time series are of significant interest in\neconomics and finance, with prominent examples including cross region\nmacroeconomic panels and firms' financial data panels. We introduce a class of\nBayesian matrix dynamic factor models that utilize matrix structures to\nidentify more interpretable factor patterns and factor impacts. Our model\naccommodates time-varying volatility, adjusts for outliers, and allows\ncross-sectional correlations in the idiosyncratic components. To determine the\ndimension of the factor matrix, we employ an importance-sampling estimator\nbased on the cross-entropy method to estimate marginal likelihoods. Through a\nseries of Monte Carlo experiments, we show the properties of the factor\nestimators and the performance of the marginal likelihood estimator in\ncorrectly identifying the true dimensions of the factor matrices. Applying our\nmodel to a macroeconomic dataset and a financial dataset, we demonstrate its\nability in unveiling interesting features within matrix-valued time series.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08354v2"
    },
    {
        "title": "Automatic Pricing and Replenishment Strategies for Vegetable Products\n  Based on Data Analysis and Nonlinear Programming",
        "authors": [
            "Mingpu Ma"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In the field of fresh produce retail, vegetables generally have a relatively\nlimited shelf life, and their quality deteriorates with time. Most vegetable\nvarieties, if not sold on the day of delivery, become difficult to sell the\nfollowing day. Therefore, retailers usually perform daily quantitative\nreplenishment based on historical sales data and demand conditions. Vegetable\npricing typically uses a \"cost-plus pricing\" method, with retailers often\ndiscounting products affected by transportation loss and quality decline. In\nthis context, reliable market demand analysis is crucial as it directly impacts\nreplenishment and pricing decisions. Given the limited retail space, a rational\nsales mix becomes essential. This paper first uses data analysis and\nvisualization techniques to examine the distribution patterns and\ninterrelationships of vegetable sales quantities by category and individual\nitem, based on provided data on vegetable types, sales records, wholesale\nprices, and recent loss rates. Next, it constructs a functional relationship\nbetween total sales volume and cost-plus pricing for vegetable categories,\nforecasts future wholesale prices using the ARIMA model, and establishes a\nsales profit function and constraints. A nonlinear programming model is then\ndeveloped and solved to provide daily replenishment quantities and pricing\nstrategies for each vegetable category for the upcoming week. Further, we\noptimize the profit function and constraints based on the actual sales\nconditions and requirements, providing replenishment quantities and pricing\nstrategies for individual items on July 1 to maximize retail profit. Finally,\nto better formulate replenishment and pricing decisions for vegetable products,\nwe discuss and forecast the data that retailers need to collect and analyses\nhow the collected data can be applied to the above issues.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.09065v1"
    },
    {
        "title": "Structural counterfactual analysis in macroeconomics: theory and\n  inference",
        "authors": [
            "Endong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a structural model-free methodology to analyze two types of\nmacroeconomic counterfactuals related to policy path deviation: hypothetical\ntrajectory and policy intervention. Our model-free approach is built on a\nstructural vector moving-average (SVMA) model that relies solely on the\nidentification of policy shocks, thereby eliminating the need to specify an\nentire structural model. Analytical solutions are derived for the\ncounterfactual parameters, and statistical inference for these parameter\nestimates is provided using the Delta method. By utilizing external\ninstruments, we introduce a projection-based method for the identification,\nestimation, and inference of these parameters. This approach connects our\ncounterfactual analysis with the Local Projection literature. A\nsimulation-based approach with nonlinear model is provided to add in addressing\nLucas' critique. The innovative model-free methodology is applied in three\ncounterfactual studies on the U.S. monetary policy: (1) a historical scenario\nanalysis for a hypothetical interest rate path in the post-pandemic era, (2) a\nfuture scenario analysis under either hawkish or dovish interest rate policy,\nand (3) an evaluation of the policy intervention effect of an oil price shock\nby zeroing out the systematic responses of the interest rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.09577v1"
    },
    {
        "title": "A Simple and Adaptive Confidence Interval when Nuisance Parameters\n  Satisfy an Inequality",
        "authors": [
            "Gregory Fletcher Cox"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Inequalities may appear in many models. They can be as simple as assuming a\nparameter is nonnegative, possibly a regression coefficient or a treatment\neffect. This paper focuses on the case that there is only one inequality and\nproposes a confidence interval that is particularly attractive, called the\ninequality-imposed confidence interval (IICI). The IICI is simple. It does not\nrequire simulations or tuning parameters. The IICI is adaptive. It reduces to\nthe usual confidence interval (calculated by adding and subtracting the\nstandard error times the $1 - \\alpha/2$ standard normal quantile) when the\ninequality is sufficiently slack. When the inequality is sufficiently violated,\nthe IICI reduces to an equality-imposed confidence interval (the usual\nconfidence interval for the submodel where the inequality holds with equality).\nAlso, the IICI is uniformly valid and has (weakly) shorter length than the\nusual confidence interval; it is never longer. The first empirical application\nconsiders a linear regression when a coefficient is known to be nonpositive. A\nsecond empirical application considers an instrumental variables regression\nwhen the endogeneity of a regressor is known to be nonnegative.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.09962v1"
    },
    {
        "title": "GPT takes the SAT: Tracing changes in Test Difficulty and Math\n  Performance of Students",
        "authors": [
            "Vikram Krishnaveti",
            "Saannidhya Rawat"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Scholastic Aptitude Test (SAT) is crucial for college admissions but its\neffectiveness and relevance are increasingly questioned. This paper enhances\nSynthetic Control methods by introducing \"Transformed Control\", a novel method\nthat employs Large Language Models (LLMs) powered by Artificial Intelligence to\ngenerate control groups. We utilize OpenAI's API to generate a control group\nwhere GPT-4, or ChatGPT, takes multiple SATs annually from 2008 to 2023. This\ncontrol group helps analyze shifts in SAT math difficulty over time, starting\nfrom the baseline year of 2008. Using parallel trends, we calculate the Average\nDifference in Scores (ADS) to assess changes in high school students' math\nperformance. Our results indicate a significant decrease in the difficulty of\nthe SAT math section over time, alongside a decline in students' math\nperformance. The analysis shows a 71-point drop in the rigor of SAT math from\n2008 to 2023, with student performance decreasing by 36 points, resulting in a\n107-point total divergence in average student math performance. We investigate\npossible mechanisms for this decline in math proficiency, such as changing\nuniversity selection criteria, increased screen time, grade inflation, and\nworsening adolescent mental health. Disparities among demographic groups show a\n104-point drop for White students, 84 points for Black students, and 53 points\nfor Asian students. Male students saw a 117-point reduction, while female\nstudents had a 100-point decrease.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.10750v1"
    },
    {
        "title": "Simple robust two-stage estimation and inference for generalized impulse\n  responses and multi-horizon causality",
        "authors": [
            "Jean-Marie Dufour",
            "Endong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper introduces a novel two-stage estimation and inference procedure\nfor generalized impulse responses (GIRs). GIRs encompass all coefficients in a\nmulti-horizon linear projection model of future outcomes of y on lagged values\n(Dufour and Renault, 1998), which include the Sims' impulse response. The\nconventional use of Least Squares (LS) with heteroskedasticity- and\nautocorrelation-consistent covariance estimation is less precise and often\nresults in unreliable finite sample tests, further complicated by the selection\nof bandwidth and kernel functions. Our two-stage method surpasses the LS\napproach in terms of estimation efficiency and inference robustness. The\nrobustness stems from our proposed covariance matrix estimates, which eliminate\nthe need to correct for serial correlation in the multi-horizon projection\nresiduals. Our method accommodates non-stationary data and allows the\nprojection horizon to grow with sample size. Monte Carlo simulations\ndemonstrate our two-stage method outperforms the LS method. We apply the\ntwo-stage method to investigate the GIRs, implement multi-horizon Granger\ncausality test, and find that economic uncertainty exerts both short-run (1-3\nmonths) and long-run (30 months) effects on economic activities.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.10820v1"
    },
    {
        "title": "A Way to Synthetic Triple Difference",
        "authors": [
            "Castiel Chen Zhuang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper discusses a practical approach that combines synthetic control\nwith triple difference to address violations of the parallel trends assumption.\nBy transforming triple difference into a DID structure, we can apply synthetic\ncontrol to a triple-difference framework, enabling more robust estimates when\nparallel trends are violated across multiple dimensions. The proposed procedure\nis applied to a real-world dataset to illustrate when and how we should apply\nthis practice, while cautions are presented afterwards. This method contributes\nto improving causal inference in policy evaluations and offers a valuable tool\nfor researchers dealing with heterogeneous treatment effects across subgroups.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.12353v2"
    },
    {
        "title": "Testing for equal predictive accuracy with strong dependence",
        "authors": [
            "Laura Coroneo",
            "Fabrizio Iacone"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We analyse the properties of the Diebold and Mariano (1995) test in the\npresence of autocorrelation in the loss differential. We show that the power of\nthe Diebold and Mariano (1995) test decreases as the dependence increases,\nmaking it more difficult to obtain statistically significant evidence of\nsuperior predictive ability against less accurate benchmarks. We also find\nthat, after a certain threshold, the test has no power and the correct null\nhypothesis is spuriously rejected. Taken together, these results caution to\nseriously consider the dependence properties of the loss differential before\nthe application of the Diebold and Mariano (1995) test.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.12662v1"
    },
    {
        "title": "Dynamic tail risk forecasting: what do realized skewness and kurtosis\n  add?",
        "authors": [
            "Giampiero Gallo",
            "Ostap Okhrin",
            "Giuseppe Storti"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper compares the accuracy of tail risk forecasts with a focus on\nincluding realized skewness and kurtosis in \"additive\" and \"multiplicative\"\nmodels. Utilizing a panel of 960 US stocks, we conduct diagnostic tests, employ\nscoring functions, and implement rolling window forecasting to evaluate the\nperformance of Value at Risk (VaR) and Expected Shortfall (ES) forecasts.\nAdditionally, we examine the impact of the window length on forecast accuracy.\nWe propose model specifications that incorporate realized skewness and kurtosis\nfor enhanced precision. Our findings provide insights into the importance of\nconsidering skewness and kurtosis in tail risk modeling, contributing to the\nexisting literature and offering practical implications for risk practitioners\nand researchers.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.13516v1"
    },
    {
        "title": "Inequality Sensitive Optimal Treatment Assignment",
        "authors": [
            "Eduardo Zambrano"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The egalitarian equivalent, $ee$, of a societal distribution of outcomes with\nmean $m$ is the outcome level such that the evaluator is indifferent between\nthe distribution of outcomes and a society in which everyone obtains an outcome\nof $ee$. For an inequality averse evaluator, $ee < m$. In this paper, I extend\nthe optimal treatment choice framework in Manski (2024) to the case where the\nwelfare evaluation is made using egalitarian equivalent measures, and derive\noptimal treatment rules for the Bayesian, maximin and minimax regret inequality\naverse evaluators. I illustrate how the methodology operates in the context of\nthe JobCorps education and training program for disadvantaged youth (Schochet,\nBurghardt, and McConnell 2008) and in Meager (2022)'s Bayesian meta analysis of\nthe microcredit literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.14776v1"
    },
    {
        "title": "Large Bayesian Tensor VARs with Stochastic Volatility",
        "authors": [
            "Joshua C. C. Chan",
            "Yaling Qi"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We consider Bayesian tensor vector autoregressions (TVARs) in which the VAR\ncoefficients are arranged as a three-dimensional array or tensor, and this\ncoefficient tensor is parameterized using a low-rank CP decomposition. We\ndevelop a family of TVARs using a general stochastic volatility specification,\nwhich includes a wide variety of commonly-used multivariate stochastic\nvolatility and COVID-19 outlier-augmented models. In a forecasting exercise\ninvolving 40 US quarterly variables, we show that these TVARs outperform the\nstandard Bayesian VAR with the Minnesota prior. The results also suggest that\nthe parsimonious common stochastic volatility model tends to forecast better\nthan the more flexible Cholesky stochastic volatility model.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.16132v1"
    },
    {
        "title": "Factors in Fashion: Factor Analysis towards the Mode",
        "authors": [
            "Zhe Sun",
            "Yundong Tu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The modal factor model represents a new factor model for dimension reduction\nin high dimensional panel data. Unlike the approximate factor model that\ntargets for the mean factors, it captures factors that influence the\nconditional mode of the distribution of the observables. Statistical inference\nis developed with the aid of mode estimation, where the modal factors and the\nloadings are estimated through maximizing a kernel-type objective function. An\neasy-to-implement alternating maximization algorithm is designed to obtain the\nestimators numerically. Two model selection criteria are further proposed to\ndetermine the number of factors. The asymptotic properties of the proposed\nestimators are established under some regularity conditions. Simulations\ndemonstrate the nice finite sample performance of our proposed estimators, even\nin the presence of heavy-tailed and asymmetric idiosyncratic error\ndistributions. Finally, the application to inflation forecasting illustrates\nthe practical merits of modal factors.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.19287v1"
    },
    {
        "title": "Synthetic Difference in Differences for Repeated Cross-Sectional Data",
        "authors": [
            "Yoann Morin"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The synthetic difference-in-differences method provides an efficient method\nto estimate a causal effect with a latent factor model. However, it relies on\nthe use of panel data. This paper presents an adaptation of the synthetic\ndifference-in-differences method for repeated cross-sectional data. The\ntreatment is considered to be at the group level so that it is possible to\naggregate data by group to compute the two types of synthetic\ndifference-in-differences weights on these aggregated data. Then, I develop and\ncompute a third type of weight that accounts for the different number of\nobservations in each cross-section. Simulation results show that the\nperformance of the synthetic difference-in-differences estimator is improved\nwhen using the third type of weights on repeated cross-sectional data.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.20199v1"
    },
    {
        "title": "New Tests of Equal Forecast Accuracy for Factor-Augmented Regressions\n  with Weaker Loadings",
        "authors": [
            "Luca Margaritella",
            "Ovidijus Stauskas"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We provide the theoretical foundation for the recently proposed tests of\nequal forecast accuracy and encompassing by Pitarakis (2023a) and Pitarakis\n(2023b), when the competing forecast specification is that of a\nfactor-augmented regression model, whose loadings are allowed to be\nhomogeneously/heterogeneously weak. This should be of interest for\npractitioners, as at the moment there is no theory available to justify the use\nof these simple and powerful tests in such context.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.20415v2"
    },
    {
        "title": "Valid Inference on Functions of Causal Effects in the Absence of\n  Microdata",
        "authors": [
            "Vedant Vohra"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Economists are often interested in functions of multiple causal effects, a\nleading example of which is evaluating the cost-effectiveness of a government\npolicy. In such settings, the benefits and costs might be captured by multiple\ncausal effects and aggregated into a scalar measure of cost-effectiveness.\nOftentimes, the microdata underlying these estimates is not accessible; only\nthe published estimates and their corresponding standard errors are available\nfor post-hoc analysis. We provide a method to conduct inference on functions of\ncausal effects when the only information available is the point estimates and\ntheir corresponding standard errors. We apply our method to conduct inference\non the Marginal Value of Public Funds (MVPF) for 8 different policies, and show\nthat even in the absence of any microdata, it is possible to conduct valid and\nmeaningful inference on the MVPF.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.00217v1"
    },
    {
        "title": "A Nonparametric Test of Heterogeneous Treatment Effects under\n  Interference",
        "authors": [
            "Julius Owusu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Statistical inference of heterogeneous treatment effects (HTEs) across\npredefined subgroups is challenging when units interact because treatment\neffects may vary by pre-treatment variables, post-treatment exposure variables\n(that measure the exposure to other units' treatment statuses), or both. Thus,\nthe conventional HTEs testing procedures may be invalid under interference. In\nthis paper, I develop statistical methods to infer HTEs and disentangle the\ndrivers of treatment effects heterogeneity in populations where units interact.\nSpecifically, I incorporate clustered interference into the potential outcomes\nmodel and propose kernel-based test statistics for the null hypotheses of (i)\nno HTEs by treatment assignment (or post-treatment exposure variables) for all\npre-treatment variables values and (ii) no HTEs by pre-treatment variables for\nall treatment assignment vectors. I recommend a multiple-testing algorithm to\ndisentangle the source of heterogeneity in treatment effects. I prove the\nasymptotic properties of the proposed test statistics. Finally, I illustrate\nthe application of the test procedures in an empirical setting using an\nexperimental data set from a Chinese weather insurance program.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.00733v1"
    },
    {
        "title": "Partially Identified Heterogeneous Treatment Effect with Selection: An\n  Application to Gender Gaps",
        "authors": [
            "Xiaolin Sun",
            "Xueyan Zhao",
            "D. S. Poskitt"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper addresses the sample selection model within the context of the\ngender gap problem, where even random treatment assignment is affected by\nselection bias. By offering a robust alternative free from distributional or\nspecification assumptions, we bound the treatment effect under the sample\nselection model with an exclusion restriction, an assumption whose validity is\ntested in the literature. This exclusion restriction allows for further\nsegmentation of the population into distinct types based on observed and\nunobserved characteristics. For each type, we derive the proportions and bound\nthe gender gap accordingly. Notably, trends in type proportions and gender gap\nbounds reveal an increasing proportion of always-working individuals over time,\nalongside variations in bounds, including a general decline across time and\nconsistently higher bounds for those in high-potential wage groups. Further\nanalysis, considering additional assumptions, highlights persistent gender gaps\nfor some types, while other types exhibit differing or inconclusive trends.\nThis underscores the necessity of separating individuals by type to understand\nthe heterogeneous nature of the gender gap.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.01159v2"
    },
    {
        "title": "Forecasting short-term inflation in Argentina with Random Forest Models",
        "authors": [
            "Federico Daniel Forte"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper examines the performance of Random Forest models in forecasting\nshort-term monthly inflation in Argentina, based on a database of monthly\nindicators since 1962. It is found that these models achieve forecast accuracy\nthat is statistically comparable to the consensus of market analysts'\nexpectations surveyed by the Central Bank of Argentina (BCRA) and to\ntraditional econometric models. One advantage of Random Forest models is that,\nas they are non-parametric, they allow for the exploration of nonlinear effects\nin the predictive power of certain macroeconomic variables on inflation. Among\nother findings, the relative importance of the exchange rate gap in forecasting\ninflation increases when the gap between the parallel and official exchange\nrates exceeds 60%. The predictive power of the exchange rate on inflation rises\nwhen the BCRA's net international reserves are negative or close to zero\n(specifically, below USD 2 billion). The relative importance of inflation\ninertia and the nominal interest rate in forecasting the following month's\ninflation increases when the nominal levels of inflation and/or interest rates\nrise.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.01175v1"
    },
    {
        "title": "A new GARCH model with a deterministic time-varying intercept",
        "authors": [
            "Niklas Ahlgren",
            "Alexander Back",
            "Timo Teräsvirta"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  It is common for long financial time series to exhibit gradual change in the\nunconditional volatility. We propose a new model that captures this type of\nnonstationarity in a parsimonious way. The model augments the volatility\nequation of a standard GARCH model by a deterministic time-varying intercept.\nIt captures structural change that slowly affects the amplitude of a time\nseries while keeping the short-run dynamics constant. We parameterize the\nintercept as a linear combination of logistic transition functions. We show\nthat the model can be derived from a multiplicative decomposition of volatility\nand preserves the financial motivation of variance decomposition. We use the\ntheory of locally stationary processes to show that the quasi maximum\nlikelihood estimator (QMLE) of the parameters of the model is consistent and\nasymptotically normally distributed. We examine the quality of the asymptotic\napproximation in a small simulation study. An empirical application to Oracle\nCorporation stock returns demonstrates the usefulness of the model. We find\nthat the persistence implied by the GARCH parameter estimates is reduced by\nincluding a time-varying intercept in the volatility equation.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.03239v2"
    },
    {
        "title": "Inference in High-Dimensional Linear Projections: Multi-Horizon Granger\n  Causality and Network Connectedness",
        "authors": [
            "Eugene Dettaa",
            "Endong Wang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper presents a Wald test for multi-horizon Granger causality within a\nhigh-dimensional sparse Vector Autoregression (VAR) framework. The null\nhypothesis focuses on the causal coefficients of interest in a local projection\n(LP) at a given horizon. Nevertheless, the post-double-selection method on LP\nmay not be applicable in this context, as a sparse VAR model does not\nnecessarily imply a sparse LP for horizon h>1. To validate the proposed test,\nwe develop two types of de-biased estimators for the causal coefficients of\ninterest, both relying on first-step machine learning estimators of the VAR\nslope parameters. The first estimator is derived from the Least Squares method,\nwhile the second is obtained through a two-stage approach that offers potential\nefficiency gains. We further derive heteroskedasticity- and\nautocorrelation-consistent (HAC) inference for each estimator. Additionally, we\npropose a robust inference method for the two-stage estimator, eliminating the\nneed to correct for serial correlation in the projection residuals. Monte Carlo\nsimulations show that the two-stage estimator with robust inference outperforms\nthe Least Squares method in terms of the Wald test size, particularly for\nlonger projection horizons. We apply our methodology to analyze the\ninterconnectedness of policy-related economic uncertainty among a large set of\ncountries in both the short and long run. Specifically, we construct a causal\nnetwork to visualize how economic uncertainty spreads across countries over\ntime. Our empirical findings reveal, among other insights, that in the short\nrun (1 and 3 months), the U.S. influences China, while in the long run (9 and\n12 months), China influences the U.S. Identifying these connections can help\nanticipate a country's potential vulnerabilities and propose proactive\nsolutions to mitigate the transmission of economic uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.04330v1"
    },
    {
        "title": "A Structural Approach to Growth-at-Risk",
        "authors": [
            "Robert Wojciechowski"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We identify the structural impulse responses of quantiles of the outcome\nvariable to a shock. Our estimation strategy explicitly distinguishes treatment\nfrom control variables, allowing us to model responses of unconditional\nquantiles while using controls for identification. Disentangling the effect of\nadding control variables on identification versus interpretation brings our\nstructural quantile impulse responses conceptually closer to structural mean\nimpulse responses. Applying our methodology to study the impact of financial\nshocks on lower quantiles of output growth confirms that financial shocks have\nan outsized effect on growth-at-risk, but the magnitude of our estimates is\nmore extreme than in previous studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.04431v1"
    },
    {
        "title": "Democratizing Strategic Planning in Master-Planned Communities",
        "authors": [
            "Christopher K. Allsup",
            "Irene S. Gabashvili"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper introduces a strategic planning tool for master-planned\ncommunities designed specifically to quantify residents' subjective preferences\nabout large investments in amenities and infrastructure projects. Drawing on\ndata obtained from brief online surveys, the tool ranks alternative plans by\nconsidering the aggregate anticipated utilization of each proposed amenity and\ncost sensitivity to it (or risk sensitivity for infrastructure plans). In\naddition, the tool estimates the percentage of households that favor the\npreferred plan and predicts whether residents would actually be willing to fund\nthe project. The mathematical underpinnings of the tool are borrowed from\nutility theory, incorporating exponential functions to model diminishing\nmarginal returns on quality, cost, and risk mitigation.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.04676v1"
    },
    {
        "title": "Large datasets for the Euro Area and its member countries and the\n  dynamic effects of the common monetary policy",
        "authors": [
            "Matteo Barigozzi",
            "Claudio Lissona",
            "Lorenzo Tonni"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We present and describe a new publicly available large dataset which\nencompasses quarterly and monthly macroeconomic time series for both the Euro\nArea (EA) as a whole and its ten primary member countries. The dataset, which\nis called EA-MD-QD, includes more than 800 time series and spans the period\nfrom January 2000 to the latest available month. Since January 2024 EA-MD-QD is\nupdated on a monthly basis and constantly revised, making it an essential\nresource for conducting policy analysis related to economic outcomes in the EA.\nTo illustrate the usefulness of EA-MD-QD, we study the country specific Impulse\nResponses of the EA wide monetary policy shock by means of the Common Component\nVAR plus either Instrumental Variables or Sign Restrictions identification\nschemes. The results reveal asymmetries in the transmission of the monetary\npolicy shock across countries, particularly between core and peripheral\ncountries. Additionally, we find comovements across Euro Area countries'\nbusiness cycles to be driven mostly by real variables, compared to nominal\nones.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.05082v1"
    },
    {
        "title": "$\\texttt{rdid}$ and $\\texttt{rdidstag}$: Stata commands for robust\n  difference-in-differences",
        "authors": [
            "Kyunghoon Ban",
            "Désiré Kédagni"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This article provides a Stata package for the implementation of the robust\ndifference-in-differences (RDID) method developed in Ban and K\\'edagni (2023).\nIt contains three main commands: $\\texttt{rdid}$, $\\texttt{rdid_dy}$,\n$\\texttt{rdidstag}$, which we describe in the introduction and the main text.\nWe illustrate these commands through simulations and empirical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.05212v1"
    },
    {
        "title": "The Transmission of Monetary Policy via Common Cycles in the Euro Area",
        "authors": [
            "Lukas Berend",
            "Jan Prüser"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We use a FAVAR model with proxy variables and sign restrictions to\ninvestigate the role of the euro area's common output and inflation cycles in\nthe transmission of monetary policy shocks. Our findings indicate that common\ncycles explain most of the variation in output and inflation across member\ncountries. However, Southern European economies exhibit a notable divergence\nfrom these cycles in the aftermath of the financial crisis. Building on this\nevidence, we demonstrate that monetary policy is homogeneously propagated to\nmember countries via the common cycles. In contrast, country-specific\ntransmission channels lead to heterogeneous country responses to monetary\npolicy shocks. Consequently, our empirical results suggest that the divergent\neffects of ECB monetary policy are attributable to heterogeneous\ncountry-specific exposures to financial markets, rather than to\ndis-synchronized economies within the euro area.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.05741v3"
    },
    {
        "title": "Nickell Meets Stambaugh: A Tale of Two Biases in Panel Predictive\n  Regressions",
        "authors": [
            "Chengwang Liao",
            "Ziwei Mei",
            "Zhentao Shi"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In panel predictive regressions with persistent covariates, coexistence of\nthe Nickell bias and the Stambaugh bias imposes challenges for hypothesis\ntesting. This paper introduces a new estimator, the IVX-X-Jackknife (IVXJ),\nwhich effectively removes this composite bias and reinstates standard\ninferential procedures. The IVXJ estimator is inspired by the IVX technique in\ntime series. In panel data where the cross section is of the same order as the\ntime dimension, the bias of the baseline panel IVX estimator can be corrected\nvia an analytical formula by leveraging an innovative X-Jackknife scheme that\ndivides the time dimension into the odd and even indices. IVXJ is the first\nprocedure that achieves unified inference across a wide range of modes of\npersistence in panel predictive regressions, whereas such unified inference is\nunattainable for the popular within-group estimator. Extended to accommodate\nlong-horizon predictions with multiple regressions, IVXJ is used to examine the\nimpact of debt levels on financial crises by panel local projection. Our\nempirics provide comparable results across different categories of debt.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.09825v1"
    },
    {
        "title": "Large Scale Longitudinal Experiments: Estimation and Inference",
        "authors": [
            "Apoorva Lal",
            "Alexander Fischer",
            "Matthew Wardrop"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Large-scale randomized experiments are seldom analyzed using panel regression\nmethods because of computational challenges arising from the presence of\nmillions of nuisance parameters. We leverage Mundlak's insight that unit\nintercepts can be eliminated by using carefully chosen averages of the\nregressors to rewrite several common estimators in a form that is amenable to\nweighted-least squares estimation with frequency weights. This renders\nregressions involving arbitrary strata intercepts tractable with very large\ndatasets, optionally with the key compression step computed out-of-memory in\nSQL. We demonstrate that these methods yield more precise estimates than other\ncommonly used estimators, and also find that the compression strategy greatly\nincreases computational efficiency. We provide in-memory (pyfixest) and\nout-of-memory (duckreg) python libraries to implement these estimators.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.09952v1"
    },
    {
        "title": "Testing the order of fractional integration in the presence of smooth\n  trends, with an application to UK Great Ratios",
        "authors": [
            "Mustafa R. Kılınç",
            "Michael Massmann",
            "Maximilian Ambros"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This note proposes semi-parametric tests for investigating whether a\nstochastic process is fractionally integrated of order $\\delta$, where\n$|\\delta| < 1/2$, when smooth trends are present in the model. We combine the\nsemi-parametric approach by Iacone, Nielsen & Taylor (2022) to model the short\nrange dependence with the use of Chebyshev polynomials by Cuestas & Gil-Alana\nto describe smooth trends. Our proposed statistics have standard limiting null\ndistributions and match the asymptotic local power of infeasible tests based on\nunobserved errors. We also establish the conditions under which an information\ncriterion can consistently estimate the order of the Chebyshev polynomial. The\nfinite sample performance is evaluated using simulations, and an empirical\napplication is given for the UK Great Ratios.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.10749v1"
    },
    {
        "title": "Closed-form estimation and inference for panels with attrition and\n  refreshment samples",
        "authors": [
            "Grigory Franguridi",
            "Lidia Kosenkova"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  It has long been established that, if a panel dataset suffers from attrition,\nauxiliary (refreshment) sampling restores full identification under additional\nassumptions that still allow for nontrivial attrition mechanisms. Such\nidentification results rely on implausible assumptions about the attrition\nprocess or lead to theoretically and computationally challenging estimation\nprocedures. We propose an alternative identifying assumption that, despite its\nnonparametric nature, suggests a simple estimation algorithm based on a\ntransformation of the empirical cumulative distribution function of the data.\nThis estimation procedure requires neither tuning parameters nor optimization\nin the first step, i.e. has a closed form. We prove that our estimator is\nconsistent and asymptotically normal and demonstrate its good performance in\nsimulations. We provide an empirical illustration with income data from the\nUnderstanding America Study.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.11263v1"
    },
    {
        "title": "Aggregation Trees",
        "authors": [
            "Riccardo Di Francesco"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Uncovering the heterogeneous effects of particular policies or \"treatments\"\nis a key concern for researchers and policymakers. A common approach is to\nreport average treatment effects across subgroups based on observable\ncovariates. However, the choice of subgroups is crucial as it poses the risk of\n$p$-hacking and requires balancing interpretability with granularity. This\npaper proposes a nonparametric approach to construct heterogeneous subgroups.\nThe approach enables a flexible exploration of the trade-off between\ninterpretability and the discovery of more granular heterogeneity by\nconstructing a sequence of nested groupings, each with an optimality property.\nBy integrating our approach with \"honesty\" and debiased machine learning, we\nprovide valid inference about the average treatment effect of each group. We\nvalidate the proposed methodology through an empirical Monte-Carlo study and\napply it to revisit the impact of maternal smoking on birth weight, revealing\nsystematic heterogeneity driven by parental and birth-related characteristics.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.11408v1"
    },
    {
        "title": "Testing Identifying Assumptions in Parametric Separable Models: A\n  Conditional Moment Inequality Approach",
        "authors": [
            "Leonard Goff",
            "Désiré Kédagni",
            "Huan Wu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, we propose a simple method for testing identifying assumptions\nin parametric separable models, namely treatment exogeneity, instrument\nvalidity, and/or homoskedasticity. We show that the testable implications can\nbe written in the intersection bounds framework, which is easy to implement\nusing the inference method proposed in Chernozhukov, Lee, and Rosen (2013), and\nthe Stata package of Chernozhukov et al. (2015). Monte Carlo simulations\nconfirm that our test is consistent and controls size. We use our proposed\nmethod to test the validity of some commonly used instrumental variables, such\nas the average price in other markets in Nevo and Rosen (2012), the Bartik\ninstrument in Card (2009), and the test rejects both instrumental variable\nmodels. When the identifying assumptions are rejected, we discuss solutions\nthat allow researchers to identify some causal parameters of interest after\nrelaxing functional form assumptions. We show that the IV model is nontestable\nif no functional form assumption is made on the outcome equation, when there\nexists a one-to-one mapping between the continuous treatment variable, the\ninstrument, and the first-stage unobserved heterogeneity.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12098v1"
    },
    {
        "title": "A Simple Interactive Fixed Effects Estimator for Short Panels",
        "authors": [
            "Robert F. Phillips",
            "Benjamin D. Williams"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We study the interactive effects (IE) model as an extension of the\nconventional additive effects (AE) model. For the AE model, the fixed effects\nestimator can be obtained by applying least squares to a regression that adds a\nlinear projection of the fixed effect on the explanatory variables (Mundlak,\n1978; Chamberlain, 1984). In this paper, we develop a novel estimator -- the\nprojection-based IE (PIE) estimator -- for the IE model that is based on a\nsimilar approach. We show that, for the IE model, fixed effects estimators that\nhave appeared in the literature are not equivalent to our PIE estimator, though\nboth can be expressed as a generalized within estimator. Unlike the fixed\neffects estimators for the IE model, the PIE estimator is consistent for a\nfixed number of time periods with no restrictions on serial correlation or\nconditional heteroskedasticity in the errors. We also derive a statistic for\ntesting the consistency of the two-way fixed effects estimator in the possible\npresence of iterative effects. Moreover, although the PIE estimator is the\nsolution to a high-dimensional nonlinear least squares problem, we show that it\ncan be computed by iterating between two steps, both of which have simple\nanalytical solutions. The computational simplicity is an important advantage\nrelative to other strategies that have been proposed for estimating the IE\nmodel for short panels. Finally, we compare the finite sample performance of IE\nestimators through simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12709v1"
    },
    {
        "title": "Counterfactual Analysis in Empirical Games",
        "authors": [
            "Brendan Kline",
            "Elie Tamer"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We address counterfactual analysis in empirical models of games with\npartially identified parameters, and multiple equilibria and/or randomized\nstrategies, by constructing and analyzing the counterfactual predictive\ndistribution set (CPDS). This framework accommodates various outcomes of\ninterest, including behavioral and welfare outcomes. It allows a variety of\nchanges to the environment to generate the counterfactual, including\nmodifications of the utility functions, the distribution of utility\ndeterminants, the number of decision makers, and the solution concept. We use a\nBayesian approach to summarize statistical uncertainty. We establish conditions\nunder which the population CPDS is sharp from the point of view of\nidentification. We also establish conditions under which the posterior CPDS is\nconsistent if the posterior distribution for the underlying model parameter is\nconsistent. Consequently, our results can be employed to conduct counterfactual\nanalysis after a preliminary step of identifying and estimating the underlying\nmodel parameter based on the existing literature. Our consistency results\ninvolve the development of a new general theory for Bayesian consistency of\nposterior distributions for mappings of sets. Although we primarily focus on a\nmodel of a strategic game, our approach is applicable to other structural\nmodels with similar features.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12731v1"
    },
    {
        "title": "Identification of a Rank-dependent Peer Effect Model",
        "authors": [
            "Eyo I. Herstad",
            "Myungkou Shin"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper develops an econometric model to analyse heterogeneity in peer\neffects in network data with endogenous spillover across units. We introduce a\nrank-dependent peer effect model that captures how the relative ranking of a\npeer outcome shapes the influence units have on one another, by modeling the\npeer effect to be linear in ordered peer outcomes. In contrast to the\ntraditional linear-in-means model, our approach allows for greater flexibility\nin peer effect by accounting for the distribution of peer outcomes as well as\nthe size of peer groups. Under a minimal condition, the rank-dependent peer\neffect model admits a unique equilibrium and is therefore tractable. Our\nsimulations show that that estimation performs well in finite samples given\nsufficient covariate strength. We then apply our model to educational data from\nNorway, where we see that higher-performing students disproportionately drive\nGPA spillovers.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.14317v1"
    },
    {
        "title": "GARCH option valuation with long-run and short-run volatility\n  components: A novel framework ensuring positive variance",
        "authors": [
            "Luca Vincenzo Ballestra",
            "Enzo D'Innocenzo",
            "Christian Tezza"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Christoffersen, Jacobs, Ornthanalai, and Wang (2008) (CJOW) proposed an\nimproved Generalized Autoregressive Conditional Heteroskedasticity (GARCH)\nmodel for valuing European options, where the return volatility is comprised of\ntwo distinct components. Empirical studies indicate that the model developed by\nCJOW outperforms widely-used single-component GARCH models and provides a\nsuperior fit to options data than models that combine conditional\nheteroskedasticity with Poisson-normal jumps. However, a significant limitation\nof this model is that it allows the variance process to become negative. Oh and\nPark [2023] partially addressed this issue by developing a related model, yet\nthe positivity of the volatility components is not guaranteed, both\ntheoretically and empirically. In this paper we introduce a new GARCH model\nthat improves upon the models by CJOW and Oh and Park [2023], ensuring the\npositivity of the return volatility. In comparison to the two earlier GARCH\napproaches, our novel methodology shows comparable in-sample performance on\nreturns data and superior performance on S&P500 options data.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.14513v1"
    },
    {
        "title": "A GARCH model with two volatility components and two driving factors",
        "authors": [
            "Luca Vincenzo Ballestra",
            "Enzo D'Innocenzo",
            "Christian Tezza"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We introduce a novel GARCH model that integrates two sources of uncertainty\nto better capture the rich, multi-component dynamics often observed in the\nvolatility of financial assets. This model provides a quasi closed-form\nrepresentation of the characteristic function for future log-returns, from\nwhich semi-analytical formulas for option pricing can be derived. A theoretical\nanalysis is conducted to establish sufficient conditions for strict\nstationarity and geometric ergodicity, while also obtaining the continuous-time\ndiffusion limit of the model. Empirical evaluations, conducted both in-sample\nand out-of-sample using S\\&P500 time series data, show that our model\noutperforms widely used single-factor models in predicting returns and option\nprices.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.14585v1"
    },
    {
        "title": "Fast and Efficient Bayesian Analysis of Structural Vector\n  Autoregressions Using the R Package bsvars",
        "authors": [
            "Tomasz Woźniak"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The R package bsvars provides a wide range of tools for empirical\nmacroeconomic and financial analyses using Bayesian Structural Vector\nAutoregressions. It uses frontier econometric techniques and C++ code to ensure\nfast and efficient estimation of these multivariate dynamic structural models,\npossibly with many variables, complex identification strategies, and non-linear\ncharacteristics. The models can be identified using adjustable exclusion\nrestrictions and heteroskedastic or non-normal shocks. They feature a flexible\nthree-level equation-specific local-global hierarchical prior distribution for\nthe estimated level of shrinkage for autoregressive and structural parameters.\nAdditionally, the package facilitates predictive and structural analyses such\nas impulse responses, forecast error variance and historical decompositions,\nforecasting, statistical verification of identification and hypotheses on\nautoregressive parameters, and analyses of structural shocks, volatilities, and\nfitted values. These features differentiate bsvars from existing R packages\nthat either focus on a specific structural model, do not consider\nheteroskedastic shocks, or lack the implementation using compiled code.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.15090v1"
    },
    {
        "title": "Predictive Quantile Regression with High-Dimensional Predictors: The\n  Variable Screening Approach",
        "authors": [
            "Hongqi Chen",
            "Ji Hyung Lee"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper advances a variable screening approach to enhance conditional\nquantile forecasts using high-dimensional predictors. We have refined and\naugmented the quantile partial correlation (QPC)-based variable screening\nproposed by Ma et al. (2017) to accommodate $\\beta$-mixing time-series data.\nOur approach is inclusive of i.i.d scenarios but introduces new convergence\nbounds for time-series contexts, suggesting the performance of QPC-based\nscreening is influenced by the degree of time-series dependence. Through Monte\nCarlo simulations, we validate the effectiveness of QPC under weak dependence.\nOur empirical assessment of variable selection for growth-at-risk (GaR)\nforecasting underscores the method's advantages, revealing that specific labor\nmarket determinants play a pivotal role in forecasting GaR. While prior\nempirical research has predominantly considered a limited set of predictors, we\nemploy the comprehensive Fred-QD dataset, retaining a richer breadth of\ninformation for GaR forecasts.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.15097v1"
    },
    {
        "title": "Semiparametric Bayesian Inference for a Conditional Moment Equality\n  Model",
        "authors": [
            "Christopher D. Walker"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Conditional moment equality models are regularly encountered in empirical\neconomics, yet they are difficult to estimate. These models map a conditional\ndistribution of data to a structural parameter via the restriction that a\nconditional mean equals zero. Using this observation, I introduce a Bayesian\ninference framework in which an unknown conditional distribution is replaced\nwith a nonparametric posterior, and structural parameter inference is then\nperformed using an implied posterior. The method has the same flexibility as\nfrequentist semiparametric estimators and does not require converting\nconditional moments to unconditional moments. Importantly, I prove a\nsemiparametric Bernstein-von Mises theorem, providing conditions under which,\nin large samples, the posterior for the structural parameter is approximately\nnormal, centered at an efficient estimator, and has variance equal to the\nChamberlain (1987) semiparametric efficiency bound. As byproducts, I show that\nBayesian uncertainty quantification methods are asymptotically optimal\nfrequentist confidence sets and derive low-level sufficient conditions for\nGaussian process priors. The latter sheds light on a key prior stability\ncondition and relates to the numerical aspects of the paper in which these\npriors are used to predict the welfare effects of price changes.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.16017v1"
    },
    {
        "title": "Dynamic Biases of Static Panel Data Estimators",
        "authors": [
            "Sylvia Klosin"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper identifies an important bias - termed dynamic bias - in fixed\neffects panel estimators that arises when dynamic feedback is ignored in the\nestimating equation. Dynamic feedback occurs if past outcomes impact current\noutcomes, a feature of many settings ranging from economic growth to\nagricultural and labor markets. When estimating equations omit past outcomes,\ndynamic bias can lead to significantly inaccurate treatment effect estimates,\neven with randomly assigned treatments. This dynamic bias in simulations is\nlarger than Nickell bias. I show that dynamic bias stems from the estimation of\nfixed effects, as their estimation generates confounding in the data. To\nrecover consistent treatment effects, I develop a flexible estimator that\nprovides fixed-T bias correction. I apply this approach to study the impact of\ntemperature shocks on GDP, a canonical example where economic theory points to\nan important feedback from past to future outcomes. Accounting for dynamic bias\nlowers the estimated effects of higher yearly temperatures on GDP growth by 10%\nand GDP levels by 120%.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.16112v1"
    },
    {
        "title": "Identifying Conduct Parameters with Separable Demand: A Counterexample\n  to Lau (1982)",
        "authors": [
            "Yuri Matsumura",
            "Suguru Otani"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We provide a counterexample to the conduct parameter identification result\nestablished in the foundational work of Lau (1982), which generalizes the\nidentification theorem of Bresnahan (1982) by relaxing the linearity\nassumptions. We identify a separable demand function that still permits\nidentification and validate this case both theoretically and through numerical\nsimulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.16998v1"
    },
    {
        "title": "A Bayesian Perspective on the Maximum Score Problem",
        "authors": [
            "Christopher D. Walker"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper presents a Bayesian inference framework for a linear index\nthreshold-crossing binary choice model that satisfies a median independence\nrestriction. The key idea is that the model is observationally equivalent to a\nprobit model with nonparametric heteroskedasticity. Consequently, Gibbs\nsampling techniques from Albert and Chib (1993) and Chib and Greenberg (2013)\nlead to a computationally attractive Bayesian inference procedure in which a\nGaussian process forms a conditionally conjugate prior for the natural\nlogarithm of the skedastic function.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.17153v1"
    },
    {
        "title": "Partially Identified Rankings from Pairwise Interactions",
        "authors": [
            "Federico Crippa",
            "Danil Fedchenko"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper considers the problem of ranking objects based on their latent\nmerits using data from pairwise interactions. Existing approaches rely on the\nrestrictive assumption that all the interactions are either observed or missed\nrandomly. We investigate what can be inferred about rankings when this\nassumption is relaxed. First, we demonstrate that in parametric models, such as\nthe popular Bradley-Terry-Luce model, rankings are point-identified if and only\nif the tournament graph is connected. Second, we show that in nonparametric\nmodels based on strong stochastic transitivity, rankings in a connected\ntournament are only partially identified. Finally, we propose two statistical\ntests to determine whether a ranking belongs to the identified set. One test is\nvalid in finite samples but computationally intensive, while the other is easy\nto implement and valid asymptotically. We illustrate our procedure using\nBrazilian employer-employee data to test whether male and female workers rank\nfirms differently when making job transitions.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.18272v1"
    },
    {
        "title": "Inference on High Dimensional Selective Labeling Models",
        "authors": [
            "Shakeeb Khan",
            "Elie Tamer",
            "Qingsong Yao"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  A class of simultaneous equation models arise in the many domains where\nobserved binary outcomes are themselves a consequence of the existing choices\nof of one of the agents in the model. These models are gaining increasing\ninterest in the computer science and machine learning literatures where they\nrefer the potentially endogenous sample selection as the {\\em selective labels}\nproblem. Empirical settings for such models arise in fields as diverse as\ncriminal justice, health care, and insurance. For important recent work in this\narea, see for example Lakkaruju et al. (2017), Kleinberg et al. (2018), and\nCoston et al.(2021) where the authors focus on judicial bail decisions, and\nwhere one observes the outcome of whether a defendant filed to return for their\ncourt appearance only if the judge in the case decides to release the defendant\non bail. Identifying and estimating such models can be computationally\nchallenging for two reasons. One is the nonconcavity of the bivariate\nlikelihood function, and the other is the large number of covariates in each\nequation. Despite these challenges, in this paper we propose a novel\ndistribution free estimation procedure that is computationally friendly in many\ncovariates settings. The new method combines the semiparametric batched\ngradient descent algorithm introduced in Khan et al.(2023) with a novel sorting\nalgorithms incorporated to control for selection bias. Asymptotic properties of\nthe new procedure are established under increasing dimension conditions in both\nequations, and its finite sample properties are explored through a simulation\nstudy and an application using judicial bail data.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.18381v2"
    },
    {
        "title": "Heterogeneous Intertemporal Treatment Effects via Dynamic Panel Data\n  Models",
        "authors": [
            "Philip Marx",
            "Elie Tamer",
            "Xun Tang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We study the identification and estimation of heterogeneous, intertemporal\ntreatment effects (TE) when potential outcomes depend on past treatments.\nFirst, applying a dynamic panel data model to observed outcomes, we show that\ninstrument-based GMM estimators, such as Arellano and Bond (1991), converge to\na non-convex (negatively weighted) aggregate of TE plus non-vanishing trends.\nWe then provide restrictions on sequential exchangeability (SE) of treatment\nand TE heterogeneity that reduce the GMM estimand to a convex (positively\nweighted) aggregate of TE. Second, we introduce an adjusted\ninverse-propensity-weighted (IPW) estimator for a new notion of average\ntreatment effect (ATE) over past observed treatments. Third, we show that when\npotential outcomes are generated by dynamic panel data models with homogeneous\nTE, such GMM estimators converge to causal parameters (even when SE is\ngenerically violated without conditioning on individual fixed effects).\nFinally, we motivate SE and compare it with parallel trends (PT) in various\nsettings with observational data (when treatments are dynamic, rational choices\nunder learning) or experimental data (when treatments are sequentially\nrandomized).\n",
        "pdf_link": "http://arxiv.org/pdf/2410.19060v1"
    },
    {
        "title": "Inference on Multiple Winners with Applications to Microcredit and\n  Economic Mobility",
        "authors": [
            "Andreas Petrou-Zeniou",
            "Azeem M. Shaikh"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  While policymakers and researchers are often concerned with conducting\ninference based on a data-dependent selection, a strictly larger class of\ninference problems arises when considering multiple data-dependent selections,\nsuch as when selecting on statistical significance or quantiles. Given this, we\nstudy the problem of conducting inference on multiple selections, which we dub\nthe inference on multiple winners problem. In this setting, we encounter both\nselective and multiple testing problems, making existing approaches either not\napplicable or too conservative. Instead, we propose a novel, two-step approach\nto the inference on multiple winners problem, with the first step modeling the\nselection of winners, and the second step using this model to conduct inference\nonly on the set of likely winners. Our two-step approach reduces over-coverage\nerror by up to 96%. We apply our two-step approach to revisit the winner's\ncurse in the creating moves to opportunity (CMTO) program, and to study\nexternal validity issues in the microcredit literature. In the CMTO\napplication, we find that, after correcting for the inference on multiple\nwinners problem, we fail to reject the possibility of null effects in the\nmajority of census tracts selected by the CMTO program. In our microcredit\napplication, we find that heterogeneity in treatment effect estimates remains\nlargely unaffected even after our proposed inference corrections.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.19212v1"
    },
    {
        "title": "Testing the effects of an unobservable factor: Do marriage prospects\n  affect college major choice?",
        "authors": [
            "Hayri Alper Arslan",
            "Brantly Callaway",
            "Tong Li"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Motivated by studying the effects of marriage prospects on students' college\nmajor choices, this paper develops a new econometric test for analyzing the\neffects of an unobservable factor in a setting where this factor potentially\ninfluences both agents' decisions and a binary outcome variable. Our test is\nbuilt upon a flexible copula-based estimation procedure and leverages the\nordered nature of latent utilities of the polychotomous choice model. Using the\nproposed method, we demonstrate that marriage prospects significantly influence\nthe college major choices of college graduates participating in the National\nLongitudinal Study of Youth (97) Survey. Furthermore, we validate the\nrobustness of our findings with alternative tests that use stated marriage\nexpectation measures from our data, thereby demonstrating the applicability and\nvalidity of our testing procedure in real-life scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.19947v1"
    },
    {
        "title": "Jacobian-free Efficient Pseudo-Likelihood (EPL) Algorithm",
        "authors": [
            "Takeshi Fukasawa"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This study proposes a simple procedure to compute Efficient Pseudo Likelihood\n(EPL) estimator proposed by Dearing and Blevins (2024) for estimating dynamic\ndiscrete games, without computing Jacobians of equilibrium constraints. EPL\nestimator is efficient, convergent, and computationally fast. However, the\noriginal algorithm requires deriving and coding the Jacobians, which are\ncumbersome and prone to coding mistakes especially when considering complicated\nmodels. The current study proposes to avoid the computation of Jacobians by\ncombining the ideas of numerical derivatives (for computing Jacobian-vector\nproducts) and the Krylov method (for solving linear equations). It shows good\ncomputational performance of the proposed method by numerical experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.20029v1"
    },
    {
        "title": "International vulnerability of inflation",
        "authors": [
            "Ignacio Garrón",
            "C. Vladimir Rodríguez-Caballero",
            "Esther Ruiz"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In a globalised world, inflation in a given country may be becoming less\nresponsive to domestic economic activity, while being increasingly determined\nby international conditions. Consequently, understanding the international\nsources of vulnerability of domestic inflation is turning fundamental for\npolicy makers. In this paper, we propose the construction of Inflation-at-risk\nand Deflation-at-risk measures of vulnerability obtained using factor-augmented\nquantile regressions estimated with international factors extracted from a\nmulti-level Dynamic Factor Model with overlapping blocks of inflations\ncorresponding to economies grouped either in a given geographical region or\naccording to their development level. The methodology is implemented to\ninflation observed monthly from 1999 to 2022 for over 115 countries. We\nconclude that, in a large number of developed countries, international factors\nare relevant to explain the right tail of the distribution of inflation, and,\nconsequently, they are more relevant for the vulnerability related to high\ninflation than for average or low inflation. However, while inflation of\ndeveloping low-income countries is hardly affected by international conditions,\nthe results for middle-income countries are mixed. Finally, based on a\nrolling-window out-of-sample forecasting exercise, we show that the predictive\npower of international factors has increased in the most recent years of high\ninflation.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.20628v2"
    },
    {
        "title": "Robust Network Targeting with Multiple Nash Equilibria",
        "authors": [
            "Guanyi Wang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Many policy problems involve designing individualized treatment allocation\nrules to maximize the equilibrium social welfare of interacting agents.\nFocusing on large-scale simultaneous decision games with strategic\ncomplementarities, we develop a method to estimate an optimal treatment\nallocation rule that is robust to the presence of multiple equilibria. Our\napproach remains agnostic about changes in the equilibrium selection mechanism\nunder counterfactual policies, and we provide a closed-form expression for the\nboundary of the set-identified equilibrium outcomes. To address the\nincompleteness that arises when an equilibrium selection mechanism is not\nspecified, we use the maximin welfare criterion to select a policy, and\nimplement this policy using a greedy algorithm. We establish a performance\nguarantee for our method by deriving a welfare regret bound, which accounts for\nsampling uncertainty and the use of the greedy algorithm. We demonstrate our\nmethod with an application to the microfinance dataset of Banerjee et al.\n(2013).\n",
        "pdf_link": "http://arxiv.org/pdf/2410.20860v2"
    },
    {
        "title": "Economic Diversification and Social Progress in the GCC Countries: A\n  Study on the Transition from Oil-Dependency to Knowledge-Based Economies",
        "authors": [
            "Mahdi Goldani",
            "Soraya Asadi Tirvan"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The Gulf Cooperation Council countries -- Oman, Bahrain, Kuwait, UAE, Qatar,\nand Saudi Arabia -- holds strategic significance due to its large oil reserves.\nHowever, these nations face considerable challenges in shifting from\noil-dependent economies to more diversified, knowledge-based systems. This\nstudy examines the progress of Gulf Cooperation Council (GCC) countries in\nachieving economic diversification and social development, focusing on the\nSocial Progress Index (SPI), which provides a broader measure of societal\nwell-being beyond just economic growth. Using data from the World Bank,\ncovering 2010 to 2023, the study employs the XGBoost machine learning model to\nforecast SPI values for the period of 2024 to 2026. Key components of the\nmethodology include data preprocessing, feature selection, and the simulation\nof independent variables through ARIMA modeling. The results highlight\nsignificant improvements in education, healthcare, and women's rights,\ncontributing to enhanced SPI performance across the GCC countries. However,\nnotable challenges persist in areas like personal rights and inclusivity. The\nstudy further indicates that despite economic setbacks caused by global\ndisruptions, including the COVID-19 pandemic and oil price volatility, GCC\nnations are expected to see steady improvements in their SPI scores through\n2027. These findings underscore the critical importance of economic\ndiversification, investment in human capital, and ongoing social reforms to\nreduce dependence on hydrocarbons and build knowledge-driven economies. This\nresearch offers valuable insights for policymakers aiming to strengthen both\nsocial and economic resilience in the region while advancing long-term\nsustainable development goals.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.21505v2"
    },
    {
        "title": "Forecasting Political Stability in GCC Countries",
        "authors": [
            "Mahdi Goldani"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Political stability is crucial for the socioeconomic development of nations,\nparticularly in geopolitically sensitive regions such as the Gulf Cooperation\nCouncil Countries, Saudi Arabia, UAE, Kuwait, Qatar, Oman, and Bahrain. This\nstudy focuses on predicting the political stability index for these six\ncountries using machine learning techniques. The study uses data from the World\nBanks comprehensive dataset, comprising 266 indicators covering economic,\npolitical, social, and environmental factors. Employing the Edit Distance on\nReal Sequence method for feature selection and XGBoost for model training, the\nstudy forecasts political stability trends for the next five years. The model\nachieves high accuracy, with mean absolute percentage error values under 10,\nindicating reliable predictions. The forecasts suggest that Oman, the UAE, and\nQatar will experience relatively stable political conditions, while Saudi\nArabia and Bahrain may continue to face negative political stability indices.\nThe findings underscore the significance of economic factors such as GDP and\nforeign investment, along with variables related to military expenditure and\ninternational tourism, as key predictors of political stability. These results\nprovide valuable insights for policymakers, enabling proactive measures to\nenhance governance and mitigate potential risks.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.21516v2"
    },
    {
        "title": "Machine Learning Debiasing with Conditional Moment Restrictions: An\n  Application to LATE",
        "authors": [
            "Facundo Argañaraz",
            "Juan Carlos Escanciano"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Models with Conditional Moment Restrictions (CMRs) are popular in economics.\nThese models involve finite and infinite dimensional parameters. The infinite\ndimensional components include conditional expectations, conditional choice\nprobabilities, or policy functions, which might be flexibly estimated using\nMachine Learning tools. This paper presents a characterization of locally\ndebiased moments for regular models defined by general semiparametric CMRs with\npossibly different conditioning variables. These moments are appealing as they\nare known to be less affected by first-step bias. Additionally, we study their\nexistence and relevance. Such results apply to a broad class of smooth\nfunctionals of finite and infinite dimensional parameters that do not\nnecessarily appear in the CMRs. As a leading application of our theory, we\ncharacterize debiased machine learning for settings of treatment effects with\nendogeneity, giving necessary and sufficient conditions. We present a large\nclass of relevant debiased moments in this context. We then propose the\nCompliance Machine Learning Estimator (CML), based on a practically convenient\northogonal relevant moment. We show that the resulting estimand can be written\nas a convex combination of conditional local average treatment effects (LATE).\nAltogether, CML enjoys three appealing properties in the LATE framework: (1)\nlocal robustness to first-stage estimation, (2) an estimand that can be\nidentified under a minimal relevance condition, and (3) a meaningful causal\ninterpretation. Our numerical experimentation shows satisfactory relative\nperformance of such an estimator. Finally, we revisit the Oregon Health\nInsurance Experiment, analyzed by Finkelstein et al. (2012). We find that the\nuse of machine learning and CML suggest larger positive effects on health care\nutilization than previously determined.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.23785v1"
    },
    {
        "title": "Estimation and Inference in Dyadic Network Formation Models with\n  Nontransferable Utilities",
        "authors": [
            "Ming Li",
            "Zhentao Shi",
            "Yapeng Zheng"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper studies estimation and inference in a dyadic network formation\nmodel with observed covariates, unobserved heterogeneity, and nontransferable\nutilities. With the presence of the high dimensional fixed effects, the maximum\nlikelihood estimator is numerically difficult to compute and suffers from the\nincidental parameter bias. We propose an easy-to-compute one-step estimator for\nthe homophily parameter of interest, which is further refined to achieve\n$\\sqrt{N}$-consistency via split-network jackknife and efficiency by the\nbootstrap aggregating (bagging) technique. We establish consistency for the\nestimator of the fixed effects and prove asymptotic normality for the\nunconditional average partial effects. Simulation studies show that our method\nworks well with finite samples, and an empirical application using the\nrisk-sharing data from Nyakatoke highlights the importance of employing proper\nstatistical inferential procedures.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.23852v1"
    },
    {
        "title": "Inference in a Stationary/Nonstationary Autoregressive\n  Time-Varying-Parameter Model",
        "authors": [
            "Donald W. K. Andrews",
            "Ming Li"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper considers nonparametric estimation and inference in first-order\nautoregressive (AR(1)) models with deterministically time-varying parameters. A\nkey feature of the proposed approach is to allow for time-varying stationarity\nin some time periods, time-varying nonstationarity (i.e., unit root or\nlocal-to-unit root behavior) in other periods, and smooth transitions between\nthe two. The estimation of the AR parameter at any time point is based on a\nlocal least squares regression method, where the relevant initial condition is\nendogenous. We obtain limit distributions for the AR parameter estimator and\nt-statistic at a given point $\\tau$ in time when the parameter exhibits unit\nroot, local-to-unity, or stationary/stationary-like behavior at time $\\tau$.\nThese results are used to construct confidence intervals and median-unbiased\ninterval estimators for the AR parameter at any specified point in time. The\nconfidence intervals have correct asymptotic coverage probabilities with the\ncoverage holding uniformly over stationary and nonstationary behavior of the\nobservations.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.00358v1"
    },
    {
        "title": "The ET Interview: Professor Joel L. Horowitz",
        "authors": [
            "Sokbae Lee"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Joel L. Horowitz has made profound contributions to many areas in\neconometrics and statistics. These include bootstrap methods, semiparametric\nand nonparametric estimation, specification testing, nonparametric instrumental\nvariables estimation, high-dimensional models, functional data analysis, and\nshape restrictions, among others. Originally trained as a physicist, Joel made\na pivotal transition to econometrics, greatly benefiting our profession.\nThroughout his career, he has collaborated extensively with a diverse range of\ncoauthors, including students, departmental colleagues, and scholars from\naround the globe. Joel was born in 1941 in Pasadena, California. He attended\nStanford for his undergraduate studies and obtained his Ph.D. in physics from\nCornell in 1967. He has been Charles E. and Emma H. Morrison Professor of\nEconomics at Northwestern University since 2001. Prior to that, he was a\nfaculty member at the University of Iowa (1982-2001). He has served as a\nco-editor of Econometric Theory (1992-2000) and Econometrica (2000-2004). He is\na Fellow of the Econometric Society and of the American Statistical\nAssociation, and an elected member of the International Statistical Institute.\nThe majority of this interview took place in London during June 2022.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.00886v1"
    },
    {
        "title": "Empirical Welfare Analysis with Hedonic Budget Constraints",
        "authors": [
            "Debopam Bhattacharya",
            "Ekaterina Oparina",
            "Qianya Xu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We analyze demand settings where heterogeneous consumers maximize utility for\nproduct attributes subject to a nonlinear budget constraint. We develop\nnonparametric methods for welfare-analysis of interventions that change the\nconstraint. Two new findings are Roy's identity for smooth, nonlinear budgets,\nwhich yields a Partial Differential Equation system, and a Slutsky-like\nsymmetry condition for demand. Under scalar unobserved heterogeneity and\nsingle-crossing preferences, the coefficient functions in the PDEs are\nnonparametrically identified, and under symmetry, lead to path-independent,\nmoney-metric welfare. We illustrate our methods with welfare evaluation of a\nhypothetical change in relationship between property rent and neighborhood\nschool-quality using British microdata.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.01064v1"
    },
    {
        "title": "Changes-In-Changes For Discrete Treatment",
        "authors": [
            "Onil Boussim"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper generalizes the changes-in-changes (CIC) model to handle discrete\ntreatments with more than two categories, extending the binary case of Athey\nand Imbens (2006). While the original CIC model is well-suited for binary\ntreatments, it cannot accommodate multi-category discrete treatments often\nfound in economic and policy settings. Although recent work has extended CIC to\ncontinuous treatments, there remains a gap for multi-category discrete\ntreatments. I introduce a generalized CIC model that adapts the rank invariance\nassumption to multiple treatment levels, allowing for robust modeling while\ncapturing the distinct effects of varying treatment intensities.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.01617v1"
    },
    {
        "title": "Estimating Nonseparable Selection Models: A Functional Contraction\n  Approach",
        "authors": [
            "Fan Wu",
            "Yi Xin"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a new method for estimating nonseparable selection models. We show\nthat, given the selection rule and the observed selected outcome distribution,\nthe potential outcome distribution can be characterized as the fixed point of\nan operator, and we prove that this operator is a functional contraction. We\npropose a two-step semiparametric maximum likelihood estimator to estimate the\nselection model and the potential outcome distribution. The consistency and\nasymptotic normality of the estimator are established. Our approach performs\nwell in Monte Carlo simulations and is applicable in a variety of empirical\nsettings where only a selected sample of outcomes is observed. Examples include\nconsumer demand models with only transaction prices, auctions with incomplete\nbid data, and Roy models with data on accepted wages.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.01799v1"
    },
    {
        "title": "On the Asymptotic Properties of Debiased Machine Learning Estimators",
        "authors": [
            "Amilcar Velez"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper studies the properties of debiased machine learning (DML)\nestimators under a novel asymptotic framework, offering insights for improving\nthe performance of these estimators in applications. DML is an estimation\nmethod suited to economic models where the parameter of interest depends on\nunknown nuisance functions that must be estimated. It requires weaker\nconditions than previous methods while still ensuring standard asymptotic\nproperties. Existing theoretical results do not distinguish between two\nalternative versions of DML estimators, DML1 and DML2. Under a new asymptotic\nframework, this paper demonstrates that DML2 asymptotically dominates DML1 in\nterms of bias and mean squared error, formalizing a previous conjecture based\non simulation results regarding their relative performance. Additionally, this\npaper provides guidance for improving the performance of DML2 in applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.01864v1"
    },
    {
        "title": "Randomly Assigned First Differences?",
        "authors": [
            "Clément de Chaisemartin"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  I consider treatment-effect estimation with a two-periods panel, using a\nfirst-difference regression of the outcome evolution $\\Delta Y_g$ on the\ntreatment evolution $\\Delta D_g$. To justify this regression, one may assume\nthat $\\Delta D_g$ is as good as randomly assigned, namely uncorrelated to the\nresidual of the first-differenced model and to the treatment's effect. This\npaper shows that if one posits a causal model in levels between the treatment\nand the outcome, then the residual of the first-differenced model is a function\nof the period-one treatment $D_{g,1}$, so the first-difference regression may\nsuffer from an omitted variable bias whenever $\\Delta D_g$ is correlated to\n$D_{g,1}$. To solve this problem, one can control for $E(\\Delta D_g|D_{g,1})$\nin the regression. I apply these results to a first-difference regression of\nthe 1999 to 2007 employment evolutions of US industries on the evolution of\ntheir Chinese imports penetration ratio, estimated on the data of Acemoglu et\nal. (2016). $\\Delta D_g$ and $D_{g,1}$ are very strongly correlated, so this\nregression may suffer from an omitted variable bias. Controlling for $E(\\Delta\nD_g|D_{g,1})$ reduces the estimated effect of Chinese imports penetration by 42\nto 51%, and makes it less significant.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.03208v5"
    },
    {
        "title": "Identification and Inference in General Bunching Designs",
        "authors": [
            "Myunghyun Song"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper develops a formal econometric framework and tools for the\nidentification and inference of a structural parameter in general bunching\ndesigns. We present both point and partial identification results, which\ngeneralize previous approaches in the literature. The key assumption for point\nidentification is the analyticity of the counterfactual density, which defines\na broader class of distributions than many well-known parametric families. In\nthe partial identification approach, the analyticity condition is relaxed and\nvarious shape restrictions can be incorporated, including those found in the\nliterature. Both of our identification results account for observable\nheterogeneity in the model, which has previously been permitted only in limited\nways. We provide a suite of counterfactual estimation and inference methods,\ntermed the generalized polynomial strategy. Our method restores the merits of\nthe original polynomial strategy proposed by Chetty et al. (2011) while\naddressing several weaknesses in the widespread practice. The efficacy of the\nproposed method is demonstrated compared to a version of the polynomial\nestimator in a series of Monte Carlo studies within the augmented isoelastic\nmodel. We revisit the data used in Saez (2010) and find substantially different\nresults relative to those from the polynomial strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.03625v2"
    },
    {
        "title": "An Adversarial Approach to Identification",
        "authors": [
            "Irene Botosaru",
            "Isaac Loh",
            "Chris Muris"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We introduce a new framework for characterizing identified sets of structural\nand counterfactual parameters in econometric models. By reformulating the\nidentification problem as a set membership question, we leverage the separating\nhyperplane theorem in the space of observed probability measures to\ncharacterize the identified set through the zeros of a discrepancy function\nwith an adversarial game interpretation. The set can be a singleton, resulting\nin point identification. A feature of many econometric models, with or without\ndistributional assumptions on the error terms, is that the probability measure\nof observed variables can be expressed as a linear transformation of the\nprobability measure of latent variables. This structure provides a unifying\nframework and facilitates computation and inference via linear programming. We\ndemonstrate the versatility of our approach by applying it to nonlinear panel\nmodels with fixed effects, with parametric and nonparametric error\ndistributions, and across various exogeneity restrictions, including strict and\nsequential.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.04239v2"
    },
    {
        "title": "Lee Bounds with a Continuous Treatment in Sample Selection",
        "authors": [
            "Ying-Ying Lee",
            "Chu-An Liu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Sample selection bias arises in causal inference when a treatment affects\nboth the outcome and the researcher's ability to observe it. This paper\ngeneralizes the sharp bounds in Lee (2009) for the average treatment effect of\na binary treatment to a continuous/multivalued treatment. We revisit the\nImbens, Rubin, and Sacerdote (2001) lottery data to study the effect of the\nprize on earnings that are only observed for the employed and the survey\nrespondents. We evaluate the Job Crops program to study the effect of training\nhours on wages. To identify the average treatment effect of always-takers who\nare selected into samples with observed outcomes regardless of the treatment\nvalue they receive, we assume that if a subject is selected at some sufficient\ntreatment values, then it remains selected at all treatment values. For\nexample, if program participants are employed with one week of training, then\nthey remain employed with any training hours. This sufficient treatment values\nassumption includes the monotone assumption on the treatment effect on\nselection as a special case. We further allow the conditional independence\nassumption and subjects with different pretreatment covariates to have\ndifferent sufficient treatment values. The practical estimation and inference\ntheory utilize the orthogonal moment function and cross-fitting for double\ndebiased machine learning.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.04312v2"
    },
    {
        "title": "Partial Identification of Distributional Treatment Effects in Panel Data\n  using Copula Equality Assumptions",
        "authors": [
            "Heshani Madigasekara",
            "D. S. Poskitt",
            "Lina Zhang",
            "Xueyan Zhao"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper aims to partially identify the distributional treatment effects\n(DTEs) that depend on the unknown joint distribution of treated and untreated\npotential outcomes. We construct the DTE bounds using panel data and allow\nindividuals to switch between the treated and untreated states more than once\nover time. Individuals are grouped based on their past treatment history, and\nDTEs are allowed to be heterogeneous across different groups. We provide two\nalternative group-wise copula equality assumptions to bound the unknown joint\nand the DTEs, both of which leverage information from the past observations.\nTestability of these two assumptions are also discussed, and test results are\npresented. We apply this method to study the treatment effect heterogeneity of\nexercising on the adults' body weight. These results demonstrate that our\nmethod improves the identification power of the DTE bounds compared to the\nexisting methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.04450v1"
    },
    {
        "title": "The role of expansion strategies and operational attributes on hotel\n  performance: a compositional approach",
        "authors": [
            "Carles Mulet-Forteza",
            "Berta Ferrer-Rosell",
            "Onofre Martorell Cunill",
            "Salvador Linares-Mustarós"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This study aims to explore the impact of expansion strategies and specific\nattributes of hotel establishments on the performance of international hotel\nchains, focusing on four key performance indicators: RevPAR, efficiency,\noccupancy, and asset turnover. Data were collected from 255 hotels across\nvarious international hotel chains, providing a comprehensive assessment of how\ndifferent expansion strategies and hotel attributes influence performance. The\nresearch employs compositional data analysis (CoDA) to address the\nmethodological limitations of traditional financial ratios in statistical\nanalysis. The findings indicate that ownership-based expansion strategies\nresult in higher operational performance, as measured by revenue per available\nroom, but yield lower economic performance due to the high capital investment\nrequired. Non-ownership strategies, such as management contracts and\nfranchising, show superior economic efficiency, offering more flexibility and\nreduced financial risk. This study contributes to the hospitality management\nliterature by applying CoDA, a novel methodological approach in this field, to\nexamine the performance of different hotel expansion strategies with a sound\nand more appropriate method. The insights provided can guide hotel managers and\ninvestors in making informed decisions to optimize both operational and\neconomic performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.04640v1"
    },
    {
        "title": "Inference for Treatment Effects Conditional on Generalized Principal\n  Strata using Instrumental Variables",
        "authors": [
            "Yuehao Bai",
            "Shunzhuang Huang",
            "Sarah Moon",
            "Andres Santos",
            "Azeem M. Shaikh",
            "Edward J. Vytlacil"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In a setting with a multi-valued outcome, treatment and instrument, this\npaper considers the problem of inference for a general class of treatment\neffect parameters. The class of parameters considered are those that can be\nexpressed as the expectation of a function of the response type conditional on\na generalized principal stratum. Here, the response type simply refers to the\nvector of potential outcomes and potential treatments, and a generalized\nprincipal stratum is a set of possible values for the response type. In\naddition to instrument exogeneity, the main substantive restriction imposed\nrules out certain values for the response types in the sense that they are\nassumed to occur with probability zero. It is shown through a series of\nexamples that this framework includes a wide variety of parameters and\nassumptions that have been considered in the previous literature. A key result\nin our analysis is a characterization of the identified set for such parameters\nunder these assumptions in terms of existence of a non-negative solution to\nlinear systems of equations with a special structure. We propose methods for\ninference exploiting this special structure and recent results in Fang et al.\n(2023).\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05220v1"
    },
    {
        "title": "Nowcasting distributions: a functional MIDAS model",
        "authors": [
            "Massimiliano Marcellino",
            "Andrea Renzetti",
            "Tommaso Tornese"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a functional MIDAS model to leverage high-frequency information\nfor forecasting and nowcasting distributions observed at a lower frequency. We\napproximate the low-frequency distribution using Functional Principal Component\nAnalysis and consider a group lasso spike-and-slab prior to identify the\nrelevant predictors in the finite-dimensional SUR-MIDAS approximation of the\nfunctional MIDAS model. In our application, we use the model to nowcast the\nU.S. households' income distribution. Our findings indicate that the model\nenhances forecast accuracy for the entire target distribution and for key\nfeatures of the distribution that signal changes in inequality.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05629v1"
    },
    {
        "title": "Firm Heterogeneity and Macroeconomic Fluctuations: a Functional VAR\n  model",
        "authors": [
            "Massimiliano Marcellino",
            "Andrea Renzetti",
            "Tommaso Tornese"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We develop a Functional Augmented Vector Autoregression (FunVAR) model to\nexplicitly incorporate firm-level heterogeneity observed in more than one\ndimension and study its interaction with aggregate macroeconomic fluctuations.\nOur methodology employs dimensionality reduction techniques for tensor data\nobjects to approximate the joint distribution of firm-level characteristics.\nMore broadly, our framework can be used for assessing predictions from\nstructural models that account for micro-level heterogeneity observed on\nmultiple dimensions. Leveraging firm-level data from the Compustat database, we\nuse the FunVAR model to analyze the propagation of total factor productivity\n(TFP) shocks, examining their impact on both macroeconomic aggregates and the\ncross-sectional distribution of capital and labor across firms.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05695v1"
    },
    {
        "title": "Return-forecasting and Volatility-forecasting Power of On-chain\n  Activities in the Cryptocurrency Market",
        "authors": [
            "Yeguang Chi",
            " Qionghua",
            " Chu",
            "Wenyan Hao"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We investigate the return-forecasting and volatility-forecasting power of\nintraday on-chain flow data for BTC, ETH, and USDT, and the associated option\nstrategies. First, we find that USDT net inflow into cryptocurrency exchanges\npositively forecasts future returns of both BTC and ETH, with the strongest\neffect at the 1-hour frequency. Second, we find that ETH net inflow into\ncryptocurrency exchanges negatively forecasts future returns of ETH. Third, we\nfind that BTC net inflow into cryptocurrency exchanges does not significantly\nforecast future returns of BTC. Finally, we confirm that selling 0DTE ETH call\noptions is a profitable trading strategy when the net inflow into\ncryptocurrency exchanges is high. Our study lends new insights into the\nemerging literature that studies the on-chain activities and their\nasset-pricing impact in the cryptocurrency market.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.06327v1"
    },
    {
        "title": "Dynamic Evolutionary Game Analysis of How Fintech in Banking Mitigates\n  Risks in Agricultural Supply Chain Finance",
        "authors": [
            "Qiang Wan",
            "Jun Cui"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper explores the impact of banking fintech on reducing financial risks\nin the agricultural supply chain, focusing on the secondary allocation of\ncommercial credit. The study constructs a three-player evolutionary game model\ninvolving banks, core enterprises, and SMEs to analyze how fintech innovations,\nsuch as big data credit assessment, blockchain, and AI-driven risk evaluation,\ninfluence financial risks and access to credit. The findings reveal that\nbanking fintech reduces financing costs and mitigates financial risks by\nimproving transaction reliability, enhancing risk identification, and\nminimizing information asymmetry. By optimizing cooperation between banks, core\nenterprises, and SMEs, fintech solutions enhance the stability of the\nagricultural supply chain, contributing to rural revitalization goals and\nsustainable agricultural development. The study provides new theoretical\ninsights and practical recommendations for improving agricultural finance\nsystems and reducing financial risks.\n  Keywords: banking fintech, agricultural supply chain, financial risk,\ncommercial credit, SMEs, evolutionary game model, big data, blockchain,\nAI-driven risk evaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07604v1"
    },
    {
        "title": "Spatial Competition on Psychological Pricing Strategies: Preliminary\n  Evidence from an Online Marketplace",
        "authors": [
            "Magdalena Schindl",
            "Felix Reichel"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  According zu Kadir et al. (2023) online marketplaces are used to buy and sell\nproducts and services, as well as to exchange money and data between users or\nthe platform. Due to the large product selection, low costs and the ease of\nshopping without physical restrictions as well as the technical possibilities,\nonline marketplaces have grown rapidly Kadir et al. (2023). Online marketplaces\nare also used in the consumer-to-consumer (C2C) sector and thus offer a broad\nuser group a marketplace, for example for used products. This article focuses\non Willhaben.at (2024), a leading C2C marketplace in Austria, as stated by\nObersteiner, Schmied, and Pamperl (2023). The empirical analysis in this course\nessay centers around the offer ads of Woom Bikes, a standardised product which\nis sold on Willhaben. Through web scraping, a dataset of approximately 826\nobservations was created, focusing on mid-to-high price segment bicycles, which\nare characterized by price stability and uniformity as we claim. This analysis\naims to create analyse ad listing prices through predictive models using\nwillhaben product listing attributes and using the spatial distribution of one\nof the product attributes.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07808v2"
    },
    {
        "title": "Impact of R&D and AI Investments on Economic Growth and Credit Rating",
        "authors": [
            "Davit Gondauri",
            "Ekaterine Mikautadze"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The research and development (R&D) phase is essential for fostering\ninnovation and aligns with long-term strategies in both public and private\nsectors. This study addresses two primary research questions: (1) assessing the\nrelationship between R&D investments and GDP through regression analysis, and\n(2) estimating the economic value added (EVA) that Georgia must generate to\nprogress from a BB to a BBB credit rating. Using World Bank data from\n2014-2022, this analysis found that increasing R&D, with an emphasis on AI, by\n30-35% has a measurable impact on GDP. Regression results reveal a coefficient\nof 7.02%, indicating a 10% increase in R&D leads to a 0.70% GDP rise, with an\n81.1% determination coefficient and a strong 90.1% correlation.\n  Georgia's EVA model was calculated to determine the additional value needed\nfor a BBB rating, comparing indicators from Greece, Hungary, India, and\nKazakhstan as benchmarks. Key economic indicators considered were nominal GDP,\nGDP per capita, real GDP growth, and fiscal indicators (government balance/GDP,\ndebt/GDP). The EVA model projects that to achieve a BBB rating within nine\nyears, Georgia requires $61.7 billion in investments. Utilizing EVA and\ncomprehensive economic indicators will support informed decision-making and\nenhance the analysis of Georgia's economic trajectory.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07817v1"
    },
    {
        "title": "On the (Mis)Use of Machine Learning with Panel Data",
        "authors": [
            "Augusto Cerqua",
            "Marco Letta",
            "Gabriele Pinto"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Machine Learning (ML) is increasingly employed to inform and support\npolicymaking interventions. This methodological article cautions practitioners\nabout common but often overlooked pitfalls associated with the uncritical\napplication of supervised ML algorithms to panel data. Ignoring the\ncross-sectional and longitudinal structure of this data can lead to\nhard-to-detect data leakage, inflated out-of-sample performance, and an\ninadvertent overestimation of the real-world usefulness and applicability of ML\nmodels. After clarifying these issues, we provide practical guidelines and best\npractices for applied researchers to ensure the correct implementation of\nsupervised ML in panel data environments, emphasizing the need to define ex\nante the primary goal of the analysis and align the ML pipeline accordingly. An\nempirical application based on over 3,000 US counties from 2000 to 2019\nillustrates the practical relevance of these points across nearly 500 models\nfor both classification and regression tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.09218v1"
    },
    {
        "title": "Difference-in-Differences with Sample Selection",
        "authors": [
            "Gayani Rathnayake",
            "Akanksha Negi",
            "Otavio Bartalotti",
            "Xueyan Zhao"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We consider identification of average treatment effects on the treated (ATT)\nwithin the difference-in-differences (DiD) framework in the presence of\nendogenous sample selection. First, we establish that the usual DiD estimand\nfails to recover meaningful treatment effects, even if selection and treatment\nassignment are independent. Next, we partially identify the ATT for individuals\nwho are always observed post-treatment regardless of their treatment status,\nand derive bounds on this parameter under different sets of assumptions about\nthe relationship between sample selection and treatment assignment. Extensions\nto the repeated cross-section and two-by-two comparisons in the staggered\nadoption case are explored. Furthermore, we provide identification results for\nthe ATT of three additional empirically relevant latent groups by incorporating\noutcome mean dominance assumptions which have intuitive appeal in applications.\nFinally, two empirical illustrations demonstrate the approach's usefulness by\nrevisiting (i) the effect of a job training program on earnings(Calonico &\nSmith, 2017) and (ii) the effect of a working-from-home policy on employee\nperformance (Bloom, Liang, Roberts, & Ying, 2015).\n",
        "pdf_link": "http://arxiv.org/pdf/2411.09221v2"
    },
    {
        "title": "Sparse Interval-valued Time Series Modeling with Machine Learning",
        "authors": [
            "Haowen Bao",
            "Yongmiao Hong",
            "Yuying Sun",
            "Shouyang Wang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  By treating intervals as inseparable sets, this paper proposes sparse machine\nlearning regressions for high-dimensional interval-valued time series. With\nLASSO or adaptive LASSO techniques, we develop a penalized minimum distance\nestimation, which covers point-based estimators are special cases. We establish\nthe consistency and oracle properties of the proposed penalized estimator,\nregardless of whether the number of predictors is diverging with the sample\nsize. Monte Carlo simulations demonstrate the favorable finite sample\nproperties of the proposed estimation. Empirical applications to\ninterval-valued crude oil price forecasting and sparse index-tracking portfolio\nconstruction illustrate the robustness and effectiveness of our method against\ncompeting approaches, including random forest and multilayer perceptron for\ninterval-valued data. Our findings highlight the potential of machine learning\ntechniques in interval-valued time series analysis, offering new insights for\nfinancial forecasting and portfolio management.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.09452v1"
    },
    {
        "title": "Sharp Testable Implications of Encouragement Designs",
        "authors": [
            "Yuehao Bai",
            "Max Tabord-Meehan"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper studies the sharp testable implications of an additive random\nutility model with a discrete multi-valued treatment and a discrete\nmulti-valued instrument, in which each value of the instrument only weakly\nincreases the utility of one choice. Borrowing the terminology used in\nrandomized experiments, we call such a setting an encouragement design. We\nderive inequalities in terms of the conditional choice probabilities that\ncharacterize when the distribution of the observed data is consistent with such\na model. Through a novel constructive argument, we further show these\ninequalities are sharp in the sense that any distribution of the observed data\nthat satisfies these inequalities is generated by this additive random utility\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.09808v1"
    },
    {
        "title": "Dynamic Causal Effects in a Nonlinear World: the Good, the Bad, and the\n  Ugly",
        "authors": [
            "Michal Kolesár",
            "Mikkel Plagborg-Møller"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Applied macroeconomists frequently use impulse response estimators motivated\nby linear models. We study whether the estimands of such procedures have a\ncausal interpretation when the true data generating process is in fact\nnonlinear. We show that vector autoregressions and linear local projections\nonto observed shocks or proxies identify weighted averages of causal effects\nregardless of the extent of nonlinearities. By contrast, identification\napproaches that exploit heteroskedasticity or non-Gaussianity of latent shocks\nare highly sensitive to departures from linearity. Our analysis is based on new\nresults on the identification of marginal treatment effects through weighted\nregressions, which may also be of interest to researchers outside\nmacroeconomics.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.10415v2"
    },
    {
        "title": "Treatment Effect Estimators as Weighted Outcomes",
        "authors": [
            "Michael C. Knaus"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Estimators that weight observed outcomes to form effect estimates have a long\ntradition. Their outcome weights are widely used in established procedures,\nsuch as checking covariate balance, characterizing target populations, or\ndetecting and managing extreme weights. This paper introduces a general\nframework for deriving such outcome weights. It establishes when and how\nnumerical equivalence between an original estimator representation as moment\ncondition and a unique weighted representation can be obtained. The framework\nis applied to derive novel outcome weights for the six seminal instances of\ndouble machine learning and generalized random forests, while recovering\nexisting results for other estimators as special cases. The analysis highlights\nthat implementation choices determine (i) the availability of outcome weights\nand (ii) their properties. Notably, standard implementations of partially\nlinear regression-based estimators, like causal forests, employ outcome weights\nthat do not sum to (minus) one in the (un)treated group, not fulfilling a\nproperty often considered desirable.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.11559v2"
    },
    {
        "title": "Clustering with Potential Multidimensionality: Inference and Practice",
        "authors": [
            "Ruonan Xu",
            "Luther Yap"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We show how clustering standard errors in one or more dimensions can be\njustified in M-estimation when there is sampling or assignment uncertainty.\nSince existing procedures for variance estimation are either conservative or\ninvalid, we propose a variance estimator that refines a conservative procedure\nand remains valid. We then interpret environments where clustering is\nfrequently employed in empirical work from our design-based perspective and\nprovide insights on their estimands and inference procedures.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13372v1"
    },
    {
        "title": "Dynamic spatial interaction models for a leader's resource allocation\n  and followers' multiple activities",
        "authors": [
            "Hanbat Jeong"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper introduces a novel spatial interaction model to explore the\ndecision-making processes of two types of agents-a leader and followers-with\ncentral and local governments serving as empirical representations. The model\naccounts for three key features: (i) resource allocations from the leader to\nthe followers and the resulting strategic interactions, (ii) followers' choices\nacross multiple activities, and (iii) interactions among these activities. We\ndevelop a network game to examine the micro-foundations of these processes. In\nthis game, followers engage in multiple activities, while the leader allocates\nresources by monitoring the externalities arising from followers' interactions.\nThe game's unique NE is the foundation for our econometric framework, providing\nequilibrium measures to understand the short-term impacts of changes in\nfollowers' characteristics and their long-term consequences. To estimate the\nagent payoff parameters, we employ the QML estimation method and examine the\nasymptotic properties of the QML estimator to ensure robust statistical\ninferences. Empirically, we investigate interactions among U.S. states in\npublic welfare expenditures (PWE) and housing and community development\nexpenditures (HCDE), focusing on how federal grants influence these\nexpenditures and the interactions among state governments. Our findings reveal\npositive spillovers in states' PWEs, complementarity between the two\nexpenditures within states, and negative cross-variable spillovers between\nthem. Additionally, we observe positive effects of federal grants on both\nexpenditures. Counterfactual simulations indicate that federal interventions\nlead to a 6.46% increase in social welfare by increasing the states' efforts on\nPWE and HCDE. However, due to the limited flexibility in federal grants, their\nmagnitudes are smaller than the proportion of federal grants within the states'\ntotal revenues.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13810v1"
    },
    {
        "title": "From Replications to Revelations: Heteroskedasticity-Robust Inference",
        "authors": [
            "Sebastian Kranz"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Analysing the Stata regression commands from 4,420 reproduction packages of\nleading economic journals, we find that, among the 40,571 regressions\nspecifying heteroskedasticity-robust standard errors, 98.1% adhere to Stata's\ndefault HC1 specification. We then compare several heteroskedasticity-robust\ninference methods with a large-scale Monte Carlo study based on regressions\nfrom 155 reproduction packages. Our results show that t-tests based on HC1 or\nHC2 with default degrees of freedom exhibit substantial over-rejection.\nInference methods with customized degrees of freedom, as proposed by Bell and\nMcCaffrey (2002), Hansen (2024), and a novel approach based on partial\nleverages, perform best. Additionally, we provide deeper insights into the role\nof leverages and partial leverages across different inference methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.14763v2"
    },
    {
        "title": "Utilization and Profitability of Tractor Services for Maize Farming in\n  Ejura-Sekyedumase Municipality, Ghana",
        "authors": [
            "Fred Nimoh",
            "Innocent Yao Yevu",
            "Attah-Nyame Essampong",
            "Asante Emmanuel Addo",
            "Addai Kevin"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Maize farming is a major livelihood activity for many farmers in Ghana.\nUnfortunately, farmers usually do not obtain the expected returns on their\ninvestment due to reliance on rudimentary, labor-intensive, and inefficient\nmethods of production. Using cross-sectional data from 359 maize farmers, this\nstudy investigates the profitability and determinants of the use of tractor\nservices for maize production in Ejura-Sekyedumase, Ashanti Region of Ghana.\nResults from descriptive and profitability analyses reveal that tractor\nservices such as ploughing and shelling are widely used, but their\nprofitability varies significantly among farmers. Key factors influencing\nprofitability include farm size, fertilizer quantity applied, and farmer\nexperience. Results from a multivariate probit analysis also showed that\nfarming experience, fertilizer quantity, and profit per acre have a positive\ninfluence on tractor service use for shelling, while household size, farm size,\nand FBO have a negative influence. Farming experience, fertilizer quantity, and\nprofit per acre positively influence tractor service use for ploughing, while\nfarm size has a negative influence. A t-test result reveals a statistically\nsignificant difference in profit between farmers who use tractor services and\nthose who do not. Specifically, farmers who utilize tractor services on their\nmaize farm had a return to cost of 9 percent more than those who do not\n(p-value < 0.05). The Kendall's result showed a moderate agreement among the\nmaize farmers (first ranked being financial issues) in their ability to\naccess/utilize tractor services on their farm.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.15797v1"
    },
    {
        "title": "A Supervised Machine Learning Approach for Assessing Grant Peer Review\n  Reports",
        "authors": [
            "Gabriel Okasa",
            "Alberto de León",
            "Michaela Strinzel",
            "Anne Jorstad",
            "Katrin Milzow",
            "Matthias Egger",
            "Stefan Müller"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Peer review in grant evaluation informs funding decisions, but the contents\nof peer review reports are rarely analyzed. In this work, we develop a\nthoroughly tested pipeline to analyze the texts of grant peer review reports\nusing methods from applied Natural Language Processing (NLP) and machine\nlearning. We start by developing twelve categories reflecting content of grant\npeer review reports that are of interest to research funders. This is followed\nby multiple human annotators' iterative annotation of these categories in a\nnovel text corpus of grant peer review reports submitted to the Swiss National\nScience Foundation. After validating the human annotation, we use the annotated\ntexts to fine-tune pre-trained transformer models to classify these categories\nat scale, while conducting several robustness and validation checks. Our\nresults show that many categories can be reliably identified by human\nannotators and machine learning approaches. However, the choice of text\nclassification approach considerably influences the classification performance.\nWe also find a high correspondence between out-of-sample classification\nperformance and human annotators' perceived difficulty in identifying\ncategories. Our results and publicly available fine-tuned transformer models\nwill allow researchers and research funders and anybody interested in peer\nreview to examine and report on the contents of these reports in a structured\nmanner. Ultimately, we hope our approach can contribute to ensuring the quality\nand trustworthiness of grant peer review.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.16662v2"
    },
    {
        "title": "A Binary IV Model for Persuasion: Profiling Persuasion Types among\n  Compliers",
        "authors": [
            "Zeyang Yu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In an empirical study of persuasion, researchers often use a binary\ninstrument to encourage individuals to consume information and take some\naction. We show that, with a binary Imbens-Angrist instrumental variable model\nand the monotone treatment response assumption, it is possible to identify the\njoint distribution of potential outcomes among compliers. This is necessary to\nidentify the percentage of mobilised voters and their statistical\ncharacteristic defined by the moments of the joint distribution of treatment\nand covariates. Specifically, we develop a method that enables researchers to\nidentify the statistical characteristic of persuasion types: always-voters,\nnever-voters, and mobilised voters among compliers. These findings extend the\nkappa weighting results in Abadie (2003). We also provide a sharp test for the\ntwo sets of identification assumptions. The test boils down to testing whether\nthere exists a nonnegative solution to a possibly under-determined system of\nlinear equations with known coefficients. An application based on Green et al.\n(2003) is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.16906v1"
    },
    {
        "title": "Normal Approximation for U-Statistics with Cross-Sectional Dependence",
        "authors": [
            "Weiguang Liu"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We apply Stein's method to investigate the normal approximation for both\nnon-degenerate and degenerate U-statistics with cross-sectionally dependent\nunderlying processes in the Wasserstein metric. We show that the convergence\nrates depend on the mixing rates, the sparsity of the cross-sectional\ndependence, and the moments of the kernel functions. Conditions are derived for\ncentral limit theorems to hold as corollaries. We demonstrate one application\nof the theoretical results with nonparametric specification test for data with\ncross-sectional dependence.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.16978v1"
    },
    {
        "title": "Ranking probabilistic forecasting models with different loss functions",
        "authors": [
            "Tomasz Serafin",
            "Bartosz Uniejewski"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this study, we introduced various statistical performance metrics, based\non the pinball loss and the empirical coverage, for the ranking of\nprobabilistic forecasting models. We tested the ability of the proposed metrics\nto determine the top performing forecasting model and investigated the use of\nwhich metric corresponds to the highest average per-trade profit in the\nout-of-sample period. Our findings show that for the considered trading\nstrategy, ranking the forecasting models according to the coverage of quantile\nforecasts used in the trading hours exhibits a superior economic performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.17743v1"
    },
    {
        "title": "Warfare Ignited Price Contagion Dynamics in Early Modern Europe",
        "authors": [
            "Emile Esmaili",
            "Michael J. Puma",
            "Francis Ludlow",
            "Poul Holm",
            "Eva Jobbova"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Economic historians have long studied market integration and contagion\ndynamics during periods of warfare and global stress, but there is a lack of\nmodel-based evidence on these phenomena. This paper uses an econometric\ncontagion model, the Diebold-Yilmaz framework, to examine the dynamics of\neconomic shocks across European markets in the early modern period. Our\nfindings suggest that key periods of violent conflicts significantly increased\nfood price spillover across cities, causing widespread disruptions across\nEurope. We also demonstrate the ability of this framework to capture relevant\nhistorical dynamics between the main trade centers of the period.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.18978v3"
    },
    {
        "title": "Peer Effects and Herd Behavior: An Empirical Study Based on the \"Double\n  11\" Shopping Festival",
        "authors": [
            "Hambur Wang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This study employs a Bayesian Probit model to empirically analyze peer\neffects and herd behavior among consumers during the \"Double 11\" shopping\nfestival, using data collected through a questionnaire survey. The results\ndemonstrate that peer effects significantly influence consumer decision-making,\nwith the probability of participation in the shopping event increasing notably\nwhen roommates are involved. Additionally, factors such as gender, online\nshopping experience, and fashion consciousness significantly impact consumers'\nherd behavior. This research not only enhances the understanding of online\nshopping behavior among college students but also provides empirical evidence\nfor e-commerce platforms to formulate targeted marketing strategies. Finally,\nthe study discusses the fragility of online consumption activities, the need\nfor adjustments in corporate marketing strategies, and the importance of\npromoting a healthy online culture.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.00233v1"
    },
    {
        "title": "Optimization of Delivery Routes for Fresh E-commerce in Pre-warehouse\n  Mode",
        "authors": [
            "Alice Harward",
            "Junjie Lin",
            "Yun Wang",
            "Xiaoke Xie"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  With the development of the economy, fresh food e-commerce has experienced\nrapid growth. One of the core competitive advantages of fresh food e-commerce\nplatforms lies in selecting an appropriate logistics distribution model. This\nstudy focuses on the front warehouse model, aiming to minimize distribution\ncosts. Considering the perishable nature and short shelf life of fresh food, a\ndistribution route optimization model is constructed, and the saving mileage\nmethod is designed to determine the optimal distribution scheme. The results\nindicate that under certain conditions, different distribution schemes\nsignificantly impact the performance of fresh food e-commerce platforms. Based\non a review of domestic and international research, this paper takes Dingdong\nMaicai as an example to systematically introduce the basic concepts of\ndistribution route optimization in fresh food e-commerce platforms under the\nfront warehouse model, analyze the advantages of logistics distribution, and\nthoroughly examine the importance of distribution routes for fresh products.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.00634v1"
    },
    {
        "title": "Iterative Distributed Multinomial Regression",
        "authors": [
            "Yanqin Fan",
            "Yigit Okar",
            "Xuetao Shi"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This article introduces an iterative distributed computing estimator for the\nmultinomial logistic regression model with large choice sets. Compared to the\nmaximum likelihood estimator, the proposed iterative distributed estimator\nachieves significantly faster computation and, when initialized with a\nconsistent estimator, attains asymptotic efficiency under a weak dominance\ncondition. Additionally, we propose a parametric bootstrap inference procedure\nbased on the iterative distributed estimator and establish its consistency.\nExtensive simulation studies validate the effectiveness of the proposed methods\nand highlight the computational efficiency of the iterative distributed\nestimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.01030v1"
    },
    {
        "title": "Locally robust semiparametric estimation of sample selection models\n  without exclusion restrictions",
        "authors": [
            "Zhewen Pan",
            "Yifan Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Existing identification and estimation methods for semiparametric sample\nselection models rely heavily on exclusion restrictions. However, it is\ndifficult in practice to find a credible excluded variable that has a\ncorrelation with selection but no correlation with the outcome. In this paper,\nwe establish a new identification result for a semiparametric sample selection\nmodel without the exclusion restriction. The key identifying assumptions are\nnonlinearity on the selection equation and linearity on the outcome equation.\nThe difference in the functional form plays the role of an excluded variable\nand provides identification power. According to the identification result, we\npropose to estimate the model by a partially linear regression with a\nnonparametrically generated regressor. To accommodate modern machine learning\nmethods in generating the regressor, we construct an orthogonalized moment by\nadding the first-step influence function and develop a locally robust estimator\nby solving the cross-fitted orthogonalized moment condition. We prove\nroot-n-consistency and asymptotic normality of the proposed estimator under\nmild regularity conditions. A Monte Carlo simulation shows the satisfactory\nperformance of the estimator in finite samples, and an application to wage\nregression illustrates its usefulness in the absence of exclusion restrictions.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.01208v1"
    },
    {
        "title": "From rotational to scalar invariance: Enhancing identifiability in\n  score-driven factor models",
        "authors": [
            "Giuseppe Buccheri",
            "Fulvio Corsi",
            "Emilija Dzuverovic"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We show that, for a certain class of scaling matrices including the commonly\nused inverse square-root of the conditional Fisher Information, score-driven\nfactor models are identifiable up to a multiplicative scalar constant under\nvery mild restrictions. This result has no analogue in parameter-driven models,\nas it exploits the different structure of the score-driven factor dynamics.\nConsequently, score-driven models offer a clear advantage in terms of economic\ninterpretability compared to parameter-driven factor models, which are\nidentifiable only up to orthogonal transformations. Our restrictions are\norder-invariant and can be generalized to scoredriven factor models with\ndynamic loadings and nonlinear factor models. We test extensively the\nidentification strategy using simulated and real data. The empirical analysis\non financial and macroeconomic data reveals a substantial increase of\nlog-likelihood ratios and significantly improved out-of-sample forecast\nperformance when switching from the classical restrictions adopted in the\nliterature to our more flexible specifications.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.01367v1"
    },
    {
        "title": "Endogenous Interference in Randomized Experiments",
        "authors": [
            "Mengsi Gao"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper investigates the identification and inference of treatment effects\nin randomized controlled trials with social interactions. Two key network\nfeatures characterize the setting and introduce endogeneity: (1) latent\nvariables may affect both network formation and outcomes, and (2) the\nintervention may alter network structure, mediating treatment effects. I make\nthree contributions. First, I define parameters within a post-treatment network\nframework, distinguishing direct effects of treatment from indirect effects\nmediated through changes in network structure. I provide a causal\ninterpretation of the coefficients in a linear outcome model. For estimation\nand inference, I focus on a specific form of peer effects, represented by the\nfraction of treated friends. Second, in the absence of endogeneity, I establish\nthe consistency and asymptotic normality of ordinary least squares estimators.\nThird, if endogeneity is present, I propose addressing it through shift-share\ninstrumental variables, demonstrating the consistency and asymptotic normality\nof instrumental variable estimators in relatively sparse networks. For denser\nnetworks, I propose a denoised estimator based on eigendecomposition to restore\nconsistency. Finally, I revisit Prina (2015) as an empirical illustration,\ndemonstrating that treatment can influence outcomes both directly and through\nnetwork structure changes.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.02183v1"
    },
    {
        "title": "Simple and Effective Portfolio Construction with Crypto Assets",
        "authors": [
            "Kasper Johansson",
            "Stephen Boyd"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We consider the problem of constructing a portfolio that combines traditional\nfinancial assets with crypto assets. We show that despite the documented\nattributes of crypto assets, such as high volatility, heavy tails, excess\nkurtosis, and skewness, a simple extension of traditional risk allocation\nprovides robust solutions for integrating these emerging assets into broader\ninvestment strategies. Examination of the risk allocation holdings suggests an\neven simpler method, analogous to the traditional 60/40 stocks/bonds\nallocation, involving a fixed allocation to crypto and traditional assets,\ndynamically diluted with cash to achieve a target risk level.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.02654v1"
    },
    {
        "title": "A Markowitz Approach to Managing a Dynamic Basket of Moving-Band\n  Statistical Arbitrages",
        "authors": [
            "Kasper Johansson",
            "Thomas Schmelzer",
            "Stephen Boyd"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We consider the problem of managing a portfolio of moving-band statistical\narbitrages (MBSAs), inspired by the Markowitz optimization framework. We show\nhow to manage a dynamic basket of MBSAs, and illustrate the method on recent\nhistorical data, showing that it can perform very well in terms of\nrisk-adjusted return, essentially uncorrelated with the market.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.02660v1"
    },
    {
        "title": "Endogenous Heteroskedasticity in Linear Models",
        "authors": [
            "Javier Alejo",
            "Antonio F. Galvao",
            "Julian Martinez-Iriarte",
            "Gabriel Montes-Rojas"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Linear regressions with endogeneity are widely used to estimate causal\neffects. This paper studies a framework that has two common issues, endogeneity\nof the regressors, and heteroskedasticity that is allowed to depend on\nendogenous regressors, i.e., endogenous heteroskedasticity. We show that the\npresence of such conditional heteroskedasticity in the structural regression\nrenders the two-stages least squares estimator inconsistent. To solve this\nissue, we propose sufficient conditions together with a control function\napproach to identify and estimate the causal parameters of interest. We\nestablish the limiting properties of the estimator, say consistency and\nasymptotic normality, and propose inference procedures. Monte Carlo simulations\nprovide evidence of the finite sample performance of the proposed methods, and\nevaluate different implementation procedures. We revisit an empirical\napplication about job training to illustrate the methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.02767v2"
    },
    {
        "title": "On Extrapolation of Treatment Effects in Multiple-Cutoff Regression\n  Discontinuity Designs",
        "authors": [
            "Yuta Okamoto",
            "Yuuki Ozaki"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Regression discontinuity (RD) designs typically identify the treatment effect\nat a single cutoff point. But when and how can we learn about treatment effects\naway from the cutoff? This paper addresses this question within a\nmultiple-cutoff RD framework. We begin by examining the plausibility of the\nconstant bias assumption proposed by Cattaneo, Keele, Titiunik, and\nVazquez-Bare (2021) through the lens of rational decision-making behavior,\nwhich suggests that a kind of similarity between groups and whether individuals\ncan influence the running variable are important factors. We then introduce an\nalternative set of assumptions and propose a broadly applicable partial\nidentification strategy. The potential applicability and usefulness of the\nproposed bounds are illustrated through two empirical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.04265v1"
    },
    {
        "title": "Cubic-based Prediction Approach for Large Volatility Matrix using\n  High-Frequency Financial Data",
        "authors": [
            "Sung Hoon Choi",
            "Donggyu Kim"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In this paper, we develop a novel method for predicting future large\nvolatility matrices based on high-dimensional factor-based It\\^o processes.\nSeveral studies have proposed volatility matrix prediction methods using\nparametric models to account for volatility dynamics. However, these methods\noften impose restrictions, such as constant eigenvectors over time. To\ngeneralize the factor structure, we construct a cubic (order-3 tensor) form of\nan integrated volatility matrix process, which can be decomposed into low-rank\ntensor and idiosyncratic tensor components. To predict conditional expected\nlarge volatility matrices, we introduce the Projected Tensor Principal\nOrthogonal componEnt Thresholding (PT-POET) procedure and establish its\nasymptotic properties. Finally, the advantages of PT-POET are also verified by\na simulation study and illustrated by applying minimum variance portfolio\nallocation using high-frequency trading data.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.04293v1"
    },
    {
        "title": "Minimum Sliced Distance Estimation in a Class of Nonregular Econometric\n  Models",
        "authors": [
            "Yanqin Fan",
            "Hyeonseok Park"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper proposes minimum sliced distance estimation in structural\neconometric models with possibly parameter-dependent supports. In contrast to\nlikelihood-based estimation, we show that under mild regularity conditions, the\nminimum sliced distance estimator is asymptotically normally distributed\nleading to simple inference regardless of the presence/absence of parameter\ndependent supports. We illustrate the performance of our estimator on an\nauction model.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.05621v1"
    },
    {
        "title": "Property of Inverse Covariance Matrix-based Financial Adjacency Matrix\n  for Detecting Local Groups",
        "authors": [
            "Minseog Oh",
            "Donggyu Kim"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In financial applications, we often observe both global and local factors\nthat are modeled by a multi-level factor model. When detecting unknown local\ngroup memberships under such a model, employing a covariance matrix as an\nadjacency matrix for local group memberships is inadequate due to the\npredominant effect of global factors. Thus, to detect a local group structure\nmore effectively, this study introduces an inverse covariance matrix-based\nfinancial adjacency matrix (IFAM) that utilizes negative values of the inverse\ncovariance matrix. We show that IFAM ensures that the edge density between\ndifferent groups vanishes, while that within the same group remains\nnon-vanishing. This reduces falsely detected connections and helps identify\nlocal group membership accurately. To estimate IFAM under the multi-level\nfactor model, we introduce a factor-adjusted GLASSO estimator to address the\nprevalent global factor effect in the inverse covariance matrix. An empirical\nstudy using returns from international stocks across 20 financial markets\ndemonstrates that incorporating IFAM effectively detects latent local groups,\nwhich helps improve the minimum variance portfolio allocation performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.05664v1"
    },
    {
        "title": "Bundle Choice Model with Endogenous Regressors: An Application to Soda\n  Tax",
        "authors": [
            "Tao Sun"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper proposes a Bayesian factor-augmented bundle choice model to\nestimate joint consumption as well as the substitutability and complementarity\nof multiple goods in the presence of endogenous regressors. The model extends\nthe two primary treatments of endogeneity in existing bundle choice models: (1)\nendogenous market-level prices and (2) time-invariant unobserved individual\nheterogeneity. A Bayesian sparse factor approach is employed to capture\nhigh-dimensional error correlations that induce taste correlation and\nendogeneity. Time-varying factor loadings allow for more general\nindividual-level and time-varying heterogeneity and endogeneity, while the\nsparsity induced by the shrinkage prior on loadings balances flexibility with\nparsimony. Applied to a soda tax in the context of complementarities, the new\napproach captures broader effects of the tax that were previously overlooked.\nResults suggest that a soda tax could yield additional health benefits by\nmarginally decreasing the consumption of salty snacks along with sugary drinks,\nextending the health benefits beyond the reduction in sugar consumption alone.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.05794v1"
    },
    {
        "title": "Estimating Spillover Effects in the Presence of Isolated Nodes",
        "authors": [
            "Bora Kim"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In estimating spillover effects under network interference, practitioners\noften use linear regression with either the number or fraction of treated\nneighbors as regressors. An often overlooked fact is that the latter is\nundefined for units without neighbors (``isolated nodes\"). The common practice\nis to impute this fraction as zero for isolated nodes. This paper shows that\nsuch practice introduces bias through theoretical derivations and simulations.\nCausal interpretations of the commonly used spillover regression coefficients\nare also provided.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.05919v1"
    },
    {
        "title": "Density forecast transformations",
        "authors": [
            "Matteo Mogliani",
            "Florens Odendahl"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The popular choice of using a $direct$ forecasting scheme implies that the\nindividual predictions do not contain information on cross-horizon dependence.\nHowever, this dependence is needed if the forecaster has to construct, based on\n$direct$ density forecasts, predictive objects that are functions of several\nhorizons ($e.g.$ when constructing annual-average growth rates from\nquarter-on-quarter growth rates). To address this issue we propose to use\ncopulas to combine the individual $h$-step-ahead predictive distributions into\na joint predictive distribution. Our method is particularly appealing to\npractitioners for whom changing the $direct$ forecasting specification is too\ncostly. In a Monte Carlo study, we demonstrate that our approach leads to a\nbetter approximation of the true density than an approach that ignores the\npotential dependence. We show the superior performance of our method in several\nempirical examples, where we construct (i) quarterly forecasts using\nmonth-on-month $direct$ forecasts, (ii) annual-average forecasts using monthly\nyear-on-year $direct$ forecasts, and (iii) annual-average forecasts using\nquarter-on-quarter $direct$ forecasts.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.06092v1"
    },
    {
        "title": "Probabilistic Targeted Factor Analysis",
        "authors": [
            "Miguel C. Herculano",
            "Santiago Montoya-Blandón"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We develop a probabilistic variant of Partial Least Squares (PLS) we call\nProbabilistic Targeted Factor Analysis (PTFA), which can be used to extract\ncommon factors in predictors that are useful to predict a set of predetermined\ntarget variables. Along with the technique, we provide an efficient\nexpectation-maximization (EM) algorithm to learn the parameters and forecast\nthe targets of interest. We develop a number of extensions to missing-at-random\ndata, stochastic volatility, and mixed-frequency data for real-time\nforecasting. In a simulation exercise, we show that PTFA outperforms PLS at\nrecovering the common underlying factors affecting both features and target\nvariables delivering better in-sample fit, and providing valid forecasts under\ncontamination such as measurement error or outliers. Finally, we provide two\napplications in Economics and Finance where PTFA performs competitively\ncompared with PLS and Principal Component Analysis (PCA) at out-of-sample\nforecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.06688v1"
    },
    {
        "title": "Inference after discretizing unobserved heterogeneity",
        "authors": [
            "Jad Beyhum",
            "Martin Mugnier"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We consider a linear panel data model with nonseparable two-way unobserved\nheterogeneity corresponding to a linear version of the model studied in\nBonhomme et al. (2022). We show that inference is possible in this setting\nusing a straightforward two-step estimation procedure inspired by existing\ndiscretization approaches. In the first step, we construct a discrete\napproximation of the unobserved heterogeneity by (k-means) clustering\nobservations separately across the individual ($i$) and time ($t$) dimensions.\nIn the second step, we estimate a linear model with two-way group fixed effects\nspecific to each cluster. Our approach shares similarities with methods from\nthe double machine learning literature, as the underlying moment conditions\nexhibit the same type of bias-reducing properties. We provide a theoretical\nanalysis of a cross-fitted version of our estimator, establishing its\nasymptotic normality at parametric rate under the condition\n$\\max(N,T)=o(\\min(N,T)^3)$. Simulation studies demonstrate that our methodology\nachieves excellent finite-sample performance, even when $T$ is negligible with\nrespect to $N$.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.07352v1"
    },
    {
        "title": "Machine Learning the Macroeconomic Effects of Financial Shocks",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber",
            "Karin Klieber",
            "Massimiliano Marcellino"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a method to learn the nonlinear impulse responses to structural\nshocks using neural networks, and apply it to uncover the effects of US\nfinancial shocks. The results reveal substantial asymmetries with respect to\nthe sign of the shock. Adverse financial shocks have powerful effects on the US\neconomy, while benign shocks trigger much smaller reactions. Instead, with\nrespect to the size of the shocks, we find no discernible asymmetries.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.07649v1"
    },
    {
        "title": "Panel Stochastic Frontier Models with Latent Group Structures",
        "authors": [
            "Kazuki Tomioka",
            "Thomas T. Yang",
            "Xibin Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Stochastic frontier models have attracted significant interest over the years\ndue to their unique feature of including a distinct inefficiency term alongside\nthe usual error term. To effectively separate these two components, strong\ndistributional assumptions are often necessary. To overcome this limitation,\nnumerous studies have sought to relax or generalize these models for more\nrobust estimation. In line with these efforts, we introduce a latent group\nstructure that accommodates heterogeneity across firms, addressing not only the\nstochastic frontiers but also the distribution of the inefficiency term. This\nframework accounts for the distinctive features of stochastic frontier models,\nand we propose a practical estimation procedure to implement it. Simulation\nstudies demonstrate the strong performance of our proposed method, which is\nfurther illustrated through an application to study the cost efficiency of the\nU.S. commercial banking sector.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.08831v1"
    },
    {
        "title": "An overview of meta-analytic methods for economic research",
        "authors": [
            "Amin Haghnejad",
            "Mahboobeh Farahati"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Meta-analysis is the use of statistical methods to combine the results of\nindividual studies to estimate the overall effect size for a specific outcome\nof interest. The direction and magnitude of this estimated effect, along with\nits confidence interval, provide insights into the phenomenon or relationship\nbeing investigated. As an extension of the standard meta-analysis,\nmeta-regression analysis incorporates multiple moderators representing\nidentifiable study characteristics into the meta-analysis model, thereby\nexplaining some of the heterogeneity in true effect sizes across studies. This\nform of meta-analysis is especially designed to quantitatively synthesize\nempirical evidence in economics. This study provides an overview of the\nmeta-analytic procedures tailored for economic research. By addressing key\nchallenges, including between-study heterogeneity, publication bias, and effect\nsize dependence, it aims to equip researchers with the tools and insights\nneeded to conduct rigorous and informative meta-analytic studies in economics\nand related disciplines.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.10608v1"
    },
    {
        "title": "Do LLMs Act as Repositories of Causal Knowledge?",
        "authors": [
            "Nick Huntington-Klein",
            "Eleanor J. Murray"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Large language models (LLMs) offer the potential to automate a large number\nof tasks that previously have not been possible to automate, including some in\nscience. There is considerable interest in whether LLMs can automate the\nprocess of causal inference by providing the information about causal links\nnecessary to build a structural model. We use the case of confounding in the\nCoronary Drug Project (CDP), for which there are several studies listing\nexpert-selected confounders that can serve as a ground truth. LLMs exhibit\nmediocre performance in identifying confounders in this setting, even though\ntext about the ground truth is in their training data. Variables that experts\nidentify as confounders are only slightly more likely to be labeled as\nconfounders by LLMs compared to variables that experts consider\nnon-confounders. Further, LLM judgment on confounder status is highly\ninconsistent across models, prompts, and irrelevant concerns like\nmultiple-choice option ordering. LLMs do not yet have the ability to automate\nthe reporting of causal links.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.10635v1"
    },
    {
        "title": "Forecasting realized covariances using HAR-type models",
        "authors": [
            "Matias Quiroz",
            "Laleh Tafakori",
            "Hans Manner"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We investigate methods for forecasting multivariate realized covariances\nmatrices applied to a set of 30 assets that were included in the DJ30 index at\nsome point, including two novel methods that use existing (univariate) log of\nrealized variance models that account for attenuation bias and time-varying\nparameters. We consider the implications of some modeling choices within the\nclass of heterogeneous autoregressive models. The following are our key\nfindings. First, modeling the logs of the marginal volatilities is strongly\npreferred over direct modeling of marginal volatility. Thus, our proposed model\nthat accounts for attenuation bias (for the log-response) provides superior\none-step-ahead forecasts over existing multivariate realized covariance\napproaches. Second, accounting for measurement errors in marginal realized\nvariances generally improves multivariate forecasting performance, but to a\nlesser degree than previously found in the literature. Third, time-varying\nparameter models based on state-space models perform almost equally well.\nFourth, statistical and economic criteria for comparing the forecasting\nperformance lead to some differences in the models' rankings, which can\npartially be explained by the turbulent post-pandemic data in our out-of-sample\nvalidation dataset using sub-sample analyses.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.10791v1"
    },
    {
        "title": "Treatment Evaluation at the Intensive and Extensive Margins",
        "authors": [
            "Phillip Heiler",
            "Asbjørn Kaufmann",
            "Bezirgen Veliyev"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper provides a solution to the evaluation of treatment effects in\nselective samples when neither instruments nor parametric assumptions are\navailable. We provide sharp bounds for average treatment effects under a\nconditional monotonicity assumption for all principal strata, i.e. units\ncharacterizing the complete intensive and extensive margins. Most importantly,\nwe allow for a large share of units whose selection is indifferent to\ntreatment, e.g. due to non-compliance. The existence of such a population is\ncrucially tied to the regularity of sharp population bounds and thus\nconventional asymptotic inference for methods such as Lee bounds can be\nmisleading. It can be solved using smoothed outer identification regions for\ninference. We provide semiparametrically efficient debiased machine learning\nestimators for both regular and smooth bounds that can accommodate\nhigh-dimensional covariates and flexible functional forms. Our study of active\nlabor market policy reveals the empirical prevalence of the aforementioned\nindifference population and supports results from previous impact analysis\nunder much weaker assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.11179v1"
    },
    {
        "title": "VAR models with an index structure: A survey with new results",
        "authors": [
            "Gianluca Cubadda"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  The main aim of this paper is to review recent advances in the multivariate\nautoregressive index model [MAI], originally proposed by\n<cite>reinsel1983some</cite>, and their applications to economic and financial\ntime series. MAI has recently gained momentum because it can be seen as a link\nbetween two popular but distinct multivariate time series approaches: vector\nautoregressive modeling [VAR] and the dynamic factor model [DFM]. Indeed, on\nthe one hand, the MAI is a VAR model with a peculiar reduced-rank structure; on\nthe other hand, it allows for identification of common components and common\nshocks in a similar way as the DFM. The focus is on recent developments of the\nMAI, which include extending the original model with individual autoregressive\nstructures, stochastic volatility, time-varying parameters,\nhigh-dimensionality, and cointegration. In addition, new insights on previous\ncontributions and a novel model are also provided.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.11278v1"
    },
    {
        "title": "Moderating the Mediation Bootstrap for Causal Inference",
        "authors": [
            "Kees Jan van Garderen",
            "Noud van Giersbergen"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Mediation analysis is a form of causal inference that investigates indirect\neffects and causal mechanisms. Confidence intervals for indirect effects play a\ncentral role in conducting inference. The problem is non-standard leading to\ncoverage rates that deviate considerably from their nominal level. The default\ninference method in the mediation model is the paired bootstrap, which\nresamples directly from the observed data. However, a residual bootstrap that\nexplicitly exploits the assumed causal structure (X->M->Y) could also be\napplied. There is also a debate whether the bias-corrected (BC) bootstrap\nmethod is superior to the percentile method, with the former showing liberal\nbehavior (actual coverage too low) in certain circumstances. Moreover,\nbootstrap methods tend to be very conservative (coverage higher than required)\nwhen mediation effects are small. Finally, iterated bootstrap methods like the\ndouble bootstrap have not been considered due to their high computational\ndemands. We investigate the issues mentioned in the simple mediation model by a\nlarge-scale simulation. Results are explained using graphical methods and the\nnewly derived finite-sample distribution. The main findings are: (i)\nconservative behavior of the bootstrap is caused by extreme dependence of the\nbootstrap distribution's shape on the estimated coefficients (ii) this\ndependence leads to counterproductive correction of the the double bootstrap.\nThe added randomness of the BC method inflates the coverage in the absence of\nmediation, but still leads to (invalid) liberal inference when the mediation\neffect is small.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.11285v1"
    },
    {
        "title": "An Analysis of the Relationship Between the Characteristics of\n  Innovative Consumers and the Degree of Serious Leisure in User Innovation",
        "authors": [
            "Taichi Abe",
            "Yasunobu Morita"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This study examines the relationship between the concept of serious leisure\nand user innovation. We adopted the characteristics of innovative consumers\nidentified by Luthje (2004)-product use experience, information exchange, and\nnew product adoption speed-to analyze their correlation with serious leisure\nengagement. The analysis utilized consumer behavior survey data from the\n\"Marketing Analysis Contest 2023\" sponsored by Nomura Research Institute,\nexamining the relationship between innovative consumer characteristics and the\ndegree of serious leisure (Serious Leisure Inventory and Measure: SLIM). Since\nthe contest data did not directly measure innovative consumer characteristics\nor serious leisure engagement, we established alternative variables for\nquantitative analysis. The results showed that the SLIM alternative variable\nhad positive correlations with diverse product experiences and early adoption\nof new products. However, no clear relationship was found with information\nexchange among consumers. These findings suggest that serious leisure practice\nmay serve as a potential antecedent to user innovation. The leisure career\nperspective of the serious leisure concept may capture the motivations of user\ninnovators that Okada and Nishikawa (2019) identified.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.13556v1"
    },
    {
        "title": "Good Controls Gone Bad: Difference-in-Differences with Covariates",
        "authors": [
            "Sunny Karim",
            "Matthew D. Webb"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This paper introduces the two-way common causal covariates (CCC) assumption,\nwhich is necessary to get an unbiased estimate of the ATT when using\ntime-varying covariates in existing Difference-in-Differences methods. The\ntwo-way CCC assumption implies that the effect of the covariates remain the\nsame between groups and across time periods. This assumption has been implied\nin previous literature, but has not been explicitly addressed. Through\ntheoretical proofs and a Monte Carlo simulation study, we show that the\nstandard TWFE and the CS-DID estimators are biased when the two-way CCC\nassumption is violated. We propose a new estimator called the Intersection\nDifference-in-differences (DID-INT) which can provide an unbiased estimate of\nthe ATT under two-way CCC violations. DID-INT can also identify the ATT under\nheterogeneous treatment effects and with staggered treatment rollout. The\nestimator relies on parallel trends of the residuals of the outcome variable,\nafter appropriately adjusting for covariates. This covariate residualization\ncan recover parallel trends that are hidden with conventional estimators.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.14447v1"
    },
    {
        "title": "Testing linearity of spatial interaction functions à la Ramsey",
        "authors": [
            "Abhimanyu Gupta",
            "Jungyoon Lee",
            "Francesca Rossi"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a computationally straightforward test for the linearity of a\nspatial interaction function. Such functions arise commonly, either as\npractitioner imposed specifications or due to optimizing behaviour by agents.\nOur test is nonparametric, but based on the Lagrange Multiplier principle and\nreminiscent of the Ramsey RESET approach. This entails estimation only under\nthe null hypothesis, which yields an easy to estimate linear spatial\nautoregressive model. Monte Carlo simulations show excellent size control and\npower. An empirical study with Finnish data illustrates the test's practical\nusefulness, shedding light on debates on the presence of tax competition among\nneighbouring municipalities.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.14778v1"
    },
    {
        "title": "Counting Defiers in Health Care with a Design-Based Likelihood for the\n  Joint Distribution of Potential Outcomes",
        "authors": [
            "Neil Christy",
            "Amanda Ellen Kowalski"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We present a design-based model of a randomized experiment in which the\nobserved outcomes are informative about the joint distribution of potential\noutcomes within the experimental sample. We derive a likelihood function that\nmaintains curvature with respect to the joint distribution of potential\noutcomes, even when holding the marginal distributions of potential outcomes\nconstant -- curvature that is not maintained in a sampling-based likelihood\nthat imposes a large sample assumption. Our proposed decision rule guesses the\njoint distribution of potential outcomes in the sample as the distribution that\nmaximizes the likelihood. We show that this decision rule is Bayes optimal\nunder a uniform prior. Our optimal decision rule differs from and significantly\noutperforms a ``monotonicity'' decision rule that assumes no defiers or no\ncompliers. In sample sizes ranging from 2 to 40, we show that the Bayes\nexpected utility of the optimal rule increases relative to the monotonicity\nrule as the sample size increases. In two experiments in health care, we show\nthat the joint distribution of potential outcomes that maximizes the likelihood\nneed not include compliers even when the average outcome in the intervention\ngroup exceeds the average outcome in the control group, and that the maximizer\nof the likelihood may include both compliers and defiers, even when the average\nintervention effect is large and statistically significant.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.16352v1"
    },
    {
        "title": "Advanced Models for Hourly Marginal CO2 Emission Factor Estimation: A\n  Synergy between Fundamental and Statistical Approaches",
        "authors": [
            "Souhir Ben Amor",
            "Smaranda Sgarciu",
            "Taimyra BatzLineiro",
            "Felix Muesgens"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Global warming is caused by increasing concentrations of greenhouse gases,\nparticularly carbon dioxide (CO2). A metric used to quantify the change in CO2\nemissions is the marginal emission factor, defined as the marginal change in\nCO2 emissions resulting from a marginal change in electricity demand over a\nspecified period. This paper aims to present two methodologies to estimate the\nmarginal emission factor in a decarbonized electricity system with high\ntemporal resolution. First, we present an energy systems model that\nincrementally calculates the marginal emission factors. Second, we examine a\nMarkov Switching Dynamic Regression model, a statistical model designed to\nestimate marginal emission factors faster and use an incremental marginal\nemission factor as a benchmark to assess its precision. For the German\nelectricity market, we estimate the marginal emissions factor time series\nhistorically (2019, 2020) using Agora Energiewende and for the future (2025,\n2030, and 2040) using estimated energy system data. The results indicate that\nthe Markov Switching Dynamic Regression model is more accurate in estimating\nmarginal emission factors than the Dynamic Linear Regression models, which are\nfrequently used in the literature. Hence, the Markov Switching Dynamic\nRegression model is a simpler alternative to the computationally intensive\nincremental marginal emissions factor, especially when short-term marginal\nemissions factor estimation is needed. The results of the marginal emission\nfactor estimation are applied to an exemplary low-emission vehicle charging\nscenario to estimate CO2 savings by shifting the charge hours to those\ncorresponding to the lower marginal emissions factor. By implementing this\nemission-minimized charging approach, an average reduction of 31% in the\nmarginal emission factor was achieved over the 5 years.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.17379v1"
    },
    {
        "title": "A large non-Gaussian structural VAR with application to Monetary Policy",
        "authors": [
            "Jan Prüser"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We propose a large structural VAR which is identified by higher moments\nwithout the need to impose economically motivated restrictions. The model\nscales well to higher dimensions, allowing the inclusion of a larger number of\nvariables. We develop an efficient Gibbs sampler to estimate the model. We also\npresent an estimator of the deviance information criterion to facilitate model\ncomparison. Finally, we discuss how economically motivated restrictions can be\nadded to the model. Experiments with artificial data show that the model\npossesses good estimation properties. Using real data we highlight the benefits\nof including more variables in the structural analysis. Specifically, we\nidentify a monetary policy shock and provide empirical evidence that prices and\neconomic output respond with a large delay to the monetary policy shock.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.17598v1"
    },
    {
        "title": "Automated Demand Forecasting in small to medium-sized enterprises",
        "authors": [
            "Thomas Gaertner",
            "Christoph Lippert",
            "Stefan Konigorski"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  In response to the growing demand for accurate demand forecasts, this\nresearch proposes a generalized automated sales forecasting pipeline tailored\nfor small- to medium-sized enterprises (SMEs). Unlike large corporations with\ndedicated data scientists for sales forecasting, SMEs often lack such\nresources. To address this, we developed a comprehensive forecasting pipeline\nthat automates time series sales forecasting, encompassing data preparation,\nmodel training, and selection based on validation results.\n  The development included two main components: model preselection and the\nforecasting pipeline. In the first phase, state-of-the-art methods were\nevaluated on a showcase dataset, leading to the selection of ARIMA, SARIMAX,\nHolt-Winters Exponential Smoothing, Regression Tree, Dilated Convolutional\nNeural Networks, and Generalized Additive Models. An ensemble prediction of\nthese models was also included. Long-Short-Term Memory (LSTM) networks were\nexcluded due to suboptimal prediction accuracy, and Facebook Prophet was\nomitted for compatibility reasons.\n  In the second phase, the proposed forecasting pipeline was tested with SMEs\nin the food and electric industries, revealing variable model performance\nacross different companies. While one project-based company derived no benefit,\nothers achieved superior forecasts compared to naive estimators.\n  Our findings suggest that no single model is universally superior. Instead, a\ndiverse set of models, when integrated within an automated validation\nframework, can significantly enhance forecasting accuracy for SMEs. These\nresults emphasize the importance of model diversity and automated validation in\naddressing the unique needs of each business. This research contributes to the\nfield by providing SMEs access to state-of-the-art sales forecasting tools,\nenabling data-driven decision-making and improving operational efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.20420v1"
    },
    {
        "title": "Regression discontinuity aggregation, with an application to the union\n  effects on inequality",
        "authors": [
            "Kirill Borusyak",
            "Matan Kolerman-Shemer"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We extend the regression discontinuity (RD) design to settings where each\nunit's treatment status is an average or aggregate across multiple\ndiscontinuity events. Such situations arise in many studies where the outcome\nis measured at a higher level of spatial or temporal aggregation (e.g., by\nstate with district-level discontinuities) or when spillovers from\ndiscontinuity events are of interest. We propose two novel estimation\nprocedures - one at the level at which the outcome is measured and the other in\nthe sample of discontinuities - and show that both identify a local average\ncausal effect under continuity assumptions similar to those of standard RD\ndesigns. We apply these ideas to study the effect of unionization on inequality\nin the United States. Using credible variation from close unionization\nelections at the establishment level, we show that a higher rate of newly\nunionized workers in a state-by-industry cell reduces wage inequality within\nthe cell.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.00428v1"
    },
    {
        "title": "Copula Central Asymmetry of Equity Portfolios",
        "authors": [
            "Lorenzo Frattarolo"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Financial crises are usually associated with increased cross-sectional\ndependence between asset returns, causing asymmetry between the lower and upper\ntail of return distribution. The detection of asymmetric dependence is now\nunderstood to be essential for market supervision, risk management, and\nportfolio allocation. I propose a non-parametric test procedure for the\nhypothesis of copula central symmetry based on the Cram\\'er-von Mises distance\nof the empirical copula and its survival counterpart, deriving the asymptotic\nproperties of the test under standard assumptions for stationary time series. I\nuse the powerful tie-break bootstrap that, as the included simulation study\nimplies, allows me to detect asymmetries with up to 25 series and the number of\nobservations corresponding to one year of daily returns. Applying the procedure\nto US portfolio returns separately for each year shows that the amount of\ncopula central asymmetry is time-varying and less present in the recent past.\nAsymmetry is more critical in portfolios based on size and less in portfolios\nbased on book-to-market and momentum. In portfolios based on industry\nclassification, asymmetry is present during market downturns, coherently with\nthe financial contagion narrative.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.00634v2"
    },
    {
        "title": "Instrumental Variables with Time-Varying Exposure: New Estimates of\n  Revascularization Effects on Quality of Life",
        "authors": [
            "Joshua D. Angrist",
            "Bruno Ferman",
            "Carol Gao",
            "Peter Hull",
            "Otavio L. Tecchio",
            "Robert W. Yeh"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  The ISCHEMIA Trial randomly assigned patients with ischemic heart disease to\nan invasive treatment strategy centered on revascularization with a control\ngroup assigned non-invasive medical therapy. As is common in such ``strategy\ntrials,'' many participants assigned to treatment remained untreated while many\nassigned to control crossed over into treatment. Intention-to-treat (ITT)\nanalyses of strategy trials preserve randomization-based comparisons, but ITT\neffects are diluted by non-compliance. Conventional per-protocol analyses that\ncondition on treatment received are likely biased by discarding random\nassignment. In trials where compliance choices are made shortly after\nassignment, instrumental variables (IV) methods solve both problems --\nrecovering an undiluted average causal effect of treatment for treated subjects\nwho comply with trial protocol. In ISCHEMIA, however, some controls were\nrevascularized as long as five years after random assignment. This paper\nextends the IV framework for strategy trials, allowing for such dynamic\nnon-random compliance behavior. IV estimates of long-run revascularization\neffects on quality of life are markedly larger than previously reported ITT and\nper-protocol estimates. We also show how to estimate complier characteristics\nin a dynamic-treatment setting. These estimates reveal increasing selection\nbias in naive time-varying per-protocol estimates of revascularization effects.\nCompliers have baseline health similar to that of the study population, while\ncontrol-group crossovers are far sicker.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.01623v1"
    },
    {
        "title": "Prediction with Differential Covariate Classification: Illustrated by\n  Racial/Ethnic Classification in Medical Risk Assessment",
        "authors": [
            "Charles F. Manski",
            "John Mullahy",
            "Atheendar S. Venkataramani"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  A common practice in evidence-based decision-making uses estimates of\nconditional probabilities P(y|x) obtained from research studies to predict\noutcomes y on the basis of observed covariates x. Given this information,\ndecisions are then based on the predicted outcomes. Researchers commonly assume\nthat the predictors used in the generation of the evidence are the same as\nthose used in applying the evidence: i.e., the meaning of x in the two\ncircumstances is the same. This may not be the case in real-world settings.\nAcross a wide-range of settings, ranging from clinical practice or education\npolicy, demographic attributes (e.g., age, race, ethnicity) are often\nclassified differently in research studies than in decision settings. This\npaper studies identification in such settings. We propose a formal framework\nfor prediction with what we term differential covariate classification (DCC).\nUsing this framework, we analyze partial identification of probabilistic\npredictions and assess how various assumptions influence the identification\nregions. We apply the findings to a range of settings, focusing mainly on\ndifferential classification of individuals' race and ethnicity in clinical\nmedicine. We find that bounds on P(y|x) can be wide, and the information needed\nto narrow them available only in special cases. These findings highlight an\nimportant problem in using evidence in decision making, a problem that has not\nyet been fully appreciated in debates on classification in public policy and\nmedicine.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.02318v1"
    },
    {
        "title": "Estimating Discrete Choice Demand Models with Sparse Market-Product\n  Shocks",
        "authors": [
            "Zhentong Lu",
            "Kenichi Shimizu"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  We propose a new approach to estimating the random coefficient logit demand\nmodel for differentiated products when the vector of market-product level\nshocks is sparse. Assuming sparsity, we establish nonparametric identification\nof the distribution of random coefficients and demand shocks under mild\nconditions. Then we develop a Bayesian procedure, which exploits the sparsity\nstructure using shrinkage priors, to conduct inference about the model\nparameters and counterfactual quantities. Comparing to the standard BLP (Berry,\nLevinsohn, & Pakes, 1995) method, our approach does not require demand\ninversion or instrumental variables (IVs), thus provides a compelling\nalternative when IVs are not available or their validity is questionable. Monte\nCarlo simulations validate our theoretical findings and demonstrate the\neffectiveness of our approach, while empirical applications reveal evidence of\nsparse demand shocks in well-known datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.02381v1"
    },
    {
        "title": "High-frequency Density Nowcasts of U.S. State-Level Carbon Dioxide\n  Emissions",
        "authors": [
            "Ignacio Garrón",
            "Andrey Ramos"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  Accurate tracking of anthropogenic carbon dioxide (CO2) emissions is crucial\nfor shaping climate policies and meeting global decarbonization targets.\nHowever, energy consumption and emissions data are released annually and with\nsubstantial publication lags, hindering timely decision-making. This paper\nintroduces a panel nowcasting framework to produce higher-frequency predictions\nof the state-level growth rate of per-capita energy consumption and CO2\nemissions in the United States (U.S.). Our approach employs a panel mixed-data\nsampling (MIDAS) model to predict per-capita energy consumption growth,\nconsidering quarterly personal income, monthly electricity consumption, and a\nweekly economic conditions index as predictors. A bridge equation linking\nper-capita CO2 emissions growth with the nowcasts of energy consumption is\nestimated using panel quantile regression methods. A pseudo out-of-sample study\n(2009-2018), simulating the real-time data release calendar, confirms the\nimproved accuracy of our nowcasts with respect to a historical benchmark. Our\nresults suggest that by leveraging the availability of higher-frequency\nindicators, we not only enhance predictive accuracy for per-capita energy\nconsumption growth but also provide more reliable estimates of the distribution\nof CO2 emissions growth.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.03380v1"
    },
    {
        "title": "Sequential Monte Carlo for Noncausal Processes",
        "authors": [
            "Gianluca Cubadda",
            "Francesco Giancaterini",
            "Stefano Grassi"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  This paper proposes a Sequential Monte Carlo approach for the Bayesian\nestimation of mixed causal and noncausal models. Unlike previous Bayesian\nestimation methods developed for these models, Sequential Monte Carlo offers\nextensive parallelization opportunities, significantly reducing estimation time\nand mitigating the risk of becoming trapped in local minima, a common issue in\nnoncausal processes. Simulation studies demonstrate the strong ability of the\nalgorithm to produce accurate estimates and correctly identify the process. In\nparticular, we propose a novel identification methodology that leverages the\nMarginal Data Density and the Bayesian Information Criterion. Unlike previous\nstudies, this methodology determines not only the causal and noncausal\npolynomial orders but also the error term distribution that best fits the data.\nFinally, Sequential Monte Carlo is applied to a bivariate process containing\nS$\\&$P Europe 350 ESG Index and Brent crude oil prices.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.03945v1"
    },
    {
        "title": "Monthly GDP Growth Estimates for the U.S. States",
        "authors": [
            "Gary Koop",
            "Stuart McIntyre",
            "James Mitchell",
            "Aristeidis Raftapostolos"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  This paper develops a mixed frequency vector autoregressive (MF-VAR) model to\nproduce nowcasts and historical estimates of monthly real state-level GDP for\nthe 50 U.S. states, plus Washington DC, from 1964 through the present day. The\nMF-VAR model incorporates state and U.S. data at the monthly, quarterly, and\nannual frequencies. Temporal and cross-sectional constraints are imposed to\nensure that the monthly state-level estimates are consistent with official\nestimates of quarterly GDP at the U.S. and state-levels. We illustrate the\nutility of the historical estimates in better understanding state business\ncycles and cross-state dependencies. We show how the model produces accurate\nnowcasts of state GDP three months ahead of the BEA's quarterly estimates,\nafter conditioning on the latest estimates of U.S. GDP.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.04607v1"
    },
    {
        "title": "Identification of dynamic treatment effects when treatment histories are\n  partially observed",
        "authors": [
            "Akanksha Negi",
            "Didier Nibbering"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  This paper proposes a class of methods for identifying and estimating dynamic\ntreatment effects when outcomes depend on the entire treatment path and\ntreatment histories are only partially observed. We advocate for the approach\nwhich we refer to as `robust' that identifies path-dependent treatment effects\nfor different mover subpopulations under misspecification of any one of three\nmodels involved (outcome, propensity score, or missing data models). Our\napproach can handle fixed, absorbing, sequential, or simultaneous treatment\nregimes where missing treatment histories may obfuscate identification of\ncausal effects. Numerical experiments demonstrate how the proposed estimator\ncompares to traditional complete-case methods. We find that the\nmissingness-adjusted estimates have negligible bias compared to their\ncomplete-case counterparts. As an illustration, we apply the proposed class of\nadjustment methods to estimate dynamic effects of COVID-19 on voter turnout in\nthe 2022 U.S. general elections. We find that counties that experienced\nabove-average number of cases in 2020 and 2021 had a statistically significant\nreduction in voter turnout compared to those that did not.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.04853v1"
    },
    {
        "title": "RUM-NN: A Neural Network Model Compatible with Random Utility\n  Maximisation for Discrete Choice Setups",
        "authors": [
            "Niousha Bagheri",
            "Milad Ghasri",
            "Michael Barlow"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  This paper introduces a framework for capturing stochasticity of choice\nprobabilities in neural networks, derived from and fully consistent with the\nRandom Utility Maximization (RUM) theory, referred to as RUM-NN. Neural network\nmodels show remarkable performance compared with statistical models; however,\nthey are often criticized for their lack of transparency and interoperability.\nThe proposed RUM-NN is introduced in both linear and nonlinear structures. The\nlinear RUM-NN retains the interpretability and identifiability of traditional\neconometric discrete choice models while using neural network-based estimation\ntechniques. The nonlinear RUM-NN extends the model's flexibility and predictive\ncapabilities to capture nonlinear relationships between variables within\nutility functions. Additionally, the RUM-NN allows for the implementation of\nvarious parametric distributions for unobserved error components in the utility\nfunction and captures correlations among error terms. The performance of RUM-NN\nin parameter recovery and prediction accuracy is rigorously evaluated using\nsynthetic datasets through Monte Carlo experiments. Additionally, RUM-NN is\nevaluated on the Swissmetro and the London Passenger Mode Choice (LPMC)\ndatasets with different sets of distribution assumptions for the error\ncomponent. The results demonstrate that RUM-NN under a linear utility structure\nand IID Gumbel error terms can replicate the performance of the Multinomial\nLogit (MNL) model, but relaxing those constraints leads to superior performance\nfor both Swissmetro and LPMC datasets. By introducing a novel estimation\napproach aligned with statistical theories, this study empowers econometricians\nto harness the advantages of neural network models.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.05221v1"
    },
    {
        "title": "Optimizing Financial Data Analysis: A Comparative Study of Preprocessing\n  Techniques for Regression Modeling of Apple Inc.'s Net Income and Stock\n  Prices",
        "authors": [
            "Kevin Ungar",
            "Camelia Oprean-Stan"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  This article presents a comprehensive methodology for processing financial\ndatasets of Apple Inc., encompassing quarterly income and daily stock prices,\nspanning from March 31, 2009, to December 31, 2023. Leveraging 60 observations\nfor quarterly income and 3774 observations for daily stock prices, sourced from\nMacrotrends and Yahoo Finance respectively, the study outlines five distinct\ndatasets crafted through varied preprocessing techniques. Through detailed\nexplanations of aggregation, interpolation (linear, polynomial, and cubic\nspline) and lagged variables methods, the study elucidates the steps taken to\ntransform raw data into analytically rich datasets. Subsequently, the article\ndelves into regression analysis, aiming to decipher which of the five data\nprocessing methods best suits capital market analysis, by employing both linear\nand polynomial regression models on each preprocessed dataset and evaluating\ntheir performance using a range of metrics, including cross-validation score,\nMSE, MAE, RMSE, R-squared, and Adjusted R-squared. The research findings reveal\nthat linear interpolation with polynomial regression emerges as the\ntop-performing method, boasting the lowest validation MSE and MAE values,\nalongside the highest R-squared and Adjusted R-squared values.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.06587v1"
    },
    {
        "title": "Forecasting for monetary policy",
        "authors": [
            "Laura Coroneo"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  This paper discusses three key themes in forecasting for monetary policy\nhighlighted in the Bernanke (2024) review: the challenges in economic\nforecasting, the conditional nature of central bank forecasts, and the\nimportance of forecast evaluation. In addition, a formal evaluation of the Bank\nof England's inflation forecasts indicates that, despite the large forecast\nerrors in recent years, they were still accurate relative to common benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.07386v1"
    },
    {
        "title": "Estimating Sequential Search Models Based on a Partial Ranking\n  Representation",
        "authors": [
            "Tinghan Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  Consumers are increasingly shopping online, and more and more datasets\ndocumenting consumer search are becoming available. While sequential search\nmodels provide a framework for utilizing such data, they present empirical\nchallenges. A key difficulty arises from the inequality conditions implied by\nthese models, which depend on multiple unobservables revealed during the search\nprocess and necessitate solving or simulating high-dimensional integrals for\nlikelihood-based estimation methods. This paper introduces a novel\nrepresentation of inequalities implied by a broad class of sequential search\nmodels, demonstrating that the empirical content of such models can be\neffectively captured through a specific partial ranking of available actions.\nThis representation reduces the complexity caused by unobservables and provides\na tractable expression for joint probabilities. Leveraging this insight, we\npropose a GHK-style simulation-based likelihood estimator that is simpler to\nimplement than existing ones. It offers greater flexibility for handling\nincomplete search data, incorporating additional ranking information, and\naccommodating complex search processes, including those involving product\ndiscovery. We show that the estimator achieves robust performance while\nmaintaining relatively low computational costs, making it a practical and\nversatile tool for researchers and practitioners.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.07514v2"
    },
    {
        "title": "The Impact of Digitalisation and Sustainability on Inclusiveness:\n  Inclusive Growth Determinants",
        "authors": [
            "Radu Rusu",
            "Camelia Oprean-Stan"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  Inclusiveness and economic development have been slowed by the pandemics and\nmilitary conflicts. This study investigates the main determinants of\ninclusiveness at the European level. A multi-method approach is used, with\nPrincipal Component Analysis (PCA) applied to create the Inclusiveness Index\nand Generalised Method of Moments (GMM) analysis used to investigate the\ndeterminants of inclusiveness. The data comprises a range of 22 years, from\n2000 to 2021, for 32 European countries. The determinants of inclusiveness and\ntheir effects were identified. First, economic growth, industrial upgrading,\nelectricity consumption, digitalisation, and the quantitative aspect of\ngovernance, all have a positive impact on inclusive growth in Europe. Second,\nthe level of CO2 emissions and inflation have a negative impact on\ninclusiveness. Tomorrow's inclusive and sustainable growth must include\ninvestments in renewable energy, digital infrastructure, inequality policies,\nsustainable governance, human capital, and inflation management. These findings\ncan help decision makers design inclusive growth policies.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.07880v1"
    },
    {
        "title": "Recovering latent linkage structures and spillover effects with\n  structural breaks in panel data models",
        "authors": [
            "Ryo Okui",
            "Yutao Sun",
            "Wendun Wang"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  This paper introduces a framework to analyze time-varying spillover effects\nin panel data. We consider panel models where a unit's outcome depends not only\non its own characteristics (private effects) but also on the characteristics of\nother units (spillover effects). The linkage of units is allowed to be latent\nand may shift at an unknown breakpoint. We propose a novel procedure to\nestimate the breakpoint, linkage structure, spillover and private effects. We\naddress the high-dimensionality of spillover effect parameters using penalized\nestimation, and estimate the breakpoint with refinement. We establish the\nsuper-consistency of the breakpoint estimator, ensuring that inferences about\nother parameters can proceed as if the breakpoint were known. The private\neffect parameters are estimated using a double machine learning method. The\nproposed method is applied to estimate the cross-country R&D spillovers, and we\nfind that the R&D spillovers become sparser after the financial crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.09517v1"
    },
    {
        "title": "Convergence Rates of GMM Estimators with Nonsmooth Moments under\n  Misspecification",
        "authors": [
            "Byunghoon Kang",
            "Seojeong Lee",
            "Juha Song"
        ],
        "category": "econ.EM",
        "published_year": "2025",
        "summary": "  The asymptotic behavior of GMM estimators depends critically on whether the\nunderlying moment condition model is correctly specified. Hong and Li (2023,\nEconometric Theory) showed that GMM estimators with nonsmooth\n(non-directionally differentiable) moment functions are at best\n$n^{1/3}$-consistent under misspecification. Through simulations, we verify the\nslower convergence rate of GMM estimators in such cases. For the two-step GMM\nestimator with an estimated weight matrix, our results align with theory.\nHowever, for the one-step GMM estimator with the identity weight matrix, the\nconvergence rate remains $\\sqrt{n}$, even under severe misspecification.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.09540v1"
    },
    {
        "title": "Identification of hedonic equilibrium and nonseparable simultaneous\n  equations",
        "authors": [
            "Victor Chernozhukov",
            "Alfred Galichon",
            "Marc Henry",
            "Brendan Pass"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  This paper derives conditions under which preferences and technology are\nnonparametrically identified in hedonic equilibrium models, where products are\ndifferentiated along more than one dimension and agents are characterized by\nseveral dimensions of unobserved heterogeneity. With products differentiated\nalong a quality index and agents characterized by scalar unobserved\nheterogeneity, single crossing conditions on preferences and technology provide\nidentifying restrictions in Ekeland, Heckman and Nesheim (2004) and Heckman,\nMatzkin and Nesheim (2010). We develop similar shape restrictions in the\nmulti-attribute case. These shape restrictions, which are based on optimal\ntransport theory and generalized convexity, allow us to identify preferences\nfor goods differentiated along multiple dimensions, from the observation of a\nsingle market. We thereby derive nonparametric identification results for\nnonseparable simultaneous equations and multi-attribute hedonic equilibrium\nmodels with (possibly) multiple dimensions of unobserved heterogeneity. One of\nour results is a proof of absolute continuity of the distribution of\nendogenously traded qualities, which is of independent interest.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.09570v6"
    },
    {
        "title": "Estimation of Graphical Models using the $L_{1,2}$ Norm",
        "authors": [
            "Khai X. Chiong",
            "Hyungsik Roger Moon"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Gaussian graphical models are recently used in economics to obtain networks\nof dependence among agents. A widely-used estimator is the Graphical Lasso\n(GLASSO), which amounts to a maximum likelihood estimation regularized using\nthe $L_{1,1}$ matrix norm on the precision matrix $\\Omega$. The $L_{1,1}$ norm\nis a lasso penalty that controls for sparsity, or the number of zeros in\n$\\Omega$. We propose a new estimator called Structured Graphical Lasso\n(SGLASSO) that uses the $L_{1,2}$ mixed norm. The use of the $L_{1,2}$ penalty\ncontrols for the structure of the sparsity in $\\Omega$. We show that when the\nnetwork size is fixed, SGLASSO is asymptotically equivalent to an infeasible\nGLASSO problem which prioritizes the sparsity-recovery of high-degree nodes.\nMonte Carlo simulation shows that SGLASSO outperforms GLASSO in terms of\nestimating the overall precision matrix and in terms of estimating the\nstructure of the graphical model. In an empirical illustration using a classic\nfirms' investment dataset, we obtain a network of firms' dependence that\nexhibits the core-periphery structure, with General Motors, General Electric\nand U.S. Steel forming the core group of firms.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.10038v2"
    },
    {
        "title": "Semiparametric Estimation of Structural Functions in Nonseparable\n  Triangular Models",
        "authors": [
            "Victor Chernozhukov",
            "Iván Fernández-Val",
            "Whitney Newey",
            "Sami Stouli",
            "Francis Vella"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Triangular systems with nonadditively separable unobserved heterogeneity\nprovide a theoretically appealing framework for the modelling of complex\nstructural relationships. However, they are not commonly used in practice due\nto the need for exogenous variables with large support for identification, the\ncurse of dimensionality in estimation, and the lack of inferential tools. This\npaper introduces two classes of semiparametric nonseparable triangular models\nthat address these limitations. They are based on distribution and quantile\nregression modelling of the reduced form conditional distributions of the\nendogenous variables. We show that average, distribution and quantile\nstructural functions are identified in these systems through a control function\napproach that does not require a large support condition. We propose a\ncomputationally attractive three-stage procedure to estimate the structural\nfunctions where the first two stages consist of quantile or distribution\nregressions. We provide asymptotic theory and uniform inference methods for\neach stage. In particular, we derive functional central limit theorems and\nbootstrap functional central limit theorems for the distribution regression\nestimators of the structural functions. These results establish the validity of\nthe bootstrap for three-stage estimators of structural functions, and lead to\nsimple inference algorithms. We illustrate the implementation and applicability\nof all our methods with numerical simulations and an empirical application to\ndemand analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.02184v3"
    },
    {
        "title": "Uniform Inference for Characteristic Effects of Large Continuous-Time\n  Linear Models",
        "authors": [
            "Yuan Liao",
            "Xiye Yang"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  We consider continuous-time models with a large panel of moment conditions,\nwhere the structural parameter depends on a set of characteristics, whose\neffects are of interest. The leading example is the linear factor model in\nfinancial economics where factor betas depend on observed characteristics such\nas firm specific instruments and macroeconomic variables, and their effects\npick up long-run time-varying beta fluctuations. We specify the factor betas as\nthe sum of characteristic effects and an orthogonal idiosyncratic parameter\nthat captures high-frequency movements. It is often the case that researchers\ndo not know whether or not the latter exists, or its strengths, and thus the\ninference about the characteristic effects should be valid uniformly over a\nbroad class of data generating processes for idiosyncratic parameters. We\nconstruct our estimation and inference in a two-step continuous-time GMM\nframework. It is found that the limiting distribution of the estimated\ncharacteristic effects has a discontinuity when the variance of the\nidiosyncratic parameter is near the boundary (zero), which makes the usual\n\"plug-in\" method using the estimated asymptotic variance only valid pointwise\nand may produce either over- or under- coveraging probabilities. We show that\nthe uniformity can be achieved by cross-sectional bootstrap. Our procedure\nallows both known and estimated factors, and also features a bias correction\nfor the effect of estimating unknown factors.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.04392v2"
    },
    {
        "title": "On monitoring development indicators using high resolution satellite\n  images",
        "authors": [
            "Potnuru Kishen Suraj",
            "Ankesh Gupta",
            "Makkunda Sharma",
            "Sourabh Bikas Paul",
            "Subhashis Banerjee"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  We develop a machine learning based tool for accurate prediction of\nsocio-economic indicators from daytime satellite imagery. The diverse set of\nindicators are often not intuitively related to observable features in\nsatellite images, and are not even always well correlated with each other. Our\npredictive tool is more accurate than using night light as a proxy, and can be\nused to predict missing data, smooth out noise in surveys, monitor development\nprogress of a region, and flag potential anomalies. Finally, we use predicted\nvariables to do robustness analysis of a regression study of high rate of\nstunting in India.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.02282v3"
    },
    {
        "title": "Forecasting of a Hierarchical Functional Time Series on Example of\n  Macromodel for Day and Night Air Pollution in Silesia Region: A Critical\n  Overview",
        "authors": [
            "Daniel Kosiorowski",
            "Dominik Mielczarek",
            "Jerzy. P. Rydlewski"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  In economics we often face a system, which intrinsically imposes a structure\nof hierarchy of its components, i.e., in modelling trade accounts related to\nforeign exchange or in optimization of regional air protection policy.\n  A problem of reconciliation of forecasts obtained on different levels of\nhierarchy has been addressed in the statistical and econometric literature for\nmany times and concerns bringing together forecasts obtained independently at\ndifferent levels of hierarchy.\n  This paper deals with this issue in case of a hierarchical functional time\nseries. We present and critically discuss a state of art and indicate\nopportunities of an application of these methods to a certain environment\nprotection problem. We critically compare the best predictor known from the\nliterature with our own original proposal. Within the paper we study a\nmacromodel describing a day and night air pollution in Silesia region divided\ninto five subregions.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.03797v1"
    },
    {
        "title": "The Calculus of Democratization and Development",
        "authors": [
            "Jacob Ferguson"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  In accordance with \"Democracy's Effect on Development: More Questions than\nAnswers\", we seek to carry out a study in following the description in the\n'Questions for Further Study.' To that end, we studied 33 countries in the\nSub-Saharan Africa region, who all went through an election which should signal\na \"step-up\" for their democracy, one in which previously homogenous regimes\ntransfer power to an opposition party that fairly won the election. After doing\nso, liberal-democracy indicators and democracy indicators were evaluated in the\nfive years prior to and after the election took place, and over that ten-year\nperiod, we examine the data for trends. If we see positive or negative trends\nover this time horizon, we are able to conclude that it was the recent increase\nin the quality of their democracy which led to it. Having investigated examples\nof this in depth, there seem to be three main archetypes which drive the\nresults. Countries with positive results to their democracy from the election\nhave generally positive effects on their development, countries with more\n\"plateau\" like results also did well, but countries for whom the descent to\nauthoritarianism was continued by this election found more negative results.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.04117v1"
    },
    {
        "title": "Assessment Voting in Large Electorates",
        "authors": [
            "Hans Gersbach",
            "Akaki Mamageishvili",
            "Oriol Tejada"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  We analyze Assessment Voting, a new two-round voting procedure that can be\napplied to binary decisions in democratic societies. In the first round, a\nrandomly-selected number of citizens cast their vote on one of the two\nalternatives at hand, thereby irrevocably exercising their right to vote. In\nthe second round, after the results of the first round have been published, the\nremaining citizens decide whether to vote for one alternative or to ab- stain.\nThe votes from both rounds are aggregated, and the final outcome is obtained by\napplying the majority rule, with ties being broken by fair randomization.\nWithin a costly voting framework, we show that large elec- torates will choose\nthe preferred alternative of the majority with high prob- ability, and that\naverage costs will be low. This result is in contrast with the literature on\none-round voting, which predicts either higher voting costs (when voting is\ncompulsory) or decisions that often do not represent the preferences of the\nmajority (when voting is voluntary).\n",
        "pdf_link": "http://arxiv.org/pdf/1712.05470v2"
    },
    {
        "title": "An Exact and Robust Conformal Inference Method for Counterfactual and\n  Synthetic Controls",
        "authors": [
            "Victor Chernozhukov",
            "Kaspar Wüthrich",
            "Yinchu Zhu"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  We introduce new inference procedures for counterfactual and synthetic\ncontrol methods for policy evaluation. We recast the causal inference problem\nas a counterfactual prediction and a structural breaks testing problem. This\nallows us to exploit insights from conformal prediction and structural breaks\ntesting to develop permutation inference procedures that accommodate modern\nhigh-dimensional estimators, are valid under weak and easy-to-verify\nconditions, and are provably robust against misspecification. Our methods work\nin conjunction with many different approaches for predicting counterfactual\nmean outcomes in the absence of the policy intervention. Examples include\nsynthetic controls, difference-in-differences, factor and matrix completion\nmodels, and (fused) time series panel data models. Our approach demonstrates an\nexcellent small-sample performance in simulations and is taken to a data\napplication where we re-evaluate the consequences of decriminalizing indoor\nprostitution. Open-source software for implementing our conformal inference\nmethods is available.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09089v10"
    },
    {
        "title": "Confidence set for group membership",
        "authors": [
            "Andreas Dzemski",
            "Ryo Okui"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Our confidence set quantifies the statistical uncertainty from data-driven\ngroup assignments in grouped panel models. It covers the true group memberships\njointly for all units with pre-specified probability and is constructed by\ninverting many simultaneous unit-specific one-sided tests for group membership.\nWe justify our approach under $N, T \\to \\infty$ asymptotics using tools from\nhigh-dimensional statistics, some of which we extend in this paper. We provide\nMonte Carlo evidence that the confidence set has adequate coverage in finite\nsamples.An empirical application illustrates the use of our confidence set.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.00332v6"
    },
    {
        "title": "A New Wald Test for Hypothesis Testing Based on MCMC outputs",
        "authors": [
            "Yong Li",
            "Xiaobin Liu",
            "Jun Yu",
            "Tao Zeng"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper, a new and convenient $\\chi^2$ wald test based on MCMC outputs\nis proposed for hypothesis testing. The new statistic can be explained as MCMC\nversion of Wald test and has several important advantages that make it very\nconvenient in practical applications. First, it is well-defined under improper\nprior distributions and avoids Jeffrey-Lindley's paradox. Second, it's\nasymptotic distribution can be proved to follow the $\\chi^2$ distribution so\nthat the threshold values can be easily calibrated from this distribution.\nThird, it's statistical error can be derived using the Markov chain Monte Carlo\n(MCMC) approach. Fourth, most importantly, it is only based on the posterior\nMCMC random samples drawn from the posterior distribution. Hence, it is only\nthe by-product of the posterior outputs and very easy to compute. In addition,\nwhen the prior information is available, the finite sample theory is derived\nfor the proposed test statistic. At last, the usefulness of the test is\nillustrated with several applications to latent variable models widely used in\neconomics and finance.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.00973v1"
    },
    {
        "title": "Comparing the Forecasting Performances of Linear Models for Electricity\n  Prices with High RES Penetration",
        "authors": [
            "Angelica Gianfreda",
            "Francesco Ravazzolo",
            "Luca Rossini"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper compares alternative univariate versus multivariate models,\nfrequentist versus Bayesian autoregressive and vector autoregressive\nspecifications, for hourly day-ahead electricity prices, both with and without\nrenewable energy sources. The accuracy of point and density forecasts are\ninspected in four main European markets (Germany, Denmark, Italy and Spain)\ncharacterized by different levels of renewable energy power generation. Our\nresults show that the Bayesian VAR specifications with exogenous variables\ndominate other multivariate and univariate specifications, in terms of both\npoint and density forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.01093v3"
    },
    {
        "title": "Dynamic and granular loss reserving with copulae",
        "authors": [
            "Matúš Maciak",
            "Ostap Okhrin",
            "Michal Pešta"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  An intensive research sprang up for stochastic methods in insurance during\nthe past years. To meet all future claims rising from policies, it is requisite\nto quantify the outstanding loss liabilities. Loss reserving methods based on\naggregated data from run-off triangles are predominantly used to calculate the\nclaims reserves. Conventional reserving techniques have some disadvantages:\nloss of information from the policy and the claim's development due to the\naggregation, zero or negative cells in the triangle; usually small number of\nobservations in the triangle; only few observations for recent accident years;\nand sensitivity to the most recent paid claims.\n  To overcome these dilemmas, granular loss reserving methods for individual\nclaim-by-claim data will be derived. Reserves' estimation is a crucial part of\nthe risk valuation process, which is now a front burner in economics. Since\nthere is a growing demand for prediction of total reserves for different types\nof claims or even multiple lines of business, a time-varying copula framework\nfor granular reserving will be established.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.01792v1"
    },
    {
        "title": "Why Markets are Inefficient: A Gambling \"Theory\" of Financial Markets\n  For Practitioners and Theorists",
        "authors": [
            "Steven D. Moffitt"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The purpose of this article is to propose a new \"theory,\" the Strategic\nAnalysis of Financial Markets (SAFM) theory, that explains the operation of\nfinancial markets using the analytical perspective of an enlightened gambler.\nThe gambler understands that all opportunities for superior performance arise\nfrom suboptimal decisions by humans, but understands also that knowledge of\nhuman decision making alone is not enough to understand market behavior --- one\nmust still model how those decisions lead to market prices. Thus are there\nthree parts to the model: gambling theory, human decision making, and strategic\nproblem solving. A new theory is necessary because at this writing in 2017,\nthere is no theory of financial markets acceptable to both practitioners and\ntheorists. Theorists' efficient market theory, for example, cannot explain\nbubbles and crashes nor the exceptional returns of famous investors and\nspeculators such as Warren Buffett and George Soros. At the same time, a new\ntheory must be sufficiently quantitative, explain market \"anomalies\" and\nprovide predictions in order to satisfy theorists. It is hoped that the SAFM\nframework will meet these requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.01948v1"
    },
    {
        "title": "Revealed Price Preference: Theory and Empirical Analysis",
        "authors": [
            "Rahul Deb",
            "Yuichi Kitamura",
            "John K. -H. Quah",
            "Jörg Stoye"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  To determine the welfare implications of price changes in demand data, we\nintroduce a revealed preference relation over prices. We show that the absence\nof cycles in this relation characterizes a consumer who trades off the utility\nof consumption against the disutility of expenditure. Our model can be applied\nwhenever a consumer's demand over a strict subset of all available goods is\nbeing analyzed; it can also be extended to settings with discrete goods and\nnonlinear prices. To illustrate its use, we apply our model to a single-agent\ndata set and to a data set with repeated cross-sections. We develop a novel\ntest of linear hypotheses on partially identified parameters to estimate the\nproportion of the population who are revealed better off due to a price change\nin the latter application. This new technique can be used for nonparametric\ncounterfactual analysis more broadly.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02702v3"
    },
    {
        "title": "On a Constructive Theory of Markets",
        "authors": [
            "Steven D. Moffitt"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This article is a prologue to the article \"Why Markets are Inefficient: A\nGambling 'Theory' of Financial Markets for Practitioners and Theorists.\" It\npresents important background for that article --- why gambling is important,\neven necessary, for real-world traders --- the reason for the superiority of\nthe strategic/gambling approach to the competing market ideologies of market\nfundamentalism and the scientific approach --- and its potential to uncover\nprofitable trading systems. Much of this article was drawn from Chapter 1 of\nthe book \"The Strategic Analysis of Financial Markets (in 2 volumes)\" World\nScientific, 2017.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02994v1"
    },
    {
        "title": "Panel Data Quantile Regression with Grouped Fixed Effects",
        "authors": [
            "Jiaying Gu",
            "Stanislav Volgushev"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper introduces estimation methods for grouped latent heterogeneity in\npanel data quantile regression. We assume that the observed individuals come\nfrom a heterogeneous population with a finite number of types. The number of\ntypes and group membership is not assumed to be known in advance and is\nestimated by means of a convex optimization problem. We provide conditions\nunder which group membership is estimated consistently and establish asymptotic\nnormality of the resulting estimators. Simulations show that the method works\nwell in finite samples when T is reasonably large. To illustrate the proposed\nmethodology we study the effects of the adoption of Right-to-Carry concealed\nweapon laws on violent crime rates using panel data of 51 U.S. states from 1977\n- 2010.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.05041v2"
    },
    {
        "title": "Censored Quantile Instrumental Variable Estimation with Stata",
        "authors": [
            "Victor Chernozhukov",
            "Iván Fernández-Val",
            "Sukjin Han",
            "Amanda Kowalski"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Many applications involve a censored dependent variable and an endogenous\nindependent variable. Chernozhukov et al. (2015) introduced a censored quantile\ninstrumental variable estimator (CQIV) for use in those applications, which has\nbeen applied by Kowalski (2016), among others. In this article, we introduce a\nStata command, cqiv, that simplifes application of the CQIV estimator in Stata.\nWe summarize the CQIV estimator and algorithm, we describe the use of the cqiv\ncommand, and we provide empirical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.05305v3"
    },
    {
        "title": "Predicting crypto-currencies using sparse non-Gaussian state space\n  models",
        "authors": [
            "Christian Hotz-Behofsits",
            "Florian Huber",
            "Thomas O. Zörner"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper we forecast daily returns of crypto-currencies using a wide\nvariety of different econometric models. To capture salient features commonly\nobserved in financial time series like rapid changes in the conditional\nvariance, non-normality of the measurement errors and sharply increasing\ntrends, we develop a time-varying parameter VAR with t-distributed measurement\nerrors and stochastic volatility. To control for overparameterization, we rely\non the Bayesian literature on shrinkage priors that enables us to shrink\ncoefficients associated with irrelevant predictors and/or perform model\nspecification in a flexible manner. Using around one year of daily data we\nperform a real-time forecasting exercise and investigate whether any of the\nproposed models is able to outperform the naive random walk benchmark. To\nassess the economic relevance of the forecasting gains produced by the proposed\nmodels we moreover run a simple trading exercise.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06373v2"
    },
    {
        "title": "USDA Forecasts: A meta-analysis study",
        "authors": [
            "Bahram Sanginabadi"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The primary goal of this study is doing a meta-analysis research on two\ngroups of published studies. First, the ones that focus on the evaluation of\nthe United States Department of Agriculture (USDA) forecasts and second, the\nones that evaluate the market reactions to the USDA forecasts. We investigate\nfour questions. 1) How the studies evaluate the accuracy of the USDA forecasts?\n2) How they evaluate the market reactions to the USDA forecasts? 3) Is there\nany heterogeneity in the results of the mentioned studies? 4) Is there any\npublication bias? About the first question, while some researchers argue that\nthe forecasts are unbiased, most of them maintain that they are biased,\ninefficient, not optimal, or not rational. About the second question, while a\nfew studies claim that the forecasts are not newsworthy, most of them maintain\nthat they are newsworthy, provide useful information, and cause market\nreactions. About the third and the fourth questions, based on our findings,\nthere are some clues that the results of the studies are heterogeneous, but we\ndidn't find enough evidences of publication bias.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06575v1"
    },
    {
        "title": "Evolution of Regional Innovation with Spatial Knowledge Spillovers:\n  Convergence or Divergence?",
        "authors": [
            "Jinwen Qiu",
            "Wenjian Liu",
            "Ning Ning"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper extends endogenous economic growth models to incorporate knowledge\nexternality. We explores whether spatial knowledge spillovers among regions\nexist, whether spatial knowledge spillovers promote regional innovative\nactivities, and whether external knowledge spillovers affect the evolution of\nregional innovations in the long run. We empirically verify the theoretical\nresults through applying spatial statistics and econometric model in the\nanalysis of panel data of 31 regions in China. An accurate estimate of the\nrange of knowledge spillovers is achieved and the convergence of regional\nknowledge growth rate is found, with clear evidences that developing regions\nbenefit more from external knowledge spillovers than developed regions.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06936v3"
    },
    {
        "title": "Nonseparable Sample Selection Models with Censored Selection Rules",
        "authors": [
            "Iván Fernández-Val",
            "Aico van Vuuren",
            "Francis Vella"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We consider identification and estimation of nonseparable sample selection\nmodels with censored selection rules. We employ a control function approach and\ndiscuss different objects of interest based on (1) local effects conditional on\nthe control function, and (2) global effects obtained from integration over\nranges of values of the control function. We derive the conditions for the\nidentification of these different objects and suggest strategies for\nestimation. Moreover, we provide the associated asymptotic theory. These\nstrategies are illustrated in an empirical investigation of the determinants of\nfemale wages in the United Kingdom.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.08961v2"
    },
    {
        "title": "Random taste heterogeneity in discrete choice models: Flexible\n  nonparametric finite mixture distributions",
        "authors": [
            "Akshay Vij",
            "Rico Krueger"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This study proposes a mixed logit model with multivariate nonparametric\nfinite mixture distributions. The support of the distribution is specified as a\nhigh-dimensional grid over the coefficient space, with equal or unequal\nintervals between successive points along the same dimension; the location of\neach point on the grid and the probability mass at that point are model\nparameters that need to be estimated. The framework does not require the\nanalyst to specify the shape of the distribution prior to model estimation, but\ncan approximate any multivariate probability distribution function to any\narbitrary degree of accuracy. The grid with unequal intervals, in particular,\noffers greater flexibility than existing multivariate nonparametric\nspecifications, while requiring the estimation of a small number of additional\nparameters. An expectation maximization algorithm is developed for the\nestimation of these models. Multiple synthetic datasets and a case study on\ntravel mode choice behavior are used to demonstrate the value of the model\nframework and estimation algorithm. Compared to extant models that incorporate\nrandom taste heterogeneity through continuous mixture distributions, the\nproposed model provides better out-of-sample predictive ability. Findings\nreveal significant differences in willingness to pay measures between the\nproposed model and extant specifications. The case study further demonstrates\nthe ability of the proposed model to endogenously recover patterns of attribute\nnon-attendance and choice set formation.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.02299v1"
    },
    {
        "title": "Long-Term Unemployed hirings: Should targeted or untargeted policies be\n  preferred?",
        "authors": [
            "Alessandra Pasquini",
            "Marco Centra",
            "Guido Pellegrini"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  To what extent, hiring incentives targeting a specific group of vulnerable\nunemployed (i.e. long term unemployed) are more effective, with respect to\ngeneralised incentives (without a definite target), to increase hirings of the\ntargeted group? Are generalized incentives able to influence hirings of the\nvulnerable group? Do targeted policies have negative side effects too important\nto accept them? Even though there is a huge literature on hiring subsidies,\nthese questions remained unresolved. We tried to answer them, comparing the\nimpact of two similar hiring policies, one oriented towards a target group and\none generalised, implemented on the italian labour market. We used\nadministrative data on job contracts, and counterfactual analysis methods. The\ntargeted policy had a positive and significant impact, while the generalized\npolicy didn't have a significant impact on the vulnerable group. Moreover, we\nconcluded the targeted policy didn't have any indirect negative side effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.03343v2"
    },
    {
        "title": "Achieving perfect coordination amongst agents in the co-action minority\n  game",
        "authors": [
            "Hardik Rajpal",
            "Deepak Dhar"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We discuss the strategy that rational agents can use to maximize their\nexpected long-term payoff in the co-action minority game. We argue that the\nagents will try to get into a cyclic state, where each of the $(2N +1)$ agent\nwins exactly $N$ times in any continuous stretch of $(2N+1)$ days. We propose\nand analyse a strategy for reaching such a cyclic state quickly, when any\ndirect communication between agents is not allowed, and only the publicly\navailable common information is the record of total number of people choosing\nthe first restaurant in the past. We determine exactly the average time\nrequired to reach the periodic state for this strategy. We show that it varies\nas $(N/\\ln 2) [1 + \\alpha \\cos (2 \\pi \\log_2 N)$], for large $N$, where the\namplitude $\\alpha$ of the leading term in the log-periodic oscillations is\nfound be $\\frac{8 \\pi^2}{(\\ln 2)^2} \\exp{(- 2 \\pi^2/\\ln 2)} \\approx\n{\\color{blue}7 \\times 10^{-11}}$.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.06770v2"
    },
    {
        "title": "The Allen--Uzawa elasticity of substitution for nonhomogeneous\n  production functions",
        "authors": [
            "Elena Burmistrova",
            "Sergey Lobanov"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This note proves that the representation of the Allen elasticity of\nsubstitution obtained by Uzawa for linear homogeneous functions holds true for\nnonhomogeneous functions. It is shown that the criticism of the Allen-Uzawa\nelasticity of substitution in the works of Blackorby, Primont, Russell is based\non an incorrect example.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.06885v1"
    },
    {
        "title": "The Security of the United Kingdom Electricity Imports under Conditions\n  of High European Demand",
        "authors": [
            "Anthony D Stephens",
            "David R Walwyn"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Energy policy in Europe has been driven by the three goals of security of\nsupply, economic competitiveness and environmental sustainability, referred to\nas the energy trilemma. Although there are clear conflicts within the trilemma,\nmember countries have acted to facilitate a fully integrated European\nelectricity market. Interconnection and cross-border electricity trade has been\na fundamental part of such market liberalisation. However, it has been\nsuggested that consumers are exposed to a higher price volatility as a\nconsequence of interconnection. Furthermore, during times of energy shortages\nand high demand, issues of national sovereignty take precedence over\ncooperation. In this article, the unique and somewhat peculiar conditions of\nearly 2017 within France, Germany and the United Kingdom have been studied to\nunderstand how the existing integration arrangements address the energy\ntrilemma. It is concluded that the dominant interests are economic and national\nsecurity; issues of environmental sustainability are neglected or overridden.\nAlthough the optimisation of European electricity generation to achieve a lower\noverall carbon emission is possible, such a goal is far from being realised.\nFurthermore, it is apparent that the United Kingdom, and other countries,\ncannot rely upon imports from other countries during periods of high demand\nand/or limited supply.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.07457v1"
    },
    {
        "title": "Forecasting the impact of state pension reforms in post-Brexit England\n  and Wales using microsimulation and deep learning",
        "authors": [
            "Agnieszka Werpachowska"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We employ stochastic dynamic microsimulations to analyse and forecast the\npension cost dependency ratio for England and Wales from 1991 to 2061,\nevaluating the impact of the ongoing state pension reforms and changes in\ninternational migration patterns under different Brexit scenarios. To fully\naccount for the recently observed volatility in life expectancies, we propose\nmortality rate model based on deep learning techniques, which discovers complex\npatterns in data and extrapolated trends. Our results show that the recent\nreforms can effectively stave off the \"pension crisis\" and bring back the\nsystem on a sounder fiscal footing. At the same time, increasingly more workers\ncan expect to spend greater share of their lifespan in retirement, despite the\neligibility age rises. The population ageing due to the observed postponement\nof death until senectitude often occurs with the compression of morbidity, and\nthus will not, perforce, intrinsically strain healthcare costs. To a lesser\ndegree, the future pension cost dependency ratio will depend on the post-Brexit\nrelations between the UK and the EU, with \"soft\" alignment on the free movement\nlowering the relative cost of the pension system compared to the \"hard\" one. In\nthe long term, however, the ratio has a rising tendency.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.09427v2"
    },
    {
        "title": "Moment Inequalities in the Context of Simulated and Predicted Variables",
        "authors": [
            "Hiroaki Kaido",
            "Jiaxuan Li",
            "Marc Rysman"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper explores the effects of simulated moments on the performance of\ninference methods based on moment inequalities. Commonly used confidence sets\nfor parameters are level sets of criterion functions whose boundary points may\ndepend on sample moments in an irregular manner. Due to this feature,\nsimulation errors can affect the performance of inference in non-standard ways.\nIn particular, a (first-order) bias due to the simulation errors may remain in\nthe estimated boundary of the confidence set. We demonstrate, through Monte\nCarlo experiments, that simulation errors can significantly reduce the coverage\nprobabilities of confidence sets in small samples. The size distortion is\nparticularly severe when the number of inequality restrictions is large. These\nresults highlight the danger of ignoring the sampling variations due to the\nsimulation errors in moment inequality models. Similar issues arise when using\npredicted variables in moment inequalities models. We propose a method for\nproperly correcting for these variations based on regularizing the intersection\nof moments in parameter space, and we show that our proposed method performs\nwell theoretically and in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03674v1"
    },
    {
        "title": "Quantifying the Economic Case for Electric Semi-Trucks",
        "authors": [
            "Shashank Sripad",
            "Venkatasubramanian Viswanathan"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  There has been considerable interest in the electrification of freight\ntransport, particularly heavy-duty trucks to downscale the greenhouse-gas (GHG)\nemissions from the transportation sector. However, the economic competitiveness\nof electric semi-trucks is uncertain as there are substantial additional\ninitial costs associated with the large battery packs required. In this work,\nwe analyze the trade-off between the initial investment and the operating cost\nfor realistic usage scenarios to compare a fleet of electric semi-trucks with a\nrange of 500 miles with a fleet of diesel trucks. For the baseline case with\n30% of fleet requiring battery pack replacements and a price differential of\nUS\\$50,000, we find a payback period of about 3 years. Based on sensitivity\nanalysis, we find that the fraction of the fleet that requires battery pack\nreplacements is a major factor. For the case with 100% replacement fraction,\nthe payback period could be as high as 5-6 years. We identify the price of\nelectricity as the second most important variable, where a price of\nUS$0.14/kWh, the payback period could go up to 5 years. Electric semi-trucks\nare expected to lead to savings due to reduced repairs and magnitude of these\nsavings could play a crucial role in the payback period as well. With increased\npenetration of autonomous vehicles, the annual mileage of semi-trucks could\nsubstantially increase and this heavily sways in favor of electric semi-trucks,\nbringing down the payback period to around 2 years at an annual mileage of\n120,000 miles. There is an undeniable economic case for electric semi-trucks\nand developing battery packs with longer cycle life and higher specific energy\nwould make this case even stronger.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.05974v1"
    },
    {
        "title": "Dissection of Bitcoin's Multiscale Bubble History from January 2012 to\n  February 2018",
        "authors": [
            "Jan-Christian Gerlach",
            "Guilherme Demos",
            "Didier Sornette"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We present a detailed bubble analysis of the Bitcoin to US Dollar price\ndynamics from January 2012 to February 2018. We introduce a robust automatic\npeak detection method that classifies price time series into periods of\nuninterrupted market growth (drawups) and regimes of uninterrupted market\ndecrease (drawdowns). In combination with the Lagrange Regularisation Method\nfor detecting the beginning of a new market regime, we identify 3 major peaks\nand 10 additional smaller peaks, that have punctuated the dynamics of Bitcoin\nprice during the analyzed time period. We explain this classification of long\nand short bubbles by a number of quantitative metrics and graphs to understand\nthe main socio-economic drivers behind the ascent of Bitcoin over this period.\nThen, a detailed analysis of the growing risks associated with the three long\nbubbles using the Log-Periodic Power Law Singularity (LPPLS) model is based on\nthe LPPLS Confidence Indicators, defined as the fraction of qualified fits of\nthe LPPLS model over multiple time windows. Furthermore, for various fictitious\n'present' times $t_2$ before the crashes, we employ a clustering method to\ngroup the predicted critical times $t_c$ of the LPPLS fits over different time\nscales, where $t_c$ is the most probable time for the ending of the bubble.\nEach cluster is proposed as a plausible scenario for the subsequent Bitcoin\nprice evolution. We present these predictions for the three long bubbles and\nthe four short bubbles that our time scale of analysis was able to resolve.\nOverall, our predictive scheme provides useful information to warn of an\nimminent crash risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06261v4"
    },
    {
        "title": "Econometric Modeling of Regional Electricity Spot Prices in the\n  Australian Market",
        "authors": [
            "Michael Stanley Smith",
            "Thomas S. Shively"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Wholesale electricity markets are increasingly integrated via high voltage\ninterconnectors, and inter-regional trade in electricity is growing. To model\nthis, we consider a spatial equilibrium model of price formation, where\nconstraints on inter-regional flows result in three distinct equilibria in\nprices. We use this to motivate an econometric model for the distribution of\nobserved electricity spot prices that captures many of their unique empirical\ncharacteristics. The econometric model features supply and inter-regional trade\ncost functions, which are estimated using Bayesian monotonic regression\nsmoothing methodology. A copula multivariate time series model is employed to\ncapture additional dependence -- both cross-sectional and serial-- in regional\nprices. The marginal distributions are nonparametric, with means given by the\nregression means. The model has the advantage of preserving the heavy\nright-hand tail in the predictive densities of price. We fit the model to\nhalf-hourly spot price data in the five interconnected regions of the\nAustralian national electricity market. The fitted model is then used to\nmeasure how both supply and price shocks in one region are transmitted to the\ndistribution of prices in all regions in subsequent periods. Finally, to\nvalidate our econometric model, we show that prices forecast using the proposed\nmodel compare favorably with those from some benchmark alternatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.08218v1"
    },
    {
        "title": "Interpreting Quantile Independence",
        "authors": [
            "Matthew A. Masten",
            "Alexandre Poirier"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  How should one assess the credibility of assumptions weaker than statistical\nindependence, like quantile independence? In the context of identifying causal\neffects of a treatment variable, we argue that such deviations should be chosen\nbased on the form of selection on unobservables they allow. For quantile\nindependence, we characterize this form of treatment selection. Specifically,\nwe show that quantile independence is equivalent to a constraint on the average\nvalue of either a latent propensity score (for a binary treatment) or the cdf\nof treatment given the unobservables (for a continuous treatment). In both\ncases, this average value constraint requires a kind of non-monotonic treatment\nselection. Using these results, we show that several common treatment selection\nmodels are incompatible with quantile independence. We introduce a class of\nassumptions which weakens quantile independence by removing the average value\nconstraint, and therefore allows for monotonic treatment selection. In a\npotential outcomes model with a binary treatment, we derive identified sets for\nthe ATT and QTT under both classes of assumptions. In a numerical example we\nshow that the average value constraint inherent in quantile independence has\nsubstantial identifying power. Our results suggest that researchers should\ncarefully consider the credibility of this non-monotonicity property when using\nquantile independence to weaken full independence.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.10957v1"
    },
    {
        "title": "Identifying Effects of Multivalued Treatments",
        "authors": [
            "Sokbae Lee",
            "Bernard Salanié"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Multivalued treatment models have typically been studied under restrictive\nassumptions: ordered choice, and more recently unordered monotonicity. We show\nhow treatment effects can be identified in a more general class of models that\nallows for multidimensional unobserved heterogeneity. Our results rely on two\nmain assumptions: treatment assignment must be a measurable function of\nthreshold-crossing rules, and enough continuous instruments must be available.\nWe illustrate our approach for several classes of models.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.00057v1"
    },
    {
        "title": "Structural Breaks in Time Series",
        "authors": [
            "Alessandro Casini",
            "Pierre Perron"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This chapter covers methodological issues related to estimation, testing and\ncomputation for models involving structural changes. Our aim is to review\ndevelopments as they relate to econometric applications based on linear models.\nSubstantial advances have been made to cover models at a level of generality\nthat allow a host of interesting practical applications. These include models\nwith general stationary regressors and errors that can exhibit temporal\ndependence and heteroskedasticity, models with trending variables and possible\nunit roots and cointegrated models, among others. Advances have been made\npertaining to computational aspects of constructing estimates, their limit\ndistributions, tests for structural changes, and methods to determine the\nnumber of changes present. A variety of topics are covered. The first part\nsummarizes and updates developments described in an earlier review, Perron\n(2006), with the exposition following heavily that of Perron (2008). Additions\nare included for recent developments: testing for common breaks, models with\nendogenous regressors (emphasizing that simply using least-squares is\npreferable over instrumental variables methods), quantile regressions, methods\nbased on Lasso, panel data models, testing for changes in forecast accuracy,\nfactors models and methods of inference based on a continuous records\nasymptotic framework. Our focus is on the so-called off-line methods whereby\none wants to retrospectively test for breaks in a given sample of data and form\nconfidence intervals about the break dates. The aim is to provide the readers\nwith an overview of methods that are of direct usefulness in practice as\nopposed to issues that are mostly of theoretical interest.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.03807v1"
    },
    {
        "title": "Nuclear Norm Regularized Estimation of Panel Regression Models",
        "authors": [
            "Hyungsik Roger Moon",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper we investigate panel regression models with interactive fixed\neffects. We propose two new estimation methods that are based on minimizing\nconvex objective functions. The first method minimizes the sum of squared\nresiduals with a nuclear (trace) norm regularization. The second method\nminimizes the nuclear norm of the residuals. We establish the consistency of\nthe two resulting estimators. Those estimators have a very important\ncomputational advantage compared to the existing least squares (LS) estimator,\nin that they are defined as minimizers of a convex objective function. In\naddition, the nuclear norm penalization helps to resolve a potential\nidentification problem for interactive fixed effect models, in particular when\nthe regressors are low-rank and the number of the factors is unknown. We also\nshow how to construct estimators that are asymptotically equivalent to the\nleast squares (LS) estimator in Bai (2009) and Moon and Weidner (2017) by using\nour nuclear norm regularized or minimized estimators as initial values for a\nfinite number of LS minimizing iteration steps. This iteration avoids any\nnon-convex minimization, while the original LS estimation problem is generally\nnon-convex, and can have multiple local minima.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.10987v3"
    },
    {
        "title": "Modified Causal Forests for Estimating Heterogeneous Causal Effects",
        "authors": [
            "Michael Lechner"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Uncovering the heterogeneity of causal effects of policies and business\ndecisions at various levels of granularity provides substantial value to\ndecision makers. This paper develops new estimation and inference procedures\nfor multiple treatment models in a selection-on-observables framework by\nmodifying the Causal Forest approach suggested by Wager and Athey (2018) in\nseveral dimensions. The new estimators have desirable theoretical,\ncomputational and practical properties for various aggregation levels of the\ncausal effects. While an Empirical Monte Carlo study suggests that they\noutperform previously suggested estimators, an application to the evaluation of\nan active labour market programme shows the value of the new methods for\napplied research.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09487v2"
    },
    {
        "title": "Salvaging Falsified Instrumental Variable Models",
        "authors": [
            "Matthew A. Masten",
            "Alexandre Poirier"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  What should researchers do when their baseline model is refuted? We provide\nfour constructive answers. First, researchers can measure the extent of\nfalsification. To do this, we consider continuous relaxations of the baseline\nassumptions of concern. We then define the falsification frontier: The smallest\nrelaxations of the baseline model which are not refuted. This frontier provides\na quantitative measure of the extent of falsification. Second, researchers can\npresent the identified set for the parameter of interest under the assumption\nthat the true model lies somewhere on this frontier. We call this the\nfalsification adaptive set. This set generalizes the standard baseline estimand\nto account for possible falsification. Third, researchers can present the\nidentified set for a specific point on this frontier. Finally, as a sensitivity\nanalysis, researchers can present identified sets for points beyond the\nfrontier. To illustrate these four ways of salvaging falsified models, we study\noveridentifying restrictions in two instrumental variable models: a homogeneous\neffects linear model, and heterogeneous effect models with either binary or\ncontinuous outcomes. In the linear model, we consider the classical\noveridentifying restrictions implied when multiple instruments are observed. We\ngeneralize these conditions by considering continuous relaxations of the\nclassical exclusion restrictions. By sufficiently weakening the assumptions, a\nfalsified baseline model becomes non-falsified. We obtain analogous results in\nthe heterogeneous effect models, where we derive identified sets for marginal\ndistributions of potential outcomes, falsification frontiers, and falsification\nadaptive sets under continuous relaxations of the instrument exogeneity\nassumptions. We illustrate our results in four different empirical\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.11598v3"
    },
    {
        "title": "Limit Theorems for Network Dependent Random Variables",
        "authors": [
            "Denis Kojevnikov",
            "Vadim Marmer",
            "Kyungchul Song"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper is concerned with cross-sectional dependence arising because\nobservations are interconnected through an observed network. Following Doukhan\nand Louhichi (1999), we measure the strength of dependence by covariances of\nnonlinearly transformed variables. We provide a law of large numbers and\ncentral limit theorem for network dependent variables. We also provide a method\nof calculating standard errors robust to general forms of network dependence.\nFor that purpose, we rely on a network heteroskedasticity and autocorrelation\nconsistent (HAC) variance estimator, and show its consistency. The results rely\non conditions characterized by tradeoffs between the rate of decay of\ndependence across a network and network's denseness. Our approach can\naccommodate data generated by network formation models, random fields on\ngraphs, conditional dependency graphs, and large functional-causal systems of\nequations.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.01059v6"
    },
    {
        "title": "Machine Learning Methods Economists Should Know About",
        "authors": [
            "Susan Athey",
            "Guido Imbens"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We discuss the relevance of the recent Machine Learning (ML) literature for\neconomics and econometrics. First we discuss the differences in goals, methods\nand settings between the ML literature and the traditional econometrics and\nstatistics literatures. Then we discuss some specific methods from the machine\nlearning literature that we view as important for empirical researchers in\neconomics. These include supervised learning methods for regression and\nclassification, unsupervised learning methods, as well as matrix completion\nmethods. Finally, we highlight newly developed methods at the intersection of\nML and econometrics, methods that typically perform better than either\noff-the-shelf ML or more traditional econometric methods when applied to\nparticular classes of problems, problems that include causal inference for\naverage treatment effects, optimal policy estimation, and estimation of the\ncounterfactual effect of price changes in consumer choice models.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.10075v1"
    },
    {
        "title": "Simpler Proofs for Approximate Factor Models of Large Dimensions",
        "authors": [
            "Jushan Bai",
            "Serena Ng"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Estimates of the approximate factor model are increasingly used in empirical\nwork. Their theoretical properties, studied some twenty years ago, also laid\nthe ground work for analysis on large dimensional panel data models with\ncross-section dependence. This paper presents simplified proofs for the\nestimates by using alternative rotation matrices, exploiting properties of low\nrank matrices, as well as the singular value decomposition of the data in\naddition to its covariance structure. These simplifications facilitate\ninterpretation of results and provide a more friendly introduction to\nresearchers new to the field. New results are provided to allow linear\nrestrictions to be imposed on factor models.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.00254v1"
    },
    {
        "title": "What can we learn about SARS-CoV-2 prevalence from testing and hospital\n  data?",
        "authors": [
            "Daniel W. Sacks",
            "Nir Menachemi",
            "Peter Embi",
            "Coady Wing"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Measuring the prevalence of active SARS-CoV-2 infections in the general\npopulation is difficult because tests are conducted on a small and non-random\nsegment of the population. However, people admitted to the hospital for\nnon-COVID reasons are tested at very high rates, even though they do not appear\nto be at elevated risk of infection. This sub-population may provide valuable\nevidence on prevalence in the general population. We estimate upper and lower\nbounds on the prevalence of the virus in the general population and the\npopulation of non-COVID hospital patients under weak assumptions on who gets\ntested, using Indiana data on hospital inpatient records linked to SARS-CoV-2\nvirological tests. The non-COVID hospital population is tested fifty times as\noften as the general population, yielding much tighter bounds on prevalence. We\nprovide and test conditions under which this non-COVID hospitalization bound is\nvalid for the general population. The combination of clinical testing data and\nhospital records may contain much more information about the state of the\nepidemic than has been previously appreciated. The bounds we calculate for\nIndiana could be constructed at relatively low cost in many other states.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.00298v2"
    },
    {
        "title": "Design-Based Uncertainty for Quasi-Experiments",
        "authors": [
            "Ashesh Rambachan",
            "Jonathan Roth"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Design-based frameworks of uncertainty are frequently used in settings where\nthe treatment is (conditionally) randomly assigned. This paper develops a\ndesign-based framework suitable for analyzing quasi-experimental settings in\nthe social sciences, in which the treatment is at least partially determined by\nidiosyncratic factors but there are concerns about endogenous selection into\ntreatment. In our framework, treatments are stochastic, but units may differ in\ntheir probabilities of receiving treatment, thereby allowing for rich forms of\nselection. We provide conditions under which the estimands of popular\nquasi-experimental estimators correspond to interpretable finite-population\ncausal parameters. We characterize the biases and distortions to inference that\narise when these conditions are violated. These results can be used to conduct\nsensitivity analyses when there are concerns about selection into treatment.\nTaken together, our results establish a rigorous foundation for\nquasi-experimental analyses that more closely aligns with the way empirical\nresearchers discuss the variation in the data.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.00602v7"
    },
    {
        "title": "Estimating TVP-VAR models with time invariant long-run multipliers",
        "authors": [
            "Denis Belomestny",
            "Ekaterina Krymova",
            "Andrey Polbin"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The main goal of this paper is to develop a methodology for estimating time\nvarying parameter vector auto-regression (TVP-VAR) models with a timeinvariant\nlong-run relationship between endogenous variables and changes in exogenous\nvariables. We propose a Gibbs sampling scheme for estimation of model\nparameters as well as time-invariant long-run multiplier parameters. Further we\ndemonstrate the applicability of the proposed method by analyzing examples of\nthe Norwegian and Russian economies based on the data on real GDP, real\nexchange rate and real oil prices. Our results show that incorporating the time\ninvariance constraint on the long-run multipliers in TVP-VAR model helps to\nsignificantly improve the forecasting performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.00718v1"
    },
    {
        "title": "Testing error distribution by kernelized Stein discrepancy in\n  multivariate time series models",
        "authors": [
            "Donghang Luo",
            "Ke Zhu",
            "Huan Gong",
            "Dong Li"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Knowing the error distribution is important in many multivariate time series\napplications. To alleviate the risk of error distribution mis-specification,\ntesting methodologies are needed to detect whether the chosen error\ndistribution is correct. However, the majority of the existing tests only deal\nwith the multivariate normal distribution for some special multivariate time\nseries models, and they thus can not be used to testing for the often observed\nheavy-tailed and skewed error distributions in applications. In this paper, we\nconstruct a new consistent test for general multivariate time series models,\nbased on the kernelized Stein discrepancy. To account for the estimation\nuncertainty and unobserved initial values, a bootstrap method is provided to\ncalculate the critical values. Our new test is easy-to-implement for a large\nscope of multivariate error distributions, and its importance is illustrated by\nsimulated and real data.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.00747v1"
    },
    {
        "title": "Nonparametric prediction with spatial data",
        "authors": [
            "Abhimanyu Gupta",
            "Javier Hidalgo"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We describe a (nonparametric) prediction algorithm for spatial data, based on\na canonical factorization of the spectral density function. We provide\ntheoretical results showing that the predictor has desirable asymptotic\nproperties. Finite sample performance is assessed in a Monte Carlo study that\nalso compares our algorithm to a rival nonparametric method based on the\ninfinite AR representation of the dynamics of the data. Finally, we apply our\nmethodology to predict house prices in Los Angeles.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04269v2"
    },
    {
        "title": "\"Big Data\" and its Origins",
        "authors": [
            "Francis X. Diebold"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Against the background of explosive growth in data volume, velocity, and\nvariety, I investigate the origins of the term \"Big Data\". Its origins are a\nbit murky and hence intriguing, involving both academics and industry,\nstatistics and computer science, ultimately winding back to lunch-table\nconversations at Silicon Graphics Inc. (SGI) in the mid 1990s. The Big Data\nphenomenon continues unabated, and the ongoing development of statistical\nmachine learning tools continues to help us confront it.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.05835v6"
    },
    {
        "title": "Bounding Infection Prevalence by Bounding Selectivity and Accuracy of\n  Tests: With Application to Early COVID-19",
        "authors": [
            "Jörg Stoye"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  I propose novel partial identification bounds on infection prevalence from\ninformation on test rate and test yield. The approach utilizes user-specified\nbounds on (i) test accuracy and (ii) the extent to which tests are targeted,\nformalized as restriction on the effect of true infection status on the odds\nratio of getting tested and thereby embeddable in logit specifications. The\nmotivating application is to the COVID-19 pandemic but the strategy may also be\nuseful elsewhere.\n  Evaluated on data from the pandemic's early stage, even the weakest of the\nnovel bounds are reasonably informative. Notably, and in contrast to\nspeculations that were widely reported at the time, they place the infection\nfatality rate for Italy well above the one of influenza by mid-April.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.06178v2"
    },
    {
        "title": "Finite-Sample Average Bid Auction",
        "authors": [
            "Haitian Xie"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The paper studies the problem of auction design in a setting where the\nauctioneer accesses the knowledge of the valuation distribution only through\nstatistical samples. A new framework is established that combines the\nstatistical decision theory with mechanism design. Two optimality criteria,\nmaxmin, and equivariance, are studied along with their implications on the form\nof auctions. The simplest form of the equivariant auction is the average bid\nauction, which set individual reservation prices proportional to the average of\nother bids and historical samples. This form of auction can be motivated by the\nGamma distribution, and it sheds new light on the estimation of the optimal\nprice, an irregular parameter. Theoretical results show that it is often\npossible to use the regular parameter population mean to approximate the\noptimal price. An adaptive average bid estimator is developed under this idea,\nand it has the same asymptotic properties as the empirical Myerson estimator.\nThe new proposed estimator has a significantly better performance in terms of\nvalue at risk and expected shortfall when the sample size is small.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.10217v2"
    },
    {
        "title": "Efficient closed-form estimation of large spatial autoregressions",
        "authors": [
            "Abhimanyu Gupta"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Newton-step approximations to pseudo maximum likelihood estimates of spatial\nautoregressive models with a large number of parameters are examined, in the\nsense that the parameter space grows slowly as a function of sample size. These\nhave the same asymptotic efficiency properties as maximum likelihood under\nGaussianity but are of closed form. Hence they are computationally simple and\nfree from compactness assumptions, thereby avoiding two notorious pitfalls of\nimplicitly defined estimates of large spatial autoregressions. For an initial\nleast squares estimate, the Newton step can also lead to weaker regularity\nconditions for a central limit theorem than those extant in the literature. A\nsimulation study demonstrates excellent finite sample gains from Newton\niterations, especially in large multiparameter models for which grid search is\ncostly. A small empirical illustration shows improvements in estimation\nprecision with real data.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.12395v4"
    },
    {
        "title": "The Identity Fragmentation Bias",
        "authors": [
            "Tesary Lin",
            "Sanjog Misra"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Consumers interact with firms across multiple devices, browsers, and\nmachines; these interactions are often recorded with different identifiers for\nthe same consumer. The failure to correctly match different identities leads to\na fragmented view of exposures and behaviors. This paper studies the identity\nfragmentation bias, referring to the estimation bias resulted from using\nfragmented data. Using a formal framework, we decompose the contributing\nfactors of the estimation bias caused by data fragmentation and discuss the\ndirection of bias. Contrary to conventional wisdom, this bias cannot be signed\nor bounded under standard assumptions. Instead, upward biases and sign\nreversals can occur even in experimental settings. We then compare several\ncorrective measures, and discuss their respective advantages and caveats.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.12849v2"
    },
    {
        "title": "Dependence-Robust Inference Using Resampled Statistics",
        "authors": [
            "Michael P. Leung"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We develop inference procedures robust to general forms of weak dependence.\nThe procedures utilize test statistics constructed by resampling in a manner\nthat does not depend on the unknown correlation structure of the data. We prove\nthat the statistics are asymptotically normal under the weak requirement that\nthe target parameter can be consistently estimated at the parametric rate. This\nholds for regular estimators under many well-known forms of weak dependence and\njustifies the claim of dependence-robustness. We consider applications to\nsettings with unknown or complicated forms of dependence, with various forms of\nnetwork dependence as leading examples. We develop tests for both moment\nequalities and inequalities.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.02097v4"
    },
    {
        "title": "Bounds on direct and indirect effects under treatment/mediator\n  endogeneity and outcome attrition",
        "authors": [
            "Martin Huber",
            "Lukáš Lafférs"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Causal mediation analysis aims at disentangling a treatment effect into an\nindirect mechanism operating through an intermediate outcome or mediator, as\nwell as the direct effect of the treatment on the outcome of interest. However,\nthe evaluation of direct and indirect effects is frequently complicated by\nnon-ignorable selection into the treatment and/or mediator, even after\ncontrolling for observables, as well as sample selection/outcome attrition. We\npropose a method for bounding direct and indirect effects in the presence of\nsuch complications using a method that is based on a sequence of linear\nprogramming problems. Considering inverse probability weighting by propensity\nscores, we compute the weights that would yield identification in the absence\nof complications and perturb them by an entropy parameter reflecting a specific\namount of propensity score misspecification to set-identify the effects of\ninterest. We apply our method to data from the National Longitudinal Survey of\nYouth 1979 to derive bounds on the explained and unexplained components of a\ngender wage gap decomposition that is likely prone to non-ignorable mediator\nselection and outcome attrition.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.05253v3"
    },
    {
        "title": "Hopf Bifurcation from new-Keynesian Taylor rule to Ramsey Optimal Policy",
        "authors": [
            "Jean-Bernard Chatelain",
            "Kirsten Ralf"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper compares different implementations of monetary policy in a\nnew-Keynesian setting. We can show that a shift from Ramsey optimal policy\nunder short-term commitment (based on a negative feedback mechanism) to a\nTaylor rule (based on a positive feedback mechanism) corresponds to a Hopf\nbifurcation with opposite policy advice and a change of the dynamic properties.\nThis bifurcation occurs because of the ad hoc assumption that interest rate is\na forward-looking variable when policy targets (inflation and output gap) are\nforward-looking variables in the new-Keynesian theory.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.07479v1"
    },
    {
        "title": "Revenue-based Attribution Modeling for Online Advertising",
        "authors": [
            "Kaifeng Zhao",
            "Seyed Hanif Mahboobi",
            "Saeed Bagheri"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  This paper examines and proposes several attribution modeling methods that\nquantify how revenue should be attributed to online advertising inputs. We\nadopt and further develop relative importance method, which is based on\nregression models that have been extensively studied and utilized to\ninvestigate the relationship between advertising efforts and market reaction\n(revenue). Relative importance method aims at decomposing and allocating\nmarginal contributions to the coefficient of determination (R^2) of regression\nmodels as attribution values. In particular, we adopt two alternative\nsubmethods to perform this decomposition: dominance analysis and relative\nweight analysis. Moreover, we demonstrate an extension of the decomposition\nmethods from standard linear model to additive model. We claim that our new\napproaches are more flexible and accurate in modeling the underlying\nrelationship and calculating the attribution values. We use simulation examples\nto demonstrate the superior performance of our new approaches over traditional\nmethods. We further illustrate the value of our proposed approaches using a\nreal advertising campaign dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.06561v1"
    },
    {
        "title": "Electricity Market Theory Based on Continuous Time Commodity Model",
        "authors": [
            "Haoyong Chen",
            "Lijia Han"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  The recent research report of U.S. Department of Energy prompts us to\nre-examine the pricing theories applied in electricity market design. The\ntheory of spot pricing is the basis of electricity market design in many\ncountries, but it has two major drawbacks: one is that it is still based on the\ntraditional hourly scheduling/dispatch model, ignores the crucial time\ncontinuity in electric power production and consumption and does not treat the\ninter-temporal constraints seriously; the second is that it assumes that the\nelectricity products are homogeneous in the same dispatch period and cannot\ndistinguish the base, intermediate and peak power with obviously different\ntechnical and economic characteristics. To overcome the shortcomings, this\npaper presents a continuous time commodity model of electricity, including spot\npricing model and load duration model. The market optimization models under the\ntwo pricing mechanisms are established with the Riemann and Lebesgue integrals\nrespectively and the functional optimization problem are solved by the\nEuler-Lagrange equation to obtain the market equilibria. The feasibility of\npricing according to load duration is proved by strict mathematical derivation.\nSimulation results show that load duration pricing can correctly identify and\nvalue different attributes of generators, reduce the total electricity\npurchasing cost, and distribute profits among the power plants more equitably.\nThe theory and methods proposed in this paper will provide new ideas and\ntheoretical foundation for the development of electric power markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.07918v1"
    },
    {
        "title": "Existence in Multidimensional Screening with General Nonlinear\n  Preferences",
        "authors": [
            "Kelvin Shuangjian Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  We generalize the approach of Carlier (2001) and provide an existence proof\nfor the multidimensional screening problem with general nonlinear preferences.\nWe first formulate the principal's problem as a maximization problem with\n$G$-convexity constraints and then use $G$-convex analysis to prove existence.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.08549v2"
    },
    {
        "title": "Calibration of Machine Learning Classifiers for Probability of Default\n  Modelling",
        "authors": [
            "Pedro G. Fonseca",
            "Hugo D. Lopes"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Binary classification is highly used in credit scoring in the estimation of\nprobability of default. The validation of such predictive models is based both\non rank ability, and also on calibration (i.e. how accurately the probabilities\noutput by the model map to the observed probabilities). In this study we cover\nthe current best practices regarding calibration for binary classification, and\nexplore how different approaches yield different results on real world credit\nscoring data. The limitations of evaluating credit scoring models using only\nrank ability metrics are explored. A benchmark is run on 18 real world\ndatasets, and results compared. The calibration techniques used are Platt\nScaling and Isotonic Regression. Also, different machine learning models are\nused: Logistic Regression, Random Forest Classifiers, and Gradient Boosting\nClassifiers. Results show that when the dataset is treated as a time series,\nthe use of re-calibration with Isotonic Regression is able to improve the long\nterm calibration better than the alternative methods. Using re-calibration, the\nnon-parametric models are able to outperform the Logistic Regression on Brier\nScore Loss.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.08901v1"
    },
    {
        "title": "Shape-Constrained Density Estimation via Optimal Transport",
        "authors": [
            "Ryan Cumings-Menon"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Constraining the maximum likelihood density estimator to satisfy a\nsufficiently strong constraint, $\\log-$concavity being a common example, has\nthe effect of restoring consistency without requiring additional parameters.\nSince many results in economics require densities to satisfy a regularity\ncondition, these estimators are also attractive for the structural estimation\nof economic models. In all of the examples of regularity conditions provided by\nBagnoli and Bergstrom (2005) and Ewerhart (2013), $\\log-$concavity is\nsufficient to ensure that the density satisfies the required conditions.\nHowever, in many cases $\\log-$concavity is far from necessary, and it has the\nunfortunate side effect of ruling out sub-exponential tail behavior.\n  In this paper, we use optimal transport to formulate a shape constrained\ndensity estimator. We initially describe the estimator using a $\\rho-$concavity\nconstraint. In this setting we provide results on consistency, asymptotic\ndistribution, convexity of the optimization problem defining the estimator, and\nformulate a test for the null hypothesis that the population density satisfies\na shape constraint. Afterward, we provide sufficient conditions for these\nresults to hold using an arbitrary shape constraint. This generalization is\nused to explore whether the California Department of Transportation's decision\nto award construction contracts with the use of a first price auction is cost\nminimizing. We estimate the marginal costs of construction firms subject to\nMyerson's (1981) regularity condition, which is a requirement for the first\nprice reverse auction to be cost minimizing. The proposed test fails to reject\nthat the regularity condition is satisfied.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.09069v2"
    },
    {
        "title": "Calibrated Projection in MATLAB: Users' Manual",
        "authors": [
            "Hiroaki Kaido",
            "Francesca Molinari",
            "Jörg Stoye",
            "Matthew Thirkettle"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  We present the calibrated-projection MATLAB package implementing the method\nto construct confidence intervals proposed by Kaido, Molinari and Stoye (2017).\nThis manual provides details on how to use the package for inference on\nprojections of partially identified parameters. It also explains how to use the\nMATLAB functions we developed to compute confidence intervals on solutions of\nnonlinear optimization problems with estimated constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.09707v1"
    },
    {
        "title": "Macroeconomics and FinTech: Uncovering Latent Macroeconomic Effects on\n  Peer-to-Peer Lending",
        "authors": [
            "Jessica Foo",
            "Lek-Heng Lim",
            "Ken Sze-Wai Wong"
        ],
        "category": "econ.EM",
        "published_year": "2017",
        "summary": "  Peer-to-peer (P2P) lending is a fast growing financial technology (FinTech)\ntrend that is displacing traditional retail banking. Studies on P2P lending\nhave focused on predicting individual interest rates or default probabilities.\nHowever, the relationship between aggregated P2P interest rates and the general\neconomy will be of interest to investors and borrowers as the P2P credit market\nmatures. We show that the variation in P2P interest rates across grade types\nare determined by three macroeconomic latent factors formed by Canonical\nCorrelation Analysis (CCA) - macro default, investor uncertainty, and the\nfundamental value of the market. However, the variation in P2P interest rates\nacross term types cannot be explained by the general economy.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.11283v1"
    },
    {
        "title": "Counterfactual Analysis under Partial Identification Using Locally\n  Robust Refinement",
        "authors": [
            "Nathan Canen",
            "Kyungchul Song"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Structural models that admit multiple reduced forms, such as game-theoretic\nmodels with multiple equilibria, pose challenges in practice, especially when\nparameters are set-identified and the identified set is large. In such cases,\nresearchers often choose to focus on a particular subset of equilibria for\ncounterfactual analysis, but this choice can be hard to justify. This paper\nshows that some parameter values can be more \"desirable\" than others for\ncounterfactual analysis, even if they are empirically equivalent given the\ndata. In particular, within the identified set, some counterfactual predictions\ncan exhibit more robustness than others, against local perturbations of the\nreduced forms (e.g. the equilibrium selection rule). We provide a\nrepresentation of this subset which can be used to simplify the implementation.\nWe illustrate our message using moment inequality models, and provide an\nempirical application based on a model with top-coded data.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.00003v3"
    },
    {
        "title": "Bayesian nonparametric graphical models for time-varying parameters VAR",
        "authors": [
            "Matteo Iacopini",
            "Luca Rossini"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Over the last decade, big data have poured into econometrics, demanding new\nstatistical methods for analysing high-dimensional data and complex non-linear\nrelationships. A common approach for addressing dimensionality issues relies on\nthe use of static graphical structures for extracting the most significant\ndependence interrelationships between the variables of interest. Recently,\nBayesian nonparametric techniques have become popular for modelling complex\nphenomena in a flexible and efficient manner, but only few attempts have been\nmade in econometrics. In this paper, we provide an innovative Bayesian\nnonparametric (BNP) time-varying graphical framework for making inference in\nhigh-dimensional time series. We include a Bayesian nonparametric dependent\nprior specification on the matrix of coefficients and the covariance matrix by\nmean of a Time-Series DPP as in Nieto-Barajas et al. (2012). Following Billio\net al. (2019), our hierarchical prior overcomes over-parametrization and\nover-fitting issues by clustering the vector autoregressive (VAR) coefficients\ninto groups and by shrinking the coefficients of each group toward a common\nlocation. Our BNP timevarying VAR model is based on a spike-and-slab\nconstruction coupled with dependent Dirichlet Process prior (DPP) and allows\nto: (i) infer time-varying Granger causality networks from time series; (ii)\nflexibly model and cluster non-zero time-varying coefficients; (iii)\naccommodate for potential non-linearities. In order to assess the performance\nof the model, we study the merits of our approach by considering a well-known\nmacroeconomic dataset. Moreover, we check the robustness of the method by\ncomparing two alternative specifications, with Dirac and diffuse spike prior\ndistributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.02140v1"
    },
    {
        "title": "Bias-Aware Inference in Fuzzy Regression Discontinuity Designs",
        "authors": [
            "Claudia Noack",
            "Christoph Rothe"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose new confidence sets (CSs) for the regression discontinuity\nparameter in fuzzy designs. Our CSs are based on local linear regression, and\nare bias-aware, in the sense that they take possible bias explicitly into\naccount. Their construction shares similarities with that of Anderson-Rubin CSs\nin exactly identified instrumental variable models, and thereby avoids issues\nwith \"delta method\" approximations that underlie most commonly used existing\ninference methods for fuzzy regression discontinuity analysis. Our CSs are\nasymptotically equivalent to existing procedures in canonical settings with\nstrong identification and a continuous running variable. However, due to their\nparticular construction they are also valid under a wide range of empirically\nrelevant conditions in which existing methods can fail, such as setups with\ndiscrete running variables, donut designs, and weak identification.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.04631v4"
    },
    {
        "title": "Posterior Average Effects",
        "authors": [
            "Stéphane Bonhomme",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Economists are often interested in estimating averages with respect to\ndistributions of unobservables, such as moments of individual fixed-effects, or\naverage partial effects in discrete choice models. For such quantities, we\npropose and study posterior average effects (PAE), where the average is\ncomputed conditional on the sample, in the spirit of empirical Bayes and\nshrinkage methods. While the usefulness of shrinkage for prediction is\nwell-understood, a justification of posterior conditioning to estimate\npopulation averages is currently lacking. We show that PAE have minimum\nworst-case specification error under various forms of misspecification of the\nparametric distribution of unobservables. In addition, we introduce a measure\nof informativeness of the posterior conditioning, which quantifies the\nworst-case specification error of PAE relative to parametric model-based\nestimators. As illustrations, we report PAE estimates of distributions of\nneighborhood effects in the US, and of permanent and transitory components in a\nmodel of income dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.06360v6"
    },
    {
        "title": "Proxy expenditure weights for Consumer Price Index: Audit sampling\n  inference for big data statistics",
        "authors": [
            "Li-Chun Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Purchase data from retail chains provide proxy measures of private household\nexpenditure on items that are the most troublesome to collect in the\ntraditional expenditure survey. Due to the sheer amount of proxy data, the bias\ndue to coverage and selection errors completely dominates the variance. We\ndevelop tests for bias based on audit sampling, which makes use of available\nsurvey data that cannot be linked to the proxy data source at the individual\nlevel. However, audit sampling fails to yield a meaningful mean squared error\nestimate, because the sampling variance is too large compared to the bias of\nthe big data estimate. We propose a novel accuracy measure that is applicable\nin such situations. This can provide a necessary part of the statistical\nargument for the uptake of big data source, in replacement of traditional\nsurvey sampling. An application to disaggregated food price index is used to\ndemonstrate the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.11208v1"
    },
    {
        "title": "A Robust Score-Driven Filter for Multivariate Time Series",
        "authors": [
            "Enzo D'Innocenzo",
            "Alessandra Luati",
            "Mario Mazzocchi"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  A multivariate score-driven filter is developed to extract signals from noisy\nvector processes. By assuming that the conditional location vector from a\nmultivariate Student's t distribution changes over time, we construct a robust\nfilter which is able to overcome several issues that naturally arise when\nmodeling heavy-tailed phenomena and, more in general, vectors of dependent\nnon-Gaussian time series. We derive conditions for stationarity and\ninvertibility and estimate the unknown parameters by maximum likelihood (ML).\nStrong consistency and asymptotic normality of the estimator are proved and the\nfinite sample properties are illustrated by a Monte-Carlo study. From a\ncomputational point of view, analytical formulae are derived, which consent to\ndevelop estimation procedures based on the Fisher scoring method. The theory is\nsupported by a novel empirical illustration that shows how the model can be\neffectively applied to estimate consumer prices from home scanner data.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.01517v3"
    },
    {
        "title": "Heterogeneous Coefficients, Control Variables, and Identification of\n  Multiple Treatment Effects",
        "authors": [
            "Whitney K. Newey",
            "Sami Stouli"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Multidimensional heterogeneity and endogeneity are important features of\nmodels with multiple treatments. We consider a heterogeneous coefficients model\nwhere the outcome is a linear combination of dummy treatment variables, with\neach variable representing a different kind of treatment. We use control\nvariables to give necessary and sufficient conditions for identification of\naverage treatment effects. With mutually exclusive treatments we find that,\nprovided the heterogeneous coefficients are mean independent from treatments\ngiven the controls, a simple identification condition is that the generalized\npropensity scores (Imbens, 2000) be bounded away from zero and that their sum\nbe bounded away from one, with probability one. Our analysis extends to\ndistributional and quantile treatment effects, as well as corresponding\ntreatment effects on the treated. These results generalize the classical\nidentification result of Rosenbaum and Rubin (1983) for binary treatments.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02314v3"
    },
    {
        "title": "Doubly Robust Semiparametric Difference-in-Differences Estimators with\n  High-Dimensional Data",
        "authors": [
            "Yang Ning",
            "Sida Peng",
            "Jing Tao"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper proposes a doubly robust two-stage semiparametric\ndifference-in-difference estimator for estimating heterogeneous treatment\neffects with high-dimensional data. Our new estimator is robust to model\nmiss-specifications and allows for, but does not require, many more regressors\nthan observations. The first stage allows a general set of machine learning\nmethods to be used to estimate the propensity score. In the second stage, we\nderive the rates of convergence for both the parametric parameter and the\nunknown function under a partially linear specification for the outcome\nequation. We also provide bias correction procedures to allow for valid\ninference for the heterogeneous treatment effects. We evaluate the finite\nsample performance with extensive simulation studies. Additionally, a real data\nanalysis on the effect of Fair Minimum Wage Act on the unemployment rate is\nperformed as an illustration of our method. An R package for implementing the\nproposed method is available on Github.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03151v1"
    },
    {
        "title": "Robust discrete choice models with t-distributed kernel errors",
        "authors": [
            "Rico Krueger",
            "Michel Bierlaire",
            "Thomas Gasos",
            "Prateek Bansal"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Outliers in discrete choice response data may result from misclassification\nand misreporting of the response variable and from choice behaviour that is\ninconsistent with modelling assumptions (e.g. random utility maximisation). In\nthe presence of outliers, standard discrete choice models produce biased\nestimates and suffer from compromised predictive accuracy. Robust statistical\nmodels are less sensitive to outliers than standard non-robust models. This\npaper analyses two robust alternatives to the multinomial probit (MNP) model.\nThe two models are robit models whose kernel error distributions are\nheavy-tailed t-distributions to moderate the influence of outliers. The first\nmodel is the multinomial robit (MNR) model, in which a generic degrees of\nfreedom parameter controls the heavy-tailedness of the kernel error\ndistribution. The second model, the generalised multinomial robit (Gen-MNR)\nmodel, is more flexible than MNR, as it allows for distinct heavy-tailedness in\neach dimension of the kernel error distribution. For both models, we derive\nGibbs samplers for posterior inference. In a simulation study, we illustrate\nthe excellent finite sample properties of the proposed Bayes estimators and\nshow that MNR and Gen-MNR produce more accurate estimates if the choice data\ncontain outliers through the lens of the non-robust MNP model. In a case study\non transport mode choice behaviour, MNR and Gen-MNR outperform MNP by\nsubstantial margins in terms of in-sample fit and out-of-sample predictive\naccuracy. The case study also highlights differences in elasticity estimates\nacross models.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.06383v3"
    },
    {
        "title": "Spatial Differencing for Sample Selection Models with Unobserved\n  Heterogeneity",
        "authors": [
            "Alexander Klein",
            "Guy Tchuente"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper derives identification, estimation, and inference results using\nspatial differencing in sample selection models with unobserved heterogeneity.\nWe show that under the assumption of smooth changes across space of the\nunobserved sub-location specific heterogeneities and inverse Mills ratio, key\nparameters of a sample selection model are identified. The smoothness of the\nsub-location specific heterogeneities implies a correlation in the outcomes. We\nassume that the correlation is restricted within a location or cluster and\nderive asymptotic results showing that as the number of independent clusters\nincreases, the estimators are consistent and asymptotically normal. We also\npropose a formula for standard error estimation. A Monte-Carlo experiment\nillustrates the small sample properties of our estimator. The application of\nour procedure to estimate the determinants of the municipality tax rate in\nFinland shows the importance of accounting for unobserved heterogeneity.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.06570v1"
    },
    {
        "title": "Manipulation-Robust Regression Discontinuity Designs",
        "authors": [
            "Takuya Ishihara",
            "Masayuki Sawada"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We present simple low-level conditions for identification in regression\ndiscontinuity designs using a potential outcome framework for the manipulation\nof the running variable. Using this framework, we replace the existing\nidentification statement with two restrictions on manipulation. Our framework\nhighlights the critical role of the continuous density of the running variable\nin identification. In particular, we establish the low-level auxiliary\nassumption of the diagnostic density test under which the design may detect\nmanipulation against identification and hence is manipulation-robust.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.07551v7"
    },
    {
        "title": "Optimal probabilistic forecasts: When do they work?",
        "authors": [
            "Gael M. Martin",
            "Rubén Loaiza-Maya",
            "David T. Frazier",
            "Worapree Maneesoonthorn",
            "Andrés Ramírez Hassan"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Proper scoring rules are used to assess the out-of-sample accuracy of\nprobabilistic forecasts, with different scoring rules rewarding distinct\naspects of forecast performance. Herein, we re-investigate the practice of\nusing proper scoring rules to produce probabilistic forecasts that are\n`optimal' according to a given score, and assess when their out-of-sample\naccuracy is superior to alternative forecasts, according to that score.\nParticular attention is paid to relative predictive performance under\nmisspecification of the predictive model. Using numerical illustrations, we\ndocument several novel findings within this paradigm that highlight the\nimportant interplay between the true data generating process, the assumed\npredictive model and the scoring rule. Notably, we show that only when a\npredictive model is sufficiently compatible with the true process to allow a\nparticular score criterion to reward what it is designed to reward, will this\napproach to forecasting reap benefits. Subject to this compatibility however,\nthe superiority of the optimal forecast will be greater, the greater is the\ndegree of misspecification. We explore these issues under a range of different\nscenarios, and using both artificially simulated and empirical data.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09592v1"
    },
    {
        "title": "Recent Developments on Factor Models and its Applications in Econometric\n  Learning",
        "authors": [
            "Jianqing Fan",
            "Kunpeng Li",
            "Yuan Liao"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper makes a selective survey on the recent development of the factor\nmodel and its application on statistical learnings. We focus on the perspective\nof the low-rank structure of factor models, and particularly draws attentions\nto estimating the model from the low-rank recovery point of view. The survey\nmainly consists of three parts: the first part is a review on new factor\nestimations based on modern techniques on recovering low-rank structures of\nhigh-dimensional models. The second part discusses statistical inferences of\nseveral factor-augmented models and applications in econometric learning\nmodels. The final part summarizes new developments dealing with unbalanced\npanels from the matrix completion perspective.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.10103v1"
    },
    {
        "title": "Nonclassical Measurement Error in the Outcome Variable",
        "authors": [
            "Christoph Breunig",
            "Stephan Martin"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study a semi-/nonparametric regression model with a general form of\nnonclassical measurement error in the outcome variable. We show equivalence of\nthis model to a generalized regression model. Our main identifying assumptions\nare a special regressor type restriction and monotonicity in the nonlinear\nrelationship between the observed and unobserved true outcome. Nonparametric\nidentification is then obtained under a normalization of the unknown link\nfunction, which is a natural extension of the classical measurement error case.\nWe propose a novel sieve rank estimator for the regression function and\nestablish its rate of convergence.\n  In Monte Carlo simulations, we find that our estimator corrects for biases\ninduced by nonclassical measurement error and provides numerically stable\nresults. We apply our method to analyze belief formation of stock market\nexpectations with survey data from the German Socio-Economic Panel (SOEP) and\nfind evidence for nonclassical measurement error in subjective belief data.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.12665v2"
    },
    {
        "title": "Identification of Conduit Countries and Community Structures in the\n  Withholding Tax Networks",
        "authors": [
            "Tembo Nakamoto",
            "Yuichi Ikeda"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Due to economic globalization, each country's economic law, including tax\nlaws and tax treaties, has been forced to work as a single network. However,\neach jurisdiction (country or region) has not made its economic law under the\nassumption that its law functions as an element of one network, so it has\nbrought unexpected results. We thought that the results are exactly\ninternational tax avoidance. To contribute to the solution of international tax\navoidance, we tried to investigate which part of the network is vulnerable.\nSpecifically, focusing on treaty shopping, which is one of international tax\navoidance methods, we attempt to identified which jurisdiction are likely to be\nused for treaty shopping from tax liabilities and the relationship between\njurisdictions which are likely to be used for treaty shopping and others. For\nthat purpose, based on withholding tax rates imposed on dividends, interest,\nand royalties by jurisdictions, we produced weighted multiple directed graphs,\ncomputed the centralities and detected the communities. As a result, we\nclarified the jurisdictions that are likely to be used for treaty shopping and\npointed out that there are community structures. The results of this study\nsuggested that fewer jurisdictions need to introduce more regulations for\nprevention of treaty abuse worldwide.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.00799v1"
    },
    {
        "title": "Pricing Engine: Estimating Causal Impacts in Real World Business\n  Settings",
        "authors": [
            "Matt Goldman",
            "Brian Quistorff"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We introduce the Pricing Engine package to enable the use of Double ML\nestimation techniques in general panel data settings. Customization allows the\nuser to specify first-stage models, first-stage featurization, second stage\ntreatment selection and second stage causal-modeling. We also introduce a\nDynamicDML class that allows the user to generate dynamic treatment-aware\nforecasts at a range of leads and to understand how the forecasts will vary as\na function of causally estimated treatment parameters. The Pricing Engine is\nbuilt on Python 3.5 and can be run on an Azure ML Workbench environment with\nthe addition of only a few Python packages. This note provides high-level\ndiscussion of the Double ML method, describes the packages intended use and\nincludes an example Jupyter notebook demonstrating application to some publicly\navailable data. Installation of the package and additional technical\ndocumentation is available at\n$\\href{https://github.com/bquistorff/pricingengine}{github.com/bquistorff/pricingengine}$.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.03285v2"
    },
    {
        "title": "Inference under Covariate-Adaptive Randomization with Multiple\n  Treatments",
        "authors": [
            "Federico A. Bugni",
            "Ivan A. Canay",
            "Azeem M. Shaikh"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper studies inference in randomized controlled trials with\ncovariate-adaptive randomization when there are multiple treatments. More\nspecifically, we study inference about the average effect of one or more\ntreatments relative to other treatments or a control. As in Bugni et al.\n(2018), covariate-adaptive randomization refers to randomization schemes that\nfirst stratify according to baseline covariates and then assign treatment\nstatus so as to achieve balance within each stratum. In contrast to Bugni et\nal. (2018), we not only allow for multiple treatments, but further allow for\nthe proportion of units being assigned to each of the treatments to vary across\nstrata. We first study the properties of estimators derived from a fully\nsaturated linear regression, i.e., a linear regression of the outcome on all\ninteractions between indicators for each of the treatments and indicators for\neach of the strata. We show that tests based on these estimators using the\nusual heteroskedasticity-consistent estimator of the asymptotic variance are\ninvalid; on the other hand, tests based on these estimators and suitable\nestimators of the asymptotic variance that we provide are exact. For the\nspecial case in which the target proportion of units being assigned to each of\nthe treatments does not vary across strata, we additionally consider tests\nbased on estimators derived from a linear regression with strata fixed effects,\ni.e., a linear regression of the outcome on indicators for each of the\ntreatments and indicators for each of the strata. We show that tests based on\nthese estimators using the usual heteroskedasticity-consistent estimator of the\nasymptotic variance are conservative, but tests based on these estimators and\nsuitable estimators of the asymptotic variance that we provide are exact. A\nsimulation study illustrates the practical relevance of our theoretical\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.04206v3"
    },
    {
        "title": "A hybrid econometric-machine learning approach for relative importance\n  analysis: Prioritizing food policy",
        "authors": [
            "Akash Malhotra"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  A measure of relative importance of variables is often desired by researchers\nwhen the explanatory aspects of econometric methods are of interest. To this\nend, the author briefly reviews the limitations of conventional econometrics in\nconstructing a reliable measure of variable importance. The author highlights\nthe relative stature of explanatory and predictive analysis in economics and\nthe emergence of fruitful collaborations between econometrics and computer\nscience. Learning lessons from both, the author proposes a hybrid approach\nbased on conventional econometrics and advanced machine learning (ML)\nalgorithms, which are otherwise, used in predictive analytics. The purpose of\nthis article is two-fold, to propose a hybrid approach to assess relative\nimportance and demonstrate its applicability in addressing policy priority\nissues with an example of food inflation in India, followed by a broader aim to\nintroduce the possibility of conflation of ML and conventional econometrics to\nan audience of researchers in economics and social sciences, in general.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.04517v3"
    },
    {
        "title": "LASSO-Driven Inference in Time and Space",
        "authors": [
            "Victor Chernozhukov",
            "Wolfgang K. Härdle",
            "Chen Huang",
            "Weining Wang"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We consider the estimation and inference in a system of high-dimensional\nregression equations allowing for temporal and cross-sectional dependency in\ncovariates and error processes, covering rather general forms of weak temporal\ndependence. A sequence of regressions with many regressors using LASSO (Least\nAbsolute Shrinkage and Selection Operator) is applied for variable selection\npurpose, and an overall penalty level is carefully chosen by a block multiplier\nbootstrap procedure to account for multiplicity of the equations and\ndependencies in the data. Correspondingly, oracle properties with a jointly\nselected tuning parameter are derived. We further provide high-quality\nde-biased simultaneous inference on the many target parameters of the system.\nWe provide bootstrap consistency results of the test procedure, which are based\non a general Bahadur representation for the $Z$-estimators with dependent data.\nSimulations demonstrate good performance of the proposed inference procedure.\nFinally, we apply the method to quantify spillover effects of textual sentiment\nindices in a financial market and to test the connectedness among sectors.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.05081v4"
    },
    {
        "title": "Stratification Trees for Adaptive Randomization in Randomized Controlled\n  Trials",
        "authors": [
            "Max Tabord-Meehan"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper proposes an adaptive randomization procedure for two-stage\nrandomized controlled trials. The method uses data from a first-wave experiment\nin order to determine how to stratify in a second wave of the experiment, where\nthe objective is to minimize the variance of an estimator for the average\ntreatment effect (ATE). We consider selection from a class of stratified\nrandomization procedures which we call stratification trees: these are\nprocedures whose strata can be represented as decision trees, with differing\ntreatment assignment probabilities across strata. By using the first wave to\nestimate a stratification tree, we simultaneously select which covariates to\nuse for stratification, how to stratify over these covariates, as well as the\nassignment probabilities within these strata. Our main result shows that using\nthis randomization procedure with an appropriate estimator results in an\nasymptotic variance which is minimal in the class of stratification trees.\nMoreover, the results we present are able to accommodate a large class of\nassignment mechanisms within strata, including stratified block randomization.\nIn a simulation study, we find that our method, paired with an appropriate\ncross-validation procedure ,can improve on ad-hoc choices of stratification. We\nconclude by applying our method to the study in Karlan and Wood (2017), where\nwe estimate stratification trees using the first wave of their experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.05127v7"
    },
    {
        "title": "A Profit Optimization Approach Based on the Use of Pumped-Hydro Energy\n  Storage Unit and Dynamic Pricing",
        "authors": [
            "Akın Taşcikaraoğlu",
            "Ozan Erdinç"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this study, an optimization problem is proposed in order to obtain the\nmaximum economic benefit from wind farms with variable and intermittent energy\ngeneration in the day ahead and balancing electricity markets. This method,\nwhich is based on the use of pumped-hydro energy storage unit and wind farm\ntogether, increases the profit from the power plant by taking advantage of the\nprice changes in the markets and at the same time supports the power system by\nsupplying a portion of the peak load demand in the system to which the plant is\nconnected. With the objective of examining the effectiveness of the proposed\nmethod, detailed simulation studies are carried out by making use of actual\nwind and price data, and the results are compared to those obtained for the\nvarious cases in which the storage unit is not available and/or the proposed\nprice-based energy management method is not applied. As a consequence, it is\ndemonstrated that the pumped-hydro energy storage units are the storage systems\ncapable of being used effectively for high-power levels and that the proposed\noptimization problem is quite successful in the cost-effective implementation\nof these systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.05211v1"
    },
    {
        "title": "Effect of Climate and Geography on worldwide fine resolution economic\n  activity",
        "authors": [
            "Alberto Troccoli"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Geography, including climatic factors, have long been considered potentially\nimportant elements in shaping socio-economic activities, alongside other\ndeterminants, such as institutions. Here we demonstrate that geography and\nclimate satisfactorily explain worldwide economic activity as measured by the\nper capita Gross Cell Product (GCP-PC) at a fine geographical resolution,\ntypically much higher than country average. A 1{\\deg} by 1{\\deg} GCP-PC dataset\nhas been key for establishing and testing a direct relationship between 'local'\ngeography/climate and GCP-PC. Not only have we tested the geography/climate\nhypothesis using many possible explanatory variables, importantly we have also\npredicted and reconstructed GCP-PC worldwide by retaining the most significant\npredictors. While this study confirms that latitude is the most important\npredictor for GCP-PC when taken in isolation, the accuracy of the GCP-PC\nprediction is greatly improved when other factors mainly related to variations\nin climatic variables, such as the variability in air pressure, rather than\naverage climatic conditions as typically used, are considered. Implications of\nthese findings include an improved understanding of why economically better-off\nsocieties are geographically placed where they are\n",
        "pdf_link": "http://arxiv.org/pdf/1806.06358v2"
    },
    {
        "title": "Factor Investing: A Bayesian Hierarchical Approach",
        "authors": [
            "Guanhao Feng",
            "Jingyu He"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper investigates asset allocation problems when returns are\npredictable. We introduce a market-timing Bayesian hierarchical (BH) approach\nthat adopts heterogeneous time-varying coefficients driven by lagged\nfundamental characteristics. Our approach includes a joint estimation of\nconditional expected returns and covariance matrix and considers estimation\nrisk for portfolio analysis. The hierarchical prior allows modeling different\nassets separately while sharing information across assets. We demonstrate the\nperformance of the U.S. equity market. Though the Bayesian forecast is slightly\nbiased, our BH approach outperforms most alternative methods in point and\ninterval prediction. Our BH approach in sector investment for the recent twenty\nyears delivers a 0.92\\% average monthly returns and a 0.32\\% significant\nJensen`s alpha. We also find technology, energy, and manufacturing are\nimportant sectors in the past decade, and size, investment, and short-term\nreversal factors are heavily weighted. Finally, the stochastic discount factor\nconstructed by our BH approach explains most anomalies.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.01015v3"
    },
    {
        "title": "Semiparametric correction for endogenous truncation bias with Vox Populi\n  based participation decision",
        "authors": [
            "Nir Billfeld",
            "Moshe Kim"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We synthesize the knowledge present in various scientific disciplines for the\ndevelopment of semiparametric endogenous truncation-proof algorithm, correcting\nfor truncation bias due to endogenous self-selection. This synthesis enriches\nthe algorithm's accuracy, efficiency and applicability. Improving upon the\ncovariate shift assumption, data are intrinsically affected and largely\ngenerated by their own behavior (cognition). Refining the concept of Vox Populi\n(Wisdom of Crowd) allows data points to sort themselves out depending on their\nestimated latent reference group opinion space. Monte Carlo simulations, based\non 2,000,000 different distribution functions, practically generating 100\nmillion realizations, attest to a very high accuracy of our model.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.06286v1"
    },
    {
        "title": "Binscatter Regressions",
        "authors": [
            "Matias D. Cattaneo",
            "Richard K. Crump",
            "Max H. Farrell",
            "Yingjie Feng"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We introduce the package Binsreg, which implements the binscatter methods\ndeveloped by Cattaneo, Crump, Farrell, and Feng (2024b,a). The package includes\nseven commands: binsreg, binslogit, binsprobit, binsqreg, binstest, binspwc,\nand binsregselect. The first four commands implement binscatter plotting, point\nestimation, and uncertainty quantification (confidence intervals and confidence\nbands) for least squares linear binscatter regression (binsreg) and for\nnonlinear binscatter regression (binslogit for Logit regression, binsprobit for\nProbit regression, and binsqreg for quantile regression). The next two commands\nfocus on pointwise and uniform inference: binstest implements hypothesis\ntesting procedures for parametric specifications and for nonparametric shape\nrestrictions of the unknown regression function, while binspwc implements\nmulti-group pairwise statistical comparisons. Finally, the command\nbinsregselect implements data-driven number of bins selectors. The commands\noffer binned scatter plots, and allow for covariate adjustment, weighting,\nclustering, and multi-sample analysis, which is useful when studying treatment\neffect heterogeneity in randomized and observational studies, among many other\nfeatures.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.09615v5"
    },
    {
        "title": "Semiparametric estimation of heterogeneous treatment effects under the\n  nonignorable assignment condition",
        "authors": [
            "Keisuke Takahata",
            "Takahiro Hoshino"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a semiparametric two-stage least square estimator for the\nheterogeneous treatment effects (HTE). HTE is the solution to certain integral\nequation which belongs to the class of Fredholm integral equations of the first\nkind, which is known to be ill-posed problem. Naive semi/nonparametric methods\ndo not provide stable solution to such problems. Then we propose to approximate\nthe function of interest by orthogonal series under the constraint which makes\nthe inverse mapping of integral to be continuous and eliminates the\nill-posedness. We illustrate the performance of the proposed estimator through\nsimulation experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.09978v1"
    },
    {
        "title": "Granger Causality Testing in High-Dimensional VARs: a\n  Post-Double-Selection Procedure",
        "authors": [
            "Alain Hecq",
            "Luca Margaritella",
            "Stephan Smeekes"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We develop an LM test for Granger causality in high-dimensional VAR models\nbased on penalized least squares estimations. To obtain a test retaining the\nappropriate size after the variable selection done by the lasso, we propose a\npost-double-selection procedure to partial out effects of nuisance variables\nand establish its uniform asymptotic validity. We conduct an extensive set of\nMonte-Carlo simulations that show our tests perform well under different data\ngenerating processes, even without sparsity. We apply our testing procedure to\nfind networks of volatility spillovers and we find evidence that causal\nrelationships become clearer in high-dimensional compared to standard\nlow-dimensional VARs.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.10991v4"
    },
    {
        "title": "Boosting: Why You Can Use the HP Filter",
        "authors": [
            "Peter C. B. Phillips",
            "Zhentao Shi"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The Hodrick-Prescott (HP) filter is one of the most widely used econometric\nmethods in applied macroeconomic research. Like all nonparametric methods, the\nHP filter depends critically on a tuning parameter that controls the degree of\nsmoothing. Yet in contrast to modern nonparametric methods and applied work\nwith these procedures, empirical practice with the HP filter almost universally\nrelies on standard settings for the tuning parameter that have been suggested\nlargely by experimentation with macroeconomic data and heuristic reasoning. As\nrecent research (Phillips and Jin, 2015) has shown, standard settings may not\nbe adequate in removing trends, particularly stochastic trends, in economic\ndata.\n  This paper proposes an easy-to-implement practical procedure of iterating the\nHP smoother that is intended to make the filter a smarter smoothing device for\ntrend estimation and trend elimination. We call this iterated HP technique the\nboosted HP filter in view of its connection to $L_{2}$-boosting in machine\nlearning. The paper develops limit theory to show that the boosted HP (bHP)\nfilter asymptotically recovers trend mechanisms that involve unit root\nprocesses, deterministic polynomial drifts, and polynomial drifts with\nstructural breaks. A stopping criterion is used to automate the iterative HP\nalgorithm, making it a data-determined method that is ready for modern\ndata-rich environments in economic research. The methodology is illustrated\nusing three real data examples that highlight the differences between simple HP\nfiltering, the data-determined boosted filter, and an alternative\nautoregressive approach. These examples show that the bHP filter is helpful in\nanalyzing a large collection of heterogeneous macroeconomic time series that\nmanifest various degrees of persistence, trend behavior, and volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.00175v3"
    },
    {
        "title": "Non-standard inference for augmented double autoregressive models with\n  null volatility coefficients",
        "authors": [
            "Feiyu Jiang",
            "Dong Li",
            "Ke Zhu"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper considers an augmented double autoregressive (DAR) model, which\nallows null volatility coefficients to circumvent the over-parameterization\nproblem in the DAR model. Since the volatility coefficients might be on the\nboundary, the statistical inference methods based on the Gaussian quasi-maximum\nlikelihood estimation (GQMLE) become non-standard, and their asymptotics\nrequire the data to have a finite sixth moment, which narrows applicable scope\nin studying heavy-tailed data. To overcome this deficiency, this paper develops\na systematic statistical inference procedure based on the self-weighted GQMLE\nfor the augmented DAR model. Except for the Lagrange multiplier test statistic,\nthe Wald, quasi-likelihood ratio and portmanteau test statistics are all shown\nto have non-standard asymptotics. The entire procedure is valid as long as the\ndata is stationary, and its usefulness is illustrated by simulation studies and\none real example.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.01798v1"
    },
    {
        "title": "Iterative Estimation of Nonparametric Regressions with Continuous\n  Endogenous Variables and Discrete Instruments",
        "authors": [
            "Samuele Centorrino",
            "Frédérique Fève",
            "Jean-Pierre Florens"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We consider a nonparametric regression model with continuous endogenous\nindependent variables when only discrete instruments are available that are\nindependent of the error term. Although this framework is very relevant for\napplied research, its implementation is challenging, as the regression function\nbecomes the solution to a nonlinear integral equation. We propose a simple\niterative procedure to estimate such models and showcase some of its asymptotic\nproperties. In a simulation experiment, we detail its implementation in the\ncase when the instrumental variable is binary. We conclude with an empirical\napplication to returns to education.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.07812v3"
    },
    {
        "title": "Smoothing quantile regressions",
        "authors": [
            "Marcelo Fernandes",
            "Emmanuel Guerre",
            "Eduardo Horta"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose to smooth the entire objective function, rather than only the\ncheck function, in a linear quantile regression context. Not only does the\nresulting smoothed quantile regression estimator yield a lower mean squared\nerror and a more accurate Bahadur-Kiefer representation than the standard\nestimator, but it is also asymptotically differentiable. We exploit the latter\nto propose a quantile density estimator that does not suffer from the curse of\ndimensionality. This means estimating the conditional density function without\nworrying about the dimension of the covariate vector. It also allows for\ntwo-stage efficient quantile regression estimation. Our asymptotic theory holds\nuniformly with respect to the bandwidth and quantile level. Finally, we propose\na rule of thumb for choosing the smoothing bandwidth that should approximate\nwell the optimal bandwidth. Simulations confirm that our smoothed quantile\nregression estimator indeed performs very well in finite samples.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08535v3"
    },
    {
        "title": "Centered and non-centered variance inflation factor",
        "authors": [
            "Román Salmerón Gómez",
            "Catalina García García y José García Pérez"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper analyzes the diagnostic of near multicollinearity in a multiple\nlinear regression from auxiliary centered regressions (with intercept) and\nnon-centered (without intercept). From these auxiliary regression, the centered\nand non-centered Variance Inflation Factors are calculated, respectively. It is\nalso presented an expression that relate both of them.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.12293v1"
    },
    {
        "title": "Heterogeneity in demand and optimal price conditioning for local rail\n  transport",
        "authors": [
            "Evgeniy M. Ozhegov",
            "Alina Ozhegova"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper describes the results of research project on optimal pricing for\nLLC \"Perm Local Rail Company\". In this study we propose a regression tree based\napproach for estimation of demand function for local rail tickets considering\nhigh degree of demand heterogeneity by various trip directions and the goals of\ntravel. Employing detailed data on ticket sales for 5 years we estimate the\nparameters of demand function and reveal the significant variation in price\nelasticity of demand. While in average the demand is elastic by price, near a\nquarter of trips is characterized by weakly elastic demand. Lower elasticity of\ndemand is correlated with lower degree of competition with other transport and\ninflexible frequency of travel.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.12859v1"
    },
    {
        "title": "Triple the gamma -- A unifying shrinkage prior for variance and variable\n  selection in sparse state space and TVP models",
        "authors": [
            "Annalisa Cadonna",
            "Sylvia Frühwirth-Schnatter",
            "Peter Knaus"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Time-varying parameter (TVP) models are very flexible in capturing gradual\nchanges in the effect of a predictor on the outcome variable. However, in\nparticular when the number of predictors is large, there is a known risk of\noverfitting and poor predictive performance, since the effect of some\npredictors is constant over time. We propose a prior for variance shrinkage in\nTVP models, called triple gamma. The triple gamma prior encompasses a number of\npriors that have been suggested previously, such as the Bayesian lasso, the\ndouble gamma prior and the Horseshoe prior. We present the desirable properties\nof such a prior and its relationship to Bayesian Model Averaging for variance\nselection. The features of the triple gamma prior are then illustrated in the\ncontext of time varying parameter vector autoregressive models, both for\nsimulated datasets and for a series of macroeconomics variables in the Euro\nArea.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.03100v1"
    },
    {
        "title": "Adaptive Dynamic Model Averaging with an Application to House Price\n  Forecasting",
        "authors": [
            "Alisa Yusupova",
            "Nicos G. Pavlidis",
            "Efthymios G. Pavlidis"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Dynamic model averaging (DMA) combines the forecasts of a large number of\ndynamic linear models (DLMs) to predict the future value of a time series. The\nperformance of DMA critically depends on the appropriate choice of two\nforgetting factors. The first of these controls the speed of adaptation of the\ncoefficient vector of each DLM, while the second enables time variation in the\nmodel averaging stage. In this paper we develop a novel, adaptive dynamic model\naveraging (ADMA) methodology. The proposed methodology employs a stochastic\noptimisation algorithm that sequentially updates the forgetting factor of each\nDLM, and uses a state-of-the-art non-parametric model combination algorithm\nfrom the prediction with expert advice literature, which offers finite-time\nperformance guarantees. An empirical application to quarterly UK house price\ndata suggests that ADMA produces more accurate forecasts than the benchmark\nautoregressive model, as well as competing DMA specifications.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.04661v1"
    },
    {
        "title": "A Regularized Factor-augmented Vector Autoregressive Model",
        "authors": [
            "Maurizio Daniele",
            "Julie Schnaitmann"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a regularized factor-augmented vector autoregressive (FAVAR) model\nthat allows for sparsity in the factor loadings. In this framework, factors may\nonly load on a subset of variables which simplifies the factor identification\nand their economic interpretation. We identify the factors in a data-driven\nmanner without imposing specific relations between the unobserved factors and\nthe underlying time series. Using our approach, the effects of structural\nshocks can be investigated on economically meaningful factors and on all\nobserved time series included in the FAVAR model. We prove consistency for the\nestimators of the factor loadings, the covariance matrix of the idiosyncratic\ncomponent, the factors, as well as the autoregressive parameters in the dynamic\nmodel. In an empirical application, we investigate the effects of a monetary\npolicy shock on a broad range of economically relevant variables. We identify\nthis shock using a joint identification of the factor model and the structural\ninnovations in the VAR model. We find impulse response functions which are in\nline with economic rationale, both on the factor aggregates and observed time\nseries level.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.06049v1"
    },
    {
        "title": "Network Data",
        "authors": [
            "Bryan S. Graham"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Many economic activities are embedded in networks: sets of agents and the\n(often) rivalrous relationships connecting them to one another. Input sourcing\nby firms, interbank lending, scientific research, and job search are four\nexamples, among many, of networked economic activities. Motivated by the\npremise that networks' structures are consequential, this chapter describes\neconometric methods for analyzing them. I emphasize (i) dyadic regression\nanalysis incorporating unobserved agent-specific heterogeneity and supporting\ncausal inference, (ii) techniques for estimating, and conducting inference on,\nsummary network parameters (e.g., the degree distribution or transitivity\nindex); and (iii) empirical models of strategic network formation admitting\ninterdependencies in preferences. Current research challenges and open\nquestions are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.06346v1"
    },
    {
        "title": "Pareto models for risk management",
        "authors": [
            "Arthur Charpentier",
            "Emmanuel Flachaire"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The Pareto model is very popular in risk management, since simple analytical\nformulas can be derived for financial downside risk measures (Value-at-Risk,\nExpected Shortfall) or reinsurance premiums and related quantities (Large Claim\nIndex, Return Period). Nevertheless, in practice, distributions are (strictly)\nPareto only in the tails, above (possible very) large threshold. Therefore, it\ncould be interesting to take into account second order behavior to provide a\nbetter fit. In this article, we present how to go from a strict Pareto model to\nPareto-type distributions. We discuss inference, and derive formulas for\nvarious measures and indices, and finally provide applications on insurance\nlosses and financial risks.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11736v1"
    },
    {
        "title": "Bayesian estimation of large dimensional time varying VARs using copulas",
        "authors": [
            "Mike Tsionas",
            "Marwan Izzeldin",
            "Lorenzo Trapani"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper provides a simple, yet reliable, alternative to the (Bayesian)\nestimation of large multivariate VARs with time variation in the conditional\nmean equations and/or in the covariance structure. With our new methodology,\nthe original multivariate, n dimensional model is treated as a set of n\nunivariate estimation problems, and cross-dependence is handled through the use\nof a copula. Thus, only univariate distribution functions are needed when\nestimating the individual equations, which are often available in closed form,\nand easy to handle with MCMC (or other techniques). Estimation is carried out\nin parallel for the individual equations. Thereafter, the individual posteriors\nare combined with the copula, so obtaining a joint posterior which can be\neasily resampled. We illustrate our approach by applying it to a large\ntime-varying parameter VAR with 25 macroeconomic variables.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.12527v1"
    },
    {
        "title": "Credit Risk: Simple Closed Form Approximate Maximum Likelihood Estimator",
        "authors": [
            "Anand Deo",
            "Sandeep Juneja"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We consider discrete default intensity based and logit type reduced form\nmodels for conditional default probabilities for corporate loans where we\ndevelop simple closed form approximations to the maximum likelihood estimator\n(MLE) when the underlying covariates follow a stationary Gaussian process. In a\npractically reasonable asymptotic regime where the default probabilities are\nsmall, say 1-3% annually, the number of firms and the time period of data\navailable is reasonably large, we rigorously show that the proposed estimator\nbehaves similarly or slightly worse than the MLE when the underlying model is\ncorrectly specified. For more realistic case of model misspecification, both\nestimators are seen to be equally good, or equally bad. Further, beyond a\npoint, both are more-or-less insensitive to increase in data. These conclusions\nare validated on empirical and simulated data. The proposed approximations\nshould also have applications outside finance, where logit-type models are used\nand probabilities of interest are small.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.12611v1"
    },
    {
        "title": "Distributional synthetic controls",
        "authors": [
            "Florian Gunsilius"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This article extends the widely-used synthetic controls estimator for\nevaluating causal effects of policy changes to quantile functions. The proposed\nmethod provides a geometrically faithful estimate of the entire counterfactual\nquantile function of the treated unit. Its appeal stems from an efficient\nimplementation via a constrained quantile-on-quantile regression. This\nconstitutes a novel concept of independent interest. The method provides a\nunique counterfactual quantile function in any scenario: for continuous,\ndiscrete or mixed distributions. It operates in both repeated cross-sections\nand panel data with as little as a single pre-treatment period. The article\nalso provides abstract identification results by showing that any synthetic\ncontrols method, classical or our generalization, provides the correct\ncounterfactual for causal models that preserve distances between the outcome\ndistributions. Working with whole quantile functions instead of aggregate\nvalues allows for tests of equality and stochastic dominance of the\ncounterfactual- and the observed distribution. It can provide causal inference\non standard outcomes like average- or quantile treatment effects, but also more\ngeneral concepts such as counterfactual Lorenz curves or interquartile ranges.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.06118v5"
    },
    {
        "title": "Entropy Balancing for Continuous Treatments",
        "authors": [
            "Stefan Tübbicke"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper introduces entropy balancing for continuous treatments (EBCT) by\nextending the original entropy balancing methodology of Hainm\\\"uller (2012). In\norder to estimate balancing weights, the proposed approach solves a globally\nconvex constrained optimization problem. EBCT weights reliably eradicate\nPearson correlations between covariates and the continuous treatment variable.\nThis is the case even when other methods based on the generalized propensity\nscore tend to yield insufficient balance due to strong selection into different\ntreatment intensities. Moreover, the optimization procedure is more successful\nin avoiding extreme weights attached to a single unit. Extensive Monte-Carlo\nsimulations show that treatment effect estimates using EBCT display similar or\nlower bias and uniformly lower root mean squared error. These properties make\nEBCT an attractive method for the evaluation of continuous treatments.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.06281v2"
    },
    {
        "title": "Oracle Efficient Estimation of Structural Breaks in Cointegrating\n  Regressions",
        "authors": [
            "Karsten Schweikert"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper, we propose an adaptive group lasso procedure to efficiently\nestimate structural breaks in cointegrating regressions. It is well-known that\nthe group lasso estimator is not simultaneously estimation consistent and model\nselection consistent in structural break settings. Hence, we use a first step\ngroup lasso estimation of a diverging number of breakpoint candidates to\nproduce weights for a second adaptive group lasso estimation. We prove that\nparameter changes are estimated consistently by group lasso and show that the\nnumber of estimated breaks is greater than the true number but still\nsufficiently close to it. Then, we use these results and prove that the\nadaptive group lasso has oracle properties if weights are obtained from our\nfirst step estimation. Simulation results show that the proposed estimator\ndelivers the expected results. An economic application to the long-run US money\ndemand function demonstrates the practical importance of this methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.07949v4"
    },
    {
        "title": "Bayesian Panel Quantile Regression for Binary Outcomes with Correlated\n  Random Effects: An Application on Crime Recidivism in Canada",
        "authors": [
            "Georges Bresson",
            "Guy Lacroix",
            "Mohammad Arshad Rahman"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This article develops a Bayesian approach for estimating panel quantile\nregression with binary outcomes in the presence of correlated random effects.\nWe construct a working likelihood using an asymmetric Laplace (AL) error\ndistribution and combine it with suitable prior distributions to obtain the\ncomplete joint posterior distribution. For posterior inference, we propose two\nMarkov chain Monte Carlo (MCMC) algorithms but prefer the algorithm that\nexploits the blocking procedure to produce lower autocorrelation in the MCMC\ndraws. We also explain how to use the MCMC draws to calculate the marginal\neffects, relative risk and odds ratio. The performance of our preferred\nalgorithm is demonstrated in multiple simulation studies and shown to perform\nextremely well. Furthermore, we implement the proposed framework to study crime\nrecidivism in Quebec, a Canadian Province, using a novel data from the\nadministrative correctional files. Our results suggest that the recently\nimplemented \"tough-on-crime\" policy of the Canadian government has been largely\nsuccessful in reducing the probability of repeat offenses in the post-policy\nperiod. Besides, our results support existing findings on crime recidivism and\noffer new insights at various quantiles.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.09295v1"
    },
    {
        "title": "Estimating Marginal Treatment Effects under Unobserved Group\n  Heterogeneity",
        "authors": [
            "Tadao Hoshino",
            "Takahide Yanagi"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper studies treatment effect models in which individuals are\nclassified into unobserved groups based on heterogeneous treatment rules. Using\na finite mixture approach, we propose a marginal treatment effect (MTE)\nframework in which the treatment choice and outcome equations can be\nheterogeneous across groups. Under the availability of instrumental variables\nspecific to each group, we show that the MTE for each group can be separately\nidentified. Based on our identification result, we propose a two-step\nsemiparametric procedure for estimating the group-wise MTE. We illustrate the\nusefulness of the proposed method with an application to economic returns to\ncollege education.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.09560v6"
    },
    {
        "title": "Arctic Amplification of Anthropogenic Forcing: A Vector Autoregressive\n  Analysis",
        "authors": [
            "Philippe Goulet Coulombe",
            "Maximilian Göbel"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  On September 15th 2020, Arctic sea ice extent (SIE) ranked second-to-lowest\nin history and keeps trending downward. The understanding of how feedback loops\namplify the effects of external CO2 forcing is still limited. We propose the\nVARCTIC, which is a Vector Autoregression (VAR) designed to capture and\nextrapolate Arctic feedback loops. VARs are dynamic simultaneous systems of\nequations, routinely estimated to predict and understand the interactions of\nmultiple macroeconomic time series. The VARCTIC is a parsimonious compromise\nbetween full-blown climate models and purely statistical approaches that\nusually offer little explanation of the underlying mechanism. Our completely\nunconditional forecast has SIE hitting 0 in September by the 2060's. Impulse\nresponse functions reveal that anthropogenic CO2 emission shocks have an\nunusually durable effect on SIE -- a property shared by no other shock. We find\nAlbedo- and Thickness-based feedbacks to be the main amplification channels\nthrough which CO2 anomalies impact SIE in the short/medium run. Further,\nconditional forecast analyses reveal that the future path of SIE crucially\ndepends on the evolution of CO2 emissions, with outcomes ranging from\nrecovering SIE to it reaching 0 in the 2050's. Finally, Albedo and Thickness\nfeedbacks are shown to play an important role in accelerating the speed at\nwhich predicted SIE is heading towards 0.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.02535v4"
    },
    {
        "title": "Detecting Latent Communities in Network Formation Models",
        "authors": [
            "Shujie Ma",
            "Liangjun Su",
            "Yichong Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper proposes a logistic undirected network formation model which\nallows for assortative matching on observed individual characteristics and the\npresence of edge-wise fixed effects. We model the coefficients of observed\ncharacteristics to have a latent community structure and the edge-wise fixed\neffects to be of low rank. We propose a multi-step estimation procedure\ninvolving nuclear norm regularization, sample splitting, iterative logistic\nregression and spectral clustering to detect the latent communities. We show\nthat the latent communities can be exactly recovered when the expected degree\nof the network is of order log n or higher, where n is the number of nodes in\nthe network. The finite sample performance of the new estimation and inference\nmethods is illustrated through both simulated and real datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03226v3"
    },
    {
        "title": "Diffusion Copulas: Identification and Estimation",
        "authors": [
            "Ruijun Bu",
            "Kaddour Hadri",
            "Dennis Kristensen"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose a new semiparametric approach for modelling nonlinear univariate\ndiffusions, where the observed process is a nonparametric transformation of an\nunderlying parametric diffusion (UPD). This modelling strategy yields a general\nclass of semiparametric Markov diffusion models with parametric dynamic copulas\nand nonparametric marginal distributions. We provide primitive conditions for\nthe identification of the UPD parameters together with the unknown\ntransformations from discrete samples. Likelihood-based estimators of both\nparametric and nonparametric components are developed and we analyze the\nasymptotic properties of these. Kernel-based drift and diffusion estimators are\nalso proposed and shown to be normally distributed in large samples. A\nsimulation study investigates the finite sample performance of our estimators\nin the context of modelling US short-term interest rates. We also present a\nsimple application of the proposed method for modelling the CBOE volatility\nindex data.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03513v1"
    },
    {
        "title": "Dynamic Shrinkage Priors for Large Time-varying Parameter Regressions\n  using Scalable Markov Chain Monte Carlo Methods",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber",
            "Gary Koop"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Time-varying parameter (TVP) regression models can involve a huge number of\ncoefficients. Careful prior elicitation is required to yield sensible posterior\nand predictive inferences. In addition, the computational demands of Markov\nChain Monte Carlo (MCMC) methods mean their use is limited to the case where\nthe number of predictors is not too large. In light of these two concerns, this\npaper proposes a new dynamic shrinkage prior which reflects the empirical\nregularity that TVPs are typically sparse (i.e. time variation may occur only\nepisodically and only for some of the coefficients). A scalable MCMC algorithm\nis developed which is capable of handling very high dimensional TVP regressions\nor TVP Vector Autoregressions. In an exercise using artificial data we\ndemonstrate the accuracy and computational efficiency of our methods. In an\napplication involving the term structure of interest rates in the eurozone, we\nfind our dynamic shrinkage prior to effectively pick out small amounts of\nparameter change and our methods to forecast well.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03906v2"
    },
    {
        "title": "Critical Values Robust to P-hacking",
        "authors": [
            "Adam McCloskey",
            "Pascal Michaillat"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  P-hacking is prevalent in reality but absent from classical hypothesis\ntesting theory. As a consequence, significant results are much more common than\nthey are supposed to be when the null hypothesis is in fact true. In this\npaper, we build a model of hypothesis testing with p-hacking. From the model,\nwe construct critical values such that, if the values are used to determine\nsignificance, and if scientists' p-hacking behavior adjusts to the new\nsignificance standards, significant results occur with the desired frequency.\nSuch robust critical values allow for p-hacking so they are larger than\nclassical critical values. To illustrate the amount of correction that\np-hacking might require, we calibrate the model using evidence from the medical\nsciences. In the calibrated model the robust critical value for any test\nstatistic is the classical critical value for the same test statistic with one\nfifth of the significance level.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.04141v8"
    },
    {
        "title": "Dynamic shrinkage in time-varying parameter stochastic volatility in\n  mean models",
        "authors": [
            "Florian Huber",
            "Michael Pfarrhofer"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Successful forecasting models strike a balance between parsimony and\nflexibility. This is often achieved by employing suitable shrinkage priors that\npenalize model complexity but also reward model fit. In this note, we modify\nthe stochastic volatility in mean (SVM) model proposed in Chan (2017) by\nintroducing state-of-the-art shrinkage techniques that allow for time-variation\nin the degree of shrinkage. Using a real-time inflation forecast exercise, we\nshow that employing more flexible prior distributions on several key parameters\nslightly improves forecast performance for the United States (US), the United\nKingdom (UK) and the Euro Area (EA). Comparing in-sample results reveals that\nour proposed model yields qualitatively similar insights to the original\nversion of the model.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.06851v1"
    },
    {
        "title": "Instrumental Variables with Treatment-Induced Selection: Exact Bias\n  Results",
        "authors": [
            "Felix Elwert",
            "Elan Segarra"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Instrumental variables (IV) estimation suffers selection bias when the\nanalysis conditions on the treatment. Judea Pearl's early graphical definition\nof instrumental variables explicitly prohibited conditioning on the treatment.\nNonetheless, the practice remains common. In this paper, we derive exact\nanalytic expressions for IV selection bias across a range of data-generating\nmodels, and for various selection-inducing procedures. We present four sets of\nresults for linear models. First, IV selection bias depends on the conditioning\nprocedure (covariate adjustment vs. sample truncation). Second, IV selection\nbias due to covariate adjustment is the limiting case of IV selection bias due\nto sample truncation. Third, in certain models, the IV and OLS estimators under\nselection bound the true causal effect in large samples. Fourth, we\ncharacterize situations where IV remains preferred to OLS despite selection on\nthe treatment. These results broaden the notion of IV selection bias beyond\nsample truncation, replace prior simulation findings with exact analytic\nformulas, and enable formal sensitivity analyses.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09583v1"
    },
    {
        "title": "An alternative to synthetic control for models with many covariates\n  under sparsity",
        "authors": [
            "Marianne Bléhaut",
            "Xavier D'Haultfoeuille",
            "Jérémy L'Hour",
            "Alexandre B. Tsybakov"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The synthetic control method is a an econometric tool to evaluate causal\neffects when only one unit is treated. While initially aimed at evaluating the\neffect of large-scale macroeconomic changes with very few available control\nunits, it has increasingly been used in place of more well-known\nmicroeconometric tools in a broad range of applications, but its properties in\nthis context are unknown. This paper introduces an alternative to the synthetic\ncontrol method, which is developed both in the usual asymptotic framework and\nin the high-dimensional scenario. We propose an estimator of average treatment\neffect that is doubly robust, consistent and asymptotically normal. It is also\nimmunized against first-step selection mistakes. We illustrate these properties\nusing Monte Carlo simulations and applications to both standard and potentially\nhigh-dimensional settings, and offer a comparison with the synthetic control\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.12225v2"
    },
    {
        "title": "Causal Impact of Masks, Policies, Behavior on Early Covid-19 Pandemic in\n  the U.S",
        "authors": [
            "Victor Chernozhukov",
            "Hiroyuki Kasaha",
            "Paul Schrimpf"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper evaluates the dynamic impact of various policies adopted by US\nstates on the growth rates of confirmed Covid-19 cases and deaths as well as\nsocial distancing behavior measured by Google Mobility Reports, where we take\ninto consideration people's voluntarily behavioral response to new information\nof transmission risks. Our analysis finds that both policies and information on\ntransmission risks are important determinants of Covid-19 cases and deaths and\nshows that a change in policies explains a large fraction of observed changes\nin social distancing behavior. Our counterfactual experiments suggest that\nnationally mandating face masks for employees on April 1st could have reduced\nthe growth rate of cases and deaths by more than 10 percentage points in late\nApril, and could have led to as much as 17 to 55 percent less deaths nationally\nby the end of May, which roughly translates into 17 to 55 thousand saved lives.\nOur estimates imply that removing non-essential business closures (while\nmaintaining school closures, restrictions on movie theaters and restaurants)\ncould have led to -20 to 60 percent more cases and deaths by the end of May. We\nalso find that, without stay-at-home orders, cases would have been larger by 25\nto 170 percent, which implies that 0.5 to 3.4 million more Americans could have\nbeen infected if stay-at-home orders had not been implemented. Finally, not\nhaving implemented any policies could have led to at least a 7 fold increase\nwith an uninformative upper bound in cases (and deaths) by the end of May in\nthe US, with considerable uncertainty over the effects of school closures,\nwhich had little cross-sectional variation.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.14168v4"
    },
    {
        "title": "Permutation Tests for Equality of Distributions of Functional Data",
        "authors": [
            "Federico A. Bugni",
            "Joel L. Horowitz"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Economic data are often generated by stochastic processes that take place in\ncontinuous time, though observations may occur only at discrete times. For\nexample, electricity and gas consumption take place in continuous time. Data\ngenerated by a continuous time stochastic process are called functional data.\nThis paper is concerned with comparing two or more stochastic processes that\ngenerate functional data. The data may be produced by a randomized experiment\nin which there are multiple treatments. The paper presents a method for testing\nthe hypothesis that the same stochastic process generates all the functional\ndata. The test described here applies to both functional data and multiple\ntreatments. It is implemented as a combination of two permutation tests. This\nensures that in finite samples, the true and nominal probabilities that each\ntest rejects a correct null hypothesis are equal. The paper presents upper and\nlower bounds on the asymptotic power of the test under alternative hypotheses.\nThe results of Monte Carlo experiments and an application to an experiment on\nbilling and pricing of natural gas illustrate the usefulness of the test.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00798v4"
    },
    {
        "title": "A comment on 'Testing Goodwin: growth cycles in ten OECD countries'",
        "authors": [
            "Matheus R. Grasselli",
            "Aditya Maheshwari"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We revisit the results of Harvie (2000) and show how correcting for a\nreporting mistake in some of the estimated parameter values leads to\nsignificantly different conclusions, including realistic parameter values for\nthe Philips curve and estimated equilibrium employment rates exhibiting on\naverage one tenth of the relative error of those obtained in Harvie (2000).\n",
        "pdf_link": "http://arxiv.org/pdf/1803.01527v1"
    },
    {
        "title": "Testing a Goodwin model with general capital accumulation rate",
        "authors": [
            "Matheus R. Grasselli",
            "Aditya Maheshwari"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We perform econometric tests on a modified Goodwin model where the capital\naccumulation rate is constant but not necessarily equal to one as in the\noriginal model (Goodwin, 1967). In addition to this modification, we find that\naddressing the methodological and reporting issues in Harvie (2000) leads to\nremarkably better results, with near perfect agreement between the estimates of\nequilibrium employment rates and the corresponding empirical averages, as well\nas significantly improved estimates of equilibrium wage shares. Despite its\nsimplicity and obvious limitations, the performance of the modified Goodwin\nmodel implied by our results show that it can be used as a starting point for\nmore sophisticated models for endogenous growth cycles.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.01536v1"
    },
    {
        "title": "Does the time horizon of the return predictive effect of investor\n  sentiment vary with stock characteristics? A Granger causality analysis in\n  the frequency domain",
        "authors": [
            "Yong Jiang",
            "Zhongbao Zhou"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Behavioral theories posit that investor sentiment exhibits predictive power\nfor stock returns, whereas there is little study have investigated the\nrelationship between the time horizon of the predictive effect of investor\nsentiment and the firm characteristics. To this end, by using a Granger\ncausality analysis in the frequency domain proposed by Lemmens et al. (2008),\nthis paper examine whether the time horizon of the predictive effect of\ninvestor sentiment on the U.S. returns of stocks vary with different firm\ncharacteristics (e.g., firm size (Size), book-to-market equity (B/M) rate,\noperating profitability (OP) and investment (Inv)). The empirical results\nindicate that investor sentiment has a long-term (more than 12 months) or\nshort-term (less than 12 months) predictive effect on stock returns with\ndifferent firm characteristics. Specifically, the investor sentiment has strong\npredictability in the stock returns for smaller Size stocks, lower B/M stocks\nand lower OP stocks, both in the short term and long term, but only has a\nshort-term predictability for higher quantile ones. The investor sentiment\nmerely has predictability for the returns of smaller Inv stocks in the short\nterm, but has a strong short-term and long-term predictability for larger Inv\nstocks. These results have important implications for the investors for the\nplanning of the short and the long run stock investment strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.02962v1"
    },
    {
        "title": "Inference on a Distribution from Noisy Draws",
        "authors": [
            "Koen Jochmans",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We consider a situation where the distribution of a random variable is being\nestimated by the empirical distribution of noisy measurements of that variable.\nThis is common practice in, for example, teacher value-added models and other\nfixed-effect models for panel data. We use an asymptotic embedding where the\nnoise shrinks with the sample size to calculate the leading bias in the\nempirical distribution arising from the presence of noise. The leading bias in\nthe empirical quantile function is equally obtained. These calculations are new\nin the literature, where only results on smooth functionals such as the mean\nand variance have been derived. We provide both analytical and jackknife\ncorrections that recenter the limit distribution and yield confidence intervals\nwith correct coverage in large samples. Our approach can be connected to\ncorrections for selection bias and shrinkage estimation and is to be contrasted\nwith deconvolution. Simulation results confirm the much-improved sampling\nbehavior of the corrected estimators. An empirical illustration on\nheterogeneity in deviations from the law of one price is equally provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04991v5"
    },
    {
        "title": "Limitations of P-Values and $R^2$ for Stepwise Regression Building: A\n  Fairness Demonstration in Health Policy Risk Adjustment",
        "authors": [
            "Sherri Rose",
            "Thomas G. McGuire"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Stepwise regression building procedures are commonly used applied statistical\ntools, despite their well-known drawbacks. While many of their limitations have\nbeen widely discussed in the literature, other aspects of the use of individual\nstatistical fit measures, especially in high-dimensional stepwise regression\nsettings, have not. Giving primacy to individual fit, as is done with p-values\nand $R^2$, when group fit may be the larger concern, can lead to misguided\ndecision making. One of the most consequential uses of stepwise regression is\nin health care, where these tools allocate hundreds of billions of dollars to\nhealth plans enrolling individuals with different predicted health care costs.\nThe main goal of this \"risk adjustment\" system is to convey incentives to\nhealth plans such that they provide health care services fairly, a component of\nwhich is not to discriminate in access or care for persons or groups likely to\nbe expensive. We address some specific limitations of p-values and $R^2$ for\nhigh-dimensional stepwise regression in this policy problem through an\nillustrated example by additionally considering a group-level fairness metric.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.05513v2"
    },
    {
        "title": "Does agricultural subsidies foster Italian southern farms? A Spatial\n  Quantile Regression Approach",
        "authors": [
            "Marusca De Castris",
            "Daniele Di Gennaro"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  During the last decades, public policies become a central pillar in\nsupporting and stabilising agricultural sector. In 1962, EU policy-makers\ndeveloped the so-called Common Agricultural Policy (CAP) to ensure\ncompetitiveness and a common market organisation for agricultural products,\nwhile 2003 reform decouple the CAP from the production to focus only on income\nstabilization and the sustainability of agricultural sector. Notwithstanding\nfarmers are highly dependent to public support, literature on the role played\nby the CAP in fostering agricultural performances is still scarce and\nfragmented. Actual CAP policies increases performance differentials between\nNorthern Central EU countries and peripheral regions. This paper aims to\nevaluate the effectiveness of CAP in stimulate performances by focusing on\nItalian lagged Regions. Moreover, agricultural sector is deeply rooted in\nplace-based production processes. In this sense, economic analysis which omit\nthe presence of spatial dependence produce biased estimates of the\nperformances. Therefore, this paper, using data on subsidies and economic\nresults of farms from the RICA dataset which is part of the Farm Accountancy\nData Network (FADN), proposes a spatial Augmented Cobb-Douglas Production\nFunction to evaluate the effects of subsidies on farm's performances. The major\ninnovation in this paper is the implementation of a micro-founded quantile\nversion of a spatial lag model to examine how the impact of the subsidies may\nvary across the conditional distribution of agricultural performances. Results\nshow an increasing shape which switch from negative to positive at the median\nand becomes statistical significant for higher quantiles. Additionally, spatial\nautocorrelation parameter is positive and significant across all the\nconditional distribution, suggesting the presence of significant spatial\nspillovers in agricultural performances.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.05659v1"
    },
    {
        "title": "Are Bitcoin Bubbles Predictable? Combining a Generalized Metcalfe's Law\n  and the LPPLS Model",
        "authors": [
            "Spencer Wheatley",
            "Didier Sornette",
            "Tobias Huber",
            "Max Reppen",
            "Robert N. Gantner"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We develop a strong diagnostic for bubbles and crashes in bitcoin, by\nanalyzing the coincidence (and its absence) of fundamental and technical\nindicators. Using a generalized Metcalfe's law based on network properties, a\nfundamental value is quantified and shown to be heavily exceeded, on at least\nfour occasions, by bubbles that grow and burst. In these bubbles, we detect a\nuniversal super-exponential unsustainable growth. We model this universal\npattern with the Log-Periodic Power Law Singularity (LPPLS) model, which\nparsimoniously captures diverse positive feedback phenomena, such as herding\nand imitation. The LPPLS model is shown to provide an ex-ante warning of market\ninstabilities, quantifying a high crash hazard and probabilistic bracket of the\ncrash time consistent with the actual corrections; although, as always, the\nprecise time and trigger (which straw breaks the camel's back) being exogenous\nand unpredictable. Looking forward, our analysis identifies a substantial but\nnot unprecedented overvaluation in the price of bitcoin, suggesting many months\nof volatile sideways bitcoin prices ahead (from the time of writing, March\n2018).\n",
        "pdf_link": "http://arxiv.org/pdf/1803.05663v1"
    },
    {
        "title": "Evaluating Conditional Cash Transfer Policies with Machine Learning\n  Methods",
        "authors": [
            "Tzai-Shuen Chen"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper presents an out-of-sample prediction comparison between major\nmachine learning models and the structural econometric model. Over the past\ndecade, machine learning has established itself as a powerful tool in many\nprediction applications, but this approach is still not widely adopted in\nempirical economic studies. To evaluate the benefits of this approach, I use\nthe most common machine learning algorithms, CART, C4.5, LASSO, random forest,\nand adaboost, to construct prediction models for a cash transfer experiment\nconducted by the Progresa program in Mexico, and I compare the prediction\nresults with those of a previous structural econometric study. Two prediction\ntasks are performed in this paper: the out-of-sample forecast and the long-term\nwithin-sample simulation. For the out-of-sample forecast, both the mean\nabsolute error and the root mean square error of the school attendance rates\nfound by all machine learning models are smaller than those found by the\nstructural model. Random forest and adaboost have the highest accuracy for the\nindividual outcomes of all subgroups. For the long-term within-sample\nsimulation, the structural model has better performance than do all of the\nmachine learning models. The poor within-sample fitness of the machine learning\nmodel results from the inaccuracy of the income and pregnancy prediction\nmodels. The result shows that the machine learning model performs better than\ndoes the structural model when there are many data to learn; however, when the\ndata are limited, the structural model offers a more sensible prediction. The\nfindings of this paper show promise for adopting machine learning in economic\npolicy analyses in the era of big data.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.06401v1"
    },
    {
        "title": "Network and Panel Quantile Effects Via Distribution Regression",
        "authors": [
            "Victor Chernozhukov",
            "Iván Fernández-Val",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper provides a method to construct simultaneous confidence bands for\nquantile functions and quantile effects in nonlinear network and panel models\nwith unobserved two-way effects, strictly exogenous covariates, and possibly\ndiscrete outcome variables. The method is based upon projection of simultaneous\nconfidence bands for distribution functions constructed from fixed effects\ndistribution regression estimators. These fixed effects estimators are debiased\nto deal with the incidental parameter problem. Under asymptotic sequences where\nboth dimensions of the data set grow at the same rate, the confidence bands for\nthe quantile functions and effects have correct joint coverage in large\nsamples. An empirical application to gravity models of trade illustrates the\napplicability of the methods to network data.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08154v3"
    },
    {
        "title": "Decentralized Pure Exchange Processes on Networks",
        "authors": [
            "Daniele Cassese",
            "Paolo Pin"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We define a class of pure exchange Edgeworth trading processes that under\nminimal assumptions converge to a stable set in the space of allocations, and\ncharacterise the Pareto set of these processes. Choosing a specific process\nbelonging to this class, that we define fair trading, we analyse the trade\ndynamics between agents located on a weighted network. We determine the\nconditions under which there always exists a one-to-one map between the set of\nnetworks and the set of limit points of the dynamics. This result is used to\nunderstand what is the effect of the network topology on the trade dynamics and\non the final allocation. We find that the positions in the network affect the\ndistribution of the utility gains, given the initial allocations\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08836v7"
    },
    {
        "title": "A Perfect Specialization Model for Gravity Equation in Bilateral Trade\n  based on Production Structure",
        "authors": [
            "Majid Einian",
            "Farshad Ranjbar Ravasan"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Although initially originated as a totally empirical relationship to explain\nthe volume of trade between two partners, gravity equation has been the focus\nof several theoretic models that try to explain it. Specialization models are\nof great importance in providing a solid theoretic ground for gravity equation\nin bilateral trade. Some research papers try to improve specialization models\nby adding imperfect specialization to model, but we believe it is unnecessary\ncomplication. We provide a perfect specialization model based on the phenomenon\nwe call tradability, which overcomes the problems with simpler initial. We\nprovide empirical evidence using estimates on panel data of bilateral trade of\n40 countries over 10 years that support the theoretical model. The empirical\nresults have implied that tradability is the only reason for deviations of data\nfrom basic perfect specialization models.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09935v1"
    },
    {
        "title": "Tests for Forecast Instability and Forecast Failure under a Continuous\n  Record Asymptotic Framework",
        "authors": [
            "Alessandro Casini"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We develop a novel continuous-time asymptotic framework for inference on\nwhether the predictive ability of a given forecast model remains stable over\ntime. We formally define forecast instability from the economic forecaster's\nperspective and highlight that the time duration of the instability bears no\nrelationship with stable period. Our approach is applicable in forecasting\nenvironment involving low-frequency as well as high-frequency macroeconomic and\nfinancial variables. As the sampling interval between observations shrinks to\nzero the sequence of forecast losses is approximated by a continuous-time\nstochastic process (i.e., an Ito semimartingale) possessing certain pathwise\nproperties. We build an hypotheses testing problem based on the local\nproperties of the continuous-time limit counterpart of the sequence of losses.\nThe null distribution follows an extreme value distribution. While controlling\nthe statistical size well, our class of test statistics feature uniform power\nover the location of the forecast failure in the sample. The test statistics\nare designed to have power against general form of insatiability and are robust\nto common forms of non-stationarity such as heteroskedasticty and serial\ncorrelation. The gains in power are substantial relative to extant methods,\nespecially when the instability is short-lasting and when occurs toward the\ntail of the sample.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.10883v2"
    },
    {
        "title": "Bi-Demographic Changes and Current Account using SVAR Modeling",
        "authors": [
            "Hassan B. Ghassan",
            "Hassan R. Al-Hajhoj",
            "Faruk Balli"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The paper aims to explore the impacts of bi-demographic structure on the\ncurrent account and growth. Using a SVAR modeling, we track the dynamic impacts\nbetween these underlying variables. New insights have been developed about the\ndynamic interrelation between population growth, current account and economic\ngrowth. The long-run net impact on economic growth of the domestic working\npopulation growth and demand labor for emigrants is positive, due to the\npredominant contribution of skilled emigrant workers. Besides, the positive\nlong-run contribution of emigrant workers to the current account growth largely\ncompensates the negative contribution from the native population, because of\nthe predominance of skilled compared to unskilled workforce. We find that a\npositive shock in demand labor for emigrant workers leads to an increasing\neffect on native active age ratio. Thus, the emigrants appear to be more\ncomplements than substitutes for native workers.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.11161v4"
    },
    {
        "title": "Mortality in a heterogeneous population - Lee-Carter's methodology",
        "authors": [
            "Kamil Jodź"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  The EU Solvency II directive recommends insurance companies to pay more\nattention to the risk management methods. The sense of risk management is the\nability to quantify risk and apply methods that reduce uncertainty. In life\ninsurance, the risk is a consequence of the random variable describing the life\nexpectancy. The article will present a proposal for stochastic mortality\nmodeling based on the Lee and Carter methodology. The maximum likelihood method\nis often used to estimate parameters in mortality models. This method assumes\nthat the population is homogeneous and the number of deaths has the Poisson\ndistribution. The aim of this article is to change assumptions about the\ndistribution of the number of deaths. The results indicate that the model can\nget a better match to historical data, when the number of deaths has a negative\nbinomial distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.11233v1"
    },
    {
        "title": "Fixed Effects and the Generalized Mundlak Estimator",
        "authors": [
            "Dmitry Arkhangelsky",
            "Guido Imbens"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We develop a new approach for estimating average treatment effects in\nobservational studies with unobserved group-level heterogeneity. We consider a\ngeneral model with group-level unconfoundedness and provide conditions under\nwhich aggregate balancing statistics -- group-level averages of functions of\ntreatments and covariates -- are sufficient to eliminate differences between\ngroups. Building on these results, we reinterpret commonly used linear\nfixed-effect regression estimators by writing them in the Mundlak form as\nlinear regression estimators without fixed effects but including group\naverages. We use this representation to develop Generalized Mundlak Estimators\n(GMEs) that capture group differences through group averages of (functions of)\nthe unit-level variables and adjust for these group differences in flexible and\nrobust ways in the spirit of the modern causal literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.02099v9"
    },
    {
        "title": "Minimizing Sensitivity to Model Misspecification",
        "authors": [
            "Stéphane Bonhomme",
            "Martin Weidner"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We propose a framework for estimation and inference when the model may be\nmisspecified. We rely on a local asymptotic approach where the degree of\nmisspecification is indexed by the sample size. We construct estimators whose\nmean squared error is minimax in a neighborhood of the reference model, based\non one-step adjustments. In addition, we provide confidence intervals that\ncontain the true parameter under local misspecification. As a tool to interpret\nthe degree of misspecification, we map it to the local power of a specification\ntest of the reference model. Our approach allows for systematic sensitivity\nanalysis when the parameter of interest may be partially or irregularly\nidentified. As illustrations, we study three applications: an empirical\nanalysis of the impact of conditional cash transfers in Mexico where\nmisspecification stems from the presence of stigma effects of the program, a\ncross-sectional binary choice model where the error distribution is\nmisspecified, and a dynamic panel data binary choice model where the number of\ntime periods is small and the distribution of individual effects is\nmisspecified.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.02161v6"
    },
    {
        "title": "Cross Validation Based Model Selection via Generalized Method of Moments",
        "authors": [
            "Junpei Komiyama",
            "Hajime Shimao"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Structural estimation is an important methodology in empirical economics, and\na large class of structural models are estimated through the generalized method\nof moments (GMM). Traditionally, selection of structural models has been\nperformed based on model fit upon estimation, which take the entire observed\nsamples. In this paper, we propose a model selection procedure based on\ncross-validation (CV), which utilizes sample-splitting technique to avoid\nissues such as over-fitting. While CV is widely used in machine learning\ncommunities, we are the first to prove its consistency in model selection in\nGMM framework. Its empirical property is compared to existing methods by\nsimulations of IV regressions and oligopoly market model. In addition, we\npropose the way to apply our method to Mathematical Programming of Equilibrium\nConstraint (MPEC) approach. Finally, we perform our method to online-retail\nsales data to compare dynamic market model to static model.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.06993v1"
    },
    {
        "title": "Take a Look Around: Using Street View and Satellite Images to Estimate\n  House Prices",
        "authors": [
            "Stephen Law",
            "Brooks Paige",
            "Chris Russell"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  When an individual purchases a home, they simultaneously purchase its\nstructural features, its accessibility to work, and the neighborhood amenities.\nSome amenities, such as air quality, are measurable while others, such as the\nprestige or the visual impression of a neighborhood, are difficult to quantify.\nDespite the well-known impacts intangible housing features have on house\nprices, limited attention has been given to systematically quantifying these\ndifficult to measure amenities. Two issues have led to this neglect. Not only\ndo few quantitative methods exist that can measure the urban environment, but\nthat the collection of such data is both costly and subjective.\n  We show that street image and satellite image data can capture these urban\nqualities and improve the estimation of house prices. We propose a pipeline\nthat uses a deep neural network model to automatically extract visual features\nfrom images to estimate house prices in London, UK. We make use of traditional\nhousing features such as age, size, and accessibility as well as visual\nfeatures from Google Street View images and Bing aerial images in estimating\nthe house price model. We find encouraging results where learning to\ncharacterize the urban quality of a neighborhood improves house price\nprediction, even when generalizing to previously unseen London boroughs.\n  We explore the use of non-linear vs. linear methods to fuse these cues with\nconventional models of house pricing, and show how the interpretability of\nlinear models allows us to directly extract proxy variables for visual\ndesirability of neighborhoods that are both of interest in their own right, and\ncould be used as inputs to other econometric methods. This is particularly\nvaluable as once the network has been trained with the training data, it can be\napplied elsewhere, allowing us to generate vivid dense maps of the visual\nappeal of London streets.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07155v2"
    },
    {
        "title": "Dynamically Optimal Treatment Allocation",
        "authors": [
            "Karun Adusumilli",
            "Friedrich Geiecke",
            "Claudio Schilter"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Dynamic decisions are pivotal to economic policy making. We show how existing\nevidence from randomized control trials can be utilized to guide personalized\ndecisions in challenging dynamic environments with budget and capacity\nconstraints. Recent advances in reinforcement learning now enable the solution\nof many complex, real-world problems for the first time. We allow for\nrestricted classes of policy functions and prove that their regret decays at\nrate n^(-0.5), the same as in the static case. Applying our methods to job\ntraining, we find that by exploiting the problem's dynamic structure, we\nachieve significantly higher welfare compared to static approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.01047v5"
    },
    {
        "title": "Estimation of Cross-Sectional Dependence in Large Panels",
        "authors": [
            "Jiti Gao",
            "Guangming Pan",
            "Yanrong Yang",
            "Bo Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Accurate estimation for extent of cross{sectional dependence in large panel\ndata analysis is paramount to further statistical analysis on the data under\nstudy. Grouping more data with weak relations (cross{sectional dependence)\ntogether often results in less efficient dimension reduction and worse\nforecasting. This paper describes cross-sectional dependence among a large\nnumber of objects (time series) via a factor model and parameterizes its extent\nin terms of strength of factor loadings. A new joint estimation method,\nbenefiting from unique feature of dimension reduction for high dimensional time\nseries, is proposed for the parameter representing the extent and some other\nparameters involved in the estimation procedure. Moreover, a joint asymptotic\ndistribution for a pair of estimators is established. Simulations illustrate\nthe effectiveness of the proposed estimation method in the finite sample\nperformance. Applications in cross-country macro-variables and stock returns\nfrom S&P 500 are studied.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.06843v1"
    },
    {
        "title": "Nonparametric Estimation and Inference in Economic and Psychological\n  Experiments",
        "authors": [
            "Raffaello Seri",
            "Samuele Centorrino",
            "Michele Bernasconi"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The goal of this paper is to provide some tools for nonparametric estimation\nand inference in psychological and economic experiments. We consider an\nexperimental framework in which each of $n$subjects provides $T$ responses to a\nvector of $T$ stimuli. We propose to estimate the unknown function $f$ linking\nstimuli to responses through a nonparametric sieve estimator. We give\nconditions for consistency when either $n$ or $T$ or both diverge. The rate of\nconvergence depends upon the error covariance structure, that is allowed to\ndiffer across subjects. With these results we derive the optimal divergence\nrate of the dimension of the sieve basis with both $n$ and $T$. We provide\nguidance about the optimal balance between the number of subjects and questions\nin a laboratory experiment and argue that a large $n$is often better than a\nlarge $T$. We derive conditions for asymptotic normality of functionals of the\nestimator of $T$ and apply them to obtain the asymptotic distribution of the\nWald test when the number of constraints under the null is finite and when it\ndiverges along with other asymptotic parameters. Lastly, we investigate the\nprevious properties when the conditional covariance matrix is replaced by an\nestimator.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.11156v3"
    },
    {
        "title": "A Factor-Augmented Markov Switching (FAMS) Model",
        "authors": [
            "Gregor Zens",
            "Maximilian Böck"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper investigates the role of high-dimensional information sets in the\ncontext of Markov switching models with time varying transition probabilities.\nMarkov switching models are commonly employed in empirical macroeconomic\nresearch and policy work. However, the information used to model the switching\nprocess is usually limited drastically to ensure stability of the model.\nIncreasing the number of included variables to enlarge the information set\nmight even result in decreasing precision of the model. Moreover, it is often\nnot clear a priori which variables are actually relevant when it comes to\ninforming the switching behavior. Building strongly on recent contributions in\nthe field of factor analysis, we introduce a general type of Markov switching\nautoregressive models for non-linear time series analysis. Large numbers of\ntime series are allowed to inform the switching process through a factor\nstructure. This factor-augmented Markov switching (FAMS) model overcomes\nestimation issues that are likely to arise in previous assessments of the\nmodeling framework. More accurate estimates of the switching behavior as well\nas improved model fit result. The performance of the FAMS model is illustrated\nin a simulated data example as well as in an US business cycle application.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.13194v2"
    },
    {
        "title": "Uncertainty in the Hot Hand Fallacy: Detecting Streaky Alternatives to\n  Random Bernoulli Sequences",
        "authors": [
            "David M. Ritzwoller",
            "Joseph P. Romano"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We study a class of permutation tests of the randomness of a collection of\nBernoulli sequences and their application to analyses of the human tendency to\nperceive streaks of consecutive successes as overly representative of positive\ndependence - the hot hand fallacy. In particular, we study permutation tests of\nthe null hypothesis of randomness (i.e., that trials are i.i.d.) based on test\nstatistics that compare the proportion of successes that directly follow k\nconsecutive successes with either the overall proportion of successes or the\nproportion of successes that directly follow k consecutive failures. We\ncharacterize the asymptotic distributions of these test statistics and their\npermutation distributions under randomness, under a set of general stationary\nprocesses, and under a class of Markov chain alternatives, which allow us to\nderive their local asymptotic power. The results are applied to evaluate the\nempirical support for the hot hand fallacy provided by four controlled\nbasketball shooting experiments. We establish that substantially larger data\nsets are required to derive an informative measurement of the deviation from\nrandomness in basketball shooting. In one experiment, for which we were able to\nobtain data, multiple testing procedures reveal that one shooter exhibits a\nshooting pattern significantly inconsistent with randomness - supplying strong\nevidence that basketball shooting is not random for all shooters all of the\ntime. However, we find that the evidence against randomness in this experiment\nis limited to this shooter. Our results provide a mathematical and statistical\nfoundation for the design and validation of experiments that directly compare\ndeviations from randomness with human beliefs about deviations from randomness,\nand thereby constitute a direct test of the hot hand fallacy.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.01406v6"
    },
    {
        "title": "Efficient Estimation by Fully Modified GLS with an Application to the\n  Environmental Kuznets Curve",
        "authors": [
            "Yicong Lin",
            "Hanno Reuvers"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper develops the asymptotic theory of a Fully Modified Generalized\nLeast Squares estimator for multivariate cointegrating polynomial regressions.\nSuch regressions allow for deterministic trends, stochastic trends and integer\npowers of stochastic trends to enter the cointegrating relations. Our fully\nmodified estimator incorporates: (1) the direct estimation of the inverse\nautocovariance matrix of the multidimensional errors, and (2) second order bias\ncorrections. The resulting estimator has the intuitive interpretation of\napplying a weighted least squares objective function to filtered data series.\nMoreover, the required second order bias corrections are convenient byproducts\nof our approach and lead to standard asymptotic inference. We also study\nseveral multivariate KPSS-type of tests for the null of cointegration. A\ncomprehensive simulation study shows good performance of the FM-GLS estimator\nand the related tests. As a practical illustration, we reinvestigate the\nEnvironmental Kuznets Curve (EKC) hypothesis for six early industrialized\ncountries as in Wagner et al. (2020).\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02552v2"
    },
    {
        "title": "Zero Black-Derman-Toy interest rate model",
        "authors": [
            "Grzegorz Krzyżanowski",
            "Ernesto Mordecki",
            "Andrés Sosa"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a modification of the classical Black-Derman-Toy (BDT) interest\nrate tree model, which includes the possibility of a jump with small\nprobability at each step to a practically zero interest rate. The corresponding\nBDT algorithms are consequently modified to calibrate the tree containing the\nzero interest rate scenarios. This modification is motivated by the recent\n2008-2009 crisis in the United States and it quantifies the risk of a future\ncrises in bond prices and derivatives. The proposed model is useful to price\nderivatives. This exercise also provides a tool to calibrate the probability of\nthis event. A comparison of option prices and implied volatilities on US\nTreasury bonds computed with both the proposed and the classical tree model is\nprovided, in six different scenarios along the different periods comprising the\nyears 2002-2017.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.04401v2"
    },
    {
        "title": "Testing the Drift-Diffusion Model",
        "authors": [
            "Drew Fudenberg",
            "Whitney K. Newey",
            "Philipp Strack",
            "Tomasz Strzalecki"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The drift diffusion model (DDM) is a model of sequential sampling with\ndiffusion (Brownian) signals, where the decision maker accumulates evidence\nuntil the process hits a stopping boundary, and then stops and chooses the\nalternative that corresponds to that boundary. This model has been widely used\nin psychology, neuroeconomics, and neuroscience to explain the observed\npatterns of choice and response times in a range of binary choice decision\nproblems. This paper provides a statistical test for DDM's with general\nboundaries. We first prove a characterization theorem: we find a condition on\nchoice probabilities that is satisfied if and only if the choice probabilities\nare generated by some DDM. Moreover, we show that the drift and the boundary\nare uniquely identified. We then use our condition to nonparametrically\nestimate the drift and the boundary and construct a test statistic.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.05824v1"
    },
    {
        "title": "Measuring international uncertainty using global vector autoregressions\n  with drifting parameters",
        "authors": [
            "Michael Pfarrhofer"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper investigates the time-varying impacts of international\nmacroeconomic uncertainty shocks. We use a global vector autoregressive\nspecification with drifting coefficients and factor stochastic volatility in\nthe errors to model six economies jointly. The measure of uncertainty is\nconstructed endogenously by estimating a scalar driving the innovation\nvariances of the latent factors, which is also included in the mean of the\nprocess. To achieve regularization, we use Bayesian techniques for estimation,\nand introduce a set of hierarchical global-local priors. The adopted priors\ncenter the model on a constant parameter specification with homoscedastic\nerrors, but allow for time-variation if suggested by likelihood information.\nMoreover, we assume coefficients across economies to be similar, but provide\nsufficient flexibility via the hierarchical prior for country-specific\nidiosyncrasies. The results point towards pronounced real and financial effects\nof uncertainty shocks in all countries, with differences across economies and\nover time.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.06325v2"
    },
    {
        "title": "Dyadic Regression",
        "authors": [
            "Bryan S. Graham"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Dyadic data, where outcomes reflecting pairwise interaction among sampled\nunits are of primary interest, arise frequently in social science research.\nRegression analyses with such data feature prominently in many research\nliteratures (e.g., gravity models of trade). The dependence structure\nassociated with dyadic data raises special estimation and, especially,\ninference issues. This chapter reviews currently available methods for\n(parametric) dyadic regression analysis and presents guidelines for empirical\nresearchers.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.09029v1"
    },
    {
        "title": "The Ridge Path Estimator for Linear Instrumental Variables",
        "authors": [
            "Nandana Sengupta",
            "Fallaw Sowell"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper presents the asymptotic behavior of a linear instrumental\nvariables (IV) estimator that uses a ridge regression penalty. The\nregularization tuning parameter is selected empirically by splitting the\nobserved data into training and test samples. Conditional on the tuning\nparameter, the training sample creates a path from the IV estimator to a prior.\nThe optimal tuning parameter is the value along this path that minimizes the IV\nobjective function for the test sample.\n  The empirically selected regularization tuning parameter becomes an estimated\nparameter that jointly converges with the parameters of interest. The\nasymptotic distribution of the tuning parameter is a nonstandard mixture\ndistribution. Monte Carlo simulations show the asymptotic distribution captures\nthe characteristics of the sampling distributions and when this ridge estimator\nperforms better than two-stage least squares.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.09237v1"
    },
    {
        "title": "Infinitely Stochastic Micro Forecasting",
        "authors": [
            "Matúš Maciak",
            "Ostap Okhrin",
            "Michal Pešta"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Forecasting costs is now a front burner in empirical economics. We propose an\nunconventional tool for stochastic prediction of future expenses based on the\nindividual (micro) developments of recorded events. Consider a firm,\nenterprise, institution, or state, which possesses knowledge about particular\nhistorical events. For each event, there is a series of several related\nsubevents: payments or losses spread over time, which all leads to an\ninfinitely stochastic process at the end. Nevertheless, the issue is that some\nalready occurred events do not have to be necessarily reported. The aim lies in\nforecasting future subevent flows coming from already reported, occurred but\nnot reported, and yet not occurred events. Our methodology is illustrated on\nquantitative risk assessment, however, it can be applied to other areas such as\nstartups, epidemics, war damages, advertising and commercials, digital\npayments, or drug prescription as manifested in the paper. As a theoretical\ncontribution, inference for infinitely stochastic processes is developed. In\nparticular, a non-homogeneous Poisson process with non-homogeneous Poisson\nprocesses as marks is used, which includes for instance the Cox process as a\nspecial case.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.10636v2"
    },
    {
        "title": "The economics of minority language use: theory and empirical evidence\n  for a language game model",
        "authors": [
            "Stefan Sperlich",
            "Jose-Ramon Uriarte"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Language and cultural diversity is a fundamental aspect of the present world.\nWe study three modern multilingual societies -- the Basque Country, Ireland and\nWales -- which are endowed with two, linguistically distant, official\nlanguages: $A$, spoken by all individuals, and $B$, spoken by a bilingual\nminority. In the three cases it is observed a decay in the use of minoritarian\n$B$, a sign of diversity loss. However, for the \"Council of Europe\" the key\nfactor to avoid the shift of $B$ is its use in all domains. Thus, we\ninvestigate the language choices of the bilinguals by means of an evolutionary\ngame theoretic model. We show that the language population dynamics has reached\nan evolutionary stable equilibrium where a fraction of bilinguals have shifted\nto speak $A$. Thus, this equilibrium captures the decline in the use of $B$. To\ntest the theory we build empirical models that predict the use of $B$ for each\nproportion of bilinguals. We show that model-based predictions fit very well\nthe observed use of Basque, Irish, and Welsh.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.11604v1"
    },
    {
        "title": "SortedEffects: Sorted Causal Effects in R",
        "authors": [
            "Shuowen Chen",
            "Victor Chernozhukov",
            "Iván Fernández-Val",
            "Ye Luo"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Chernozhukov et al. (2018) proposed the sorted effect method for nonlinear\nregression models. This method consists of reporting percentiles of the partial\neffects in addition to the average commonly used to summarize the heterogeneity\nin the partial effects. They also proposed to use the sorted effects to carry\nout classification analysis where the observational units are classified as\nmost and least affected if their causal effects are above or below some tail\nsorted effects. The R package SortedEffects implements the estimation and\ninference methods therein and provides tools to visualize the results. This\nvignette serves as an introduction to the package and displays basic\nfunctionality of the functions within.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.00836v3"
    },
    {
        "title": "Using Wasserstein Generative Adversarial Networks for the Design of\n  Monte Carlo Simulations",
        "authors": [
            "Susan Athey",
            "Guido Imbens",
            "Jonas Metzger",
            "Evan Munro"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  When researchers develop new econometric methods it is common practice to\ncompare the performance of the new methods to those of existing methods in\nMonte Carlo studies. The credibility of such Monte Carlo studies is often\nlimited because of the freedom the researcher has in choosing the design. In\nrecent years a new class of generative models emerged in the machine learning\nliterature, termed Generative Adversarial Networks (GANs) that can be used to\nsystematically generate artificial data that closely mimics real economic\ndatasets, while limiting the degrees of freedom for the researcher and\noptionally satisfying privacy guarantees with respect to their training data.\nIn addition if an applied researcher is concerned with the performance of a\nparticular statistical method on a specific data set (beyond its theoretical\nproperties in large samples), she may wish to assess the performance, e.g., the\ncoverage rate of confidence intervals or the bias of the estimator, using\nsimulated data which resembles her setting. Tol illustrate these methods we\napply Wasserstein GANs (WGANs) to compare a number of different estimators for\naverage treatment effects under unconfoundedness in three distinct settings\n(corresponding to three real data sets) and present a methodology for assessing\nthe robustness of the results. In this example, we find that (i) there is not\none estimator that outperforms the others in all three settings, so researchers\nshould tailor their analytic approach to a given setting, and (ii) systematic\nsimulation studies can be helpful for selecting among competing methods in this\nsituation.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.02210v3"
    },
    {
        "title": "Dynamics of reallocation within India's income distribution",
        "authors": [
            "Anand Sahasranaman",
            "Henrik Jeldtoft Jensen"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We investigate the nature and extent of reallocation occurring within the\nIndian income distribution, with a particular focus on the dynamics of the\nbottom of the distribution. Specifically, we use a stochastic model of\nGeometric Brownian Motion with a reallocation parameter that was constructed to\ncapture the quantum and direction of composite redistribution implied in the\nincome distribution. It is well known that inequality has been rising in India\nin the recent past, but the assumption has been that while the rich benefit\nmore than proportionally from economic growth, the poor are also better off\nthan before. Findings from our model refute this, as we find that since the\nearly 2000s reallocation has consistently been negative, and that the Indian\nincome distribution has entered a regime of perverse redistribution of\nresources from the poor to the rich. Outcomes from the model indicate not only\nthat income shares of the bottom decile (~1%) and bottom percentile (~0.03%)\nare at historic lows, but also that real incomes of the bottom decile (-2.5%)\nand percentile (-6%) have declined in the 2000s. We validate these findings\nusing income distribution data and find support for our contention of\npersistent negative reallocation in the 2000s. We characterize these findings\nin the context of increasing informalization of the workforce in the formal\nmanufacturing and service sectors, as well as the growing economic insecurity\nof the agricultural workforce in India. Significant structural changes will be\nrequired to address this phenomenon.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.04452v4"
    },
    {
        "title": "Estimation and Applications of Quantile Regression for Binary\n  Longitudinal Data",
        "authors": [
            "Mohammad Arshad Rahman",
            "Angela Vossmeyer"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper develops a framework for quantile regression in binary\nlongitudinal data settings. A novel Markov chain Monte Carlo (MCMC) method is\ndesigned to fit the model and its computational efficiency is demonstrated in a\nsimulation study. The proposed approach is flexible in that it can account for\ncommon and individual-specific parameters, as well as multivariate\nheterogeneity associated with several covariates. The methodology is applied to\nstudy female labor force participation and home ownership in the United States.\nThe results offer new insights at the various quantiles, which are of interest\nto policymakers and researchers alike.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.05560v1"
    },
    {
        "title": "Statistical inference for statistical decisions",
        "authors": [
            "Charles F. Manski"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The Wald development of statistical decision theory addresses decision making\nwith sample data. Wald's concept of a statistical decision function (SDF)\nembraces all mappings of the form [data -> decision]. An SDF need not perform\nstatistical inference; that is, it need not use data to draw conclusions about\nthe true state of nature. Inference-based SDFs have the sequential form [data\n-> inference -> decision]. This paper motivates inference-based SDFs as\npractical procedures for decision making that may accomplish some of what Wald\nenvisioned. The paper first addresses binary choice problems, where all SDFs\nmay be viewed as hypothesis tests. It next considers as-if optimization, which\nuses a point estimate of the true state as if the estimate were accurate. It\nthen extends this idea to as-if maximin and minimax-regret decisions, which use\npoint estimates of some features of the true state as if they were accurate.\nThe paper primarily uses finite-sample maximum regret to evaluate the\nperformance of inference-based SDFs. To illustrate abstract ideas, it presents\nspecific findings concerning treatment choice and point prediction with sample\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.06853v1"
    },
    {
        "title": "Distributional conformal prediction",
        "authors": [
            "Victor Chernozhukov",
            "Kaspar Wüthrich",
            "Yinchu Zhu"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a robust method for constructing conditionally valid prediction\nintervals based on models for conditional distributions such as quantile and\ndistribution regression. Our approach can be applied to important prediction\nproblems including cross-sectional prediction, k-step-ahead forecasts,\nsynthetic controls and counterfactual prediction, and individual treatment\neffects prediction. Our method exploits the probability integral transform and\nrelies on permuting estimated ranks. Unlike regression residuals, ranks are\nindependent of the predictors, allowing us to construct conditionally valid\nprediction intervals under heteroskedasticity. We establish approximate\nconditional validity under consistent estimation and provide approximate\nunconditional validity under model misspecification, overfitting, and with time\nseries data. We also propose a simple \"shape\" adjustment of our baseline method\nthat yields optimal prediction intervals.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.07889v3"
    },
    {
        "title": "Discerning Solution Concepts",
        "authors": [
            "Nail Kashaev",
            "Bruno Salcedo"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The empirical analysis of discrete complete-information games has relied on\nbehavioral restrictions in the form of solution concepts, such as Nash\nequilibrium. Choosing the right solution concept is crucial not just for\nidentification of payoff parameters, but also for the validity and\ninformativeness of counterfactual exercises and policy implications. We say\nthat a solution concept is discernible if it is possible to determine whether\nit generated the observed data on the players' behavior and covariates. We\npropose a set of conditions that make it possible to discern solution concepts.\nIn particular, our conditions are sufficient to tell whether the players'\nchoices emerged from Nash equilibria. We can also discern between\nrationalizable behavior, maxmin behavior, and collusive behavior. Finally, we\nidentify the correlation structure of unobserved shocks in our model using a\nnovel approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.09320v1"
    },
    {
        "title": "A Peek into the Unobservable: Hidden States and Bayesian Inference for\n  the Bitcoin and Ether Price Series",
        "authors": [
            "Constandina Koki",
            "Stefanos Leonardos",
            "Georgios Piliouras"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Conventional financial models fail to explain the economic and monetary\nproperties of cryptocurrencies due to the latter's dual nature: their usage as\nfinancial assets on the one side and their tight connection to the underlying\nblockchain structure on the other. In an effort to examine both components via\na unified approach, we apply a recently developed Non-Homogeneous Hidden Markov\n(NHHM) model with an extended set of financial and blockchain specific\ncovariates on the Bitcoin (BTC) and Ether (ETH) price data. Based on the\nobservable series, the NHHM model offers a novel perspective on the underlying\nmicrostructure of the cryptocurrency market and provides insight on\nunobservable parameters such as the behavior of investors, traders and miners.\nThe algorithm identifies two alternating periods (hidden states) of inherently\ndifferent activity -- fundamental versus uninformed or noise traders -- in the\nBitcoin ecosystem and unveils differences in both the short/long run dynamics\nand in the financial characteristics of the two states, such as significant\nexplanatory variables, extreme events and varying series autocorrelation. In a\nsomewhat unexpected result, the Bitcoin and Ether markets are found to be\ninfluenced by markedly distinct indicators despite their perceived correlation.\nThe current approach backs earlier findings that cryptocurrencies are unlike\nany conventional financial asset and makes a first step towards understanding\ncryptocurrency markets via a more comprehensive lens.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.10957v2"
    },
    {
        "title": "Monotonicity-Constrained Nonparametric Estimation and Inference for\n  First-Price Auctions",
        "authors": [
            "Jun Ma",
            "Vadim Marmer",
            "Artyom Shneyerov",
            "Pai Xu"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a new nonparametric estimator for first-price auctions with\nindependent private values that imposes the monotonicity constraint on the\nestimated inverse bidding strategy. We show that our estimator has a smaller\nasymptotic variance than that of Guerre, Perrigne and Vuong's (2000) estimator.\nIn addition to establishing pointwise asymptotic normality of our estimator, we\nprovide a bootstrap-based approach to constructing uniform confidence bands for\nthe density function of latent valuations.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.12974v1"
    },
    {
        "title": "Group Average Treatment Effects for Observational Studies",
        "authors": [
            "Daniel Jacob"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The paper proposes an estimator to make inference of heterogeneous treatment\neffects sorted by impact groups (GATES) for non-randomised experiments. The\ngroups can be understood as a broader aggregation of the conditional average\ntreatment effect (CATE) where the number of groups is set in advance. In\neconomics, this approach is similar to pre-analysis plans. Observational\nstudies are standard in policy evaluation from labour markets, educational\nsurveys and other empirical studies. To control for a potential selection-bias,\nwe implement a doubly-robust estimator in the first stage. We use machine\nlearning methods to learn the conditional mean functions as well as the\npropensity score. The group average treatment effect is then estimated via a\nlinear projection model. The linear model is easy to interpret, provides\np-values and confidence intervals, and limits the danger of finding spurious\nheterogeneity due to small subgroups in the CATE. To control for confounding in\nthe linear model, we use Neyman-orthogonal moments to partial out the effect\nthat covariates have on both, the treatment assignment and the outcome. The\nresult is a best linear predictor for effect heterogeneity based on impact\ngroups. We find that our proposed method has lower absolute errors as well as\nsmaller bias than the benchmark doubly-robust estimator. We further introduce a\nbagging type averaging for the CATE function for each observation to avoid\nbiases through sample splitting. The advantage of the proposed method is a\nrobust linear estimation of heterogeneous group treatment effects in\nobservational studies.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.02688v5"
    },
    {
        "title": "Randomization tests of copula symmetry",
        "authors": [
            "Brendan K. Beare",
            "Juwon Seo"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  New nonparametric tests of copula exchangeability and radial symmetry are\nproposed. The novel aspect of the tests is a resampling procedure that exploits\ngroup invariance conditions associated with the relevant symmetry hypothesis.\nThey may be viewed as feasible versions of randomization tests of symmetry, the\nlatter being inapplicable due to the unobservability of margins. Our tests are\nsimple to compute, control size asymptotically, consistently detect arbitrary\nforms of asymmetry, and do not require the specification of a tuning parameter.\nSimulations indicate excellent small sample properties compared to existing\nprocedures involving the multiplier bootstrap.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.05307v1"
    },
    {
        "title": "Semiparametric Estimation of Correlated Random Coefficient Models\n  without Instrumental Variables",
        "authors": [
            "Samuele Centorrino",
            "Aman Ullah",
            "Jing Xue"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We study a linear random coefficient model where slope parameters may be\ncorrelated with some continuous covariates. Such a model specification may\noccur in empirical research, for instance, when quantifying the effect of a\ncontinuous treatment observed at two time periods. We show one can carry\nidentification and estimation without instruments. We propose a semiparametric\nestimator of average partial effects and of average treatment effects on the\ntreated. We showcase the small sample properties of our estimator in an\nextensive simulation study. Among other things, we reveal that it compares\nfavorably with a control function estimator. We conclude with an application to\nthe effect of malaria eradication on economic development in Colombia.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.06857v1"
    },
    {
        "title": "Causal Inference Under Approximate Neighborhood Interference",
        "authors": [
            "Michael P. Leung"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper studies causal inference in randomized experiments under network\ninterference. Commonly used models of interference posit that treatments\nassigned to alters beyond a certain network distance from the ego have no\neffect on the ego's response. However, this assumption is violated in common\nmodels of social interactions. We propose a substantially weaker model of\n\"approximate neighborhood interference\" (ANI) under which treatments assigned\nto alters further from the ego have a smaller, but potentially nonzero, effect\non the ego's response. We formally verify that ANI holds for well-known models\nof social interactions. Under ANI, restrictions on the network topology, and\nasymptotics under which the network size increases, we prove that standard\ninverse-probability weighting estimators consistently estimate useful exposure\neffects and are approximately normal. For inference, we consider a network HAC\nvariance estimator. Under a finite population model, we show that the estimator\nis biased but that the bias can be interpreted as the variance of unit-level\nexposure effects. This generalizes Neyman's well-known result on conservative\nvariance estimation to settings with interference.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.07085v4"
    },
    {
        "title": "Inference in Models of Discrete Choice with Social Interactions Using\n  Network Data",
        "authors": [
            "Michael P. Leung"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper studies inference in models of discrete choice with social\ninteractions when the data consists of a single large network. We provide\ntheoretical justification for the use of spatial and network HAC variance\nestimators in applied work, the latter constructed by using network path\ndistance in place of spatial distance. Toward this end, we prove new central\nlimit theorems for network moments in a large class of social interactions\nmodels. The results are applicable to discrete games on networks and dynamic\nmodels where social interactions enter through lagged dependent variables. We\nillustrate our results in an empirical application and simulation study.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.07106v1"
    },
    {
        "title": "Statistical Inference on Partially Linear Panel Model under Unobserved\n  Linearity",
        "authors": [
            "Ruiqi Liu",
            "Ben Boukai",
            "Zuofeng Shang"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  A new statistical procedure, based on a modified spline basis, is proposed to\nidentify the linear components in the panel data model with fixed effects.\nUnder some mild assumptions, the proposed procedure is shown to consistently\nestimate the underlying regression function, correctly select the linear\ncomponents, and effectively conduct the statistical inference. When compared to\nexisting methods for detection of linearity in the panel model, our approach is\ndemonstrated to be theoretically justified as well as practically convenient.\nWe provide a computational algorithm that implements the proposed procedure\nalong with a path-based solution method for linearity detection, which avoids\nthe burden of selecting the tuning parameter for the penalty term. Monte Carlo\nsimulations are conducted to examine the finite sample performance of our\nproposed procedure with detailed findings that confirm our theoretical results\nin the paper. Applications to Aggregate Production and Environmental Kuznets\nCurve data also illustrate the necessity for detecting linearity in the\npartially linear panel model.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.08830v1"
    },
    {
        "title": "A Scrambled Method of Moments",
        "authors": [
            "Jean-Jacques Forneron"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Quasi-Monte Carlo (qMC) methods are a powerful alternative to classical\nMonte-Carlo (MC) integration. Under certain conditions, they can approximate\nthe desired integral at a faster rate than the usual Central Limit Theorem,\nresulting in more accurate estimates. This paper explores these methods in a\nsimulation-based estimation setting with an emphasis on the scramble of Owen\n(1995). For cross-sections and short-panels, the resulting Scrambled Method of\nMoments simply replaces the random number generator with the scramble\n(available in most softwares) to reduce simulation noise. Scrambled Indirect\nInference estimation is also considered. For time series, qMC may not apply\ndirectly because of a curse of dimensionality on the time dimension. A simple\nalgorithm and a class of moments which circumvent this issue are described.\nAsymptotic results are given for each algorithm. Monte-Carlo examples\nillustrate these results in finite samples, including an income process with\n\"lots of heterogeneity.\"\n",
        "pdf_link": "http://arxiv.org/pdf/1911.09128v1"
    },
    {
        "title": "Hybrid quantile estimation for asymmetric power GARCH models",
        "authors": [
            "Guochang Wang",
            "Ke Zhu",
            "Guodong Li",
            "Wai Keung Li"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Asymmetric power GARCH models have been widely used to study the higher order\nmoments of financial returns, while their quantile estimation has been rarely\ninvestigated. This paper introduces a simple monotonic transformation on its\nconditional quantile function to make the quantile regression tractable. The\nasymptotic normality of the resulting quantile estimators is established under\neither stationarity or non-stationarity. Moreover, based on the estimation\nprocedure, new tests for strict stationarity and asymmetry are also\nconstructed. This is the first try of the quantile estimation for\nnon-stationary ARCH-type models in the literature. The usefulness of the\nproposed methodology is illustrated by simulation results and real data\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.09343v1"
    },
    {
        "title": "An Integrated Early Warning System for Stock Market Turbulence",
        "authors": [
            "Peiwan Wang",
            "Lu Zong",
            "Ye Ma"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This study constructs an integrated early warning system (EWS) that\nidentifies and predicts stock market turbulence. Based on switching ARCH\n(SWARCH) filtering probabilities of the high volatility regime, the proposed\nEWS first classifies stock market crises according to an indicator function\nwith thresholds dynamically selected by the two-peak method. A hybrid algorithm\nis then developed in the framework of a long short-term memory (LSTM) network\nto make daily predictions that alert turmoils. In the empirical evaluation\nbased on ten-year Chinese stock data, the proposed EWS yields satisfying\nresults with the test-set accuracy of $96.6\\%$ and on average $2.4$ days of the\nforewarned period. The model's stability and practical value in real-time\ndecision-making are also proven by the cross-validation and back-testing.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.12596v1"
    },
    {
        "title": "Estimates of derivatives of (log) densities and related objects",
        "authors": [
            "Joris Pinkse",
            "Karl Schurter"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We estimate the density and its derivatives using a local polynomial\napproximation to the logarithm of an unknown density $f$. The estimator is\nguaranteed to be nonnegative and achieves the same optimal rate of convergence\nin the interior as well as the boundary of the support of $f$. The estimator is\ntherefore well-suited to applications in which nonnegative density estimates\nare required, such as in semiparametric maximum likelihood estimation. In\naddition, we show that our estimator compares favorably with other kernel-based\nmethods, both in terms of asymptotic performance and computational ease.\nSimulation results confirm that our method can perform similarly in finite\nsamples to these alternative methods when they are used with optimal inputs,\ni.e. an Epanechnikov kernel and optimally chosen bandwidth sequence. Further\nsimulation evidence demonstrates that, if the researcher modifies the inputs\nand chooses a larger bandwidth, our approach can even improve upon these\noptimized alternatives, asymptotically. We provide code in several languages.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.01328v1"
    },
    {
        "title": "Ensemble Learning with Statistical and Structural Models",
        "authors": [
            "Jiaming Mao",
            "Jingzhi Xu"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Statistical and structural modeling represent two distinct approaches to data\nanalysis. In this paper, we propose a set of novel methods for combining\nstatistical and structural models for improved prediction and causal inference.\nOur first proposed estimator has the doubly robustness property in that it only\nrequires the correct specification of either the statistical or the structural\nmodel. Our second proposed estimator is a weighted ensemble that has the\nability to outperform both models when they are both misspecified. Experiments\ndemonstrate the potential of our estimators in various settings, including\nfist-price auctions, dynamic models of entry and exit, and demand estimation\nwith instrumental variables.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.05308v1"
    },
    {
        "title": "Horseshoe Prior Bayesian Quantile Regression",
        "authors": [
            "David Kohns",
            "Tibor Szendrei"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper extends the horseshoe prior of Carvalho et al. (2010) to Bayesian\nquantile regression (HS-BQR) and provides a fast sampling algorithm for\ncomputation in high dimensions. The performance of the proposed HS-BQR is\nevaluated on Monte Carlo simulations and a high dimensional Growth-at-Risk\n(GaR) forecasting application for the U.S. The Monte Carlo design considers\nseveral sparsity and error structures. Compared to alternative shrinkage\npriors, the proposed HS-BQR yields better (or at worst similar) performance in\ncoefficient bias and forecast error. The HS-BQR is particularly potent in\nsparse designs and in estimating extreme quantiles. As expected, the\nsimulations also highlight that identifying quantile specific location and\nscale effects for individual regressors in dense DGPs requires substantial\ndata. In the GaR application, we forecast tail risks as well as complete\nforecast densities using the McCracken and Ng (2020) database. Quantile\nspecific and density calibration score functions show that the HS-BQR provides\nthe best performance, especially at short and medium run horizons. The ability\nto produce well calibrated density forecasts and accurate downside risk\nmeasures in large data contexts makes the HS-BQR a promising tool for\nnowcasting applications and recession modelling.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.07655v2"
    },
    {
        "title": "Sparse HP Filter: Finding Kinks in the COVID-19 Contact Rate",
        "authors": [
            "Sokbae Lee",
            "Yuan Liao",
            "Myung Hwan Seo",
            "Youngki Shin"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In this paper, we estimate the time-varying COVID-19 contact rate of a\nSusceptible-Infected-Recovered (SIR) model. Our measurement of the contact rate\nis constructed using data on actively infected, recovered and deceased cases.\nWe propose a new trend filtering method that is a variant of the\nHodrick-Prescott (HP) filter, constrained by the number of possible kinks. We\nterm it the $\\textit{sparse HP filter}$ and apply it to daily data from five\ncountries: Canada, China, South Korea, the UK and the US. Our new method yields\nthe kinks that are well aligned with actual events in each country. We find\nthat the sparse HP filter provides a fewer kinks than the $\\ell_1$ trend\nfilter, while both methods fitting data equally well. Theoretically, we\nestablish risk consistency of both the sparse HP and $\\ell_1$ trend filters.\nUltimately, we propose to use time-varying $\\textit{contact growth rates}$ to\ndocument and monitor outbreaks of COVID-19.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.10555v2"
    },
    {
        "title": "Conflict in Africa during COVID-19: social distancing, food\n  vulnerability and welfare response",
        "authors": [
            "Roxana Gutiérrez-Romero"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study the effect of social distancing, food vulnerability, welfare and\nlabour COVID-19 policy responses on riots, violence against civilians and\nfood-related conflicts. Our analysis uses georeferenced data for 24 African\ncountries with monthly local prices and real-time conflict data reported in the\nArmed Conflict Location and Event Data Project (ACLED) from January 2015 until\nearly May 2020. Lockdowns and recent welfare policies have been implemented in\nlight of COVID-19, but in some contexts also likely in response to ongoing\nconflicts. To mitigate the potential risk of endogeneity, we use instrumental\nvariables. We exploit the exogeneity of global commodity prices, and three\nvariables that increase the risk of COVID-19 and efficiency in response such as\ncountries colonial heritage, male mortality rate attributed to air pollution\nand prevalence of diabetes in adults. We find that the probability of\nexperiencing riots, violence against civilians, food-related conflicts and food\nlooting has increased since lockdowns. Food vulnerability has been a\ncontributing factor. A 10% increase in the local price index is associated with\nan increase of 0.7 percentage points in violence against civilians.\nNonetheless, for every additional anti-poverty measure implemented in response\nto COVID-19 the probability of experiencing violence against civilians, riots\nand food-related conflicts declines by approximately 0.2 percentage points.\nThese anti-poverty measures also reduce the number of fatalities associated\nwith these conflicts. Overall, our findings reveal that food vulnerability has\nincreased conflict risks, but also offer an optimistic view of the importance\nof the state in providing an extensive welfare safety net.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.10696v1"
    },
    {
        "title": "The Macroeconomy as a Random Forest",
        "authors": [
            "Philippe Goulet Coulombe"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  I develop Macroeconomic Random Forest (MRF), an algorithm adapting the\ncanonical Machine Learning (ML) tool to flexibly model evolving parameters in a\nlinear macro equation. Its main output, Generalized Time-Varying Parameters\n(GTVPs), is a versatile device nesting many popular nonlinearities\n(threshold/switching, smooth transition, structural breaks/change) and allowing\nfor sophisticated new ones. The approach delivers clear forecasting gains over\nnumerous alternatives, predicts the 2008 drastic rise in unemployment, and\nperforms well for inflation. Unlike most ML-based methods, MRF is directly\ninterpretable -- via its GTVPs. For instance, the successful unemployment\nforecast is due to the influence of forward-looking variables (e.g., term\nspreads, housing starts) nearly doubling before every recession. Interestingly,\nthe Phillips curve has indeed flattened, and its might is highly cyclical.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.12724v3"
    },
    {
        "title": "Asset Prices and Capital Share Risks: Theory and Evidence",
        "authors": [
            "Joseph P. Byrne",
            "Boulis M. Ibrahim",
            "Xiaoyu Zong"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  An asset pricing model using long-run capital share growth risk has recently\nbeen found to successfully explain U.S. stock returns. Our paper adopts a\nrecursive preference utility framework to derive an heterogeneous asset pricing\nmodel with capital share risks.While modeling capital share risks, we account\nfor the elevated consumption volatility of high income stockholders. Capital\nrisks have strong volatility effects in our recursive asset pricing model.\nEmpirical evidence is presented in which capital share growth is also a source\nof risk for stock return volatility. We uncover contrasting unconditional and\nconditional asset pricing evidence for capital share risks.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.14023v1"
    },
    {
        "title": "A Model of the Fed's View on Inflation",
        "authors": [
            "Thomas Hasenzagl",
            "Filippo Pellegrino",
            "Lucrezia Reichlin",
            "Giovanni Ricco"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We develop a medium-size semi-structural time series model of inflation\ndynamics that is consistent with the view - often expressed by central banks -\nthat three components are important: a trend anchored by long-run expectations,\na Phillips curve and temporary fluctuations in energy prices. We find that a\nstable long-term inflation trend and a well identified steep Phillips curve are\nconsistent with the data, but they imply potential output declining since the\nnew millennium and energy prices affecting headline inflation not only via the\nPhillips curve but also via an independent expectational channel. A\nhigh-frequency energy price cycle can be related to global factors affecting\nthe commodity market, and often overpowers the Phillips curve thereby\nexplaining the inflation puzzles of the last ten years.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.14110v1"
    },
    {
        "title": "Matching Multidimensional Types: Theory and Application",
        "authors": [
            "Veli Safak"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Becker (1973) presents a bilateral matching model in which scalar types\ndescribe agents. For this framework, he establishes the conditions under which\npositive sorting between agents' attributes is the unique market outcome.\nBecker's celebrated sorting result has been applied to address many economic\nquestions. However, recent empirical studies in the fields of health,\nhousehold, and labor economics suggest that agents have multiple\noutcome-relevant attributes. In this paper, I study a matching model with\nmultidimensional types. I offer multidimensional generalizations of concordance\nand supermodularity to construct three multidimensional sorting patterns and\ntwo classes of multidimensional complementarities. For each of these sorting\npatterns, I identify the sufficient conditions which guarantee its optimality.\nIn practice, we observe sorting patterns between observed attributes that are\naggregated over unobserved characteristics. To reconcile theory with practice,\nI establish the link between production complementarities and the aggregated\nsorting patterns. Finally, I examine the relationship between agents' health\nstatus and their spouses' education levels among U.S. households within the\nframework for multidimensional matching markets. Preliminary analysis reveals a\nweak positive association between agents' health status and their spouses'\neducation levels. This weak positive association is estimated to be a product\nof three factors: (a) an attraction between better-educated individuals, (b) an\nattraction between healthier individuals, and (c) a weak positive association\nbetween agents' health status and their education levels. The attraction\nchannel suggests that the insurance risk associated with a two-person family\nplan is higher than the aggregate risk associated with two individual policies.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.14243v1"
    },
    {
        "title": "Identification and Formal Privacy Guarantees",
        "authors": [
            "Tatiana Komarova",
            "Denis Nekipelov"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Empirical economic research crucially relies on highly sensitive individual\ndatasets. At the same time, increasing availability of public individual-level\ndata makes it possible for adversaries to potentially de-identify anonymized\nrecords in sensitive research datasets. Most commonly accepted formal\ndefinition of an individual non-disclosure guarantee is referred to as\ndifferential privacy. It restricts the interaction of researchers with the data\nby allowing them to issue queries to the data. The differential privacy\nmechanism then replaces the actual outcome of the query with a randomised\noutcome.\n  The impact of differential privacy on the identification of empirical\neconomic models and on the performance of estimators in nonlinear empirical\nEconometric models has not been sufficiently studied. Since privacy protection\nmechanisms are inherently finite-sample procedures, we define the notion of\nidentifiability of the parameter of interest under differential privacy as a\nproperty of the limit of experiments. It is naturally characterized by the\nconcepts from the random sets theory.\n  We show that particular instances of regression discontinuity design may be\nproblematic for inference with differential privacy as parameters turn out to\nbe neither point nor partially identified. The set of differentially private\nestimators converges weakly to a random set. Our analysis suggests that many\nother estimators that rely on nuisance parameters may have similar properties\nwith the requirement of differential privacy. We show that identification\nbecomes possible if the target parameter can be deterministically located\nwithin the random set. In that case, a full exploration of the random set of\nthe weak limits of differentially private estimators can allow the data curator\nto select a sequence of instances of differentially private estimators\nconverging to the target parameter in probability.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.14732v2"
    },
    {
        "title": "Exact Trend Control in Estimating Treatment Effects Using Panel Data\n  with Heterogenous Trends",
        "authors": [
            "Chirok Han"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  For a panel model considered by Abadie et al. (2010), the counterfactual\noutcomes constructed by Abadie et al., Hsiao et al. (2012), and Doudchenko and\nImbens (2017) may all be confounded by uncontrolled heterogenous trends. Based\non exact-matching on the trend predictors, I propose new methods of estimating\nthe model-specific treatment effects, which are free from heterogenous trends.\nWhen applied to Abadie et al.'s (2010) model and data, the new estimators\nsuggest considerably smaller effects of California's tobacco control program.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08988v1"
    },
    {
        "title": "Invidious Comparisons: Ranking and Selection as Compound Decisions",
        "authors": [
            "Jiaying Gu",
            "Roger Koenker"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  There is an innate human tendency, one might call it the \"league table\nmentality,\" to construct rankings. Schools, hospitals, sports teams, movies,\nand myriad other objects are ranked even though their inherent\nmulti-dimensionality would suggest that -- at best -- only partial orderings\nwere possible. We consider a large class of elementary ranking problems in\nwhich we observe noisy, scalar measurements of merit for $n$ objects of\npotentially heterogeneous precision and are asked to select a group of the\nobjects that are \"most meritorious.\" The problem is naturally formulated in the\ncompound decision framework of Robbins's (1956) empirical Bayes theory, but it\nalso exhibits close connections to the recent literature on multiple testing.\nThe nonparametric maximum likelihood estimator for mixture models (Kiefer and\nWolfowitz (1956)) is employed to construct optimal ranking and selection rules.\nPerformance of the rules is evaluated in simulations and an application to\nranking U.S kidney dialysis centers.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.12550v3"
    },
    {
        "title": "Bayesian analysis of seasonally cointegrated VAR model",
        "authors": [
            "Justyna Wróblewska"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The paper aims at developing the Bayesian seasonally cointegrated model for\nquarterly data. We propose the prior structure, derive the set of full\nconditional posterior distributions, and propose the sampling scheme. The\nidentification of cointegrating spaces is obtained \\emph{via} orthonormality\nrestrictions imposed on vectors spanning them. In the case of annual frequency,\nthe cointegrating vectors are complex, which should be taken into account when\nidentifying them. The point estimation of the cointegrating spaces is also\ndiscussed. The presented methods are illustrated by a simulation experiment and\nare employed in the analysis of money and prices in the Polish economy.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.14820v2"
    },
    {
        "title": "Bias-Aware Inference in Regularized Regression Models",
        "authors": [
            "Timothy B. Armstrong",
            "Michal Kolesár",
            "Soonwoo Kwon"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We consider inference on a scalar regression coefficient under a constraint\non the magnitude of the control coefficients. A class of estimators based on a\nregularized propensity score regression is shown to exactly solve a tradeoff\nbetween worst-case bias and variance. We derive confidence intervals (CIs)\nbased on these estimators that are bias-aware: they account for the possible\nbias of the estimator. Under homoskedastic Gaussian errors, these estimators\nand CIs are near-optimal in finite samples for MSE and CI length. We also\nprovide conditions for asymptotic validity of the CI with unknown and possibly\nheteroskedastic error distribution, and derive novel optimal rates of\nconvergence under high-dimensional asymptotics that allow the number of\nregressors to increase more quickly than the number of observations. Extensive\nsimulations and an empirical application illustrate the performance of our\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.14823v2"
    },
    {
        "title": "Assessing the Sensitivity of Synthetic Control Treatment Effect\n  Estimates to Misspecification Error",
        "authors": [
            "Billy Ferguson",
            "Brad Ross"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose a sensitivity analysis for Synthetic Control (SC) treatment effect\nestimates to interrogate the assumption that the SC method is well-specified,\nnamely that choosing weights to minimize pre-treatment prediction error yields\naccurate predictions of counterfactual post-treatment outcomes. Our data-driven\nprocedure recovers the set of treatment effects consistent with the assumption\nthat the misspecification error incurred by the SC method is at most the\nobservable misspecification error incurred when using the SC estimator to\npredict the outcomes of some control unit. We show that under one definition of\nmisspecification error, our procedure provides a simple, geometric motivation\nfor comparing the estimated treatment effect to the distribution of placebo\nresiduals to assess estimate credibility. When we apply our procedure to\nseveral canonical studies that report SC estimates, we broadly confirm the\nconclusions drawn by the source papers.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.15367v3"
    },
    {
        "title": "Assessing Sensitivity to Unconfoundedness: Estimation and Inference",
        "authors": [
            "Matthew A. Masten",
            "Alexandre Poirier",
            "Linqi Zhang"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper provides a set of methods for quantifying the robustness of\ntreatment effects estimated using the unconfoundedness assumption (also known\nas selection on observables or conditional independence). Specifically, we\nestimate and do inference on bounds on various treatment effect parameters,\nlike the average treatment effect (ATE) and the average effect of treatment on\nthe treated (ATT), under nonparametric relaxations of the unconfoundedness\nassumption indexed by a scalar sensitivity parameter c. These relaxations allow\nfor limited selection on unobservables, depending on the value of c. For large\nenough c, these bounds equal the no assumptions bounds. Using a non-standard\nbootstrap method, we show how to construct confidence bands for these bound\nfunctions which are uniform over all values of c. We illustrate these methods\nwith an empirical application to effects of the National Supported Work\nDemonstration program. We implement these methods in a companion Stata module\nfor easy use in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.15716v1"
    },
    {
        "title": "Adaptive Random Bandwidth for Inference in CAViaR Models",
        "authors": [
            "Alain Hecq",
            "Li Sun"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper investigates the size performance of Wald tests for CAViaR models\n(Engle and Manganelli, 2004). We find that the usual estimation strategy on\ntest statistics yields inaccuracies. Indeed, we show that existing density\nestimation methods cannot adapt to the time-variation in the conditional\nprobability densities of CAViaR models. Consequently, we develop a method\ncalled adaptive random bandwidth which can approximate time-varying conditional\nprobability densities robustly for inference testing on CAViaR models based on\nthe asymptotic normality of the model parameter estimator. This proposed method\nalso avoids the problem of choosing an optimal bandwidth in estimating\nprobability densities, and can be extended to multivariate quantile regressions\nstraightforward.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.01636v1"
    },
    {
        "title": "Linear programming approach to nonparametric inference under shape\n  restrictions: with an application to regression kink designs",
        "authors": [
            "Harold D. Chiang",
            "Kengo Kato",
            "Yuya Sasaki",
            "Takuya Ura"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We develop a novel method of constructing confidence bands for nonparametric\nregression functions under shape constraints. This method can be implemented\nvia a linear programming, and it is thus computationally appealing. We\nillustrate a usage of our proposed method with an application to the regression\nkink design (RKD). Econometric analyses based on the RKD often suffer from wide\nconfidence intervals due to slow convergence rates of nonparametric derivative\nestimators. We demonstrate that economic models and structures motivate shape\nrestrictions, which in turn contribute to shrinking the confidence interval for\nan analysis of the causal effects of unemployment insurance benefits on\nunemployment durations.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.06586v1"
    },
    {
        "title": "Approximate Bayes factors for unit root testing",
        "authors": [
            "Magris Martin",
            "Iosifidis Alexandros"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper introduces a feasible and practical Bayesian method for unit root\ntesting in financial time series. We propose a convenient approximation of the\nBayes factor in terms of the Bayesian Information Criterion as a\nstraightforward and effective strategy for testing the unit root hypothesis.\nOur approximate approach relies on few assumptions, is of general\napplicability, and preserves a satisfactory error rate. Among its advantages,\nit does not require the prior distribution on model's parameters to be\nspecified. Our simulation study and empirical application on real exchange\nrates show great accordance between the suggested simple approach and both\nBayesian and non-Bayesian alternatives.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10048v2"
    },
    {
        "title": "Estimation and Inference by Stochastic Optimization: Three Examples",
        "authors": [
            "Jean-Jacques Forneron",
            "Serena Ng"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper illustrates two algorithms designed in Forneron & Ng (2020): the\nresampled Newton-Raphson (rNR) and resampled quasi-Newton (rqN) algorithms\nwhich speed-up estimation and bootstrap inference for structural models. An\nempirical application to BLP shows that computation time decreases from nearly\n5 hours with the standard bootstrap to just over 1 hour with rNR, and only 15\nminutes using rqN. A first Monte-Carlo exercise illustrates the accuracy of the\nmethod for estimation and inference in a probit IV regression. A second\nexercise additionally illustrates statistical efficiency gains relative to\nstandard estimation for simulation-based estimation using a dynamic panel\nregression example.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10443v1"
    },
    {
        "title": "Misguided Use of Observed Covariates to Impute Missing Covariates in\n  Conditional Prediction: A Shrinkage Problem",
        "authors": [
            "Charles F Manski",
            "Michael Gmeiner",
            "Anat Tamburc"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Researchers regularly perform conditional prediction using imputed values of\nmissing data. However, applications of imputation often lack a firm foundation\nin statistical theory. This paper originated when we were unable to find\nanalysis substantiating claims that imputation of missing data has good\nfrequentist properties when data are missing at random (MAR). We focused on the\nuse of observed covariates to impute missing covariates when estimating\nconditional means of the form E(y|x, w). Here y is an outcome whose\nrealizations are always observed, x is a covariate whose realizations are\nalways observed, and w is a covariate whose realizations are sometimes\nunobserved. We examine the probability limit of simple imputation estimates of\nE(y|x, w) as sample size goes to infinity. We find that these estimates are not\nconsistent when covariate data are MAR. To the contrary, the estimates suffer\nfrom a shrinkage problem. They converge to points intermediate between the\nconditional mean of interest, E(y|x, w), and the mean E(y|x) that conditions\nonly on x. We use a type of genotype imputation to illustrate.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.11334v1"
    },
    {
        "title": "Bridging factor and sparse models",
        "authors": [
            "Jianqing Fan",
            "Ricardo Masini",
            "Marcelo C. Medeiros"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Factor and sparse models are two widely used methods to impose a\nlow-dimensional structure in high-dimensions. However, they are seemingly\nmutually exclusive. We propose a lifting method that combines the merits of\nthese two models in a supervised learning methodology that allows for\nefficiently exploring all the information in high-dimensional datasets. The\nmethod is based on a flexible model for high-dimensional panel data, called\nfactor-augmented regression model with observable and/or latent common factors,\nas well as idiosyncratic components. This model not only includes both\nprincipal component regression and sparse regression as specific models but\nalso significantly weakens the cross-sectional dependence and facilitates model\nselection and interpretability. The method consists of several steps and a\nnovel test for (partial) covariance structure in high dimensions to infer the\nremaining cross-section dependence at each step. We develop the theory for the\nmodel and demonstrate the validity of the multiplier bootstrap for testing a\nhigh-dimensional (partial) covariance structure. The theory is supported by a\nsimulation study and applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.11341v4"
    },
    {
        "title": "Hierarchical Regularizers for Mixed-Frequency Vector Autoregressions",
        "authors": [
            "Alain Hecq",
            "Marie Ternes",
            "Ines Wilms"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Mixed-frequency Vector AutoRegressions (MF-VAR) model the dynamics between\nvariables recorded at different frequencies. However, as the number of series\nand high-frequency observations per low-frequency period grow, MF-VARs suffer\nfrom the \"curse of dimensionality\". We curb this curse through a regularizer\nthat permits hierarchical sparsity patterns by prioritizing the inclusion of\ncoefficients according to the recency of the information they contain.\nAdditionally, we investigate the presence of nowcasting relations by sparsely\nestimating the MF-VAR error covariance matrix. We study predictive Granger\ncausality relations in a MF-VAR for the U.S. economy and construct a coincident\nindicator of GDP growth. Supplementary Materials for this article are available\nonline.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.11780v2"
    },
    {
        "title": "Network Cluster-Robust Inference",
        "authors": [
            "Michael P. Leung"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Since network data commonly consists of observations from a single large\nnetwork, researchers often partition the network into clusters in order to\napply cluster-robust inference methods. Existing such methods require clusters\nto be asymptotically independent. Under mild conditions, we prove that, for\nthis requirement to hold for network-dependent data, it is necessary and\nsufficient that clusters have low conductance, the ratio of edge boundary size\nto volume. This yields a simple measure of cluster quality. We find in\nsimulations that when clusters have low conductance, cluster-robust methods\ncontrol size better than HAC estimators. However, for important classes of\nnetworks lacking low-conductance clusters, the former can exhibit substantial\nsize distortion. To determine the number of low-conductance clusters and\nconstruct them, we draw on results in spectral graph theory that connect\nconductance to the spectrum of the graph Laplacian. Based on these results, we\npropose to use the spectrum to determine the number of low-conductance clusters\nand spectral clustering to construct them.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01470v4"
    },
    {
        "title": "High-dimensional estimation of quadratic variation based on penalized\n  realized variance",
        "authors": [
            "Kim Christensen",
            "Mikkel Slot Nielsen",
            "Mark Podolskij"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In this paper, we develop a penalized realized variance (PRV) estimator of\nthe quadratic variation (QV) of a high-dimensional continuous It\\^{o}\nsemimartingale. We adapt the principle idea of regularization from linear\nregression to covariance estimation in a continuous-time high-frequency\nsetting. We show that under a nuclear norm penalization, the PRV is computed by\nsoft-thresholding the eigenvalues of realized variance (RV). It therefore\nencourages sparsity of singular values or, equivalently, low rank of the\nsolution. We prove our estimator is minimax optimal up to a logarithmic factor.\nWe derive a concentration inequality, which reveals that the rank of PRV is --\nwith a high probability -- the number of non-negligible eigenvalues of the QV.\nMoreover, we also provide the associated non-asymptotic analysis for the spot\nvariance. We suggest an intuitive data-driven bootstrap procedure to select the\nshrinkage parameter. Our theory is supplemented by a simulation study and an\nempirical application. The PRV detects about three-five factors in the equity\nmarket, with a notable rank decrease during times of distress in financial\nmarkets. This is consistent with most standard asset pricing models, where a\nlimited amount of systematic factors driving the cross-section of stock returns\nare perturbed by idiosyncratic errors, rendering the QV -- and also RV -- of\nfull rank.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.03237v1"
    },
    {
        "title": "Extremal points of Lorenz curves and applications to inequality analysis",
        "authors": [
            "Amparo Baíllo",
            "Javier Cárcamo",
            "Carlos Mora-Corral"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We find the set of extremal points of Lorenz curves with fixed Gini index and\ncompute the maximal $L^1$-distance between Lorenz curves with given values of\ntheir Gini coefficients. As an application we introduce a bidimensional index\nthat simultaneously measures relative inequality and dissimilarity between two\npopulations. This proposal employs the Gini indices of the variables and an\n$L^1$-distance between their Lorenz curves. The index takes values in a\nright-angled triangle, two of whose sides characterize perfect relative\ninequality-expressed by the Lorenz ordering between the underlying\ndistributions. Further, the hypotenuse represents maximal distance between the\ntwo distributions. As a consequence, we construct a chart to, graphically,\neither see the evolution of (relative) inequality and distance between two\nincome distributions over time or to compare the distribution of income of a\nspecific population between a fixed time point and a range of years. We prove\nthe mathematical results behind the above claims and provide a full description\nof the asymptotic properties of the plug-in estimator of this index. Finally,\nwe apply the proposed bidimensional index to several real EU-SILC income\ndatasets to illustrate its performance in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.03286v1"
    },
    {
        "title": "Prediction of financial time series using LSTM and data denoising\n  methods",
        "authors": [
            "Qi Tang",
            "Tongmei Fan",
            "Ruchen Shi",
            "Jingyan Huang",
            "Yidan Ma"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  In order to further overcome the difficulties of the existing models in\ndealing with the non-stationary and nonlinear characteristics of high-frequency\nfinancial time series data, especially its weak generalization ability, this\npaper proposes an ensemble method based on data denoising methods, including\nthe wavelet transform (WT) and singular spectrum analysis (SSA), and long-term\nshort-term memory neural network (LSTM) to build a data prediction model, The\nfinancial time series is decomposed and reconstructed by WT and SSA to denoise.\nUnder the condition of denoising, the smooth sequence with effective\ninformation is reconstructed. The smoothing sequence is introduced into LSTM\nand the predicted value is obtained. With the Dow Jones industrial average\nindex (DJIA) as the research object, the closing price of the DJIA every five\nminutes is divided into short-term (1 hour), medium-term (3 hours) and\nlong-term (6 hours) respectively. . Based on root mean square error (RMSE),\nmean absolute error (MAE), mean absolute percentage error (MAPE) and absolute\npercentage error standard deviation (SDAPE), the experimental results show that\nin the short-term, medium-term and long-term, data denoising can greatly\nimprove the accuracy and stability of the prediction, and can effectively\nimprove the generalization ability of LSTM prediction model. As WT and SSA can\nextract useful information from the original sequence and avoid overfitting,\nthe hybrid model can better grasp the sequence pattern of the closing price of\nthe DJIA. And the WT-LSTM model is better than the benchmark LSTM model and\nSSA-LSTM model.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.03505v1"
    },
    {
        "title": "Modeling tail risks of inflation using unobserved component quantile\n  regressions",
        "authors": [
            "Michael Pfarrhofer"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper proposes methods for Bayesian inference in time-varying parameter\n(TVP) quantile regression (QR) models featuring conditional heteroskedasticity.\nI use data augmentation schemes to render the model conditionally Gaussian and\ndevelop an efficient Gibbs sampling algorithm. Regularization of the\nhigh-dimensional parameter space is achieved via flexible dynamic shrinkage\npriors. A simple version of TVP-QR based on an unobserved component model is\napplied to dynamically trace the quantiles of the distribution of inflation in\nthe United States, the United Kingdom and the euro area. In an out-of-sample\nforecast exercise, I find the proposed model to be competitive and perform\nparticularly well for higher-order and tail forecasts. A detailed analysis of\nthe resulting predictive distributions reveals that they are sometimes skewed\nand occasionally feature heavy tails.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.03632v2"
    },
    {
        "title": "The impact of online machine-learning methods on long-term investment\n  decisions and generator utilization in electricity markets",
        "authors": [
            "Alexander J. M. Kell",
            "A. Stephen McGough",
            "Matthew Forshaw"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  Electricity supply must be matched with demand at all times. This helps\nreduce the chances of issues such as load frequency control and the chances of\nelectricity blackouts. To gain a better understanding of the load that is\nlikely to be required over the next 24h, estimations under uncertainty are\nneeded. This is especially difficult in a decentralized electricity market with\nmany micro-producers which are not under central control.\n  In this paper, we investigate the impact of eleven offline learning and five\nonline learning algorithms to predict the electricity demand profile over the\nnext 24h. We achieve this through integration within the long-term agent-based\nmodel, ElecSim. Through the prediction of electricity demand profile over the\nnext 24h, we can simulate the predictions made for a day-ahead market. Once we\nhave made these predictions, we sample from the residual distributions and\nperturb the electricity market demand using the simulation, ElecSim. This\nenables us to understand the impact of errors on the long-term dynamics of a\ndecentralized electricity market.\n  We show we can reduce the mean absolute error by 30% using an online\nalgorithm when compared to the best offline algorithm, whilst reducing the\nrequired tendered national grid reserve required. This reduction in national\ngrid reserves leads to savings in costs and emissions. We also show that large\nerrors in prediction accuracy have a disproportionate error on investments made\nover a 17-year time frame, as well as electricity mix.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.04327v1"
    },
    {
        "title": "Extension of the Lagrange multiplier test for error cross-section\n  independence to large panels with non normal errors",
        "authors": [
            "Zhaoyuan Li",
            "Jianfeng Yao"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  This paper reexamines the seminal Lagrange multiplier test for cross-section\nindependence in a large panel model where both the number of cross-sectional\nunits n and the number of time series observations T can be large. The first\ncontribution of the paper is an enlargement of the test with two extensions:\nfirstly the new asymptotic normality is derived in a simultaneous limiting\nscheme where the two dimensions (n, T) tend to infinity with comparable\nmagnitudes; second, the result is valid for general error distribution (not\nnecessarily normal). The second contribution of the paper is a new test\nstatistic based on the sum of the fourth powers of cross-section correlations\nfrom OLS residuals, instead of their squares used in the Lagrange multiplier\nstatistic. This new test is generally more powerful, and the improvement is\nparticularly visible against alternatives with weak or sparse cross-section\ndependence. Both simulation study and real data analysis are proposed to\ndemonstrate the advantages of the enlarged Lagrange multiplier test and the\npower enhanced test in comparison with the existing procedures.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.06075v1"
    },
    {
        "title": "The Chained Difference-in-Differences",
        "authors": [
            "Christophe Bellégo",
            "David Benatia",
            "Vincent Dortet-Bernardet"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies the identification, estimation, and inference of long-term\n(binary) treatment effect parameters when balanced panel data is not available,\nor consists of only a subset of the available data. We develop a new estimator:\nthe chained difference-in-differences, which leverages the overlapping\nstructure of many unbalanced panel data sets. This approach consists in\naggregating a collection of short-term treatment effects estimated on multiple\nincomplete panels. Our estimator accommodates (1) multiple time periods, (2)\nvariation in treatment timing, (3) treatment effect heterogeneity, (4) general\nmissing data patterns, and (5) sample selection on observables. We establish\nthe asymptotic properties of the proposed estimator and discuss identification\nand efficiency gains in comparison to existing methods. Finally, we illustrate\nits relevance through (i) numerical simulations, and (ii) an application about\nthe effects of an innovation policy in France.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01085v3"
    },
    {
        "title": "Fitting mixed logit random regret minimization models using maximum\n  simulated likelihood",
        "authors": [
            "Ziyue Zhu",
            "Álvaro A. Gutiérrez-Vargas",
            "Martina Vandebroek"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This article describes the mixrandregret command, which extends the\nrandregret command introduced in Guti\\'errez-Vargas et al. (2021, The Stata\nJournal 21: 626-658) incorporating random coefficients for Random Regret\nMinimization models. The newly developed command mixrandregret allows the\ninclusion of random coefficients in the regret function of the classical RRM\nmodel introduced in Chorus (2010, European Journal of Transport and\nInfrastructure Research 10: 181-196). The command allows the user to specify a\ncombination of fixed and random coefficients. In addition, the user can specify\nnormal and log-normal distributions for the random coefficients using the\ncommands' options. The models are fitted using simulated maximum likelihood\nusing numerical integration to approximate the choice probabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01091v1"
    },
    {
        "title": "Testing for Coefficient Randomness in Local-to-Unity Autoregressions",
        "authors": [
            "Mikihito Nishi"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this study, we propose a test for the coefficient randomness in\nautoregressive models where the autoregressive coefficient is local to unity,\nwhich is empirically relevant given the results of earlier studies. Under this\nspecification, we theoretically analyze the effect of the correlation between\nthe random coefficient and disturbance on tests' properties, which remains\nlargely unexplored in the literature. Our analysis reveals that the correlation\ncrucially affects the power of tests for coefficient randomness and that tests\nproposed by earlier studies can perform poorly when the degree of the\ncorrelation is moderate to large. The test we propose in this paper is designed\nto have a power function robust to the correlation. Because the asymptotic null\ndistribution of our test statistic depends on the correlation $\\psi$ between\nthe disturbance and its square as earlier tests do, we also propose a modified\nversion of the test statistic such that its asymptotic null distribution is\nfree from the nuisance parameter $\\psi$. The modified test is shown to have\nbetter power properties than existing ones in large and finite samples.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.04853v2"
    },
    {
        "title": "Doubly-Robust Inference for Conditional Average Treatment Effects with\n  High-Dimensional Controls",
        "authors": [
            "Adam Baybutt",
            "Manu Navjeevan"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Plausible identification of conditional average treatment effects (CATEs) may\nrely on controlling for a large number of variables to account for confounding\nfactors. In these high-dimensional settings, estimation of the CATE requires\nestimating first-stage models whose consistency relies on correctly specifying\ntheir parametric forms. While doubly-robust estimators of the CATE exist,\ninference procedures based on the second stage CATE estimator are not\ndoubly-robust. Using the popular augmented inverse propensity weighting signal,\nwe propose an estimator for the CATE whose resulting Wald-type confidence\nintervals are doubly-robust. We assume a logistic model for the propensity\nscore and a linear model for the outcome regression, and estimate the\nparameters of these models using an $\\ell_1$ (Lasso) penalty to address the\nhigh dimensional covariates. Our proposed estimator remains consistent at the\nnonparametric rate and our proposed pointwise and uniform confidence intervals\nremain asymptotically valid even if one of the logistic propensity score or\nlinear outcome regression models are misspecified. These results are obtained\nunder similar conditions to existing analyses in the high-dimensional and\nnonparametric literatures.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.06283v1"
    },
    {
        "title": "Statistical inference for the logarithmic spatial heteroskedasticity\n  model with exogenous variables",
        "authors": [
            "Bing Su",
            "Fukang Zhu",
            "Ke Zhu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The spatial dependence in mean has been well studied by plenty of models in a\nlarge strand of literature, however, the investigation of spatial dependence in\nvariance is lagging significantly behind. The existing models for the spatial\ndependence in variance are scarce, with neither probabilistic structure nor\nstatistical inference procedure being explored. To circumvent this deficiency,\nthis paper proposes a new generalized logarithmic spatial heteroscedasticity\nmodel with exogenous variables (denoted by the log-SHE model) to study the\nspatial dependence in variance. For the log-SHE model, its spatial near-epoch\ndependence (NED) property is investigated, and a systematic statistical\ninference procedure is provided, including the maximum likelihood and\ngeneralized method of moments estimators, the Wald, Lagrange multiplier and\nlikelihood-ratio-type D tests for model parameter constraints, and the\noveridentification test for the model diagnostic checking. Using the tool of\nspatial NED, the asymptotics of all proposed estimators and tests are\nestablished under regular conditions. The usefulness of the proposed\nmethodology is illustrated by simulation results and a real data example on the\nhouse selling price.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.06658v1"
    },
    {
        "title": "Noisy, Non-Smooth, Non-Convex Estimation of Moment Condition Models",
        "authors": [
            "Jean-Jacques Forneron"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  A practical challenge for structural estimation is the requirement to\naccurately minimize a sample objective function which is often non-smooth,\nnon-convex, or both. This paper proposes a simple algorithm designed to find\naccurate solutions without performing an exhaustive search. It augments each\niteration from a new Gauss-Newton algorithm with a grid search step. A finite\nsample analysis derives its optimization and statistical properties\nsimultaneously using only econometric assumptions. After a finite number of\niterations, the algorithm automatically transitions from global to fast local\nconvergence, producing accurate estimates with high probability. Simulated\nexamples and an empirical application illustrate the results.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.07196v2"
    },
    {
        "title": "Digital Divide: Empirical Study of CIUS 2020",
        "authors": [
            "Joann Jasiak",
            "Peter MacKenzie",
            "Purevdorj Tuvaandorj"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  As Canada and other major economies consider implementing \"digital money\" or\nCentral Bank Digital Currencies, understanding how demographic and geographic\nfactors influence public engagement with digital technologies becomes\nincreasingly important. This paper uses data from the 2020 Canadian Internet\nUse Survey and employs survey-adapted Lasso inference methods to identify\nindividual socio-economic and demographic characteristics determining the\ndigital divide in Canada. We also introduce a score to measure and compare the\ndigital literacy of various segments of Canadian population. Our findings\nreveal that disparities in the use of e.g. online banking, emailing, and\ndigital payments exist across different demographic and socio-economic groups.\nIn addition, we document the effects of COVID-19 pandemic on internet use in\nCanada and describe changes in the characteristics of Canadian internet users\nover the last decade.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.07855v3"
    },
    {
        "title": "Inference for Two-stage Experiments under Covariate-Adaptive\n  Randomization",
        "authors": [
            "Jizhou Liu"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper studies inference in two-stage randomized experiments under\ncovariate-adaptive randomization. In the initial stage of this experimental\ndesign, clusters (e.g., households, schools, or graph partitions) are\nstratified and randomly assigned to control or treatment groups based on\ncluster-level covariates. Subsequently, an independent second-stage design is\ncarried out, wherein units within each treated cluster are further stratified\nand randomly assigned to either control or treatment groups, based on\nindividual-level covariates. Under the homogeneous partial interference\nassumption, I establish conditions under which the proposed\ndifference-in-``average of averages'' estimators are consistent and\nasymptotically normal for the corresponding average primary and spillover\neffects and develop consistent estimators of their asymptotic variances.\nCombining these results establishes the asymptotic validity of tests based on\nthese estimators. My findings suggest that ignoring covariate information in\nthe design stage can result in efficiency loss, and commonly used inference\nmethods that ignore or improperly use covariate information can lead to either\nconservative or invalid inference. Then, I apply these results to studying\noptimal use of covariate information under covariate-adaptive randomization in\nlarge samples, and demonstrate that a specific generalized matched-pair design\nachieves minimum asymptotic variance for each proposed estimator. Finally, I\ndiscuss covariate adjustment, which incorporates additional baseline covariates\nnot used for treatment assignment. The practical relevance of the theoretical\nresults is illustrated through a simulation study and an empirical application.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.09016v6"
    },
    {
        "title": "ddml: Double/debiased machine learning in Stata",
        "authors": [
            "Achim Ahrens",
            "Christian B. Hansen",
            "Mark E. Schaffer",
            "Thomas Wiemann"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We introduce the package ddml for Double/Debiased Machine Learning (DDML) in\nStata. Estimators of causal parameters for five different econometric models\nare supported, allowing for flexible estimation of causal effects of endogenous\nvariables in settings with unknown functional forms and/or many exogenous\nvariables. ddml is compatible with many existing supervised machine learning\nprograms in Stata. We recommend using DDML in combination with stacking\nestimation which combines multiple machine learners into a final predictor. We\nprovide Monte Carlo evidence to support our recommendation.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.09397v3"
    },
    {
        "title": "Processes analogous to ecological interactions and dispersal shape the\n  dynamics of economic activities",
        "authors": [
            "Victor Boussange",
            "Didier Sornette",
            "Heike Lischke",
            "Loïc Pellissier"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  The processes of ecological interactions, dispersal and mutations shape the\ndynamics of biological communities, and analogous eco-evolutionary processes\nacting upon economic entities have been proposed to explain economic change.\nThis hypothesis is compelling because it explains economic change through\nendogenous mechanisms, but it has not been quantitatively tested at the global\neconomy level. Here, we use an inverse modelling technique and 59 years of\neconomic data covering 77 countries to test whether the collective dynamics of\nnational economic activities can be characterised by eco-evolutionary\nprocesses. We estimate the statistical support of dynamic community models in\nwhich the dynamics of economic activities are coupled with positive and\nnegative interactions between the activities, the spatial dispersal of the\nactivities, and their transformations into other economic activities. We find\nstrong support for the models capturing positive interactions between economic\nactivities and spatial dispersal of the activities across countries. These\nresults suggest that processes akin to those occurring in ecosystems play a\nsignificant role in the dynamics of economic systems. The strength-of-evidence\nobtained for each model varies across countries and may be caused by\ndifferences in the distance between countries, specific institutional contexts,\nand historical contingencies. Overall, our study provides a new quantitative,\nbiologically inspired framework to study the forces shaping economic change.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.09486v1"
    },
    {
        "title": "Hierarchical Regularizers for Reverse Unrestricted Mixed Data Sampling\n  Regressions",
        "authors": [
            "Alain Hecq",
            "Marie Ternes",
            "Ines Wilms"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Reverse Unrestricted MIxed DAta Sampling (RU-MIDAS) regressions are used to\nmodel high-frequency responses by means of low-frequency variables. However,\ndue to the periodic structure of RU-MIDAS regressions, the dimensionality grows\nquickly if the frequency mismatch between the high- and low-frequency variables\nis large. Additionally the number of high-frequency observations available for\nestimation decreases. We propose to counteract this reduction in sample size by\npooling the high-frequency coefficients and further reduce the dimensionality\nthrough a sparsity-inducing convex regularizer that accounts for the temporal\nordering among the different lags. To this end, the regularizer prioritizes the\ninclusion of lagged coefficients according to the recency of the information\nthey contain. We demonstrate the proposed method on two empirical applications,\none on realized volatility forecasting with macroeconomic data and another on\ndemand forecasting for a bicycle-sharing system with ridership data on other\ntransportation types.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.10592v2"
    },
    {
        "title": "Multidimensional dynamic factor models",
        "authors": [
            "Matteo Barigozzi",
            "Filippo Pellegrino"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper generalises dynamic factor models for multidimensional dependent\ndata. In doing so, it develops an interpretable technique to study complex\ninformation sources ranging from repeated surveys with a varying number of\nrespondents to panels of satellite images. We specialise our results to model\nmicroeconomic data on US households jointly with macroeconomic aggregates. This\nresults in a powerful tool able to generate localised predictions,\ncounterfactuals and impulse response functions for individual households,\naccounting for traditional time-series complexities depicted in the state-space\nliterature. The model is also compatible with the growing focus of policymakers\nfor real-time economic analysis as it is able to process observations online,\nwhile handling missing values and asynchronous data releases.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.12499v1"
    },
    {
        "title": "Bridging the Covid-19 Data and the Epidemiological Model using\n  Time-Varying Parameter SIRD Model",
        "authors": [
            "Cem Cakmakli",
            "Yasin Simsek"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  This paper extends the canonical model of epidemiology, the SIRD model, to\nallow for time-varying parameters for real-time measurement and prediction of\nthe trajectory of the Covid-19 pandemic. Time variation in model parameters is\ncaptured using the generalized autoregressive score modeling structure designed\nfor the typical daily count data related to the pandemic. The resulting\nspecification permits a flexible yet parsimonious model with a low\ncomputational cost. The model is extended to allow for unreported cases using a\nmixed-frequency setting. Results suggest that these cases' effects on the\nparameter estimates might be sizeable. Full sample results show that the\nflexible framework accurately captures the successive waves of the pandemic. A\nreal-time exercise indicates that the proposed structure delivers timely and\nprecise information on the pandemic's current stance. This superior\nperformance, in turn, transforms into accurate predictions of the confirmed and\ndeath cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.13692v1"
    },
    {
        "title": "Change Point Estimation in Panel Data with Time-Varying Individual\n  Effects",
        "authors": [
            "Otilia Boldea",
            "Bettina Drepper",
            "Zhuojiong Gan"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper proposes a method for estimating multiple change points in panel\ndata models with unobserved individual effects via ordinary least-squares\n(OLS). Typically, in this setting, the OLS slope estimators are inconsistent\ndue to the unobserved individual effects bias. As a consequence, existing\nmethods remove the individual effects before change point estimation through\ndata transformations such as first-differencing. We prove that under reasonable\nassumptions, the unobserved individual effects bias has no impact on the\nconsistent estimation of change points. Our simulations show that since our\nmethod does not remove any variation in the dataset before change point\nestimation, it performs better in small samples compared to first-differencing\nmethods. We focus on short panels because they are commonly used in practice,\nand allow for the unobserved individual effects to vary over time. Our method\nis illustrated via two applications: the environmental Kuznets curve and the\nU.S. house price expectations after the financial crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.03109v1"
    },
    {
        "title": "Engineering and Economic Analysis for Electric Vehicle Charging\n  Infrastructure --- Placement, Pricing, and Market Design",
        "authors": [
            "Chao Luo"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This dissertation is to study the interplay between large-scale electric\nvehicle (EV) charging and the power system. We address three important issues\npertaining to EV charging and integration into the power system: (1) charging\nstation placement, (2) pricing policy and energy management strategy, and (3)\nelectricity trading market and distribution network design to facilitate\nintegrating EV and renewable energy source (RES) into the power system.\n  For charging station placement problem, we propose a multi-stage consumer\nbehavior based placement strategy with incremental EV penetration rates and\nmodel the EV charging industry as an oligopoly where the entire market is\ndominated by a few charging service providers (oligopolists). The optimal\nplacement policy for each service provider is obtained by solving a Bayesian\ngame.\n  For pricing and energy management of EV charging stations, we provide\nguidelines for charging service providers to determine charging price and\nmanage electricity reserve to balance the competing objectives of improving\nprofitability, enhancing customer satisfaction, and reducing impact on the\npower system. Two algorithms --- stochastic dynamic programming (SDP) algorithm\nand greedy algorithm (benchmark algorithm) are applied to derive the pricing\nand electricity procurement strategy.\n  We design a novel electricity trading market and distribution network, which\nsupports seamless RES integration, grid to vehicle (G2V), vehicle to grid\n(V2G), vehicle to vehicle (V2V), and distributed generation (DG) and storage.\nWe apply a sharing economy model to the electricity sector to stimulate\ndifferent entities to exchange and monetize their underutilized electricity. A\nfitness-score (FS)-based supply-demand matching algorithm is developed by\nconsidering consumer surplus, electricity network congestion, and economic\ndispatch.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.03897v1"
    },
    {
        "title": "Sensitivity Analysis using Approximate Moment Condition Models",
        "authors": [
            "Timothy B. Armstrong",
            "Michal Kolesár"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We consider inference in models defined by approximate moment conditions. We\nshow that near-optimal confidence intervals (CIs) can be formed by taking a\ngeneralized method of moments (GMM) estimator, and adding and subtracting the\nstandard error times a critical value that takes into account the potential\nbias from misspecification of the moment conditions. In order to optimize\nperformance under potential misspecification, the weighting matrix for this GMM\nestimator takes into account this potential bias, and therefore differs from\nthe one that is optimal under correct specification. To formally show the\nnear-optimality of these CIs, we develop asymptotic efficiency bounds for\ninference in the locally misspecified GMM setting. These bounds may be of\nindependent interest, due to their implications for the possibility of using\nmoment selection procedures when conducting inference in moment condition\nmodels. We apply our methods in an empirical application to automobile demand,\nand show that adjusting the weighting matrix can shrink the CIs by a factor of\n3 or more.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.07387v5"
    },
    {
        "title": "Optimal Bandwidth Choice for Robust Bias Corrected Inference in\n  Regression Discontinuity Designs",
        "authors": [
            "Sebastian Calonico",
            "Matias D. Cattaneo",
            "Max H. Farrell"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Modern empirical work in Regression Discontinuity (RD) designs often employs\nlocal polynomial estimation and inference with a mean square error (MSE)\noptimal bandwidth choice. This bandwidth yields an MSE-optimal RD treatment\neffect estimator, but is by construction invalid for inference. Robust bias\ncorrected (RBC) inference methods are valid when using the MSE-optimal\nbandwidth, but we show they yield suboptimal confidence intervals in terms of\ncoverage error. We establish valid coverage error expansions for RBC confidence\ninterval estimators and use these results to propose new inference-optimal\nbandwidth choices for forming these intervals. We find that the standard\nMSE-optimal bandwidth for the RD point estimator is too large when the goal is\nto construct RBC confidence intervals with the smallest coverage error. We\nfurther optimize the constant terms behind the coverage error to derive new\noptimal choices for the auxiliary bandwidth required for RBC inference. Our\nexpansions also establish that RBC inference yields higher-order refinements\n(relative to traditional undersmoothing) in the context of RD designs. Our main\nresults cover sharp and sharp kink RD designs under conditional\nheteroskedasticity, and we discuss extensions to fuzzy and other RD designs,\nclustered sampling, and pre-intervention covariates adjustments. The\ntheoretical findings are illustrated with a Monte Carlo experiment and an\nempirical application, and the main methodological results are available in\n\\texttt{R} and \\texttt{Stata} packages.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.00236v4"
    },
    {
        "title": "Shape-Enforcing Operators for Point and Interval Estimators",
        "authors": [
            "Xi Chen",
            "Victor Chernozhukov",
            "Iván Fernández-Val",
            "Scott Kostyshak",
            "Ye Luo"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  A common problem in econometrics, statistics, and machine learning is to\nestimate and make inference on functions that satisfy shape restrictions. For\nexample, distribution functions are nondecreasing and range between zero and\none, height growth charts are nondecreasing in age, and production functions\nare nondecreasing and quasi-concave in input quantities. We propose a method to\nenforce these restrictions ex post on point and interval estimates of the\ntarget function by applying functional operators. If an operator satisfies\ncertain properties that we make precise, the shape-enforced point estimates are\ncloser to the target function than the original point estimates and the\nshape-enforced interval estimates have greater coverage and shorter length than\nthe original interval estimates. We show that these properties hold for six\ndifferent operators that cover commonly used shape restrictions in practice:\nrange, convexity, monotonicity, monotone convexity, quasi-convexity, and\nmonotone quasi-convexity. We illustrate the results with two empirical\napplications to the estimation of a height growth chart for infants in India\nand a production function for chemical firms in China.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.01038v5"
    },
    {
        "title": "Change-Point Testing for Risk Measures in Time Series",
        "authors": [
            "Lin Fan",
            "Peter W. Glynn",
            "Markus Pelger"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We propose novel methods for change-point testing for nonparametric\nestimators of expected shortfall and related risk measures in weakly dependent\ntime series. We can detect general multiple structural changes in the tails of\nmarginal distributions of time series under general assumptions.\nSelf-normalization allows us to avoid the issues of standard error estimation.\nThe theoretical foundations for our methods are functional central limit\ntheorems, which we develop under weak assumptions. An empirical study of S&P\n500 and US Treasury bond returns illustrates the practical use of our methods\nin detecting and quantifying market instability via the tails of financial time\nseries.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.02303v2"
    },
    {
        "title": "House Price Modeling with Digital Census",
        "authors": [
            "Enwei Zhu",
            "Stanislav Sobolevsky"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Urban house prices are strongly associated with local socioeconomic factors.\nIn literature, house price modeling is based on socioeconomic variables from\ntraditional census, which is not real-time, dynamic and comprehensive. Inspired\nby the emerging concept of \"digital census\" - using large-scale digital records\nof human activities to measure urban population dynamics and socioeconomic\nconditions, we introduce three typical datasets, namely 311 complaints, crime\ncomplaints and taxi trips, into house price modeling. Based on the individual\nhousing sales data in New York City, we provide comprehensive evidence that\nthese digital census datasets can substantially improve the modeling\nperformances on both house price levels and changes, regardless whether\ntraditional census is included or not. Hence, digital census can serve as both\neffective alternatives and complements to traditional census for house price\nmodeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.03834v1"
    },
    {
        "title": "Regression Discontinuity Designs Using Covariates",
        "authors": [
            "Sebastian Calonico",
            "Matias D. Cattaneo",
            "Max H. Farrell",
            "Rocio Titiunik"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We study regression discontinuity designs when covariates are included in the\nestimation. We examine local polynomial estimators that include discrete or\ncontinuous covariates in an additive separable way, but without imposing any\nparametric restrictions on the underlying population regression functions. We\nrecommend a covariate-adjustment approach that retains consistency under\nintuitive conditions, and characterize the potential for estimation and\ninference improvements. We also present new covariate-adjusted mean squared\nerror expansions and robust bias-corrected inference procedures, with\nheteroskedasticity-consistent and cluster-robust standard errors. An empirical\nillustration and an extensive simulation study is presented. All methods are\nimplemented in \\texttt{R} and \\texttt{Stata} software packages.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.03904v1"
    },
    {
        "title": "Valid Simultaneous Inference in High-Dimensional Settings (with the hdm\n  package for R)",
        "authors": [
            "Philipp Bach",
            "Victor Chernozhukov",
            "Martin Spindler"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Due to the increasing availability of high-dimensional empirical applications\nin many research disciplines, valid simultaneous inference becomes more and\nmore important. For instance, high-dimensional settings might arise in economic\nstudies due to very rich data sets with many potential covariates or in the\nanalysis of treatment heterogeneities. Also the evaluation of potentially more\ncomplicated (non-linear) functional forms of the regression relationship leads\nto many potential variables for which simultaneous inferential statements might\nbe of interest. Here we provide a review of classical and modern methods for\nsimultaneous inference in (high-dimensional) settings and illustrate their use\nby a case study using the R package hdm. The R package hdm implements valid\njoint powerful and efficient hypothesis tests for a potentially large number of\ncoeffcients as well as the construction of simultaneous confidence intervals\nand, therefore, provides useful methods to perform valid post-selection\ninference based on the LASSO.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.04951v1"
    },
    {
        "title": "Control Variables, Discrete Instruments, and Identification of\n  Structural Functions",
        "authors": [
            "Whitney Newey",
            "Sami Stouli"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Control variables provide an important means of controlling for endogeneity\nin econometric models with nonseparable and/or multidimensional heterogeneity.\nWe allow for discrete instruments, giving identification results under a\nvariety of restrictions on the way the endogenous variable and the control\nvariables affect the outcome. We consider many structural objects of interest,\nsuch as average or quantile treatment effects. We illustrate our results with\nan empirical application to Engel curve estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.05706v2"
    },
    {
        "title": "Focused econometric estimation for noisy and small datasets: A Bayesian\n  Minimum Expected Loss estimator approach",
        "authors": [
            "Andres Ramirez-Hassan",
            "Manuel Correa-Giraldo"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Central to many inferential situations is the estimation of rational\nfunctions of parameters. The mainstream in statistics and econometrics\nestimates these quantities based on the plug-in approach without consideration\nof the main objective of the inferential situation. We propose the Bayesian\nMinimum Expected Loss (MELO) approach focusing explicitly on the function of\ninterest, and calculating its frequentist variability. Asymptotic properties of\nthe MELO estimator are similar to the plug-in approach. Nevertheless,\nsimulation exercises show that our proposal is better in situations\ncharacterized by small sample sizes and noisy models. In addition, we observe\nin the applications that our approach gives lower standard errors than\nfrequently used alternatives when datasets are not very informative.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.06996v1"
    },
    {
        "title": "An Automated Approach Towards Sparse Single-Equation Cointegration\n  Modelling",
        "authors": [
            "Stephan Smeekes",
            "Etienne Wijler"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  In this paper we propose the Single-equation Penalized Error Correction\nSelector (SPECS) as an automated estimation procedure for dynamic\nsingle-equation models with a large number of potentially (co)integrated\nvariables. By extending the classical single-equation error correction model,\nSPECS enables the researcher to model large cointegrated datasets without\nnecessitating any form of pre-testing for the order of integration or\ncointegrating rank. Under an asymptotic regime in which both the number of\nparameters and time series observations jointly diverge to infinity, we show\nthat SPECS is able to consistently estimate an appropriate linear combination\nof the cointegrating vectors that may occur in the underlying DGP. In addition,\nSPECS is shown to enable the correct recovery of sparsity patterns in the\nparameter space and to posses the same limiting distribution as the OLS oracle\nprocedure. A simulation study shows strong selective capabilities, as well as\nsuperior predictive performance in the context of nowcasting compared to\nhigh-dimensional models that ignore cointegration. An empirical application to\nnowcasting Dutch unemployment rates using Google Trends confirms the strong\npractical performance of our procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.08889v3"
    },
    {
        "title": "Multivariate Stochastic Volatility Model with Realized Volatilities and\n  Pairwise Realized Correlations",
        "authors": [
            "Yuta Yamauchi",
            "Yasuhiro Omori"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Although stochastic volatility and GARCH (generalized autoregressive\nconditional heteroscedasticity) models have successfully described the\nvolatility dynamics of univariate asset returns, extending them to the\nmultivariate models with dynamic correlations has been difficult due to several\nmajor problems. First, there are too many parameters to estimate if available\ndata are only daily returns, which results in unstable estimates. One solution\nto this problem is to incorporate additional observations based on intraday\nasset returns, such as realized covariances. Second, since multivariate asset\nreturns are not synchronously traded, we have to use the largest time intervals\nsuch that all asset returns are observed in order to compute the realized\ncovariance matrices. However, in this study, we fail to make full use of the\navailable intraday informations when there are less frequently traded assets.\nThird, it is not straightforward to guarantee that the estimated (and the\nrealized) covariance matrices are positive definite. Our contributions are the\nfollowing: (1) we obtain the stable parameter estimates for the dynamic\ncorrelation models using the realized measures, (2) we make full use of\nintraday informations by using pairwise realized correlations, (3) the\ncovariance matrices are guaranteed to be positive definite, (4) we avoid the\narbitrariness of the ordering of asset returns, (5) we propose the flexible\ncorrelation structure model (e.g., such as setting some correlations to be zero\nif necessary), and (6) the parsimonious specification for the leverage effect\nis proposed. Our proposed models are applied to the daily returns of nine U.S.\nstocks with their realized volatilities and pairwise realized correlations and\nare shown to outperform the existing models with respect to portfolio\nperformances.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.09928v2"
    },
    {
        "title": "Complete Subset Averaging with Many Instruments",
        "authors": [
            "Seojeong Lee",
            "Youngki Shin"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We propose a two-stage least squares (2SLS) estimator whose first stage is\nthe equal-weighted average over a complete subset with $k$ instruments among\n$K$ available, which we call the complete subset averaging (CSA) 2SLS. The\napproximate mean squared error (MSE) is derived as a function of the subset\nsize $k$ by the Nagar (1959) expansion. The subset size is chosen by minimizing\nthe sample counterpart of the approximate MSE. We show that this method\nachieves the asymptotic optimality among the class of estimators with different\nsubset sizes. To deal with averaging over a growing set of irrelevant\ninstruments, we generalize the approximate MSE to find that the optimal $k$ is\nlarger than otherwise. An extensive simulation experiment shows that the\nCSA-2SLS estimator outperforms the alternative estimators when instruments are\ncorrelated. As an empirical illustration, we estimate the logistic demand\nfunction in Berry, Levinsohn, and Pakes (1995) and find the CSA-2SLS estimate\nis better supported by economic theory than the alternative estimates.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.08083v6"
    },
    {
        "title": "Heterogenous Coefficients, Discrete Instruments, and Identification of\n  Treatment Effects",
        "authors": [
            "Whitney K. Newey",
            "Sami Stouli"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Multidimensional heterogeneity and endogeneity are important features of a\nwide class of econometric models. We consider heterogenous coefficients models\nwhere the outcome is a linear combination of known functions of treatment and\nheterogenous coefficients. We use control variables to obtain identification\nresults for average treatment effects. With discrete instruments in a\ntriangular model we find that average treatment effects cannot be identified\nwhen the number of support points is less than or equal to the number of\ncoefficients. A sufficient condition for identification is that the second\nmoment matrix of the treatment functions given the control is nonsingular with\nprobability one. We relate this condition to identification of average\ntreatment effects with multiple treatments.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.09837v1"
    },
    {
        "title": "Estimation of a Heterogeneous Demand Function with Berkson Errors",
        "authors": [
            "Richard Blundell",
            "Joel Horowitz",
            "Matthias Parey"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  Berkson errors are commonplace in empirical microeconomics. In consumer\ndemand this form of measurement error occurs when the price an individual pays\nis measured by the (weighted) average price paid by individuals in a specified\ngroup (e.g., a county), rather than the true transaction price. We show the\nimportance of such measurement errors for the estimation of demand in a setting\nwith nonseparable unobserved heterogeneity. We develop a consistent estimator\nusing external information on the true distribution of prices. Examining the\ndemand for gasoline in the U.S., we document substantial within-market price\nvariability, and show that there are significant spatial differences in the\nmagnitude of Berkson errors across regions of the U.S. Accounting for Berkson\nerrors is found to be quantitatively important for estimating price effects and\nfor welfare calculations. Imposing the Slutsky shape constraint greatly reduces\nthe sensitivity to Berkson errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.10690v2"
    },
    {
        "title": "Simple Local Polynomial Density Estimators",
        "authors": [
            "Matias D. Cattaneo",
            "Michael Jansson",
            "Xinwei Ma"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  This paper introduces an intuitive and easy-to-implement nonparametric\ndensity estimator based on local polynomial techniques. The estimator is fully\nboundary adaptive and automatic, but does not require pre-binning or any other\ntransformation of the data. We study the main asymptotic properties of the\nestimator, and use these results to provide principled estimation, inference,\nand bandwidth selection methods. As a substantive application of our results,\nwe develop a novel discontinuity in density testing procedure, an important\nproblem in regression discontinuity designs and other program evaluation\nsettings. An illustrative empirical application is given. Two companion Stata\nand R software packages are provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.11512v2"
    },
    {
        "title": "Distribution Regression with Sample Selection, with an Application to\n  Wage Decompositions in the UK",
        "authors": [
            "Victor Chernozhukov",
            "Iván Fernández-Val",
            "Siyi Luo"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We develop a distribution regression model under endogenous sample selection.\nThis model is a semi-parametric generalization of the Heckman selection model.\nIt accommodates much richer effects of the covariates on outcome distribution\nand patterns of heterogeneity in the selection process, and allows for drastic\ndepartures from the Gaussian error structure, while maintaining the same level\ntractability as the classical model. The model applies to continuous, discrete\nand mixed outcomes. We provide identification, estimation, and inference\nmethods, and apply them to obtain wage decomposition for the UK. Here we\ndecompose the difference between the male and female wage distributions into\ncomposition, wage structure, selection structure, and selection sorting\neffects. After controlling for endogenous employment selection, we still find\nsubstantial gender wage gap -- ranging from 21% to 40% throughout the (latent)\noffered wage distribution that is not explained by composition. We also uncover\npositive sorting for single men and negative sorting for married women that\naccounts for a substantive fraction of the gender wage gap at the top of the\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.11603v6"
    },
    {
        "title": "Selection and the Distribution of Female Hourly Wages in the U.S",
        "authors": [
            "Iván Fernández-Val",
            "Franco Peracchi",
            "Aico van Vuuren",
            "Francis Vella"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We analyze the role of selection bias in generating the changes in the\nobserved distribution of female hourly wages in the United States using CPS\ndata for the years 1975 to 2020. We account for the selection bias from the\nemployment decision by modeling the distribution of the number of working hours\nand estimating a nonseparable model of wages. We decompose changes in the wage\ndistribution into composition, structural and selection effects. Composition\neffects have increased wages at all quantiles while the impact of the\nstructural effects varies by time period and quantile. Changes in the role of\nselection only appear at the lower quantiles of the wage distribution. The\nevidence suggests that there is positive selection in the 1970s which\ndiminishes until the later 1990s. This reduces wages at lower quantiles and\nincreases wage inequality. Post 2000 there appears to be an increase in\npositive sorting which reduces the selection effects on wage inequality.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.00419v5"
    },
    {
        "title": "Modeling Dynamic Transport Network with Matrix Factor Models: with an\n  Application to International Trade Flow",
        "authors": [
            "Elynn Y. Chen",
            "Rong Chen"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  International trade research plays an important role to inform trade policy\nand shed light on wider issues relating to poverty, development, migration,\nproductivity, and economy. With recent advances in information technology,\nglobal and regional agencies distribute an enormous amount of internationally\ncomparable trading data among a large number of countries over time, providing\na goldmine for empirical analysis of international trade. Meanwhile, an array\nof new statistical methods are recently developed for dynamic network analysis.\nHowever, these advanced methods have not been utilized for analyzing such\nmassive dynamic cross-country trading data. International trade data can be\nviewed as a dynamic transport network because it emphasizes the amount of goods\nmoving across a network. Most literature on dynamic network analysis\nconcentrates on the connectivity network that focuses on link formation or\ndeformation rather than the transport moving across the network. We take a\ndifferent perspective from the pervasive node-and-edge level modeling: the\ndynamic transport network is modeled as a time series of relational matrices.\nWe adopt a matrix factor model of \\cite{wang2018factor}, with a specific\ninterpretation for the dynamic transport network. Under the model, the observed\nsurface network is assumed to be driven by a latent dynamic transport network\nwith lower dimensions. The proposed method is able to unveil the latent dynamic\nstructure and achieve the objective of dimension reduction. We applied the\nproposed framework and methodology to a data set of monthly trading volumes\namong 24 countries and regions from 1982 to 2015. Our findings shed light on\ntrading hubs, centrality, trends and patterns of international trade and show\nmatching change points to trading policies. The dataset also provides a fertile\nground for future research on international trade.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.00769v1"
    },
    {
        "title": "Mastering Panel 'Metrics: Causal Impact of Democracy on Growth",
        "authors": [
            "Shuowen Chen",
            "Victor Chernozhukov",
            "Iván Fernández-Val"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The relationship between democracy and economic growth is of long-standing\ninterest. We revisit the panel data analysis of this relationship by Acemoglu,\nNaidu, Restrepo and Robinson (forthcoming) using state of the art econometric\nmethods. We argue that this and lots of other panel data settings in economics\nare in fact high-dimensional, resulting in principal estimators -- the fixed\neffects (FE) and Arellano-Bond (AB) estimators -- to be biased to the degree\nthat invalidates statistical inference. We can however remove these biases by\nusing simple analytical and sample-splitting methods, and thereby restore valid\nstatistical inference. We find that the debiased FE and AB estimators produce\nsubstantially higher estimates of the long-run effect of democracy on growth,\nproviding even stronger support for the key hypothesis in Acemoglu, Naidu,\nRestrepo and Robinson (forthcoming). Given the ubiquitous nature of panel data,\nwe conclude that the use of debiased panel data estimators should substantially\nimprove the quality of empirical inference in economics.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.03821v1"
    },
    {
        "title": "A dynamic factor model approach to incorporate Big Data in state space\n  models for official statistics",
        "authors": [
            "Caterina Schiavoni",
            "Franz Palm",
            "Stephan Smeekes",
            "Jan van den Brakel"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In this paper we consider estimation of unobserved components in state space\nmodels using a dynamic factor approach to incorporate auxiliary information\nfrom high-dimensional data sources. We apply the methodology to unemployment\nestimation as done by Statistics Netherlands, who uses a multivariate state\nspace model to produce monthly figures for the unemployment using series\nobserved with the labour force survey (LFS). We extend the model by including\nauxiliary series of Google Trends about job-search and economic uncertainty,\nand claimant counts, partially observed at higher frequencies. Our factor model\nallows for nowcasting the variable of interest, providing reliable unemployment\nestimates in real-time before LFS data become available.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.11355v2"
    },
    {
        "title": "Permutation inference with a finite number of heterogeneous clusters",
        "authors": [
            "Andreas Hagemann"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  I introduce a simple permutation procedure to test conventional (non-sharp)\nhypotheses about the effect of a binary treatment in the presence of a finite\nnumber of large, heterogeneous clusters when the treatment effect is identified\nby comparisons across clusters. The procedure asymptotically controls size by\napplying a level-adjusted permutation test to a suitable statistic. The\nadjustments needed for most empirically relevant situations are tabulated in\nthe paper. The adjusted permutation test is easy to implement in practice and\nperforms well at conventional levels of significance with at least four treated\nclusters and a similar number of control clusters. It is particularly robust to\nsituations where some clusters are much more variable than others. Examples and\nan empirical application are provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.01049v2"
    },
    {
        "title": "An Econometric Perspective on Algorithmic Subsampling",
        "authors": [
            "Sokbae Lee",
            "Serena Ng"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Datasets that are terabytes in size are increasingly common, but computer\nbottlenecks often frustrate a complete analysis of the data. While more data\nare better than less, diminishing returns suggest that we may not need\nterabytes of data to estimate a parameter or test a hypothesis. But which rows\nof data should we analyze, and might an arbitrary subset of rows preserve the\nfeatures of the original data? This paper reviews a line of work that is\ngrounded in theoretical computer science and numerical linear algebra, and\nwhich finds that an algorithmically desirable sketch, which is a randomly\nchosen subset of the data, must preserve the eigenstructure of the data, a\nproperty known as a subspace embedding. Building on this work, we study how\nprediction and inference can be affected by data sketching within a linear\nregression setup. We show that the sketching error is small compared to the\nsample size effect which a researcher can control. As a sketch size that is\nalgorithmically optimal may not be suitable for prediction and inference, we\nuse statistical arguments to provide 'inference conscious' guides to the sketch\nsize. When appropriately implemented, an estimator that pools over different\nsketches can be nearly as efficient as the infeasible one using the full\nsample.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.01954v4"
    },
    {
        "title": "Heterogeneous Regression Models for Clusters of Spatial Dependent Data",
        "authors": [
            "Zhihua Ma",
            "Yishu Xue",
            "Guanyu Hu"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In economic development, there are often regions that share similar economic\ncharacteristics, and economic models on such regions tend to have similar\ncovariate effects. In this paper, we propose a Bayesian clustered regression\nfor spatially dependent data in order to detect clusters in the covariate\neffects. Our proposed method is based on the Dirichlet process which provides a\nprobabilistic framework for simultaneous inference of the number of clusters\nand the clustering configurations. The usage of our method is illustrated both\nin simulation studies and an application to a housing cost dataset of Georgia.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.02212v4"
    },
    {
        "title": "Identification and Estimation of Discrete Choice Models with Unobserved\n  Choice Sets",
        "authors": [
            "Victor H. Aguiar",
            "Nail Kashaev"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  We propose a framework for nonparametric identification and estimation of\ndiscrete choice models with unobserved choice sets. We recover the joint\ndistribution of choice sets and preferences from a panel dataset on choices. We\nassume that either the latent choice sets are sparse or that the panel is\nsufficiently long. Sparsity requires the number of possible choice sets to be\nrelatively small. It is satisfied, for instance, when the choice sets are\nnested, or when they form a partition. Our estimation procedure is\ncomputationally fast and uses mixed-integer optimization to recover the sparse\nsupport of choice sets. Analyzing the ready-to-eat cereal industry using a\nhousehold scanner dataset, we find that ignoring the unobservability of choice\nsets can lead to biased estimates of preferences due to significant latent\nheterogeneity in choice sets.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.04853v3"
    },
    {
        "title": "Information processing constraints in travel behaviour modelling: A\n  generative learning approach",
        "authors": [
            "Melvin Wong",
            "Bilal Farooq"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Travel decisions tend to exhibit sensitivity to uncertainty and information\nprocessing constraints. These behavioural conditions can be characterized by a\ngenerative learning process. We propose a data-driven generative model version\nof rational inattention theory to emulate these behavioural representations. We\noutline the methodology of the generative model and the associated learning\nprocess as well as provide an intuitive explanation of how this process\ncaptures the value of prior information in the choice utility specification. We\ndemonstrate the effects of information heterogeneity on a travel choice,\nanalyze the econometric interpretation, and explore the properties of our\ngenerative model. Our findings indicate a strong correlation with rational\ninattention behaviour theory, which suggest that individuals may ignore certain\nexogenous variables and rely on prior information for evaluating decisions\nunder uncertainty. Finally, the principles demonstrated in this study can be\nformulated as a generalized entropy and utility based multinomial logit model.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.07036v2"
    },
    {
        "title": "Shrinkage in the Time-Varying Parameter Model Framework Using the R\n  Package shrinkTVP",
        "authors": [
            "Peter Knaus",
            "Angela Bitto-Nemling",
            "Annalisa Cadonna",
            "Sylvia Frühwirth-Schnatter"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Time-varying parameter (TVP) models are widely used in time series analysis\nto flexibly deal with processes which gradually change over time. However, the\nrisk of overfitting in TVP models is well known. This issue can be dealt with\nusing appropriate global-local shrinkage priors, which pull time-varying\nparameters towards static ones. In this paper, we introduce the R package\nshrinkTVP (Knaus, Bitto-Nemling, Cadonna, and Fr\\\"uhwirth-Schnatter 2019),\nwhich provides a fully Bayesian implementation of shrinkage priors for TVP\nmodels, taking advantage of recent developments in the literature, in\nparticular that of Bitto and Fr\\\"uhwirth-Schnatter (2019). The package\nshrinkTVP allows for posterior simulation of the parameters through an\nefficient Markov Chain Monte Carlo (MCMC) scheme. Moreover, summary and\nvisualization methods, as well as the possibility of assessing predictive\nperformance through log predictive density scores (LPDSs), are provided. The\ncomputationally intensive tasks have been implemented in C++ and interfaced\nwith R. The paper includes a brief overview of the models and shrinkage priors\nimplemented in the package. Furthermore, core functionalities are illustrated,\nboth with simulated and real data.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.07065v3"
    },
    {
        "title": "A Vine-copula extension for the HAR model",
        "authors": [
            "Martin Magris"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  The heterogeneous autoregressive (HAR) model is revised by modeling the joint\ndistribution of the four partial-volatility terms therein involved. Namely,\ntoday's, yesterday's, last week's and last month's volatility components. The\njoint distribution relies on a (C-) Vine copula construction, allowing to\nconveniently extract volatility forecasts based on the conditional expectation\nof today's volatility given its past terms. The proposed empirical application\ninvolves more than seven years of high-frequency transaction prices for ten\nstocks and evaluates the in-sample, out-of-sample and one-step-ahead forecast\nperformance of our model for daily realized-kernel measures. The model proposed\nin this paper is shown to outperform the HAR counterpart under different models\nfor marginal distributions, copula construction methods, and forecasting\nsettings.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.08522v1"
    },
    {
        "title": "Predicting credit default probabilities using machine learning\n  techniques in the face of unequal class distributions",
        "authors": [
            "Anna Stelzer"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This study conducts a benchmarking study, comparing 23 different statistical\nand machine learning methods in a credit scoring application. In order to do\nso, the models' performance is evaluated over four different data sets in\ncombination with five data sampling strategies to tackle existing class\nimbalances in the data. Six different performance measures are used to cover\ndifferent aspects of predictive performance. The results indicate a strong\nsuperiority of ensemble methods and show that simple sampling strategies\ndeliver better results than more sophisticated ones.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12996v1"
    },
    {
        "title": "Boosting High Dimensional Predictive Regressions with Time Varying\n  Parameters",
        "authors": [
            "Kashif Yousuf",
            "Serena Ng"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  High dimensional predictive regressions are useful in wide range of\napplications. However, the theory is mainly developed assuming that the model\nis stationary with time invariant parameters. This is at odds with the\nprevalent evidence for parameter instability in economic time series, but\ntheories for parameter instability are mainly developed for models with a small\nnumber of covariates. In this paper, we present two $L_2$ boosting algorithms\nfor estimating high dimensional models in which the coefficients are modeled as\nfunctions evolving smoothly over time and the predictors are locally\nstationary. The first method uses componentwise local constant estimators as\nbase learner, while the second relies on componentwise local linear estimators.\nWe establish consistency of both methods, and address the practical issues of\nchoosing the bandwidth for the base learners and the number of boosting\niterations. In an extensive application to macroeconomic forecasting with many\npotential predictors, we find that the benefits to modeling time variation are\nsubstantial and they increase with the forecast horizon. Furthermore, the\ntiming of the benefits suggests that the Great Moderation is associated with\nsubstantial instability in the conditional mean of various economic series.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.03109v1"
    },
    {
        "title": "Application of Machine Learning in Forecasting International Trade\n  Trends",
        "authors": [
            "Feras Batarseh",
            "Munisamy Gopinath",
            "Ganesh Nalluru",
            "Jayson Beckman"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  International trade policies have recently garnered attention for limiting\ncross-border exchange of essential goods (e.g. steel, aluminum, soybeans, and\nbeef). Since trade critically affects employment and wages, predicting future\npatterns of trade is a high-priority for policy makers around the world. While\ntraditional economic models aim to be reliable predictors, we consider the\npossibility that Machine Learning (ML) techniques allow for better predictions\nto inform policy decisions. Open-government data provide the fuel to power the\nalgorithms that can explain and forecast trade flows to inform policies. Data\ncollected in this article describe international trade transactions and\ncommonly associated economic factors. Machine learning (ML) models deployed\ninclude: ARIMA, GBoosting, XGBoosting, and LightGBM for predicting future trade\npatterns, and K-Means clustering of countries according to economic factors.\nUnlike short-term and subjective (straight-line) projections and medium-term\n(aggre-gated) projections, ML methods provide a range of data-driven and\ninterpretable projections for individual commodities. Models, their results,\nand policies are introduced and evaluated for prediction quality.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.03112v1"
    },
    {
        "title": "Latent Dirichlet Analysis of Categorical Survey Responses",
        "authors": [
            "Evan Munro",
            "Serena Ng"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Beliefs are important determinants of an individual's choices and economic\noutcomes, so understanding how they comove and differ across individuals is of\nconsiderable interest. Researchers often rely on surveys that report individual\nbeliefs as qualitative data. We propose using a Bayesian hierarchical latent\nclass model to analyze the comovements and observed heterogeneity in\ncategorical survey responses. We show that the statistical model corresponds to\nan economic structural model of information acquisition, which guides\ninterpretation and estimation of the model parameters. An algorithm based on\nstochastic optimization is proposed to estimate a model for repeated surveys\nwhen responses follow a dynamic structure and conjugate priors are not\nappropriate. Guidance on selecting the number of belief types is also provided.\nTwo examples are considered. The first shows that there is information in the\nMichigan survey responses beyond the consumer sentiment index that is\nofficially published. The second shows that belief types constructed from\nsurvey responses can be used in a subsequent analysis to estimate heterogeneous\nreturns to education.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.04883v3"
    },
    {
        "title": "Identifying Network Ties from Panel Data: Theory and an Application to\n  Tax Competition",
        "authors": [
            "Aureo de Paula",
            "Imran Rasul",
            "Pedro Souza"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Social interactions determine many economic behaviors, but information on\nsocial ties does not exist in most publicly available and widely used datasets.\nWe present results on the identification of social networks from observational\npanel data that contains no information on social ties between agents. In the\ncontext of a canonical social interactions model, we provide sufficient\nconditions under which the social interactions matrix, endogenous and exogenous\nsocial effect parameters are all globally identified. While this result is\nrelevant across different estimation strategies, we then describe how\nhigh-dimensional estimation techniques can be used to estimate the interactions\nmodel based on the Adaptive Elastic Net GMM method. We employ the method to\nstudy tax competition across US states. We find the identified social\ninteractions matrix implies tax competition differs markedly from the common\nassumption of competition between geographically neighboring states, providing\nfurther insights for the long-standing debate on the relative roles of factor\nmobility and yardstick competition in driving tax setting behavior across\nstates. Most broadly, our identification and application show the analysis of\nsocial interactions can be extended to economic realms where no network data\nexists.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.07452v4"
    },
    {
        "title": "Multi-Stage Compound Real Options Valuation in Residential PV-Battery\n  Investment",
        "authors": [
            "Yiju Ma",
            "Kevin Swandi",
            "Archie Chapman",
            "Gregor Verbic"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  Strategic valuation of efficient and well-timed network investments under\nuncertain electricity market environment has become increasingly challenging,\nbecause there generally exist multiple interacting options in these\ninvestments, and failing to systematically consider these options can lead to\ndecisions that undervalue the investment. In our work, a real options valuation\n(ROV) framework is proposed to determine the optimal strategy for executing\nmultiple interacting options within a distribution network investment, to\nmitigate the risk of financial losses in the presence of future uncertainties.\nTo demonstrate the characteristics of the proposed framework, we determine the\noptimal strategy to economically justify the investment in residential\nPV-battery systems for additional grid supply during peak demand periods. The\noptions to defer, and then expand, are considered as multi-stage compound\noptions, since the option to expand is a subsequent option of the former. These\noptions are valued via the least squares Monte Carlo method, incorporating\nuncertainty over growing power demand, varying diesel fuel price, and the\ndeclining cost of PV-battery technology as random variables. Finally, a\nsensitivity analysis is performed to demonstrate how the proposed framework\nresponds to uncertain events. The proposed framework shows that executing the\ninteracting options at the optimal timing increases the investment value.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.09132v1"
    },
    {
        "title": "Fast and Flexible Bayesian Inference in Time-varying Parameter\n  Regression Models",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber",
            "Gary Koop",
            "Luca Onorante"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In this paper, we write the time-varying parameter (TVP) regression model\ninvolving K explanatory variables and T observations as a constant coefficient\nregression model with KT explanatory variables. In contrast with much of the\nexisting literature which assumes coefficients to evolve according to a random\nwalk, a hierarchical mixture model on the TVPs is introduced. The resulting\nmodel closely mimics a random coefficients specification which groups the TVPs\ninto several regimes. These flexible mixtures allow for TVPs that feature a\nsmall, moderate or large number of structural breaks. We develop\ncomputationally efficient Bayesian econometric methods based on the singular\nvalue decomposition of the KT regressors. In artificial data, we find our\nmethods to be accurate and much faster than standard approaches in terms of\ncomputation time. In an empirical exercise involving inflation forecasting\nusing a large number of predictors, we find our models to forecast better than\nalternative approaches and document different patterns of parameter change than\nare found with approaches which assume random walk evolution of parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.10779v4"
    },
    {
        "title": "Analyzing China's Consumer Price Index Comparatively with that of United\n  States",
        "authors": [
            "Zhenzhong Wang",
            "Yundong Tu",
            "Song Xi Chen"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  This paper provides a thorough analysis on the dynamic structures and\npredictability of China's Consumer Price Index (CPI-CN), with a comparison to\nthose of the United States. Despite the differences in the two leading\neconomies, both series can be well modeled by a class of Seasonal\nAutoregressive Integrated Moving Average Model with Covariates (S-ARIMAX). The\nCPI-CN series possess regular patterns of dynamics with stable annual cycles\nand strong Spring Festival effects, with fitting and forecasting errors largely\ncomparable to their US counterparts. Finally, for the CPI-CN, the diffusion\nindex (DI) approach offers improved predictions than the S-ARIMAX models.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.13301v1"
    },
    {
        "title": "Estimating the Effect of Central Bank Independence on Inflation Using\n  Longitudinal Targeted Maximum Likelihood Estimation",
        "authors": [
            "Philipp F. M. Baumann",
            "Michael Schomaker",
            "Enzo Rossi"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The notion that an independent central bank reduces a country's inflation is\na controversial hypothesis. To date, it has not been possible to satisfactorily\nanswer this question because the complex macroeconomic structure that gives\nrise to the data has not been adequately incorporated into statistical\nanalyses. We develop a causal model that summarizes the economic process of\ninflation. Based on this causal model and recent data, we discuss and identify\nthe assumptions under which the effect of central bank independence on\ninflation can be identified and estimated. Given these and alternative\nassumptions, we estimate this effect using modern doubly robust effect\nestimators, i.e., longitudinal targeted maximum likelihood estimators. The\nestimation procedure incorporates machine learning algorithms and is tailored\nto address the challenges associated with complex longitudinal macroeconomic\ndata. We do not find strong support for the hypothesis that having an\nindependent central bank for a long period of time necessarily lowers\ninflation. Simulation studies evaluate the sensitivity of the proposed methods\nin complex settings when certain assumptions are violated and highlight the\nimportance of working with appropriate learning algorithms for estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.02208v7"
    },
    {
        "title": "Impact of Congestion Charge and Minimum Wage on TNCs: A Case Study for\n  San Francisco",
        "authors": [
            "Sen Li",
            "Kameshwar Poolla",
            "Pravin Varaiya"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper describes the impact on transportation network companies (TNCs) of\nthe imposition of a congestion charge and a driver minimum wage. The impact is\nassessed using a market equilibrium model to calculate the changes in the\nnumber of passenger trips and trip fare, number of drivers employed, the TNC\nplatform profit, the number of TNC vehicles, and city revenue. Two charges are\nconsidered: (a) a charge per TNC trip similar to an excise tax, and (b) a\ncharge per vehicle operating hour (whether or not it has a passenger) similar\nto a road tax. Both charges reduce the number of TNC trips, but this reduction\nis limited by the wage floor, and the number of TNC vehicles reduced is not\nsignificant. The time-based charge is preferable to the trip-based charge\nsince, by penalizing idle vehicle time, the former increases vehicle occupancy.\nIn a case study for San Francisco, the time-based charge is found to be Pareto\nsuperior to the trip-based charge as it yields higher passenger surplus, higher\nplatform profits, and higher tax revenue for the city.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.02550v4"
    },
    {
        "title": "Backward CUSUM for Testing and Monitoring Structural Change with an\n  Application to COVID-19 Pandemic Data",
        "authors": [
            "Sven Otto",
            "Jörg Breitung"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  It is well known that the conventional cumulative sum (CUSUM) test suffers\nfrom low power and large detection delay. In order to improve the power of the\ntest, we propose two alternative statistics. The backward CUSUM detector\nconsiders the recursive residuals in reverse chronological order, whereas the\nstacked backward CUSUM detector sequentially cumulates a triangular array of\nbackwardly cumulated residuals. A multivariate invariance principle for partial\nsums of recursive residuals is given, and the limiting distributions of the\ntest statistics are derived under local alternatives. In the retrospective\ncontext, the local power of the tests is shown to be substantially higher than\nthat of the conventional CUSUM test if a break occurs in the middle or at the\nend of the sample. When applied to monitoring schemes, the detection delay of\nthe stacked backward CUSUM is found to be much shorter than that of the\nconventional monitoring CUSUM procedure. Furthermore, we propose an estimator\nof the break date based on the backward CUSUM detector and show that in\nmonitoring exercises this estimator tends to outperform the usual maximum\nlikelihood estimator. Finally, an application of the methodology to COVID-19\ndata is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.02682v3"
    },
    {
        "title": "Complete Subset Averaging for Quantile Regressions",
        "authors": [
            "Ji Hyung Lee",
            "Youngki Shin"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose a novel conditional quantile prediction method based on complete\nsubset averaging (CSA) for quantile regressions. All models under consideration\nare potentially misspecified and the dimension of regressors goes to infinity\nas the sample size increases. Since we average over the complete subsets, the\nnumber of models is much larger than the usual model averaging method which\nadopts sophisticated weighting schemes. We propose to use an equal weight but\nselect the proper size of the complete subset based on the leave-one-out\ncross-validation method. Building upon the theory of Lu and Su (2015), we\ninvestigate the large sample properties of CSA and show the asymptotic\noptimality in the sense of Li (1987). We check the finite sample performance\nvia Monte Carlo simulations and empirical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.03299v3"
    },
    {
        "title": "Targeting customers under response-dependent costs",
        "authors": [
            "Johannes Haupt",
            "Stefan Lessmann"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This study provides a formal analysis of the customer targeting problem when\nthe cost for a marketing action depends on the customer response and proposes a\nframework to estimate the decision variables for campaign profit optimization.\nTargeting a customer is profitable if the impact and associated profit of the\nmarketing treatment are higher than its cost. Despite the growing literature on\nuplift models to identify the strongest treatment-responders, no research has\ninvestigated optimal targeting when the costs of the treatment are unknown at\nthe time of the targeting decision. Stochastic costs are ubiquitous in direct\nmarketing and customer retention campaigns because marketing incentives are\nconditioned on a positive customer response. This study makes two contributions\nto the literature, which are evaluated on an e-commerce coupon targeting\ncampaign. First, we formally analyze the targeting decision problem under\nresponse-dependent costs. Profit-optimal targeting requires an estimate of the\ntreatment effect on the customer and an estimate of the customer response\nprobability under treatment. The empirical results demonstrate that the\nconsideration of treatment cost substantially increases campaign profit when\nused for customer targeting in combination with an estimate of the average or\ncustomer-level treatment effect. Second, we propose a framework to jointly\nestimate the treatment effect and the response probability by combining methods\nfor causal inference with a hurdle mixture model. The proposed causal hurdle\nmodel achieves competitive campaign profit while streamlining model building.\nCode is available at https://github.com/Humboldt-WI/response-dependent-costs.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.06271v2"
    },
    {
        "title": "Stochastic Frontier Analysis with Generalized Errors: inference, model\n  comparison and averaging",
        "authors": [
            "Kamil Makieła",
            "Błażej Mazur"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Contribution of this paper lies in the formulation and estimation of a\ngeneralized model for stochastic frontier analysis (SFA) that nests virtually\nall forms used and includes some that have not been considered so far. The\nmodel is based on the generalized t distribution for the observation error and\nthe generalized beta distribution of the second kind for the\ninefficiency-related term. We use this general error structure framework for\nformal testing, to compare alternative specifications and to conduct model\naveraging. This allows us to deal with model specification uncertainty, which\nis one of the main unresolved issues in SFA, and to relax a number of\npotentially restrictive assumptions embedded within existing SF models. We also\ndevelop Bayesian inference methods that are less restrictive compared to the\nones used so far and demonstrate feasible approximate alternatives based on\nmaximum likelihood.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.07150v2"
    },
    {
        "title": "Testing Many Restrictions Under Heteroskedasticity",
        "authors": [
            "Stanislav Anatolyev",
            "Mikkel Sølvsten"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose a hypothesis test that allows for many tested restrictions in a\nheteroskedastic linear regression model. The test compares the conventional F\nstatistic to a critical value that corrects for many restrictions and\nconditional heteroskedasticity. This correction uses leave-one-out estimation\nto correctly center the critical value and leave-three-out estimation to\nappropriately scale it. The large sample properties of the test are established\nin an asymptotic framework where the number of tested restrictions may be fixed\nor may grow with the sample size, and can even be proportional to the number of\nobservations. We show that the test is asymptotically valid and has non-trivial\nasymptotic power against the same local alternatives as the exact F test when\nthe latter is valid. Simulations corroborate these theoretical findings and\nsuggest excellent size control in moderately small samples, even under strong\nheteroskedasticity.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.07320v3"
    },
    {
        "title": "Experimental Design under Network Interference",
        "authors": [
            "Davide Viviano"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper studies the design of two-wave experiments in the presence of\nspillover effects when the researcher aims to conduct precise inference on\ntreatment effects. We consider units connected through a single network, local\ndependence among individuals, and a general class of estimands encompassing\naverage treatment and average spillover effects. We introduce a statistical\nframework for designing two-wave experiments with networks, where the\nresearcher optimizes over participants and treatment assignments to minimize\nthe variance of the estimators of interest, using a first-wave (pilot)\nexperiment to estimate the variance. We derive guarantees for inference on\ntreatment effects and regret guarantees on the variance obtained from the\nproposed design mechanism. Our results illustrate the existence of a trade-off\nin the choice of the pilot study and formally characterize the pilot's size\nrelative to the main experiment. Simulations using simulated and real-world\nnetworks illustrate the advantages of the method.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.08421v4"
    },
    {
        "title": "Sequential monitoring for cointegrating regressions",
        "authors": [
            "Lorenzo Trapani",
            "Emily Whitehouse"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We develop monitoring procedures for cointegrating regressions, testing the\nnull of no breaks against the alternatives that there is either a change in the\nslope, or a change to non-cointegration. After observing the regression for a\ncalibration sample m, we study a CUSUM-type statistic to detect the presence of\nchange during a monitoring horizon m+1,...,T. Our procedures use a class of\nboundary functions which depend on a parameter whose value affects the delay in\ndetecting the possible break. Technically, these procedures are based on almost\nsure limiting theorems whose derivation is not straightforward. We therefore\ndefine a monitoring function which - at every point in time - diverges to\ninfinity under the null, and drifts to zero under alternatives. We cast this\nsequence in a randomised procedure to construct an i.i.d. sequence, which we\nthen employ to define the detector function. Our monitoring procedure rejects\nthe null of no break (when correct) with a small probability, whilst it rejects\nwith probability one over the monitoring horizon in the presence of breaks.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.12182v1"
    },
    {
        "title": "Specification tests for generalized propensity scores using double\n  projections",
        "authors": [
            "Pedro H. C. Sant'Anna",
            "Xiaojun Song"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper proposes a new class of nonparametric tests for the correct\nspecification of models based on conditional moment restrictions, paying\nparticular attention to generalized propensity score models. The test procedure\nis based on two different projection arguments, leading to test statistics that\nare suitable to setups with many covariates, and are (asymptotically) invariant\nto the estimation method used to estimate the nuisance parameters. We show that\nour proposed tests are able to detect a broad class of local alternatives\nconverging to the null at the usual parametric rate and illustrate its\nattractive power properties via simulations. We also extend our proposal to\ntest parametric or semiparametric single-index-type models.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13803v2"
    },
    {
        "title": "Robust Empirical Bayes Confidence Intervals",
        "authors": [
            "Timothy B. Armstrong",
            "Michal Kolesár",
            "Mikkel Plagborg-Møller"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We construct robust empirical Bayes confidence intervals (EBCIs) in a normal\nmeans problem. The intervals are centered at the usual linear empirical Bayes\nestimator, but use a critical value accounting for shrinkage. Parametric EBCIs\nthat assume a normal distribution for the means (Morris, 1983b) may\nsubstantially undercover when this assumption is violated. In contrast, our\nEBCIs control coverage regardless of the means distribution, while remaining\nclose in length to the parametric EBCIs when the means are indeed Gaussian. If\nthe means are treated as fixed, our EBCIs have an average coverage guarantee:\nthe coverage probability is at least $1 - \\alpha$ on average across the $n$\nEBCIs for each of the means. Our empirical application considers the effects of\nU.S. neighborhoods on intergenerational mobility.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.03448v4"
    },
    {
        "title": "Forecasts with Bayesian vector autoregressions under real time\n  conditions",
        "authors": [
            "Michael Pfarrhofer"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper investigates the sensitivity of forecast performance measures to\ntaking a real time versus pseudo out-of-sample perspective. We use monthly\nvintages for the United States (US) and the Euro Area (EA) and estimate a set\nof vector autoregressive (VAR) models of different sizes with constant and\ntime-varying parameters (TVPs) and stochastic volatility (SV). Our results\nsuggest differences in the relative ordering of model performance for point and\ndensity forecasts depending on whether real time data or truncated final\nvintages in pseudo out-of-sample simulations are used for evaluating forecasts.\nNo clearly superior specification for the US or the EA across variable types\nand forecast horizons can be identified, although larger models featuring TVPs\nappear to be affected the least by missing values and data revisions. We\nidentify substantial differences in performance metrics with respect to whether\nforecasts are produced for the US or the EA.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.04984v1"
    },
    {
        "title": "Multi-frequency-band tests for white noise under heteroskedasticity",
        "authors": [
            "Mengya Liu",
            "Fukan Zhu",
            "Ke Zhu"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper proposes a new family of multi-frequency-band (MFB) tests for the\nwhite noise hypothesis by using the maximum overlap discrete wavelet packet\ntransform (MODWPT). The MODWPT allows the variance of a process to be\ndecomposed into the variance of its components on different equal-length\nfrequency sub-bands, and the MFB tests then measure the distance between the\nMODWPT-based variance ratio and its theoretical null value jointly over several\nfrequency sub-bands. The resulting MFB tests have the chi-squared asymptotic\nnull distributions under mild conditions, which allow the data to be\nheteroskedastic. The MFB tests are shown to have the desirable size and power\nperformance by simulation studies, and their usefulness is further illustrated\nby two applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.09161v1"
    },
    {
        "title": "Non-linear interlinkages and key objectives amongst the Paris Agreement\n  and the Sustainable Development Goals",
        "authors": [
            "Felix Laumann",
            "Julius von Kügelgen",
            "Mauricio Barahona"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The United Nations' ambitions to combat climate change and prosper human\ndevelopment are manifested in the Paris Agreement and the Sustainable\nDevelopment Goals (SDGs), respectively. These are inherently inter-linked as\nprogress towards some of these objectives may accelerate or hinder progress\ntowards others. We investigate how these two agendas influence each other by\ndefining networks of 18 nodes, consisting of the 17 SDGs and climate change,\nfor various groupings of countries. We compute a non-linear measure of\nconditional dependence, the partial distance correlation, given any subset of\nthe remaining 16 variables. These correlations are treated as weights on edges,\nand weighted eigenvector centralities are calculated to determine the most\nimportant nodes. We find that SDG 6, clean water and sanitation, and SDG 4,\nquality education, are most central across nearly all groupings of countries.\nIn developing regions, SDG 17, partnerships for the goals, is strongly\nconnected to the progress of other objectives in the two agendas whilst,\nsomewhat surprisingly, SDG 8, decent work and economic growth, is not as\nimportant in terms of eigenvector centrality.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.09318v1"
    },
    {
        "title": "Revealing Cluster Structures Based on Mixed Sampling Frequencies",
        "authors": [
            "Yeonwoo Rho",
            "Yun Liu",
            "Hie Joo Ahn"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  This paper proposes a new linearized mixed data sampling (MIDAS) model and\ndevelops a framework to infer clusters in a panel regression with mixed\nfrequency data. The linearized MIDAS estimation method is more flexible and\nsubstantially simpler to implement than competing approaches. We show that the\nproposed clustering algorithm successfully recovers true membership in the\ncross-section, both in theory and in simulations, without requiring prior\nknowledge of the number of clusters. This methodology is applied to a\nmixed-frequency Okun's law model for state-level data in the U.S. and uncovers\nfour meaningful clusters based on the dynamic features of state-level labor\nmarkets.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.09770v2"
    },
    {
        "title": "Maximum Likelihood Estimation of Stochastic Frontier Models with\n  Endogeneity",
        "authors": [
            "Samuele Centorrino",
            "María Pérez-Urdiales"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose and study a maximum likelihood estimator of stochastic frontier\nmodels with endogeneity in cross-section data when the composite error term may\nbe correlated with inputs and environmental variables. Our framework is a\ngeneralization of the normal half-normal stochastic frontier model with\nendogeneity. We derive the likelihood function in closed form using three\nfundamental assumptions: the existence of control functions that fully capture\nthe dependence between regressors and unobservables; the conditional\nindependence of the two error components given the control functions; and the\nconditional distribution of the stochastic inefficiency term given the control\nfunctions being a folded normal distribution. We also provide a Battese-Coelli\nestimator of technical efficiency. Our estimator is computationally fast and\neasy to implement. We study some of its asymptotic properties, and we showcase\nits finite sample behavior in Monte-Carlo simulations and an empirical\napplication to farmers in Nepal.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12369v3"
    },
    {
        "title": "Structural Regularization",
        "authors": [
            "Jiaming Mao",
            "Zhesheng Zheng"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose a novel method for modeling data by using structural models based\non economic theory as regularizers for statistical models. We show that even if\na structural model is misspecified, as long as it is informative about the\ndata-generating mechanism, our method can outperform both the (misspecified)\nstructural model and un-structural-regularized statistical models. Our method\npermits a Bayesian interpretation of theory as prior knowledge and can be used\nboth for statistical prediction and causal inference. It contributes to\ntransfer learning by showing how incorporating theory into statistical modeling\ncan significantly improve out-of-domain predictions and offers a way to\nsynthesize reduced-form and structural approaches for causal effect estimation.\nSimulation experiments demonstrate the potential of our method in various\nsettings, including first-price auctions, dynamic models of entry and exit, and\ndemand estimation with instrumental variables. Our method has potential\napplications not only in economics, but in other scientific disciplines whose\ntheoretical models offer important insight but are subject to significant\nmisspecification concerns.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12601v4"
    },
    {
        "title": "Regression Discontinuity Design with Multivalued Treatments",
        "authors": [
            "Carolina Caetano",
            "Gregorio Caetano",
            "Juan Carlos Escanciano"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study identification and estimation in the Regression Discontinuity Design\n(RDD) with a multivalued treatment variable. We also allow for the inclusion of\ncovariates. We show that without additional information, treatment effects are\nnot identified. We give necessary and sufficient conditions that lead to\nidentification of LATEs as well as of weighted averages of the conditional\nLATEs. We show that if the first stage discontinuities of the multiple\ntreatments conditional on covariates are linearly independent, then it is\npossible to identify multivariate weighted averages of the treatment effects\nwith convenient identifiable weights. If, moreover, treatment effects do not\nvary with some covariates or a flexible parametric structure can be assumed, it\nis possible to identify (in fact, over-identify) all the treatment effects. The\nover-identification can be used to test these assumptions. We propose a simple\nestimator, which can be programmed in packaged software as a Two-Stage Least\nSquares regression, and packaged standard errors and tests can also be used.\nFinally, we implement our approach to identify the effects of different types\nof insurance coverage on health care utilization, as in Card, Dobkin and\nMaestas (2008).\n",
        "pdf_link": "http://arxiv.org/pdf/2007.00185v1"
    },
    {
        "title": "Structural Gaussian mixture vector autoregressive model with application\n  to the asymmetric effects of monetary policy shocks",
        "authors": [
            "Savi Virolainen"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  A structural Gaussian mixture vector autoregressive model is introduced. The\nshocks are identified by combining simultaneous diagonalization of the reduced\nform error covariance matrices with constraints on the time-varying impact\nmatrix. This leads to flexible identification conditions, and some of the\nconstraints are also testable. The empirical application studies asymmetries in\nthe effects of the U.S. monetary policy shock and finds strong asymmetries with\nrespect to the sign and size of the shock and to the initial state of the\neconomy. The accompanying CRAN distributed R package gmvarkit provides a\ncomprehensive set of tools for numerical analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.04713v7"
    },
    {
        "title": "Variable Selection in Macroeconomic Forecasting with Many Predictors",
        "authors": [
            "Zhenzhong Wang",
            "Zhengyuan Zhu",
            "Cindy Yu"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  In the data-rich environment, using many economic predictors to forecast a\nfew key variables has become a new trend in econometrics. The commonly used\napproach is factor augment (FA) approach. In this paper, we pursue another\ndirection, variable selection (VS) approach, to handle high-dimensional\npredictors. VS is an active topic in statistics and computer science. However,\nit does not receive as much attention as FA in economics. This paper introduces\nseveral cutting-edge VS methods to economic forecasting, which includes: (1)\nclassical greedy procedures; (2) l1 regularization; (3) gradient descent with\nsparsification and (4) meta-heuristic algorithms. Comprehensive simulation\nstudies are conducted to compare their variable selection accuracy and\nprediction performance under different scenarios. Among the reviewed methods, a\nmeta-heuristic algorithm called sequential Monte Carlo algorithm performs the\nbest. Surprisingly the classical forward selection is comparable to it and\nbetter than other more sophisticated algorithms. In addition, we apply these VS\nmethods on economic forecasting and compare with the popular FA approach. It\nturns out for employment rate and CPI inflation, some VS methods can achieve\nconsiderable improvement over FA, and the selected predictors can be well\nexplained by economic theories.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.10160v1"
    },
    {
        "title": "Treatment Effects with Targeting Instruments",
        "authors": [
            "Sokbae Lee",
            "Bernard Salanié"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Multivalued treatments are commonplace in applications. We explore the use of\ndiscrete-valued instruments to control for selection bias in this setting. Our\ndiscussion revolves around the concept of targeting: which instruments target\nwhich treatments. It allows us to establish conditions under which\ncounterfactual averages and treatment effects are point- or\npartially-identified for composite complier groups. We illustrate the\nusefulness of our framework by applying it to data from the Head Start Impact\nStudy. Under a plausible positive selection assumption, we derive informative\nbounds that suggest less beneficial effects of Head Start expansions than the\nparametric estimates of Kline and Walters (2016).\n",
        "pdf_link": "http://arxiv.org/pdf/2007.10432v5"
    },
    {
        "title": "Deep Dynamic Factor Models",
        "authors": [
            "Paolo Andreini",
            "Cosimo Izzo",
            "Giovanni Ricco"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  A novel deep neural network framework -- that we refer to as Deep Dynamic\nFactor Model (D$^2$FM) --, is able to encode the information available, from\nhundreds of macroeconomic and financial time-series into a handful of\nunobserved latent states. While similar in spirit to traditional dynamic factor\nmodels (DFMs), differently from those, this new class of models allows for\nnonlinearities between factors and observables due to the autoencoder neural\nnetwork structure. However, by design, the latent states of the model can still\nbe interpreted as in a standard factor model. Both in a fully real-time\nout-of-sample nowcasting and forecasting exercise with US data and in a Monte\nCarlo experiment, the D$^2$FM improves over the performances of a\nstate-of-the-art DFM.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11887v2"
    },
    {
        "title": "bootUR: An R Package for Bootstrap Unit Root Tests",
        "authors": [
            "Stephan Smeekes",
            "Ines Wilms"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  Unit root tests form an essential part of any time series analysis. We\nprovide practitioners with a single, unified framework for comprehensive and\nreliable unit root testing in the R package bootUR.The package's backbone is\nthe popular augmented Dickey-Fuller test paired with a union of rejections\nprinciple, which can be performed directly on single time series or multiple\n(including panel) time series. Accurate inference is ensured through the use of\nbootstrap methods. The package addresses the needs of both novice users, by\nproviding user-friendly and easy-to-implement functions with sensible default\noptions, as well as expert users, by giving full user-control to adjust the\ntests to one's desired settings. Our parallelized C++ implementation ensures\nthat all unit root tests are scalable to datasets containing many time series.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.12249v5"
    },
    {
        "title": "Total Error and Variability Measures for the Quarterly Workforce\n  Indicators and LEHD Origin-Destination Employment Statistics in OnTheMap",
        "authors": [
            "Kevin L. McKinney",
            "Andrew S. Green",
            "Lars Vilhuber",
            "John M. Abowd"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We report results from the first comprehensive total quality evaluation of\nfive major indicators in the U.S. Census Bureau's Longitudinal\nEmployer-Household Dynamics (LEHD) Program Quarterly Workforce Indicators\n(QWI): total flow-employment, beginning-of-quarter employment, full-quarter\nemployment, average monthly earnings of full-quarter employees, and total\nquarterly payroll. Beginning-of-quarter employment is also the main tabulation\nvariable in the LEHD Origin-Destination Employment Statistics (LODES) workplace\nreports as displayed in OnTheMap (OTM), including OnTheMap for Emergency\nManagement. We account for errors due to coverage; record-level non-response;\nedit and imputation of item missing data; and statistical disclosure\nlimitation. The analysis reveals that the five publication variables under\nstudy are estimated very accurately for tabulations involving at least 10 jobs.\nTabulations involving three to nine jobs are a transition zone, where cells may\nbe fit for use with caution. Tabulations involving one or two jobs, which are\ngenerally suppressed on fitness-for-use criteria in the QWI and synthesized in\nLODES, have substantial total variability but can still be used to estimate\nstatistics for untabulated aggregates as long as the job count in the aggregate\nis more than 10.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.13275v1"
    },
    {
        "title": "Distributionally Robust Instrumental Variables Estimation",
        "authors": [
            "Zhaonan Qu",
            "Yongchan Kwon"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Instrumental variables (IV) estimation is a fundamental method in\neconometrics and statistics for estimating causal effects in the presence of\nunobserved confounding. However, challenges such as untestable model\nassumptions and poor finite sample properties have undermined its reliability\nin practice. Viewing common issues in IV estimation as distributional\nuncertainties, we propose DRIVE, a distributionally robust IV estimation\nmethod. We show that DRIVE minimizes a square root variant of ridge regularized\ntwo stage least squares (TSLS) objective when the ambiguity set is based on a\nWasserstein distance. In addition, we develop a novel asymptotic theory for\nthis estimator, showing that it achieves consistency without requiring the\nregularization parameter to vanish. This novel property ensures that the\nestimator is robust to distributional uncertainties that persist in large\nsamples. We further derive the asymptotic distribution of Wasserstein DRIVE and\npropose data-driven procedures to select the regularization parameter based on\ntheoretical results. Simulation studies demonstrate the superior finite sample\nperformance of Wasserstein DRIVE in terms of estimation error and out-of-sample\nprediction. Due to its regularization and robustness properties, Wasserstein\nDRIVE presents an appealing option when the practitioner is uncertain about\nmodel assumptions or distributional shifts in data.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.15634v2"
    },
    {
        "title": "Machine Learning Panel Data Regressions with Heavy-tailed Dependent\n  Data: Theory and Application",
        "authors": [
            "Andrii Babii",
            "Ryan T. Ball",
            "Eric Ghysels",
            "Jonas Striaukas"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The paper introduces structured machine learning regressions for heavy-tailed\ndependent panel data potentially sampled at different frequencies. We focus on\nthe sparse-group LASSO regularization. This type of regularization can take\nadvantage of the mixed frequency time series panel data structures and improve\nthe quality of the estimates. We obtain oracle inequalities for the pooled and\nfixed effects sparse-group LASSO panel data estimators recognizing that\nfinancial and economic data can have fat tails. To that end, we leverage on a\nnew Fuk-Nagaev concentration inequality for panel data consisting of\nheavy-tailed $\\tau$-mixing processes.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.03600v2"
    },
    {
        "title": "The Laws of Motion of the Broker Call Rate in the United States",
        "authors": [
            "Alex Garivaltis"
        ],
        "category": "econ.EM",
        "published_year": "2019",
        "summary": "  In this paper, which is the third installment of the author's trilogy on\nmargin loan pricing, we analyze $1,367$ monthly observations of the U.S. broker\ncall money rate, which is the interest rate at which stock brokers can borrow\nto fund their margin loans to retail clients. We describe the basic features\nand mean-reverting behavior of this series and juxtapose the\nempirically-derived laws of motion with the author's prior theories of margin\nloan pricing (Garivaltis 2019a-b). This allows us to derive stochastic\ndifferential equations that govern the evolution of the margin loan interest\nrate and the leverage ratios of sophisticated brokerage clients (namely,\ncontinuous time Kelly gamblers). Finally, we apply Merton's (1974) arbitrage\ntheory of corporate liability pricing to study theoretical constraints on the\nrisk premia that could be generated in the market for call money. Apparently,\nif there is no arbitrage in the U.S. financial markets, the implication is that\nthe total volume of call loans must constitute north of $70\\%$ of the value of\nall leveraged portfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.00946v2"
    },
    {
        "title": "Adversarial Generalized Method of Moments",
        "authors": [
            "Greg Lewis",
            "Vasilis Syrgkanis"
        ],
        "category": "econ.EM",
        "published_year": "2018",
        "summary": "  We provide an approach for learning deep neural net representations of models\ndescribed via conditional moment restrictions. Conditional moment restrictions\nare widely used, as they are the language by which social scientists describe\nthe assumptions they make to enable causal inference. We formulate the problem\nof estimating the underling model as a zero-sum game between a modeler and an\nadversary and apply adversarial training. Our approach is similar in nature to\nGenerative Adversarial Networks (GAN), though here the modeler is learning a\nrepresentation of a function that satisfies a continuum of moment conditions\nand the adversary is identifying violating moments. We outline ways of\nconstructing effective adversaries in practice, including kernels centered by\nk-means clustering, and random forests. We examine the practical performance of\nour approach in the setting of non-parametric instrumental variable regression.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.07164v2"
    },
    {
        "title": "Stable Probability Weighting: Large-Sample and Finite-Sample Estimation\n  and Inference Methods for Heterogeneous Causal Effects of Multivalued\n  Treatments Under Limited Overlap",
        "authors": [
            "Ganesh Karapakula"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  In this paper, I try to tame \"Basu's elephants\" (data with extreme selection\non observables). I propose new practical large-sample and finite-sample methods\nfor estimating and inferring heterogeneous causal effects (under\nunconfoundedness) in the empirically relevant context of limited overlap. I\ndevelop a general principle called \"Stable Probability Weighting\" (SPW) that\ncan be used as an alternative to the widely used Inverse Probability Weighting\n(IPW) technique, which relies on strong overlap. I show that IPW (or its\naugmented version), when valid, is a special case of the more general SPW (or\nits doubly robust version), which adjusts for the extremeness of the\nconditional probabilities of the treatment states. The SPW principle can be\nimplemented using several existing large-sample parametric, semiparametric, and\nnonparametric procedures for conditional moment models. In addition, I provide\nnew finite-sample results that apply when unconfoundedness is plausible within\nfine strata. Since IPW estimation relies on the problematic reciprocal of the\nestimated propensity score, I develop a \"Finite-Sample Stable Probability\nWeighting\" (FPW) set-estimator that is unbiased in a sense. I also propose new\nfinite-sample inference methods for testing a general class of weak null\nhypotheses. The associated computationally convenient methods, which can be\nused to construct valid confidence sets and to bound the finite-sample\nconfidence distribution, are of independent interest. My large-sample and\nfinite-sample frameworks extend to the setting of multivalued treatments.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.05703v2"
    },
    {
        "title": "An Adversarial Approach to Structural Estimation",
        "authors": [
            "Tetsuya Kaji",
            "Elena Manresa",
            "Guillaume Pouliot"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We propose a new simulation-based estimation method, adversarial estimation,\nfor structural models. The estimator is formulated as the solution to a minimax\nproblem between a generator (which generates simulated observations using the\nstructural model) and a discriminator (which classifies whether an observation\nis simulated). The discriminator maximizes the accuracy of its classification\nwhile the generator minimizes it. We show that, with a sufficiently rich\ndiscriminator, the adversarial estimator attains parametric efficiency under\ncorrect specification and the parametric rate under misspecification. We\nadvocate the use of a neural network as a discriminator that can exploit\nadaptivity properties and attain fast rates of convergence. We apply our method\nto the elderly's saving decision model and show that our estimator uncovers the\nbequest motive as an important source of saving across the wealth distribution,\nnot only for the rich.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.06169v3"
    },
    {
        "title": "Interpretable Neural Networks for Panel Data Analysis in Economics",
        "authors": [
            "Yucheng Yang",
            "Zhong Zheng",
            "Weinan E"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  The lack of interpretability and transparency are preventing economists from\nusing advanced tools like neural networks in their empirical research. In this\npaper, we propose a class of interpretable neural network models that can\nachieve both high prediction accuracy and interpretability. The model can be\nwritten as a simple function of a regularized number of interpretable features,\nwhich are outcomes of interpretable functions encoded in the neural network.\nResearchers can design different forms of interpretable functions based on the\nnature of their tasks. In particular, we encode a class of interpretable\nfunctions named persistent change filters in the neural network to study time\nseries cross-sectional data. We apply the model to predicting individual's\nmonthly employment status using high-dimensional administrative data. We\nachieve an accuracy of 94.5% in the test set, which is comparable to the best\nperformed conventional machine learning methods. Furthermore, the\ninterpretability of the model allows us to understand the mechanism that\nunderlies the prediction: an individual's employment status is closely related\nto whether she pays different types of insurances. Our work is a useful step\ntowards overcoming the black-box problem of neural networks, and provide a new\ntool for economists to study administrative and proprietary big data.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.05311v3"
    },
    {
        "title": "Binary Choice with Asymmetric Loss in a Data-Rich Environment: Theory\n  and an Application to Racial Justice",
        "authors": [
            "Andrii Babii",
            "Xi Chen",
            "Eric Ghysels",
            "Rohit Kumar"
        ],
        "category": "econ.EM",
        "published_year": "2020",
        "summary": "  We study the binary choice problem in a data-rich environment with asymmetric\nloss functions. The econometrics literature covers nonparametric binary choice\nproblems but does not offer computationally attractive solutions in data-rich\nenvironments. The machine learning literature has many algorithms but is\nfocused mostly on loss functions that are independent of covariates. We show\nthat theoretically valid decisions on binary outcomes with general loss\nfunctions can be achieved via a very simple loss-based reweighting of the\nlogistic regression or state-of-the-art machine learning techniques. We apply\nour analysis to racial justice in pretrial detention.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08463v5"
    },
    {
        "title": "Inference for multi-valued heterogeneous treatment effects when the\n  number of treated units is small",
        "authors": [
            "Marina Dias",
            "Demian Pouzo"
        ],
        "category": "econ.EM",
        "published_year": "2021",
        "summary": "  We propose a method for conducting asymptotically valid inference for\ntreatment effects in a multi-valued treatment framework where the number of\nunits in the treatment arms can be small and do not grow with the sample size.\nWe accomplish this by casting the model as a semi-/non-parametric conditional\nquantile model and using known finite sample results about the law of the\nindicator function that defines the conditional quantile. Our framework allows\nfor structural functions that are non-additively separable, with flexible\nfunctional forms and heteroskedasticy in the residuals, and it also encompasses\ncommonly used designs like difference in difference. We study the finite sample\nbehavior of our test in a Monte Carlo study and we also apply our results to\nassessing the effect of weather events on GDP growth.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.10965v1"
    },
    {
        "title": "Tensor Principal Component Analysis",
        "authors": [
            "Andrii Babii",
            "Eric Ghysels",
            "Junsu Pan"
        ],
        "category": "econ.EM",
        "published_year": "2022",
        "summary": "  In this paper, we develop new methods for analyzing high-dimensional tensor\ndatasets. A tensor factor model describes a high-dimensional dataset as a sum\nof a low-rank component and an idiosyncratic noise, generalizing traditional\nfactor models for panel data. We propose an estimation algorithm, called tensor\nprincipal component analysis (TPCA), which generalizes the traditional PCA\napplicable to panel data. The algorithm involves unfolding the tensor into a\nsequence of matrices along different dimensions and applying PCA to the\nunfolded matrices. We provide theoretical results on the consistency and\nasymptotic distribution for the TPCA estimator of loadings and factors. We also\nintroduce a novel test for the number of factors in a tensor factor model. The\nTPCA and the test feature good performance in Monte Carlo experiments and are\napplied to sorted portfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.12981v2"
    },
    {
        "title": "SGMM: Stochastic Approximation to Generalized Method of Moments",
        "authors": [
            "Xiaohong Chen",
            "Sokbae Lee",
            "Yuan Liao",
            "Myung Hwan Seo",
            "Youngki Shin",
            "Myunghyun Song"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  We introduce a new class of algorithms, Stochastic Generalized Method of\nMoments (SGMM), for estimation and inference on (overidentified) moment\nrestriction models. Our SGMM is a novel stochastic approximation alternative to\nthe popular Hansen (1982) (offline) GMM, and offers fast and scalable\nimplementation with the ability to handle streaming datasets in real time. We\nestablish the almost sure convergence, and the (functional) central limit\ntheorem for the inefficient online 2SLS and the efficient SGMM. Moreover, we\npropose online versions of the Durbin-Wu-Hausman and Sargan-Hansen tests that\ncan be seamlessly integrated within the SGMM framework. Extensive Monte Carlo\nsimulations show that as the sample size increases, the SGMM matches the\nstandard (offline) GMM in terms of estimation accuracy and gains over\ncomputational efficiency, indicating its practical value for both large-scale\nand online datasets. We demonstrate the efficacy of our approach by a proof of\nconcept using two well known empirical examples with large sample sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.13564v2"
    },
    {
        "title": "Free Discontinuity Regression: With an Application to the Economic\n  Effects of Internet Shutdowns",
        "authors": [
            "Florian Gunsilius",
            "David Van Dijcke"
        ],
        "category": "econ.EM",
        "published_year": "2023",
        "summary": "  Discontinuities in regression functions can reveal important insights. In\nmany contexts, like geographic settings, such discontinuities are multivariate\nand unknown a priori. We propose a non-parametric regression method that\nestimates the location and size of discontinuities by segmenting the regression\nsurface. This estimator is based on a convex relaxation of the Mumford-Shah\nfunctional, for which we establish identification and convergence. We use it to\nshow that an internet shutdown in India resulted in a reduction of economic\nactivity by 25--35%, greatly surpassing previous estimates and shedding new\nlight on the true cost of such shutdowns for digital economies globally.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.14630v2"
    },
    {
        "title": "Sparse spanning portfolios and under-diversification with second-order\n  stochastic dominance",
        "authors": [
            "Stelios Arvanitis",
            "Olivier Scaillet",
            "Nikolas Topaloglou"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  We develop and implement methods for determining whether relaxing sparsity\nconstraints on portfolios improves the investment opportunity set for\nrisk-averse investors. We formulate a new estimation procedure for sparse\nsecond-order stochastic spanning based on a greedy algorithm and Linear\nProgramming. We show the optimal recovery of the sparse solution asymptotically\nwhether spanning holds or not. From large equity datasets, we estimate the\nexpected utility loss due to possible under-diversification, and find that\nthere is no benefit from expanding a sparse opportunity set beyond 45 assets.\nThe optimal sparse portfolio invests in 10 industry sectors and cuts tail risk\nwhen compared to a sparse mean-variance portfolio. On a rolling-window basis,\nthe number of assets shrinks to 25 assets in crisis periods, while standard\nfactor models cannot explain the performance of the sparse portfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.01951v2"
    },
    {
        "title": "A Note on Doubly Robust Estimator in Regression Continuity Designs",
        "authors": [
            "Masahiro Kato"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  This note introduces a doubly robust (DR) estimator for regression\ndiscontinuity (RD) designs. RD designs provide a quasi-experimental framework\nfor estimating treatment effects, where treatment assignment depends on whether\na running variable surpasses a predefined cutoff. A common approach in RD\nestimation is the use of nonparametric regression methods, such as local linear\nregression. However, the validity of these methods still relies on the\nconsistency of the nonparametric estimators. In this study, we propose the\nDR-RD estimator, which combines two distinct estimators for the conditional\nexpected outcomes. The primary advantage of the DR-RD estimator lies in its\nability to ensure the consistency of the treatment effect estimation as long as\nat least one of the two estimators is consistent. Consequently, our DR-RD\nestimator enhances robustness of treatment effect estimators in RD designs.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07978v3"
    },
    {
        "title": "Program Evaluation with Remotely Sensed Outcomes",
        "authors": [
            "Ashesh Rambachan",
            "Rahul Singh",
            "Davide Viviano"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  While traditional program evaluations typically rely on surveys to measure\noutcomes, certain economic outcomes such as living standards or environmental\nquality may be infeasible or costly to collect. As a result, recent empirical\nwork estimates treatment effects using remotely sensed variables (RSVs), such\nmobile phone activity or satellite images, instead of ground-truth outcome\nmeasurements. Common practice predicts the economic outcome from the RSV, using\nan auxiliary sample of labeled RSVs, and then uses such predictions as the\noutcome in the experiment. We prove that this approach leads to biased\nestimates of treatment effects when the RSV is a post-outcome variable. We\nnonparametrically identify the treatment effect, using an assumption that\nreflects the logic of recent empirical research: the conditional distribution\nof the RSV remains stable across both samples, given the outcome and treatment.\nOur results do not require researchers to know or consistently estimate the\nrelationship between the RSV, outcome, and treatment, which is typically\nmis-specified with unstructured data. We form a representation of the RSV for\ndownstream causal inference by predicting the outcome and predicting the\ntreatment, with better predictions leading to more precise causal estimates. We\nre-evaluate the efficacy of a large-scale public program in India, showing that\nthe program's measured effects on local consumption and poverty can be\nreplicated using satellite\n",
        "pdf_link": "http://arxiv.org/pdf/2411.10959v1"
    },
    {
        "title": "Anomaly Detection in California Electricity Price Forecasting: Enhancing\n  Accuracy and Reliability Using Principal Component Analysis",
        "authors": [
            "Joseph Nyangon",
            "Ruth Akintunde"
        ],
        "category": "econ.EM",
        "published_year": "2024",
        "summary": "  Accurate and reliable electricity price forecasting has significant practical\nimplications for grid management, renewable energy integration, power system\nplanning, and price volatility management. This study focuses on enhancing\nelectricity price forecasting in California's grid, addressing challenges from\ncomplex generation data and heteroskedasticity. Utilizing principal component\nanalysis (PCA), we analyze CAISO's hourly electricity prices and demand from\n2016-2021 to improve day-ahead forecasting accuracy. Initially, we apply\ntraditional outlier analysis with the interquartile range method, followed by\nrobust PCA (RPCA) for more effective outlier elimination. This approach\nimproves data symmetry and reduces skewness. We then construct multiple linear\nregression models using both raw and PCA-transformed features. The model with\ntransformed features, refined through traditional and SAS Sparse Matrix outlier\nremoval methods, shows superior forecasting performance. The SAS Sparse Matrix\nmethod, in particular, significantly enhances model accuracy. Our findings\ndemonstrate that PCA-based methods are key in advancing electricity price\nforecasting, supporting renewable integration and grid management in day-ahead\nmarkets.\n  Keywords: Electricity price forecasting, principal component analysis (PCA),\npower system planning, heteroskedasticity, renewable energy integration.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.07787v1"
    }
]