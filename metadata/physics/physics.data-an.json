[
    {
        "title": "Monte Carlo Implementation of Gaussian Process Models for Bayesian\n  Regression and Classification",
        "authors": [
            "Radford M. Neal"
        ],
        "category": "physics.data-an",
        "published_year": "1997",
        "summary": "  Gaussian processes are a natural way of defining prior distributions over\nfunctions of one or more input variables. In a simple nonparametric regression\nproblem, where such a function gives the mean of a Gaussian distribution for an\nobserved response, a Gaussian process model can easily be implemented using\nmatrix computations that are feasible for datasets of up to about a thousand\ncases. Hyperparameters that define the covariance function of the Gaussian\nprocess can be sampled using Markov chain methods. Regression models where the\nnoise has a t distribution and logistic or probit models for classification\napplications can be implemented by sampling as well for latent values\nunderlying the observations. Software is now available that implements these\nmethods using covariance functions with hierarchical parameterizations. Models\ndefined in this way can discover high-level properties of the data, such as\nwhich inputs are relevant to predicting the response.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9701026v2"
    },
    {
        "title": "The Analysis of Data from Continuous Probability Distributions",
        "authors": [
            "Timothy E. Holy"
        ],
        "category": "physics.data-an",
        "published_year": "1997",
        "summary": "  Conventional statistics begins with a model, and assigns a likelihood of\nobtaining any particular set of data. The opposite approach, beginning with the\ndata and assigning a likelihood to any particular model, is explored here for\nthe case of points drawn randomly from a continuous probability distribution. A\nscalar field theory is used to assign a likelihood over the space of\nprobability distributions. The most likely distribution may be calculated,\nproviding an estimate of the underlying distribution and a convenient graphical\nrepresentation of the raw data. Fluctuations around this maximum likelihood\nestimate are characterized by a robust measure of goodness-of-fit. Its\ndistribution may be calculated by integrating over fluctuations. The resulting\nmethod of data analysis has some advantages over conventional approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9706015v1"
    },
    {
        "title": "Objective Bayesian Statistics",
        "authors": [
            "O. -A. Al-Hujaj",
            "H. L. Harney"
        ],
        "category": "physics.data-an",
        "published_year": "1997",
        "summary": "  Bayesian inference --- although becoming popular in physics and chemistry ---\nis hampered up to now by the vagueness of its notion of prior probability. Some\nof its supporters argue that this vagueness is the unavoidable consequence of\nthe subjectivity of judgements --- even scientific ones. We argue that priors\ncan be defined uniquely if the statistical model at hand possesses a symmetry\nand if the ensuing confidence intervals are subjected to a frequentist\ncriterion. Moreover, it is shown via an example taken from recent experimental\nnuclear physics, that this procedure can be extended to models with broken\nsymmetry.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9706025v1"
    },
    {
        "title": "Experiments on Critical Phenomena in a Noisy Exit Problem",
        "authors": [
            "D. G. Luchinsky",
            "R. S. Maier",
            "R. Mannella",
            "P. V. E. McClintock",
            "D. L. Stein"
        ],
        "category": "physics.data-an",
        "published_year": "1997",
        "summary": "  We consider noise-driven exit from a domain of attraction in a\ntwo-dimensional bistable system lacking detailed balance. Through analog and\ndigital stochastic simulations, we find a theoretically predicted bifurcation\nof the most probable exit path as the parameters of the system are changed, and\na corresponding nonanalyticity of the activation energy. We also investigate\nthe extent to which the bifurcation is related to the local breaking of\ntime-reversal invariance.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9707002v1"
    },
    {
        "title": "BAYES-LIN: An object-oriented environment for Bayes linear local\n  computation",
        "authors": [
            "Darren J Wilkinson"
        ],
        "category": "physics.data-an",
        "published_year": "1997",
        "summary": "  BAYES-LIN is an extension of the LISP-STAT object-oriented statistical\ncomputing environment, which adds to LISP-STAT some object prototypes\nappropriate for carrying out local computation via message-passing between\nclique-tree nodes of Bayes linear belief networks. Currently the BAYES-LIN\nsystem represents a rather low-level set of tools for a back-end computational\nengine, together with diagnostic graphics for understanding the effects of\nadjustments on the moral graph. A GUI front end, allowing interactive\nformulation of DAG models could be easily added, but is currently missing from\nthe system. This document provides a very brief introduction to the system, by\nmeans of a work-through of two example computations, followed by a list of\nvariables, functions, objects and methods provided by the system.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9710015v1"
    },
    {
        "title": "Non-commutative time-frequency tomography",
        "authors": [
            "V. I. Man'ko",
            "R. Vilela Mendes"
        ],
        "category": "physics.data-an",
        "published_year": "1997",
        "summary": "  The characterization of non-stationary signals requires joint time and\nfrequency information. However, time (t) and frequency (omega) being\nnon-commuting variables there cannot be a joint probability density in the\n(t,omega) plane and the time-frequency distributions, that have been proposed,\nhave difficult interpretation problems arising from negative or complex values\nand spurious components. As an alternative we propose to obtain time-frequency\ninformation by looking at the marginal distributions along rotated directions\nin the (t,omega) plane. The rigorous probability interpretation of the marginal\ndistributions avoids all interpretation ambiguities. Applications to signal\nanalysis and signal detection are discussed as well as an extension of the\nmethod to other pairs of non-commuting variables.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9712022v1"
    },
    {
        "title": "Characteristic functions and process identification by neural networks",
        "authors": [
            "Joaquim A. Dente",
            "R. Vilela Mendes"
        ],
        "category": "physics.data-an",
        "published_year": "1997",
        "summary": "  Principal component analysis (PCA) algorithms use neural networks to extract\nthe eigenvectors of the correlation matrix from the data. However, if the\nprocess is non-Gaussian, PCA algorithms or their higher order generalisations\nprovide only incomplete or misleading information on the statistical properties\nof the data. To handle such situations we propose neural network algorithms,\nwith an hybrid (supervised and unsupervised) learning scheme, which constructs\nthe characteristic function of the probability distribution and the transition\nfunctions of the stochastic process. Illustrative examples are presented, which\ninclude Cauchy and Levy-type processes\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9712035v1"
    },
    {
        "title": "Cross Validated Non parametric Bayesianism by Markov Chain Monte Carlo",
        "authors": [
            "Carlos C. Rodriguez"
        ],
        "category": "physics.data-an",
        "published_year": "1997",
        "summary": "  Completely automatic and adaptive non-parametric inference is a pie in the\nsky. The frequentist approach, best exemplified by the kernel estimators, has\nexcellent asymptotic characteristics but it is very sensitive to the choice of\nsmoothness parameters. On the other hand the Bayesian approach, best\nexemplified by the mixture of gaussians models, is optimal given the observed\ndata but it is very sensitive to the choice of prior. In 1984 the author\nproposed to use the Cross-Validated gaussian kernel as the likelihood for the\nsmoothness scale parameter h, and obtained a closed formula for the posterior\nmean of h based on Jeffreys's rule as the prior. The practical operational\ncharacteristics of this bayes' rule for the smoothness parameter remained\nunknown for all these years due to the combinatorial complexity of the formula.\nIt is shown in this paper that a version of the metropolis algorithm can be\nused to approximate the value of h producing remarkably good completely\nautomatic and adaptive kernel estimators. A close study of the form of the\ncross validated likelihood suggests a modification and a new approach to\nBayesian Non-parametrics in general.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9712041v2"
    },
    {
        "title": "Using projections and correlations to approximate probability\n  distributions",
        "authors": [
            "Dean Karlen"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  A method to approximate continuous multi-dimensional probability density\nfunctions (PDFs) using their projections and correlations is described. The\nmethod is particularly useful for event classification when estimates of\nsystematic uncertainties are required and for the application of an unbinned\nmaximum likelihood analysis when an analytic model is not available. A simple\ngoodness of fit test of the approximation can be used, and simulated event\nsamples that follow the approximate PDFs can be efficiently generated. The\nsource code for a FORTRAN-77 implementation of this method is available.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9805018v1"
    },
    {
        "title": "On the determination of probability density functions by using Neural\n  Networks",
        "authors": [
            "Lluis Garrido",
            "Aurelio Juste"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  It is well known that the output of a Neural Network trained to disentangle\nbetween two classes has a probabilistic interpretation in terms of the\na-posteriori Bayesian probability, provided that a unary representation is\ntaken for the output patterns. This fact is used to make Neural Networks\napproximate probability density functions from examples in an unbinned way,\ngiving a better performace than ``standard binned procedures''. In addition,\nthe mapped p.d.f. has an analytical expression.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9807018v1"
    },
    {
        "title": "Objections to the Unified Approach to the Computation of Classical\n  Confidence Limits",
        "authors": [
            "Günter Zech"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  Conventional classical confidence intervals in specific cases are unphysical.\nA solution to this problem has recently been published by Feldman and Cousins.\nWe show that there are cases where the new approach is not applicable and that\nit does not remove the basic deficiencies of classical confidence limits.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9809035v1"
    },
    {
        "title": "Estimating probability densities from short samples: a parametric\n  maximum likelihood approach",
        "authors": [
            "T. Dudok de Wit",
            "E. Floriani"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  A parametric method similar to autoregressive spectral estimators is proposed\nto determine the probability density function (pdf) of a random set. The method\nproceeds by maximizing the likelihood of the pdf, yielding estimates that\nperform equally well in the tails as in the bulk of the distribution. It is\ntherefore well suited for the analysis short sets drawn from smooth pdfs and\nstands out by the simplicity of its computational scheme. Its advantages and\nlimitations are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9810018v1"
    },
    {
        "title": "Depth Profile Reconstruction from Rutherford Backscattering Data",
        "authors": [
            "U. V. Toussaint",
            "K. Krieger",
            "R. Fischer",
            "V. Dose"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  An adaptive kernel method in the Bayesian framework together with a new\nsimulation program for Rutherford backscattering spectroscopy (RBS) have been\napplied to the analysis of RBS data. Even in the case of strongly overlapping\nRBS peaks a depth profile reconstruction without noise fitting has been\nachieved. The adaptive kernel method leads to the simplest depth profile\nconsistent with the data. Erosion and redeposition rates of carbon divertor\nplates in the fusion experiment ASDEX Upgrade could be determined by\nRBS-analysis of thin film probes before and after exposition to plasma\ndischarges.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9811003v1"
    },
    {
        "title": "The Signal Estimator Limit Setting Method",
        "authors": [
            "S. Jin",
            "P. McNamara"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  A new method of background subtraction is presented which uses the concept of\na signal estimator to construct a confidence level which is always conservative\nand which is never better than e^-s. The new method yields stronger exclusions\nthan the Bayesian method with a flat prior distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9812030v1"
    },
    {
        "title": "Improved Probability Method for Estimating Signal in the Presence of\n  Background",
        "authors": [
            "Byron P. Roe",
            "Michael B. Woodroofe"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  A suggestion is made for improving the Feldman Cousins method of estimating\nsignal counts in the presence of background. The method concentrates on finding\nessential information about the signal and ignoring extraneous information\nabout background. An appropriate method is found which uses the condition that\nthe number of background events obtained does not exceed the total number of\nevents obtained. Several alternative approaches are explored.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9812036v3"
    },
    {
        "title": "Anomalous jumping in a double-well potential",
        "authors": [
            "P. D. Ditlevsen"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  Noise induced jumping between meta-stable states in a potential depends on\nthe structure of the noise. For an $\\alpha$-stable noise, jumping triggered by\nsingle extreme events contributes to the transition probability. This is also\ncalled Levy flights and might be of importance in triggering sudden changes in\ngeophysical flow and perhaps even climatic changes. The steady state statistics\nis also influenced by the noise structure leading to a non-Gibbs distribution\nfor an $\\alpha$-stable noise.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9901002v1"
    },
    {
        "title": "Time Series Forecasting: A Multivariate Stochastic Approach",
        "authors": [
            "Stefano Sello"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  This note deals with a multivariate stochastic approach to forecast the\nbehaviour of a cyclic time series. Particular attention is devoted to the\nproblem of the prediction of time behaviour of sunspot numbers for the current\n23th cycle. The idea is to consider the previous known n cycles as n particular\nrealizations of a given stochastic process. The aim is to predict the future\nbehaviour of the current n+1th realization given a portion of the curve and the\nstructure of the previous n realizations. The model derived is based on the\ncross-correlations between the current n+1th realization and the previous n\nones and the solution of the related least squares problem. As example we\napplied the method to smoothed monthly sunspots numbers from SIDC archives, in\norder to predict the behaviour of the current 23th solar cycle.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9901050v2"
    },
    {
        "title": "HEMAS: a Monte Carlo code for hadronic, electromagnetic and TeV muon\n  components in air shower",
        "authors": [
            "E. Scapparone"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  The features of the HEMAS code are presented. The results of the comparison\nbetween the Monte Carlo expectation and the experimental data are shown.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9902043v1"
    },
    {
        "title": "Definition of Nonequilibrium Entropy of General Systems",
        "authors": [
            "Xiaochun Mei"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  The definition of nonequilibrium entropy is provided for the general\nnonequilibrium processes by connecting thermodynamics with statistical physics,\nand the principle of entropy increment in the nonequilibrium processes is also\nproved in the paper. The result shows that the definition of nonequilibrium\nentropy is not unique.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9903029v2"
    },
    {
        "title": "Analytic Confidence Level Calculations using the Likelihood Ratio and\n  Fourier Transform",
        "authors": [
            "Hongbo Hu",
            "Jason Nielsen"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  The interpretation of new particle search results involves a confidence level\ncalculation on either the discovery hypothesis or the background-only (\"null\")\nhypothesis. A typical approach uses toy Monte Carlo experiments to build an\nexpected experiment estimator distribution against which an observed\nexperiment's estimator may be compared. In this note, a new approach is\npresented which calculates analytically the experiment estimator distribution\nvia a Fourier transform, using the likelihood ratio as an ordering estimator.\nThe analytic approach enjoys an enormous speed advantage over the toy Monte\nCarlo method, making it possible to quickly and precisely calculate confidence\nlevel results.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9906010v1"
    },
    {
        "title": "Artificial Neural Network Modeling of Forest Tree Growth",
        "authors": [
            "Christopher Gordon"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  The problem of modeling forest tree growth curves with an artificial neural\nnetwork (NN) is examined. The NN parametric form is shown to be a suitable\nmodel if each forest tree plot is assumed to consist of several differently\ngrowing sub-plots. The predictive Bayesian approach is used in estimating the\nNN output.\n  Data from the correlated curve trend (CCT) experiments are used. The NN\npredictions are compared with those of one of the best parametric solutions,\nthe Schnute model. Analysis of variance (ANOVA) methods are used to evaluate\nwhether any observed differences are statistically significant. From a\nFrequentist perspective the differences between the Schnute and NN approach are\nfound not to be significant. However, a Bayesian ANOVA indicates that there is\na 93% probability of the NN approach producing better predictions on average.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9906012v1"
    },
    {
        "title": "When do finite sample effects significantly affect entropy estimates ?",
        "authors": [
            "T. Dudok de Wit"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  An expression is proposed for determining the error caused on entropy\nestimates by finite sample effects. This expression is based on the Ansatz that\nthe ranked distribution of probabilities tends to follow an empirical Zipf law.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9906064v1"
    },
    {
        "title": "Publication Bias (The \"File-Drawer Problem\") in Scientific Inference",
        "authors": [
            "Jeffrey D. Scargle"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  Publication bias arises whenever the probability that a study is published\ndepends on the statistical significance of its results. This bias, often called\nthe file-drawer effect since the unpublished results are imagined to be tucked\naway in researchers' file cabinets, is potentially a severe impediment to\ncombining the statistical results of studies collected from the literature.\nWith almost any reasonable quantitative model for publication bias, only a\nsmall number of studies lost in the file-drawer will produce a significant\nbias. This result contradicts the well known Fail Safe File Drawer (FSFD)\nmethod for setting limits on the potential harm of publication bias, widely\nused in social, medical and psychic research. This method incorrectly treats\nthe file drawer as unbiased, and almost always misestimates the seriousness of\npublication bias. A large body of not only psychic research, but medical and\nsocial science studies, has mistakenly relied on this method to validate\nclaimed discoveries. Statistical combination can be trusted only if it is known\nwith certainty that all studies that have been carried out are included. Such\ncertainty is virtually impossible to achieve in literature surveys.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9909033v1"
    },
    {
        "title": "Mixtures of Gaussian process priors",
        "authors": [
            "J. C. Lemm"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  Nonparametric Bayesian approaches based on Gaussian processes have recently\nbecome popular in the empirical learning community. They encompass many\nclassical methods of statistics, like Radial Basis Functions or various\nsplines, and are technically convenient because Gaussian integrals can be\ncalculated analytically. Restricting to Gaussian processes, however, forbids\nfor example the implemention of genuine nonconcave priors. Mixtures of Gaussian\nprocess priors, on the other hand, allow the flexible implementation of complex\nand situation specific, also nonconcave \"a priori\" information. This is\nessential for tasks with, compared to their complexity, a small number of\navailable training data. The paper concentrates on the formalism for Gaussian\nregression problems where prior mixture models provide a generalisation of\nclassical quadratic, typically smoothness related, regularisation approaches\nbeing more flexible without having a much larger computational complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9911077v1"
    },
    {
        "title": "Kalman Filter Track Fits and Track Breakpoint Analysis",
        "authors": [
            "Pierre Astier",
            "Alessandro Cardini",
            "Robert D. Cousins",
            "Antoine Letessier-Selvon",
            "Boris A. Popov",
            "Tatiana Vinogradova"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  We give an overview of track fitting using the Kalman filter method in the\nNOMAD detector at CERN, and emphasize how the wealth of by-product information\ncan be used to analyze track breakpoints (discontinuities in track parameters\ncaused by scattering, decay, etc.). After reviewing how this information has\nbeen previously exploited by others, we describe extensions which add power to\nbreakpoint detection and characterization. We show how complete fits to the\nentire track, with breakpoint parameters added, can be easily obtained from the\ninformation from unbroken fits. Tests inspired by the Fisher F-test can then be\nused to judge breakpoints. Signed quantities (such as change in momentum at the\nbreakpoint) can supplement unsigned quantities such as the various chisquares.\nWe illustrate the method with electrons from real data, and with Monte Carlo\nsimulations of pion decays.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9912034v1"
    },
    {
        "title": "Application of Conditioning to the Gaussian-with-Boundary Problem in the\n  Unified Approach to Confidence Intervals",
        "authors": [
            "Robert D. Cousins"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  Roe and Woodroofe (RW) have suggested that certain conditional probabilities\nbe incorporated into the ``unified approach'' for constructing confidence\nintervals, previously described by Feldman and Cousins (FC). RW illustrated\nthis conditioning technique using one of the two prototype problems in the FC\npaper, that of Poisson processes with background. The main effect was on the\nupper curve in the confidence belt. In this paper, we attempt to apply this\nstyle of conditioning to the other prototype problem, that of Gaussian errors\nwith a bounded physical region. We find that the lower curve on the confidence\nbelt is also moved significantly, in an undesirable manner.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0001031v1"
    },
    {
        "title": "The Equilibrium Distribution of Gas Molecules Adsorbed on an Active\n  Surface",
        "authors": [
            "Stephen L. Adler",
            "Indrajit Mitra"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  We evaluate the exact equilibrium distribution of gas molecules adsorbed on\nan active surface with an infinite number of attachment sites. Our result is a\nPoisson distribution having mean $X = {\\mu P P_s \\over P_e}$, with $\\mu$ the\nmean gas density, $ P_s$ the sticking probability, $P_e$ the evaporation\nprobability in a time interval $\\tau$, and $P$ Smoluchowski's exit probability\nin time interval $\\tau$ for the surface in question. We then solve for the case\nof a finite number of attachment sites using the mean field approximation,\nrecovering in this case the Langmuir isotherm.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0003002v1"
    },
    {
        "title": "On mixing times for stratified walks on the d-cube",
        "authors": [
            "Nancy L. Garcia",
            "Jose L. Palacios"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  Using the electric and coupling approaches, we derive a series of results\nconcerning the mixing times for the stratified random walk on the d-cube,\ninspired in the results of Chung and Graham (1997) Stratified random walks on\nthe n-cube.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0003006v1"
    },
    {
        "title": "XAFS spectroscopy. I. Extracting the fine structure from the absorption\n  spectra",
        "authors": [
            "K. V. Klementev"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  Three independent techniques are used to separate fine structure from the\nabsorption spectra, the background function in which is approximated by (i)\nsmoothing spline. We propose a new reliable criterion for determination of\nsmoothing parameter and the method for raising of stability with respect to\nk_min variation; (ii) interpolation spline with the varied knots; (iii) the\nline obtained from bayesian smoothing. This methods considers various prior\ninformation and includes a natural way to determine the errors of XAFS\nextraction. Particular attention has been given to the estimation of\nuncertainties in XAFS data. Experimental noise is shown to be essentially\nsmaller than the errors of the background approximation, and it is the latter\nthat determines the variances of structural parameters in subsequent fitting.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0003086v1"
    },
    {
        "title": "XAFS spectroscopy. II. Statistical evaluations in the fitting problems",
        "authors": [
            "K. V. Klementev"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  The problem of error analysis is addressed in stages beginning with the case\nof uncorrelated parameters and proceeding to the Bayesian problem that takes\ninto account all possible correlations when a great deal of prior information\nabout the accessible parametr space is available. The formulas for the standard\ndeviations and deviations with arbitrary confidence levels are derived.\nUnderestimation of the errors of XAFS-function extraction is shown to be a\nsource of unjustified optimistic errors of fitting parameters. The applications\nof statistical chi^2- and F-tests to the fitting problems are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0003087v1"
    },
    {
        "title": "Tsallis' entropy maximization procedure revisited",
        "authors": [
            "S. Martinez",
            "F. Nicolas",
            "F. Pennini",
            "A. Plastino"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  The proper way of averaging is an important question with regards to Tsallis'\nThermostatistics. Three different procedures have been thus far employed in the\npertinent literature. The third one, i.e., the Tsallis-Mendes-Plastino (TMP)\nnormalization procedure, exhibits clear advantages with respect to earlier\nones. In this work, we advance a distinct (from the TMP-one) way of handling\nthe Lagrange multipliers involved in the extremization process that leads to\nTsallis' statistical operator. It is seen that the new approach considerably\nsimplifies the pertinent analysis without losing the beautiful properties of\nthe Tsallis-Mendes-Plastino formalism.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0003098v2"
    },
    {
        "title": "Singularities in kinetic theory",
        "authors": [
            "C. Y. Chen"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  It is revealed that distribution functions of practical gases relate to\nsingularities and such singularities can, with molecular motion, spread to the\nentire region of interest. It is also shown that even common continuous\ndistribution functions involve a similar quasi-discontinuity difficulty.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0006009v1"
    },
    {
        "title": "On the Confidence Interval for the parameter of Poisson Distribution",
        "authors": [
            "S. I. Bityukov",
            "N. V. Krasnikov",
            "V. A. Taperechkina"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  The possibility of construction of continuous analogue of Poisson\ndistribution with the search of bounds of confidence intervals for parameter of\nPoisson distribution is discussed. Also, in the article is shown that the true\nvalue of a parameter of Poisson distribution for the observed value $\\hat x$\nhas Gamma distribution with the scale parameter, which is equal to one, and the\nshape parameter, which is equal to $\\hat x$. The results of numerical\nconstruction of confidence intervals are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0008082v3"
    },
    {
        "title": "A Generalization of the Maximum Noise Fraction Transform",
        "authors": [
            "Christopher Gordon"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  A generalization of the maximum noise fraction (MNF) transform is proposed.\nPowers of each band are included as new bands before the MNF transform is\nperformed. The generalized MNF (GMNF) is shown to perform better than the MNF\non a time dependent airborne electromagnetic (AEM) data filtering problem.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0009019v1"
    },
    {
        "title": "Maximally Informative Statistics",
        "authors": [
            "David R. Wolf",
            "Edward I. George"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  In this paper we propose a Bayesian, information theoretic approach to\ndimensionality reduction. The approach is formulated as a variational principle\non mutual information, and seamlessly addresses the notions of sufficiency,\nrelevance, and representation. Maximally informative statistics are shown to\nminimize a Kullback-Leibler distance between posterior distributions.\nIllustrating the approach, we derive the maximally informative one dimensional\nstatistic for a random sample from the Cauchy distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0010039v1"
    },
    {
        "title": "Computer simulation approach to reliability and accuracy in EXAFS\n  structural determinations",
        "authors": [
            "Paolo Ghigna",
            "Melissa di Muri",
            "Giorgio Spinolo"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  The frequency distribution of different parameters of an EXAFS spectrum can\nbe directly sampled by analysing a population of simulated spectra produced by\nadding computer-generated noise to a reference pattern. The procedure gives\nstatistical estimators of the parameter obtained with different data processing\nstrategies to test the performance of a strategy, to evaluate the bias\nintroduced by random noise, and to clarify the amount of information actually\ncontained in an experimental spectrum. Examples are given for the two simple\nlocal structures of an Ag atom sorrounded by two oxygens or by six iodines.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0010048v1"
    },
    {
        "title": "Optimal Recovery of Local Truth",
        "authors": [
            "Carlos C. Rodriguez"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  Probability mass curves the data space with horizons. Let f be a multivariate\nprobability density function with continuous second order partial derivatives.\nConsider the problem of estimating the true value of f(z) > 0 at a single point\nz, from n independent observations. It is shown that, the fastest possible\nestimators (like the k-nearest neighbor and kernel) have minimum asymptotic\nmean square errors when the space of observations is thought as conformally\ncurved. The optimal metric is shown to be generated by the Hessian of f in the\nregions where the Hessian is definite. Thus, the peaks and valleys of f are\nsurrounded by singular horizons when the Hessian changes signature from\nRiemannian to pseudo-Riemannian. Adaptive estimators based on the optimal\nvariable metric show considerable theoretical and practical improvements over\ntraditional methods. The formulas simplify dramatically when the dimension of\nthe data space is 4. The similarities with General Relativity are striking but\npossibly illusory at this point. However, these results suggest that\nnonparametric density estimation may have something new to say about current\nphysical theory.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0010063v1"
    },
    {
        "title": "Role and meaning of subjective probability: some comments on common\n  misconceptions",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  Criticisms of so called `subjective probability' come on the one hand from\nthose who maintain that probability in physics has only a frequentistic\ninterpretation, and, on the other, from those who tend to `objectivise'\nBayesian theory, arguing, e.g., that subjective probabilities are indeed based\n`only on private introspection'. Some of the common misconceptions on\nsubjective probability will be commented upon in support of the thesis that\ncoherence is the most crucial, universal and `objective' way to assess our\nconfidence on events of any kind.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0010064v1"
    },
    {
        "title": "A Good Measure for Bayesian Inference",
        "authors": [
            "Hanns L. Harney"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  The Gaussian theory of errors has been generalized to situations, where the\nGaussian distribution and, hence, the Gaussian rules of error propagation are\ninadequate. The generalizations are based on Bayes' theorem and a suitable\nmeasure. The following text sketches some chapters of a monograph that is\npresently prepared. We concentrate on the material that is - to the best of our\nknowledge - not yet in the statistical literature. See especially the extension\nof form invariance to discrete data in section 4, the criterion on the\ncompatibility between a proposed distribution and sparse data in section 7 and\nthe ``discovery'' of probability amplitudes in section 9.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0103030v1"
    },
    {
        "title": "Complexity Through Nonextensivity",
        "authors": [
            "William Bialek",
            "Ilya Nemenman",
            "Naftali Tishby"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  The problem of defining and studying complexity of a time series has\ninterested people for years. In the context of dynamical systems, Grassberger\nhas suggested that a slow approach of the entropy to its extensive asymptotic\nlimit is a sign of complexity. We investigate this idea further by information\ntheoretic and statistical mechanics techniques and show that these arguments\ncan be made precise, and that they generalize many previous approaches to\ncomplexity, in particular unifying ideas from the physics literature with ideas\nfrom learning and coding theory; there are even connections of this statistical\napproach to algorithmic or Kolmogorov complexity. Moreover, a set of simple\naxioms similar to those used by Shannon in his development of information\ntheory allows us to prove that the divergent part of the subextensive component\nof the entropy is a unique complexity measure. We classify time series by their\ncomplexities and demonstrate that beyond the `logarithmic' complexity classes\nwidely anticipated in the literature there are qualitatively more complex,\n`power--law' classes which deserve more attention.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0103076v1"
    },
    {
        "title": "Quantum Clustering",
        "authors": [
            "David Horn",
            "Assaf Gottlieb"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  We propose a novel clustering method that is based on physical intuition\nderived from quantum mechanics. Starting with given data points, we construct a\nscale-space probability function. Viewing the latter as the lowest eigenstate\nof a Schrodinger equation, we use simple analytic operations to derive a\npotential function whose minima determine cluster centers. The method has one\nparameter, determining the scale over which cluster structures are searched. We\ndemonstrate it on data analyzed in two dimensions (chosen from the eigenvectors\nof the correlation matrix). The method is applicable in higher dimensions by\nlimiting the evaluation of the Schrodinger potential to the locations of data\npoints. In this case the method may be formulated in terms of distances between\ndata points.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0107063v1"
    },
    {
        "title": "Entropy and inference, revisited",
        "authors": [
            "Ilya Nemenman",
            "Fariel Shafee",
            "William Bialek"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  We study properties of popular near-uniform (Dirichlet) priors for learning\nundersampled probability distributions on discrete nonmetric spaces and show\nthat they lead to disastrous results. However, an Occam-style phase space\nargument expands the priors into their infinite mixture and resolves most of\nthe observed problems. This leads to a surprisingly good estimator of entropies\nof discrete distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0108025v2"
    },
    {
        "title": "Quasi-optimal observables: Attaining the quality of maximal likelihood\n  in parameter estimation when only a MC event generator is available",
        "authors": [
            "F. V. Tkachov"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  A new method of quasi-optimal observables allows one to approach the quality\nof data processing usually associated with the method of maximal likelihood\nwithin the simpler algorithmic context of generalized moments.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0108030v1"
    },
    {
        "title": "On a quantitative method to analyse dynamical and measurement noise",
        "authors": [
            "M. Siefert",
            "J. Peinke",
            "R. Friedrich"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  This letter reports on a new method of analysing experimentally gained time\nseries with respect to different types of noise involved, namely, we show that\nit is possible to differentiate between dynamical and measurement noise. This\nmethod does not depend on previous knowledge of model equations. For the\ncomplicated case of a chaotic dynamics spoiled at the same time by dynamical\nand measurement noise, we even show how to extract from data the magnitude of\nboth types of noise. As a further result, we present a new criterion to verify\nthe correct embedding for chaotic dynamics with dynamical noise.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0108034v2"
    },
    {
        "title": "Characterization of a Low Frequency Power Spectral Density f^(-gamma) in\n  a Threshold Model",
        "authors": [
            "Erika D'Ambrosio"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  his study investigates the modifications of the thermal spectrum, at low\nfrequency, induced by an external damping on a system in heat contact with\ninternal fluctuating impurities. Those impurities can move among locations and\ntheir oscillations are associated with a loss function depending on the model.\nThe fluctuation properties of the system are provided by a potential function\nshaped by wells, in such a way that jumps between the stationary positions are\nallowed. The power spectral density associated with this dissipation mechanism\nshows a f^(-gamma)tail. The interest of this problem is that many systems are\ncharacterized by a typical f^(-\\gamma) spectral tail at low frequency. The\nmodel presented in this article is based on a threshold type behaviour and its\ngenerality allows applications in several fields. The effects of an external\nforce, introduced to produce damping, are studied by using both analytical\ntechniques and numerical simulations. The results obtained with the present\nmodel show that no reduction of the power spectral density is appreciable below\nthe main peak of the spectral density.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0110019v1"
    },
    {
        "title": "Reconstruction of dynamical equations for traffic flow",
        "authors": [
            "S. Kriso",
            "R. Friedrich",
            "J. Peinke",
            "P. Wagner"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  Traffic flow data collected by an induction loop detector on the highway\nclose to Koeln-Nord are investigated with respect to their dynamics including\nthe stochastic content. In particular we present a new method, with which the\nflow dynamics can be extracted directly from the measured data. As a result a\nLangevin equation for the traffic flow is obtained. From the deterministic part\nof the flow dynamics, stable fixed points are extracted and set into relation\nwith common features of the fundamental diagram.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0110084v1"
    },
    {
        "title": "Bayesian inference for inverse problems",
        "authors": [
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  Traditionally, the MaxEnt workshops start by a tutorial day. This paper\nsummarizes my talk during 2001'th workshop at John Hopkins University. The main\nidea in this talk is to show how the Bayesian inference can naturally give us\nall the necessary tools we need to solve real inverse problems: starting by\nsimple inversion where we assume to know exactly the forward model and all the\ninput model parameters up to more realistic advanced problems of myopic or\nblind inversion where we may be uncertain about the forward model and we may\nhave noisy data. Starting by an introduction to inverse problems through a few\nexamples and explaining their ill posedness nature, I briefly presented the\nmain classical deterministic methods such as data matching and classical\nregularization methods to show their limitations. I then presented the main\nclassical probabilistic methods based on likelihood, information theory and\nmaximum entropy and the Bayesian inference framework for such problems. I show\nthat the Bayesian framework, not only generalizes all these methods, but also\ngives us natural tools, for example, for inferring the uncertainty of the\ncomputed solutions, for the estimation of the hyperparameters or for handling\nmyopic or blind inversion problems. Finally, through a deconvolution problem\nexample, I presented a few state of the art methods based on Bayesian inference\nparticularly designed for some of the mass spectrometry data processing\nproblems.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0110093v1"
    },
    {
        "title": "Penalized maximum likelihood for multivariate Gaussian mixture",
        "authors": [
            "Hichem Snoussi",
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  In this paper, we first consider the parameter estimation of a multivariate\nrandom process distribution using multivariate Gaussian mixture law. The labels\nof the mixture are allowed to have a general probability law which gives the\npossibility to modelize a temporal structure of the process under study. We\ngeneralize the case of univariate Gaussian mixture in [Ridolfi99] to show that\nthe likelihood is unbounded and goes to infinity when one of the covariance\nmatrices approaches the boundary of singularity of the non negative definite\nmatrices set. We characterize the parameter set of these singularities. As a\nsolution to this degeneracy problem, we show that the penalization of the\nlikelihood by an Inverse Wishart prior on covariance matrices results to a\npenalized or maximum a posteriori criterion which is bounded. Then, the\nexistence of positive definite matrices optimizing this criterion can be\nguaranteed. We also show that with a modified EM procedure or with a Bayesian\nsampling scheme, we can constrain covariance matrices to belong to a particular\nsubclass of covariance matrices. Finally, we study degeneracies in the source\nseparation problem where the characterization of parameter singularity set is\nmore complex. We show, however, that Inverse Wishart prior on covariance\nmatrices eliminates the degeneracies in this case too.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111007v1"
    },
    {
        "title": "Bayesian source separation with mixture of Gaussians prior for sources\n  and Gaussian prior for mixture coefficients",
        "authors": [
            "Hichem Snoussi",
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  In this contribution, we present new algorithms to source separation for the\ncase of noisy instantaneous linear mixture, within the Bayesian statistical\nframework. The source distribution prior is modeled by a mixture of Gaussians\n[Moulines97] and the mixing matrix elements distributions by a Gaussian\n[Djafari99a]. We model the mixture of Gaussians hierarchically by mean of\nhidden variables representing the labels of the mixture. Then, we consider the\njoint a posteriori distribution of sources, mixing matrix elements, labels of\nthe mixture and other parameters of the mixture with appropriate prior\nprobability laws to eliminate degeneracy of the likelihood function of variance\nparameters and we propose two iterative algorithms to estimate jointly sources,\nmixing matrix and hyperparameters: Joint MAP (Maximum a posteriori) algorithm\nand penalized EM algorithm. The illustrative example is taken in [Macchi99] to\ncompare with other algorithms proposed in literature. Keywords: Source\nseparation, Gaussian mixture, classification, JMAP algorithm, Penalized EM\nalgorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111018v1"
    },
    {
        "title": "Model selection for inverse problems: Best choice of basis functions and\n  model order selection",
        "authors": [
            "A. Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  A complete solution for an inverse problem needs five main steps: choice of\nbasis functions for discretization, determination of the order of the model,\nestimation of the hyperparameters, estimation of the solution, and finally,\ncharacterization of the proposed solution. Many works have been done for the\nthree last steps. The first two have been neglected for a while, in part due to\nthe complexity of the problem. However, in many inverse problems, particularly\nwhen the number of data is very low, a good choice of the basis functions and a\ngood selection of the order become primary. In this paper, we first propose a\ncomplete solution within a Bayesian framework. Then, we apply the proposed\nmethod to an inverse elastic electron scattering problem.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111020v1"
    },
    {
        "title": "Entropy in Signal Processing (Entropie en Traitement du Signal)",
        "authors": [
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  R\\'esum\\'e: Le principal objet de cette communication est de faire une\nr\\'etro perspective succincte de l'utilisation de l'entropie et du principe du\nmaximum d'entropie dans le domaine du traitement du signal. Apr\\`es un bref\nrappel de quelques d\\'efinitions et du principe du maximum d'entropie, nous\nverrons successivement comment l'entropie est utilis\\'ee en s\\'eparation de\nsources, en mod\\'elisation de signaux, en analyse spectrale et pour la\nr\\'esolution des probl\\`emes inverses lin\\'eaires. Mots cl\\'es : Entropie,\nEntropie crois\\'ee, Distance de Kullback, Information mutuelle, Estimation\nspectrale, Probl\\`emes inverses\n  Abstract: The main object of this work is to give a brief overview of the\ndifferent ways the entropy has been used in signal and image processing. After\na short introduction of different quantities related to the entropy and the\nmaximum entropy principle, we will study their use in different fields of\nsignal processing such as: source separation, model order selection, spectral\nestimation and, finally, general linear inverse problems. Keywords : Entropy,\nRelative entropy, Kullback distance, Mutual information, Spectral estimation,\nInverse problems.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111021v1"
    },
    {
        "title": "Shape reconstruction in X-ray tomography from a small number of\n  projections using deformable models",
        "authors": [
            "A. Mohammad-Djafari",
            "Ken Sauer"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  X-ray tomographic image reconstruction consists of determining an object\nfunction from its projections. In many applications such as non-destructive\ntesting, we look for a fault region (air) in a homogeneous, known background\n(metal). The image reconstruction problem then becomes the determination of the\nshape of the default region. Two approaches can be used: modeling the image as\na binary Markov random field and estimating the pixels of the image, or\nmodeling the shape of the fault and estimating it directly from the\nprojections. In this work we model the fault shape by a deformable polygonal\ndisc or a deformable polyhedral volume and propose a new method for directly\nestimating the coordinates of its vertices from a very limited number of its\nprojections. The basic idea is not new, but in other competing methods, in\ngeneral, the fault shape is modeled by a small number of parameters (polygonal\nshapes with very small number of vertices, snakes and deformable templates) and\nthese parameters are estimated either by least squares or by maximum likelihood\nmethods. We propose modeling the shape of the fault region by a polygon with a\nlarge number of vertices, allowing modeling of nearly any shape and estimation\nof its vertices' coordinates directly from the projections by defining the\nsolution as the minimizer of an appropriate regularized criterion. This\nformulation can also be interpreted as a maximum a posteriori (MAP) estimate in\na Bayesian estimation framework. To optimize this criterion we use either a\nsimulated annealing or a special purpose deterministic algorithm based on\niterated conditional modes (ICM). The simulated results are very encouraging,\nespecially when the number and the angles of projections are very limited.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111117v1"
    },
    {
        "title": "Probabilistic methods for data fusion",
        "authors": [
            "A. Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  The main object of this paper is to show how we can use classical\nprobabilistic methods such as Maximum Entropy (ME), maximum likelihood (ML)\nand/or Bayesian (BAYES) approaches to do microscopic and macroscopic data\nfusion. Actually ME can be used to assign a probability law to an unknown\nquantity when we have macroscopic data (expectations) on it. ML can be used to\nestimate the parameters of a probability law when we have microscopic data\n(direct observation). BAYES can be used to update a prior probability law when\nwe have microscopic data through the likelihood. When we have both microscopic\nand macroscopic data we can use first ME to assign a prior and then use BAYES\nto update it to the posterior law thus doing the desired data fusion. However,\nin practical data fusion applications, we may still need some engineering\nfeeling to propose realistic data fusion solutions. Some simple examples in\nsensor data fusion and image reconstruction using different kind of data are\npresented to illustrate these ideas. Keywords: Data fusion, Maximum entropy,\nMaximum likelihood, Bayesian data fusion, EM algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111118v1"
    },
    {
        "title": "A Bayesian Approach for the Determination of the Charge Density from\n  Elastic Electron Scattering Data",
        "authors": [
            "A. Mohammad-Djafari",
            "H. G. Miller"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  The problem of the determination of the charge density from limited\ninformation about the charge form factor is an ill-posed inverse problem. A\nBayesian probabilistic approach to this problem which permits to take into\naccount both errors and prior information about the solution is presented. We\nwill show that many classical methods can be considered as special cases of the\nproposed approach. We address also the problem of the basis function choice for\nthe discretization and the uncertainty of the solution. Some numerical results\nfor an analytical model are presented to show the performance of the proposed\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111119v1"
    },
    {
        "title": "A Bayesian Approach to Shape Reconstruction of a Compact Object from a\n  Few Number of Projections",
        "authors": [
            "A. Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  Image reconstruction in X ray tomography consists in determining an object\nfrom its projections. In many applications such as non destructive testing, we\nlook for an image who has a constant value inside a region (default) and\nanother constant value outside that region (homogeneous region surrounding the\ndefault). The image reconstruction problem becomes then the determination of\nthe shape of that region. In this work we model the object (the default region)\nas a polygonal disc and propose a new method for the estimation of the\ncoordinates of its vertices directly from a very limited number of its\nprojections. Keywords: Computed Imaging, Tomography, Shape reconstruction, Non\ndestructive testing, Regularization, Bayesian estimation, Deformable contours.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111120v1"
    },
    {
        "title": "New Advances in Bayesian Calculation for Linear and Nonlinear Inverse\n  Problems",
        "authors": [
            "A. Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  The Bayesian approach has proved to be a coherent approach to handle ill\nposed Inverse problems. However, the Bayesian calculations need either an\noptimization or an integral calculation. The maximum a posteriori (MAP)\nestimation requires the minimization of a compound criterion which, in general,\nhas two parts: a data fitting part and a prior part. In many situations the\ncriterion to be minimized becomes multimodal. The cost of the Simulated\nAnnealing (SA) based techniques is in general huge for inverse problems.\nRecently a deterministic optimization technique, based on Graduated Non\nConvexity (GNC), have been proposed to overcome this difficulty. The objective\nof this paper is to show two specific implementations of this technique for the\nfollowing situations: -- Linear inverse problems where the solution is modeled\nas a piecewise continuous function. The non convexity of the criterion is then\ndue to the special choice of the prior; -- A nonlinear inverse problem which\narises in inverse scattering where the non convexity of the criterion is due to\nthe likelihood part. Keywords: Inverse problems, Regularization, Bayesian\ncalculation, Global optimization, Graduated Non Convexity.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111121v1"
    },
    {
        "title": "A Comparison of Two Approaches: Maximum Entropy on the Mean (MEM) and\n  Bayesian Estimation (BAYES) for Inverse Problems",
        "authors": [
            "A. Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  To handle with inverse problems, two probabilistic approaches have been\nproposed: the maximum entropy on the mean (MEM) and the Bayesian estimation\n(BAYES). The main object of this presentation is to compare these two\napproaches which are in fact two different inference procedures to define the\nsolution of an inverse problem as the optimizer of a compound criterion.\nKeywords: Inverse problems, Maximum Entropy on the Mean, Bayesian inference,\nConvex analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111122v1"
    },
    {
        "title": "A full Bayesian approach for inverse problems",
        "authors": [
            "A. Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  The main object of this paper is to present some general concepts of Bayesian\ninference and more specifically the estimation of the hyperparameters in\ninverse problems. We consider a general linear situation where we are given\nsome data $\\yb$ related to the unknown parameters $\\xb$ by $\\yb=\\Ab \\xb+\\nb$\nand where we can assign the probability laws $p(\\xb|\\thetab)$,\n$p(\\yb|\\xb,\\betab)$, $p(\\betab)$ and $p(\\thetab)$. The main discussion is then\nhow to infer $\\xb$, $\\thetab$ and $\\betab$ either individually or any\ncombinations of them. Different situations are considered and discussed. As an\nimportant example, we consider the case where $\\theta$ and $\\beta$ are the\nprecision parameters of the Gaussian laws to whom we assign Gamma priors and we\npropose some new and practical algorithms to estimate them simultaneously.\nComparisons and links with other classical methods such as maximum likelihood\nare presented. Keywords: Bayesian inference, Hyperparameter estimation, Inverse\nproblems, Maximum likelihood.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111123v1"
    },
    {
        "title": "Scale Invariant Markov Models for Bayesian Inversion of Linear Inverse\n  Problems",
        "authors": [
            "Stéphane Brette",
            "Ali Mohammad-Djafari",
            "Jérôme Idier"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  In a Bayesian approach for solving linear inverse problems one needs to\nspecify the prior laws for calculation of the posterior law. A cost function\ncan also be defined in order to have a common tool for various Bayesian\nestimators which depend on the data and the hyperparameters. The Gaussian case\nexcepted, these estimators are not linear and so depend on the scale of the\nmeasurements. In this paper a weaker property than linearity is imposed on the\nBayesian estimator, namely the scale invariance property (SIP).First, we state\nsome results on linear estimation and then we introduce and justify a scale\ninvariance axiom. We show that arbitrary choice of scale measurement can be\navoided if the estimator has this SIP. Some examples of classical\nregularization procedures are shown to be scale invariant. Then we investigate\ngeneral conditions on classes of Bayesian estimators which satisfy this SIP, as\nwell as their consequences on the cost function and prior laws. We also show\nthat classical methods for hyperparameters estimation (i.e., Maximum Likelihood\nand Generalized Maximum Likelihood) can be introduced for hyperparameters\nestimation, and we verify the SIP property for them. Finally we discuss how to\nchoose the prior laws to obtain scale invariant Bayesian estimators. For this,\nwe consider two cases of prior laws: {\\em entropic prior laws} and {\\em\nfirst-order Markov models}. In related preceding works\n[Mohammad-Djafari90,Mohammad-Djafari93], the SIP constraints have been studied\nfor the case of entropic prior laws. In this paper extension to the case of\nfirst-order Markov models is provided. KEYWORDS: Bayesian estimation, Scale\ninvariance, Markov modelling, Inverse Problems, Image reconstruction, Prior\nmodel selection\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111124v1"
    },
    {
        "title": "A scale invariant Bayesian method to solve linear inverse problems",
        "authors": [
            "A. Mohammad-Djafari",
            "Jérôme Idier"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  In this paper we propose a new Bayesian estimation method to solve linear\ninverse problems in signal and image restoration and reconstruction problems\nwhich has the property to be scale invariant. In general, Bayesian estimators\nare {\\em nonlinear} functions of the observed data. The only exception is the\nGaussian case. When dealing with linear inverse problems the linearity is\nsometimes a too strong property, while {\\em scale invariance} often remains a\ndesirable property. As everybody knows one of the main difficulties with using\nthe Bayesian approach in real applications is the assignment of the direct\n(prior) probability laws before applying the Bayes' rule. We discuss here how\nto choose prior laws to obtain scale invariant Bayesian estimators. In this\npaper we discuss and propose a familly of generalized exponential probability\ndistributions functions for the direct probabilities (the prior $p(\\xb)$ and\nthe likelihood $p(\\yb|\\xb)$), for which the posterior $p(\\xb|\\yb)$, and,\nconsequently, the main posterior estimators are scale invariant. Among many\nproperties, generalized exponential can be considered as the maximum entropy\nprobability distributions subject to the knowledge of a finite set of\nexpectation values of some knwon functions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111125v1"
    },
    {
        "title": "A Matlab Program to Calculate the Maximum Entropy Distributions",
        "authors": [
            "A. Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  The classical Maximum Entropy (ME) problem consists of determining a\nprobability distribution function (pdf) from a finite set of expectations of\nknown functions. The solution depends on $N+1$ Lagrange multipliers which are\ndetermined by solving the set of nonlinear equations formed by the $N$ data\nconstraints and the normalization constraint. In this short communication we\ngive three Matlab programs to calculate these Lagrange multipliers. The first\nconsiders the general case where the functions can be any functions. The second\nconsiders the special case of power functions $x^n$. In this case the data are\nthe geometrical moments of $p(x)$. The third considers the special case of\nFourier series functions $\\exp(-j n \\omega x)$. In this case the data are the\ntrigonometrical moments of $p(x)$. Some examples are also given to illustrate\nthe usefullness of these programs.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111126v1"
    },
    {
        "title": "Statistical inference and modeling with the S distribution",
        "authors": [
            "Sergej V. Aksenov",
            "Michael A. Savageau"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  We consider the problem of statistical inference for the S distribution and\nintroduce new minimum distance estimators for the four parameters of the S\ndistribution using Kolmogorov-Smirnov, Cramer-von Mises and related distance\nmetrics. Approximate goodness-of-fit and confidence intervals for parameters\nare calculated using bootstrap methods. We discuss further how the S\ndistribution can be used to solve various problems of statistical modeling\nassociated with parameter inference, including goodness-of-fit tests, Monte\nCarlo simulations and modeling trends in the distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0112046v1"
    },
    {
        "title": "Entropic Priors for Discrete Probabilistic Networks and for Mixtures of\n  Gaussians Models",
        "authors": [
            "Carlos C. Rodriguez"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  The ongoing unprecedented exponential explosion of available computing power,\nhas radically transformed the methods of statistical inference. What used to be\na small minority of statisticians advocating for the use of priors and a strict\nadherence to bayes theorem, it is now becoming the norm across disciplines. The\nevolutionary direction is now clear. The trend is towards more realistic,\nflexible and complex likelihoods characterized by an ever increasing number of\nparameters. This makes the old question of: What should the prior be? to\nacquire a new central importance in the modern bayesian theory of inference.\nEntropic priors provide one answer to the problem of prior selection. The\ngeneral definition of an entropic prior has existed since 1988, but it was not\nuntil 1998 that it was found that they provide a new notion of complete\nignorance. This paper re-introduces the family of entropic priors as minimizers\nof mutual information between the data and the parameters, as in\n[rodriguez98b], but with a small change and a correction. The general formalism\nis then applied to two large classes of models: Discrete probabilistic networks\nand univariate finite mixtures of gaussians. It is also shown how to perform\ninference by efficiently sampling the corresponding posterior distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0201016v1"
    },
    {
        "title": "Comment on \"Indispensable Finite Time Correlations for Fokker-Planck\n  Equations from Time Series Data\"",
        "authors": [
            "R. Friedrich",
            "Ch. Renner",
            "M. Siefert",
            "J. Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Comment on \"Indispensable Finite Time Correlations for Fokker-Planck\nEquations from Time Series Data\"\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0203005v1"
    },
    {
        "title": "Stochastic analysis of road surface roughness",
        "authors": [
            "M. Waechter",
            "F. Riess",
            "H. Kantz",
            "J. Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  This paper was withdrawn by the authors due to significant new findings. A\nnew paper on the same topic has been submitted as physics/0310159.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0203068v2"
    },
    {
        "title": "Evolving Networks with Multi-species Nodes and Spread in the Number of\n  Initial Links",
        "authors": [
            "Jong-Won Kim",
            "Brian Hunt",
            "Edward Ott"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  We consider models for growing networks incorporating two effects not\npreviously considered: (i) different species of nodes, with each species having\ndifferent properties (such as different attachment probabilities to other node\nspecies); and (ii) when a new node is born, its number of links to old nodes is\nrandom with a given probability distribution. Our numerical simulations show\ngood agreement with analytic solutions. As an application of our model, we\ninvestigate the movie-actor network with movies considered as nodes and actors\nas links.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0205002v1"
    },
    {
        "title": "Decomposition of multicomponent mass spectra using Bayesian probability\n  theory",
        "authors": [
            "H. D. Kang",
            "R. Preuss",
            "T. Schwarz-Selinger",
            "V. Dose"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  We present a method for the decomposition of mass spectra of mixture gases\nusing Bayesian probability theory. The method works without any calibration\nmeasurement and therefore applies also to the analysis of spectra containing\nunstable species. For the example of mixtures of three different hydrocarbon\ngases the algorithm provides concentrations and cracking coefficients of each\nmixture component as well as their confidence intervals. The amount of\ninformation needed to obtain reliable results and its relation to the accuracy\nof our analysis are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0205004v1"
    },
    {
        "title": "Computer simulations discussed in physical terms and terminology",
        "authors": [
            "D. Bar"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  As known, any numerical simulation is composed of two parts: (1) the initial\npart of writing the relevant code and (2) the running of this code on the\ncomputer screen. The second part of running the program is extensively\ndiscussed theoretically and technically in the relevant literature. In this\nwork we pay special attention to the less discussed first part and show that it\nmay be discussed in a terminology and notation which describe physical\nphenomena. As examples we discuss the two cases of simulating (1) the harmonic\noscillator and (2) the electron-photon interaction which results in the known\nLamb shift.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0205010v4"
    },
    {
        "title": "A two-dimensional rough surface: Experiments on a pile of rice",
        "authors": [
            "C. M. Aegerter",
            "R. Gunther",
            "R. J. Wijngaarden"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Dynamical roughening of interfaces has received much attention in recent\nyears. However, experiments have been restricted to one dimensional (1d)\nsystems. Moreover, theoretical studies of the two dimensional (2d) case have\nbeen highly inconclusive. Here we introduce an experimental 2d system, with\nwhich the theories can be tested. As is shown, the surface of a 2d pile of rice\nshows roughening behaviour in both space and time, with a roughness exponent\n$\\alpha_{2d}$ = 0.39(3) and a growth exponent $\\beta_{2d}$ = 0.27(3).\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0205015v1"
    },
    {
        "title": "Inference of entropies of discrete random variables with unknown\n  cardinalities",
        "authors": [
            "Ilya Nemenman"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  We examine the recently introduced NSB estimator of entropies of severely\nundersampled discrete variables and devise a procedure for calculating the\ninvolved integrals. We discover that the output of the estimator has a well\ndefined limit for large cardinalities of the variables being studied. Thus one\ncan estimate entropies with no a priori assumptions about these cardinalities,\nand a closed form solution for such estimates is given.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0207009v1"
    },
    {
        "title": "The effects of related experiments",
        "authors": [
            "D. Bar"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  The effects of the experiment itself upon the obtained results and,\nespecially, the influence of a large number of experiments are extensively\ndiscussed in the literature. We show that the important factor that stands at\nthe basis of these effects is that the involved experiments are related and not\nindependent and detached from each other. This relationship takes, as shown\nhere, different forms for different situations and is found in entirely\ndifferent physical regimes such as the quantum and classical ones.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0208054v4"
    },
    {
        "title": "Slow relaxation in weakly open vertex-splitting rational polygons",
        "authors": [
            "Valery B. Kokshenev",
            "Eduardo Vicentini"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  The problem of splitting effects by vertex angles is discussed for\nnonintegrable rational polygonal billiards. A statistical analysis of the decay\ndynamics in weakly open polygons is given through the orbit survival\nprobability. Two distinct channels for the late-time relaxation of type\n1/t^delta are established. The primary channel, associated with the universal\nrelaxation of ''regular'' orbits, with delta = 1, is common for both the closed\nand open, chaotic and nonchaotic billiards. The secondary relaxation channel,\nwith delta > 1, is originated from ''irregular'' orbits and is due to the\nrationality of vertices.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0208063v1"
    },
    {
        "title": "What is the true dropped calls rate when in the test it was found to be\n  zero?",
        "authors": [
            "M. V. Simkin",
            "J. Olness"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  We study the distributions of dropped calls rates for different wireless\n(cellular) carriers in different markets. Our statistics comprises over 700\ndifferent market/carrier combinations. We find that the dropped calls rates\ndistribution is very close to lognormal. We derive an equation for the most\nprobable dropped calls rate for particular carrier in particular market, which\ndepends on the number of dropped calls observed, total number of calls and the\nparameters of the lognormal distribution. We apply this analysis to blocked and\n\"no service\" calls as well.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0210022v1"
    },
    {
        "title": "NeXus Software Status",
        "authors": [
            "Mark Koennecke",
            "Uwe Filges"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  NeXus is a joint effort of both the synchrotron and neutron scattering\ncommunity to devlop a common data exchange format based on HDF. In order to\nsimplify access to NeXus-files a NeXus-API is provided. This NeXus-API has been\nredesigned and expanded to cover both HDF versions 4 and 5. Only small changes\nto the API were necessary in order to accomplish. A new NeXus-API for IDL based\non IDL C interface has been implemented.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0210072v1"
    },
    {
        "title": "Parameter identification using the Hilbert transform",
        "authors": [
            "Andrew Allison",
            "Derek Abbott"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Many physical systems can be adequately modelled using a second order\napproximation. The problem of plant identification reduces to the problem of\nestimating the position of a single pair of complex conjugate poles. One\napproach to the problem is to apply the method of least squares to the time\ndomain data. This type of computation is best carried out in \"batch\" mode and\napplies to an entire data set. Another approach would be to design an adaptive\nfilter and to use autoregressive, AR, techniques. This would be well suited to\ncontinuous real-time data and could track slow changes on the underlying plant.\nI this paper we present a very fast but approximate technique for the\nestimation of the position of a single pair of complex conjugate poles, using\nthe Hilbert transform to reconstruct the analytic signal.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0210108v1"
    },
    {
        "title": "GUI Tools for an Enhanced User Experience",
        "authors": [
            "P. A. Kienzle"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  For instruments with many occasional users, it is important to have easy to\nuse software. To support the frequent users it is important to be flexible.\nUsing a scripting language to design a GUI and exposing it to the user allows\nus to do both. We present our work on a GUI for reflectometry data analysis and\nreduction written in Tcl/Tk and Octave, with underlying C code for the\nnumerically intensive portions. As well as being easier to train new users, the\nnew software allows existing users to do in minutes what used to take hours.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0211036v1"
    },
    {
        "title": "The right tool for the job",
        "authors": [
            "P. A. Kienzle"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Tcl/tk provides for fast and flexible interface design but slow and\ncumbersome vector processing. Octave provides fast and flexible vector\nprocessing but slow and cumbersome interface design. Calling octave from tcl\ngives you the flexibility to do a broad range of fast numerical manipulations\nas part of an embedded GUI. We present a way to communicate between them.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0211037v1"
    },
    {
        "title": "Yet Another Analysis of Dice Problems",
        "authors": [
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  During the MaxEnt 2002 workshop in Moscow, Idaho, Tony Vignaux asked again a\nfew simple questions about using Maximum Entropy or Bayesian approaches for the\nfamous Dice problems which have been analyzed many times through this workshop\nand also in other places. Here, there is another analysis of these problems. I\nhope that, this paper will answer a few questions of Tony and other\nparticipants of the workshop on the situations where we can use Maximum Entropy\nor Bayesian approaches or even the cases where we can actually use both of\nthem.\n  Keywords: Dice problems and probability theory, Maximum Likelihood, Bayesian\ninference, Maximum A Posteriori, Entropy, Maximum entropy, Maximum entropy in\nthe mean.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0211049v1"
    },
    {
        "title": "MCMC joint separation and segmentation of hidden Markov fields",
        "authors": [
            "Hichem Snoussi",
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  In this contribution, we consider the problem of the blind separation of\nnoisy instantaneously mixed images. The images are modelized by hidden Markov\nfields with unknown parameters. Given the observed images, we give a Bayesian\nformulation and we propose to solve the resulting data augmentation problem by\nimplementing a Monte Carlo Markov Chain (MCMC) procedure. We separate the\nunknown variables into two categories:\n  1. The parameters of interest which are the mixing matrix, the noise\ncovariance and the parameters of the sources distributions. 2. The hidden\nvariables which are the unobserved sources and the unobserved pixels\nclassification labels.\n  The proposed algorithm provides in the stationary regime samples drawn from\nthe posterior distributions of all the variables involved in the problem\nleading to a flexibility in the cost function choice.\n  We discuss and characterize some problems of non identifiability and\ndegeneracies of the parameters likelihood and the behavior of the MCMC\nalgorithm in this case.\n  Finally, we show the results for both synthetic and real data to illustrate\nthe feasibility of the proposed solution. keywords: MCMC, blind source\nseparation, hidden Markov fields, segmentation, Bayesian approach\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0211051v1"
    },
    {
        "title": "Wavelet Domain Image Separation",
        "authors": [
            "Ali Mohammad-Djafari",
            "Mahieddine Ichir"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  In this paper, we consider the problem of blind signal and image separation\nusing a sparse representation of the images in the wavelet domain. We consider\nthe problem in a Bayesian estimation framework using the fact that the\ndistribution of the wavelet coefficients of real world images can naturally be\nmodeled by an exponential power probability density function. The Bayesian\napproach which has been used with success in blind source separation gives also\nthe possibility of including any prior information we may have on the mixing\nmatrix elements as well as on the hyperparameters (parameters of the prior laws\nof the noise and the sources). We consider two cases: first the case where the\nwavelet coefficients are assumed to be i.i.d. and second the case where we\nmodel the correlation between the coefficients of two adjacent scales by a\nfirst order Markov chain. This paper only reports on the first case, the second\ncase results will be reported in a near future. The estimation computations are\ndone via a Monte Carlo Markov Chain (MCMC) procedure. Some simulations show the\nperformances of the proposed method. Keywords: Blind source separation,\nwavelets, Bayesian estimation, MCMC Hasting-Metropolis algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0211052v2"
    },
    {
        "title": "Fisher Information With Respect to Cumulants",
        "authors": [
            "S. Prasad",
            "N. C. Menicucci"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Fisher information is a measure of the best precision with which a parameter\ncan be estimated from statistical data. It can also be defined for a continuous\nrandom variable without reference to any parameters, in which case it has a\nphysically compelling interpretation of representing the highest precision with\nwhich the first cumulant of the random variable, i.e., its mean, can be\nestimated from its statistical realizations. We construct a complete hierarchy\nof information measures that determine the best precision with which all of the\ncumulants of a random variable -- and thus its complete probability\ndistribution -- can be estimated from its statistical realizations. Several\nproperties of these information measures and their generating functions are\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0212035v2"
    },
    {
        "title": "Comment on \"Including Systematic Uncertainties in Confidence Interval\n  Construction for Poisson Statistics\"",
        "authors": [
            "Gary C. Hill"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The incorporation of systematic uncertainties into confidence interval\ncalculations has been addressed recently in a paper by Conrad et al. (Physical\nReview D 67 (2003) 012002). In their work, systematic uncertainities in\ndetector efficiencies and background flux predictions were incorporated\nfollowing the hybrid frequentist-Bayesian prescription of Cousins and Highland,\nbut using the likelihood ratio ordering of Feldman and Cousins in order to\nproduce \"unified\" confidence intervals. In general, the resulting intervals\nbehaved as one would intuitively expect, i.e. increased with increasing\nuncertainties. However, it was noted that for numbers of observed events less\nthan or of order of the expected background, the intervals could sometimes\nbehave in a completely counter-intuitive fashion -- being seen to initially\ndecrease in the face of increasing uncertainties, but only for the case of\nincreasing signal efficiency uncertainty. In this comment, we show that the\nproblematic behaviour is due to integration over the signal efficiency\nuncertainty while maximising the best fit alternative hypothesis likelihood. If\nthe alternative hypothesis likelihood is determined by unconditionally\nmaximising with respect to both the unknown signal and signal efficiency\nuncertainty, the limits display the correct intuitive behaviour.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0302057v1"
    },
    {
        "title": "Transient of the kinetic spherical model between two temperatures",
        "authors": [
            "Chunshan He",
            "Zhibing Li"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We solve the dynamic equation for the kinetic spherical model that initially\nis in an arbitrary equilibrium state and then is left to evolve in a heat-bath\nwith another temperature. Flows of the Renormalizational group are determined.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0303060v1"
    },
    {
        "title": "The power law character of off-site power failures",
        "authors": [
            "A. John Arul",
            "C. Senthil Kumar",
            "S. Marimuthu",
            "Om Pal Singh"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  A study on the behavior of off-site AC power failure recovery times at three\nnuclear plant sites is presented. It is shown, that power law is appropriate\nfor the representation of failure frequency-duration correlation function of\noff-site power failure events, based on simple assumptions about component\nfailure and repair rates. It is also found that the annual maxima of power\nfailure duration follow Frechet distribution, which is a type II asymptotic\ndistribution, strengthening our assumption of power law for the parent\ndistribution. The extreme value distributions obtained are used to extrapolate\nfor failure durations beyond the observed range.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0303067v1"
    },
    {
        "title": "Inference for bounded parameters",
        "authors": [
            "D. A. S. Fraser",
            "N. Reid",
            "A. C. M. Wong"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The estimation of signal frequency count in the presence of background noise\nhas had much discussion in the recent physics literature, and Mandelkern [1]\nbrings the central issues to the statistical community, leading in turn to\nextensive discussion by statisticians. The primary focus however in [1] and the\naccompanying discussion is on the construction of a confidence interval. We\nargue that the likelihood function and $p$-value function provide a\ncomprehensive presentation of the information available from the model and the\ndata. This is illustrated for Gaussian and Poisson models with lower bounds for\nthe mean parameter.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0303111v2"
    },
    {
        "title": "Internet websites statistics expressed in the framework of the\n  Ursell-Mayer cluster formalism",
        "authors": [
            "D. Bar"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We show that it is possible to generalize the Ursell-Mayer cluster formalism\nso that it may cover also the statistics of Internet websites. Our starting\npoint is the introduction of an extra variable that is assumed to take account,\nas will be explained, of the nature of the Internet statistics. We then show,\nfollowing the arguments in Mayer, that one may obtain a phase transition-like\nphenomena\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0304021v2"
    },
    {
        "title": "Bayesian Inference in Processing Experimental Data: Principles and Basic\n  Applications",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  This report introduces general ideas and some basic methods of the Bayesian\nprobability theory applied to physics measurements. Our aim is to make the\nreader familiar, through examples rather than rigorous formalism, with concepts\nsuch as: model comparison (including the automatic Ockham's Razor filter\nprovided by the Bayesian approach); parametric inference; quantification of the\nuncertainty about the value of physical quantities, also taking into account\nsystematic effects; role of marginalization; posterior characterization;\npredictive distributions; hierarchical modelling and hyperparameters; Gaussian\napproximation of the posterior and recovery of conventional methods, especially\nmaximum likelihood and chi-square fits under well defined conditions; conjugate\npriors, transformation invariance and maximum entropy motivated priors; Monte\nCarlo estimates of expectation, including a short introduction to Markov Chain\nMonte Carlo methods.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0304102v1"
    },
    {
        "title": "Multiscale Trend Analysis",
        "authors": [
            "I. Zaliapin",
            "A. Gabrielov",
            "V. Keilis-Borok"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  This paper introduces a multiscale analysis based on optimal piecewise linear\napproximations of time series. An optimality criterion is formulated and on its\nbase a computationally effective algorithm is constructed for decomposition of\na time series into a hierarchy of trends (local linear approximations) at\ndifferent scales. The top of the hierarchy is the global linear approximation\nover the whole observational interval, the bottom is the original time series.\nEach internal level of the hierarchy corresponds to a piecewise linear\napproximation of analyzed series. Possible applications of the introduced\nMultiscale Trend Analysis (MTA) go far beyond the linear interpolation problem:\nThis paper develops and illustrates methods of self-affine, hierarchical, and\ncorrelation analyses of time series.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0305013v2"
    },
    {
        "title": "Bayesian inference of nanoparticle-broadened x-ray line profiles",
        "authors": [
            "N. Armstrong",
            "W. Kalceff",
            "J. P. Cline",
            "J. Bonevich"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  A single and self-contained method for determining the crystallite-size\ndistribution and shape from experimental x-ray line profile data is presented.\nWe have shown that the crystallite-size distribution can be determined without\nassuming a functional form for the size distribution, determining instead the\nsize distribution with the least assumptions by applying the Bayesian/MaxEnt\nmethod. The Bayesian/MaxEnt method is tested using both simulated and\nexperimental CeO$_{2}$ data. The results demonstrate that the proposed method\ncan determine size distributions, while making the least number of assumptions.\nThe comparison of the Bayesian/MaxEnt results from experimental CeO$_2$ with\nTEM results is favorable\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0305018v1"
    },
    {
        "title": "Fitting a Sum of Exponentials to Numerical Data",
        "authors": [
            "Bernhard Kaufmann"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  A finite sum of exponential functions may be expressed by a linear\ncombination of powers of the independent variable and by successive integrals\nof the sum. This is proved for the general case and the connection between the\nparameters in the sum and the coefficients in the linear combination is\nhighlighted. The fitting of exponential functions to a given data- set is\ntherefore reduced to a multilinear approximation procedure. The results of this\napproximation do not only provide the necessary information to compute the\nfactors in the exponents and the weights of the exponential terms but also they\nare used to estimate the errors in the factors.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0305019v1"
    },
    {
        "title": "Next-Generation EU DataGrid Data Management Services",
        "authors": [
            "Diana Bosio",
            "James Casey",
            "Akos Frohner",
            "Leanne Guy",
            "Peter Kunszt",
            "Erwin Laure",
            "Sophie Lemaitre",
            "Levi Lucio",
            "Heinz Stockinger",
            "Kurt Stockinger",
            "William Bell",
            "David Cameron",
            "Gavin McCance",
            "Paul Millar",
            "Joni Hahkala",
            "Niklas Karlsson",
            "Ville Nenonen",
            "Mika Silander",
            "Olle Mulmo",
            "Gian-Luca Volpato",
            "Giuseppe Andronico"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We describe the architecture and initial implementation of the\nnext-generation of Grid Data Management Middleware in the EU DataGrid (EDG)\nproject.\n  The new architecture stems out of our experience and the users requirements\ngathered during the two years of running our initial set of Grid Data\nManagement Services. All of our new services are based on the Web Service\ntechnology paradigm, very much in line with the emerging Open Grid Services\nArchitecture (OGSA). We have modularized our components and invested a great\namount of effort towards a secure, extensible and robust service, starting from\nthe design but also using a streamlined build and testing framework.\n  Our service components are: Replica Location Service, Replica Metadata\nService, Replica Optimization Service, Replica Subscription and high-level\nreplica management. The service security infrastructure is fully GSI-enabled,\nhence compatible with the existing Globus Toolkit 2-based services; moreover,\nit allows for fine-grained authorization mechanisms that can be adjusted\ndepending on the service semantics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0305134v2"
    },
    {
        "title": "Go4 v2 Analysis Framework",
        "authors": [
            "J. Adamczewski",
            "M. Al-Turany",
            "D. Bertini",
            "H. G. Essel",
            "S. Linev"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  Go4 developed at GSI is an analysis framework with a general purpose non\nblocking GUI. Go4 is based on ROOT. The GUI is implemented in Qt using GSI's\nQtROOT interface. Analysis and GUI run in separate tasks communicating through\nasynchronous object channels. A Go4 analysis may use any ROOT features. It can\nbe organized in steps, which can be controlled from the GUI according to the\nuser specifications. Each step is composed of event objects, the event\nprocessing, and event IO. Go4 composite event classes allow the construction of\narbitrary complex events hierarchically composed of objects. The IO of the\ncomposite event objects to and from ROOT trees/branches is provided without\nexplicit programming. The Go4 tree viewer can browse arbitrary hierarchy levels\nof composite events. The GUI provides hooks to attach user written GUIs. These\nGUIs have access to all objects of the analysis, i.e. events for asynchronous\nevent display. Using the Qt designer the development of such GUIs is very\nefficient. The Go4 fit package (API and GUI) is a powerful and extendable tool\nto model and fit experimental data. The Go4 framework is especially useful for\non-line monitoring. The HADES experiment at GSI integrated the existing ROOT\nbased analysis into Go4 and is using it on-line with dedicated GUIs.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306022v1"
    },
    {
        "title": "Revison Control in the Grid Era - the unmet challenge",
        "authors": [
            "Arthur Kreymer"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  As we move to distribute High Energy Physics computing tasks throughout the\nglobal Grid, we are encountering ever more severe difficulties installing and\nselecting appropriate versions of the supporting products. Problems show up at\nevery level: the base operating systems and tools, general purpose utilities\nlike root, and specific releases of application code. I will discuss some\nspecific examples, including what we've learned in commissioning the SAM\nsoftware for use by CDF. I will show why revision control can be a truly\ndifficult problem, and will discuss what we've been doing to measure and\ncontrol the situation.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306036v1"
    },
    {
        "title": "The full detector simulation for the Atlas experiment: status and\n  outlook",
        "authors": [
            "A. Rimoldi",
            "A. Dell'Acqua"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The simulation of the ATLAS detector is a major challenge, given the\ncomplexity of the detector and the demanding environment of the LHC. The\napparatus, one of the biggest and most complex ever designed, requires a\ndetailed, flexible and, if possible, fast simulation which is needed already\ntoday to deal with questions related to design optimization, to issues raised\nby staging scenarios, and of course to enable detailed physics studies to lay\nthe basis for the first physics discoveries. Scalability and robustness stand\nout as the most critical issues that are to be faced in the implementation of\nsuch a simulation. In this paper we present the status of the present\nsimulation and the adopted solutions in terms of speed optimization,\ncentralization of services, framework facilities and persistency solutions.\nEmphasis is put on the global performance when the different detector\ncomponents are collected together in a full and detailed simulation. The\nreference tool adopted is Geant4.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306086v1"
    },
    {
        "title": "Reconstruction of electrons with the Gaussian-sum filter in the CMS\n  tracker at LHC",
        "authors": [
            "W. Adam",
            "R. Frühwirth",
            "A. Strandlie",
            "T. Todorov"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The bremsstrahlung energy loss distribution of electrons propagating in\nmatter is highly non Gaussian. Because the Kalman filter relies solely on\nGaussian probability density functions, it might not be an optimal\nreconstruction algorithm for electron tracks. A Gaussian-sum filter (GSF)\nalgorithm for electron track reconstruction in the CMS tracker has therefore\nbeen developed. The basic idea is to model the bremsstrahlung energy loss\ndistribution by a Gaussian mixture rather than a single Gaussian. It is shown\nthat the GSF is able to improve the momentum resolution of electrons compared\nto the standard Kalman filter. The momentum resolution and the quality of the\nestimated error are studied with various types of mixture models of the energy\nloss distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306087v1"
    },
    {
        "title": "CMS Data Analysis: Current Status and Future Strategy",
        "authors": [
            "Vincenzo Innocente"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We present the current status of CMS data analysis architecture and describe\nwork on future Grid-based distributed analysis prototypes. CMS has two main\nsoftware frameworks related to data analysis: COBRA, the main framework, and\nIGUANA, the interactive visualisation framework. Software using these\nframeworks is used today in the world-wide production and analysis of CMS data.\nWe describe their overall design and present examples of their current use with\nemphasis on interactive analysis. CMS is currently developing remote analysis\nprototypes, including one based on Clarens, a Grid-enabled client-server tool.\nUse of the prototypes by CMS physicists will guide us in forming a\nGrid-enriched analysis strategy. The status of this work is presented, as is an\noutline of how we plan to leverage the power of our existing frameworks in the\nmigration of CMS software to the Grid.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306091v1"
    },
    {
        "title": "TPC tracking and particle identification in high-density environment",
        "authors": [
            "M. Ivanov",
            "K. Safarik",
            "Y. Belikov",
            "J. Bracinik"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  Track finding and fitting algorithm in the ALICE Time projection chamber\n(TPC) based on Kalman-filtering is presented. Implementation of particle\nidentification (PID) using d$E$/d$x$ measurement is discussed. Filtering and\nPID algorithm is able to cope with non-Gaussian noise as well as with ambiguous\nmeasurements in a high-density environment. The occupancy can reach up to 40%\nand due to the overlaps, often the points along the track are lost and others\nare significantly displaced. In the present algorithm, first, clusters are\nfound and the space points are reconstructed. The shape of a cluster provides\ninformation about overlap factor. Fast spline unfolding algorithm is applied\nfor points with distorted shapes. Then, the expected space point error is\nestimated using information about the cluster shape and track parameters.\nFurthermore, available information about local track overlap is used. Tests are\nperformed on simulation data sets to validate the analysis and to gain\npractical experience with the algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306108v2"
    },
    {
        "title": "The PROOF Distributed Parallel Analysis Framework based on ROOT",
        "authors": [
            "Maarten Ballintijn",
            "Rene Brun",
            "Fons Rademakers",
            "Gunther Roland"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The development of the Parallel ROOT Facility, PROOF, enables a physicist to\nanalyze and understand much larger data sets on a shorter time scale. It makes\nuse of the inherent parallelism in event data and implements an architecture\nthat optimizes I/O and CPU utilization in heterogeneous clusters with\ndistributed storage. The system provides transparent and interactive access to\ngigabytes today. Being part of the ROOT framework PROOF inherits the benefits\nof a performant object storage system and a wealth of statistical and\nvisualization tools. This paper describes the key principles of the PROOF\narchitecture and the implementation of the system. We will illustrate its\nfeatures using a simple example and present measurements of the scalability of\nthe system. Finally we will discuss how PROOF can be interfaced and make use of\nthe different Grid solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306110v1"
    },
    {
        "title": "LCIO - A persistency framework for linear collider simulation studies",
        "authors": [
            "Frank Gaede",
            "Ties Behnke",
            "Norman Graf",
            "Tony Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  Almost all groups involved in linear collider detector studies have their own\nsimulation software framework. Using a common persistency scheme would allow to\neasily share results and compare reconstruction algorithms. We present such a\npersistency framework, called LCIO (Linear Collider I/O). The framework has to\nfulfill the requirements of the different groups today and be flexible enough\nto be adapted to future needs. To that end we define an `abstract object\npersistency layer' that will be used by the applications. A first\nimplementation, based on a sequential file format (SIO) is completely separated\nfrom the interface, thus allowing to support additional formats if necessary.\nThe interface is defined with the AID (Abstract Interface Definition) tool from\nfreehep.org that allows creation of Java and C++ code synchronously. In order\nto make use of legacy software a Fortran interface is also provided. We present\nthe design and implementation of LCIO.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306114v1"
    },
    {
        "title": "The RooFit toolkit for data modeling",
        "authors": [
            "Wouter Verkerke",
            "David Kirkby"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  RooFit is a library of C++ classes that facilitate data modeling in the ROOT\nenvironment. Mathematical concepts such as variables, (probability density)\nfunctions and integrals are represented as C++ objects. The package provides a\nflexible framework for building complex fit models through classes that mimic\nmath operators, and is straightforward to extend. For all constructed models\nRooFit provides a concise yet powerful interface for fitting (binned and\nunbinned likelihood, chi^2), plotting and toy Monte Carlo generation as well as\nsophisticated tools to manage large scale projects. RooFit has matured into an\nindustrial strength tool capable of running the BABAR experiment's most\ncomplicated fits and is now available to all users on SourceForge.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306116v1"
    },
    {
        "title": "The new object oriented analysis framework for H1",
        "authors": [
            "M. Peez"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  During the years 2000 and 2001 the HERA machine and the H1 experiment\nperformed substantial luminosity upgrades. To cope with the increased demands\non data handling an effort was made to redesign and modernize the analysis\nsoftware. Main goals were to lower turn-around time for physics analysis by\nproviding a single framework for data storage, event selection, physics and\nevent display. The new object oriented analysis environment based on the RooT\nframework provides a data access front-end for the new data storage scheme and\na new event display. The analysis data is stored in four different layers of\nseparate files. Each layer represents a different level of abstraction, i.e.\nreconstruction output, physics particles, event summary information and user\nspecific information. Links between the layers allow correlating quantities of\ndifferent layers. Currently, this framework is used for data analyses of the\nprevious collected data and for standard data production of the currently\ncollected data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306124v1"
    },
    {
        "title": "Status of the physics validation studies using Geant4 in ATLAS",
        "authors": [
            "Andrea Dell'Acqua"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The new simulation for the ATLAS detector at LHC is performed using Geant4 in\na complete OO/C++ environment. In this framework the simulation of the various\ntest beams for the different ATLAS subdetectors offers an excellent opportunity\nto perform physics validation studies over a wide range of physics domains: the\nelectromagnetic processes, the individual hadronic interactions, the\nelectromagnetic and hadronic signals in calorimeters. The simulation is\nimplemented by paying special attention to all details of the experimental\nlayout and by testing all possible physics processes which may be of relevance\nto the specific detector under test: the resulting simulation programs are\noften more detailed than the corresponding Geant3-based simulation suites. In\nthis paper we present relevant features of muon, electron and pion signals in\nvarious ATLAS detectors. All remaining discrepancies between Geant4 and\ntest-beam data are currently being addressed and progress is continuous. This\nwork shows that Geant4 is becoming a mature and useful product for simulating\nspecific features of large-scale detector systems.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306127v1"
    },
    {
        "title": "Estimation of Goodness-of-Fit in Multidimensional Analysis Using\n  Distance to Nearest Neighbor",
        "authors": [
            "Ilya Narsky"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  A new method for calculation of goodness of multidimensional fits in particle\nphysics experiments is proposed. This method finds the smallest and largest\nclusters of nearest neighbors for observed data points. The cluster size is\nused to estimate the goodness-of-fit and the cluster location provides clues\nabout possible problems with data modeling. The performance of the new method\nis compared to that of the likelihood method and Kolmogorov-Smirnov test using\ntoy Monte Carlo studies. The new method is applied to estimate the\ngoodness-of-fit in a B->Kll analysis at BaBar.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306171v1"
    },
    {
        "title": "Detecting Stochastic Information of Electrocardiograms",
        "authors": [
            "Rafael M. Gutiérrez",
            "Luis A. Sandoval"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  In this work we present a method to detect, identify and characterize\nstochastic information contained in an electrocardiogram (ECG). We assume, as\nit is well known, that the ECG has information corresponding to many different\nprocesses related to the cardiac activity. We analyze scaling and Markov\nprocesses properties of the detected stochastic information using the power\nspectrum of the ECG and the Fokker-Planck equation respectively. The detected\nstochastic information is then characterized by three measures. First, the\nslope of the power spectrum in a particular range of frequencies as a scaling\nparameter. Second, an empirical estimation of the drift and diffusion\ncoefficients of the Fokker-Planck equation through the Kramers-Moyal\ncoefficients which define the evolution of the probability distribution of the\ndetected stochastic information.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0307040v1"
    },
    {
        "title": "Maximum Entropy and Bayesian Data Analysis: Entropic Priors",
        "authors": [
            "Ariel Caticha",
            "Roland Preuss"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The problem of assigning probability distributions which objectively reflect\nthe prior information available about experiments is one of the major stumbling\nblocks in the use of Bayesian methods of data analysis. In this paper the\nmethod of Maximum (relative) Entropy (ME) is used to translate the information\ncontained in the known form of the likelihood into a prior distribution for\nBayesian inference. The argument is inspired and guided by intuition gained\nfrom the successful use of ME methods in statistical mechanics. For experiments\nthat cannot be repeated the resulting \"entropic prior\" is formally identical\nwith the Einstein fluctuation formula. For repeatable experiments, however, the\nexpected value of the entropy of the likelihood turns out to be relevant\ninformation that must be included in the analysis. The important case of a\nGaussian likelihood is treated in detail.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0307055v1"
    },
    {
        "title": "Maximum Probability and Maximum Entropy methods: Bayesian interpretation",
        "authors": [
            "M. Grendar, Jr.",
            "M. Grendar"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  (Jaynes') Method of (Shannon-Kullback's) Relative Entropy Maximization (REM\nor MaxEnt) can be - at least in the discrete case - according to the Maximum\nProbability Theorem (MPT) viewed as an asymptotic instance of the Maximum\nProbability method (MaxProb). A simple bayesian interpretation of MaxProb is\ngiven here. MPT carries the interpretation over into REM.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0308005v2"
    },
    {
        "title": "Maximum Entropy method with non-linear moment constraints: challenges",
        "authors": [
            "M. Grendar, Jr.",
            "M. Grendar"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  Traditionally, the Method of (Shannon-Kullback's) Relative Entropy\nMaximization (REM) is considered with linear moment constraints. In this work,\nthe method is studied under frequency moment constraints which are non-linear\nin probabilities. The constraints challenge some justifications of REM since a)\naxiomatic systems are developed for classical linear moment constraints, b) the\nfeasible set of distributions which is defined by frequency moment constraints\nadmits several entropy maximizing distributions (I-projections), hence\nprobabilistic justification of REM via Conditioned Weak Law of Large Numbers\ncannot be invoked. However, REM is not left completely unjustified in this\nsetting, since Entropy Concentration Theorem and Maximum Probability Theorem\ncan be applied.\n  Maximum Renyi-Tsallis' entropy method (maxTent) enters this work because of\nnon-linearity of X-frequency moment constraints which are used in Non-extensive\nThermodynamics. It is shown here that under X-frequency moment constraints\nmaxTent distribution can be unique and different than the I-projection. This\nimplies that maxTent does not choose the most probable distribution and that\nthe maxTent distribution is asymptotically conditionally improbable. What are\nadherents of maxTent accomplishing when they maximize Renyi's or Tsallis'\nentropy?\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0308006v1"
    },
    {
        "title": "Exclusion regions and their power",
        "authors": [
            "L. Fleysher",
            "R. Fleysher",
            "T. J. Haines",
            "A. I. Mincer",
            "P. Nemethy"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The question of exclusion region construction in new phenomenon searches has\nbeen causing considerable discussions for many years and yet no clear\nmathematical definition of the problem has been stated so far. In this paper we\nformulate the problem in mathematical terms and propose a solution to the\nproblem within the framework of statistical tests. The proposed solution avoids\nproblems of the currently used procedures.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0308067v1"
    },
    {
        "title": "Remarks on statistical aspects of safety analysis of complex systems",
        "authors": [
            "L. Pal",
            "M. Makai"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We analyze safety problems of complex systems using the methods of\nmathematical statistics for testing the output variables of a code simulating\nthe operation of the system under consideration when the input variables are\nuncertain. We have defined a black box model of the code and derived formulas\nto calculate the number of runs needed for a given confidence level to achieve\na preassigned measure of safety. In order to show the capabilities of different\nstatistical methods, firstly we have investigated one output variable with\nunknown and known distribution functions. The general conclusion has been that\nthe different methods do not bring about large differences in the number of\nruns needed to ensure a given level of safety. Analyzing the case of several\nstatistically dependent output variables we have arrived at the conclusion that\nthe testing of the variables separately may lead to false, safety related\ndecisions with unforseen consequences. We have advised two methods: the sign\ntest and the tolerance interval methods for testing more than one mutually\ndependent output variables.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0308086v2"
    },
    {
        "title": "Contents of Physics Related E-Print Archives",
        "authors": [
            "E. R. Prakasan",
            "Anil Kumar",
            "Anil Sagar",
            "Lalit Mohan",
            "Sanjay Kumar Singh",
            "V. L. Kalyane",
            "Vijai Kumar"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The frontiers of physics related e-print archives (1994-2002) at\nhttp://www.arxiv.org/archives/physics web service are explored from 7770\nsubmissions. No. of e-prints in the six research disciplines besides physics\n(5390) were: Condensed matter(754), Quantum physics(279), Astrophysics(222),\nChemical physics(129), High energy physics Phenomenology(118), and High energy\nphysics-Theory(100)). By keyword contents following major sub-fields have high\nfrequency: Atomic physics(1258), General physics(1121), Chemical physics(892),\nAccelerator physics(769), Optics(686), Biological physics(674), and\nComputational physics(607). Interdomainary co-word cluster analysis revealed\nhigher e-print contents for: Classical physics-General physics(108), Quantum\nphysics-Optics(53), and High energy physics (Phenomenology)Atomic physics(49).\nProminent contributors were B. G. Sidharth (India), V. V. Flambaum (Australia),\nAntonina N. Fedorova (Russia), and Michael G. Zeitlin (Russia).\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0308107v1"
    },
    {
        "title": "A comparison of methods for confidence intervals",
        "authors": [
            "A. D. Bukin"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  Comparisons are carried out of the confidence intervals constructed with\nNeyman's frequentist method and with the \\Delta L=1/2 likelihood method, using\nthe example of low-statistics life time estimates.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0309077v5"
    },
    {
        "title": "Frequentist Hypothesis Testing with Background Uncertainty",
        "authors": [
            "Kyle S. Cranmer"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We consider the standard Neyman-Pearson hypothesis test of a\nsignal-plus-background hypothesis and background-only hypothesis in the\npresence of uncertainty on the background-only prediction. Surprisingly, this\nproblem has not been addressed in the recent conferences on statistical\ntechniques in high-energy physics -- although the its confidence-interval\nequivalent has been. We discuss the issues of power, similar tests, coverage,\nand ordering rules. The method presented is compared to the Cousins-Highland\ntechnique, the ratio of Poisson means, and ``profile'' method.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0310108v1"
    },
    {
        "title": "Multivariate Analysis from a Statistical Point of View",
        "authors": [
            "Kyle S. Cranmer"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  Multivariate Analysis is an increasingly common tool in experimental high\nenergy physics; however, many of the common approaches were borrowed from other\nfields. We clarify what the goal of a multivariate algorithm should be for the\nsearch for a new particle and compare different approaches. We also translate\nthe Neyman-Pearson theory into the language of statistical learning theory.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0310110v2"
    },
    {
        "title": "Event Selection Using an Extended Fisher Discriminant Method",
        "authors": [
            "Byron P. Roe"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  This note discusses the problem of choosing between hypotheses in a situation\nwith many, correlated non-normal variables. A new method is introduced to\nshrink the many variables into a smaller subset of variables with zero mean,\nunit variance, and zero correlation coefficient between variables. These new\nvariables are well suited to use in a neural net.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0310145v1"
    },
    {
        "title": "Stochastic analysis of surface roughness",
        "authors": [
            "M. Waechter",
            "F. Riess",
            "H. Kantz",
            "J. Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  For the characterization of surface height profiles we present a new\nstochastic approach which is based on the theory of Markov processes. With this\nanalysis we achieve a characterization of the complexity of the surface\nroughness by means of a Fokker-Planck or Langevin equation, providing the\ncomplete stochastic information of multiscale joint probabilities. The method\nwas applied to different road surface profiles which were measured with high\nresolution. Evidence of Markov properties is shown. Estimations for the\nparameters of the Fokker-Planck equation are based on pure, parameter free data\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0310159v1"
    },
    {
        "title": "Pitfalls of Goodness-of-Fit from Likelihood",
        "authors": [
            "Joel Heinrich"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The value of the likelihood is occasionally used by high energy physicists as\na statistic to measure goodness-of-fit in unbinned maximum likelihood fits.\nSimple examples are presented that illustrate why this (seemingly intuitive)\nmethod fails in practice to achieve the desired goal.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0310167v1"
    },
    {
        "title": "Bayesian Wavelet Based Signal and Image Separation",
        "authors": [
            "Mahieddine M. Ichir",
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  In this contribution, we consider the problem of blind source separation in a\nBayesian estimation framework. The wavelet representation allows us to assign\nan adequate prior distribution to the wavelet coefficients of the sources. MCMC\nalgorithms are implemented to test the validity of the proposed approach, and\nthe non linear approximation of the wavelet transform is exploited to aleviate\nthe algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0311033v2"
    },
    {
        "title": "Temperature and pressure in nonextensive thermostatistics",
        "authors": [
            "Qiuping A. Wang",
            "Laurent Nivanen",
            "Alain Le Mehaute",
            "Michel Pezeril"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The definitions of the temperature in the nonextensive statistical\nthermodynamics based on Tsallis entropy are analyzed. A definition of pressure\nis proposed for nonadditive systems by using a nonadditive effective volume.\nThe thermodynamics of nonadditive photon gas is discussed on this basis. We\nshow that the Stefan-Boltzmann law can be preserved within nonextensive\nthermodynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0311034v1"
    },
    {
        "title": "Statistical Issues in Particle Physics -- A View from BaBar",
        "authors": [
            "Frank C. Porter"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The statistical methods used in deriving physics results in the BaBar\ncollaboration are reviewed, with especial emphasis on areas where practice is\nnot uniform in particle physics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0311092v1"
    },
    {
        "title": "An Unbinned Goodness-of-Fit Test Based on the Random Walk",
        "authors": [
            "K. Kinoshita"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We describe a test statistic for unbinned goodness-of-fit of data in one\ndimension. The statistic is based on the two-dimensional Random Walk. The\nrejection power of this test is explored both for simple and compound\nhypotheses and, for the examples explored, it is found to be comparable to that\nfor the chisquare test. We discuss briefly how it may be possible to extend\nthis test to multi-dimensional data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0312014v2"
    },
    {
        "title": "Predictability of Volcano Eruption: lessons from a basaltic effusive\n  volcano",
        "authors": [
            "J. -R. Grasso",
            "I. Zaliapin"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  Volcano eruption forecast remains a challenging and controversial problem\ndespite the fact that data from volcano monitoring significantly increased in\nquantity and quality during the last decades.This study uses pattern\nrecognition techniques to quantify the predictability of the 15 Piton de la\nFournaise (PdlF) eruptions in the 1988-2001 period using increase of the daily\nseismicity rate as a precursor. Lead time of this prediction is a few days to\nweeks. Using the daily seismicity rate, we formulate a simple prediction rule,\nuse it for retrospective prediction of the 15 eruptions,and test the prediction\nquality with error diagrams. The best prediction performance corresponds to\naveraging the daily seismicity rate over 5 days and issuing a prediction alarm\nfor 5 days. 65% of the eruptions are predicted for an alarm duration less than\n20% of the time considered. Even though this result is concomitant of a large\nnumber of false alarms, it is obtained with a crude counting of daily events\nthat are available from most volcano observatories\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0312034v1"
    },
    {
        "title": "Challenges in Moving the LEP Higgs Statistics to the LHC",
        "authors": [
            "K. S. Cranmer",
            "B. Mellado",
            "W. Quayle",
            "Sau Lan Wu"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We examine computational, conceptual, and philosophical issues in moving the\nstatistical techniques used in the LEP Higgs working group to the LHC.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0312050v1"
    },
    {
        "title": "Measures of Significance in HEP and Astrophysics",
        "authors": [
            "J. T. Linnemann"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  I compare and discuss critically several measures of statistical significance\nin common use in astrophysics and in high energy physics. I also exhibit some\nrelationships among them.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0312059v2"
    },
    {
        "title": "Likelihood inference in the presence of nuisance parameters",
        "authors": [
            "N. Reid",
            "D. A. S. Fraser"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We describe some recent approaches to likelihood based inference in the\npresence of nuisance parameters. Our approach is based on plotting the\nlikelihood function and the $p$-value function, using recently developed third\norder approximations. Orthogonal parameters and adjustments to profile\nlikelihood are also discussed. Connections to classical approaches of\nconditional and marginal inference are outlined.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0312079v1"
    },
    {
        "title": "Constructing Ensembles of Pseudo-Experiments",
        "authors": [
            "Luc Demortier"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The frequentist interpretation of measurement results requires the\nspecification of an ensemble of independent replications of the same\nexperiment. For complex calculations of bias, coverage, significance, etc.,\nthis ensemble is often simulated by running Monte Carlo pseudo-experiments. In\norder to be valid, the latter must obey the Frequentist Principle and the\nAnticipation Criterion. We formulate these two principles and describe some of\ntheir consequences in relation to stopping rules, conditioning, and nuisance\nparameters. The discussion is illustrated with examples taken from high-energy\nphysics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0312100v2"
    },
    {
        "title": "How to Claim a Discovery",
        "authors": [
            "Wolfgang Rolke",
            "Angel Lopez"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We describe a statistical hypothesis test for the presence of a signal. The\ntest allows the researcher to fix the signal location and/or width a priori, or\nperform a search to find the signal region that maximizes the signal. The\nbackground rate and/or distribution can be known or might be estimated from the\ndata. Cuts can be used to bring out the signal.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0312141v1"
    },
    {
        "title": "Dynamic Models and Nonlinear Filtering of Wave Propagation in Random\n  Fields",
        "authors": [
            "Haiqing Wei"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  In this paper, a general model of wireless channels is established based on\nthe physics of wave propagation. Then the problems of inverse scattering and\nchannel prediction are formulated as nonlinear filtering problems. The\nsolutions to the nonlinear filtering problems are given in the form of dynamic\nevolution equations of the estimated quantities. Finally, examples are provided\nto illustrate the practical applications of the proposed theory.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0401006v1"
    },
    {
        "title": "On the CODATA Recommended Values of the Fundamental Physical Constants:\n  V3.2(1998) & V4.0(2002)",
        "authors": [
            "A. S. Siver",
            "V. V. Ezhela"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  With the help of special program package PAREVAL designed in {\\sl\nMathematica} system we reproduce values of basic fundamental physical constants\nobtained by NIST and recommended by CODATA for international usage since 1998.\nIn our adjustment we use the input data and methods published by NIST in 1998.\nIt is shown, that the detected earlier inaccuracy of published by NIST\ncorrelations (that made the NIST 1998&2002 results doubtful) are, most\nprobably, due to inadmissible independent rounding. The simple estimate of the\ncritical numbers of decimal digits in the independently rounded correlation\ncoefficients is obtained. Further independent rounding of ``critically\nrounded'' correlations can lead to the non positive semi definite correlation\nmatrices and hence is inadmissable. It is demonstrated by a few examples that\nthe poor presentation of the correlated random quantities in the scientific\nliterature is a common bad practice and is argued (once again) that the common\nstandard for presentation numerical values of correlated quantities in\npublications and sites is urgently and badly needed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0401064v1"
    },
    {
        "title": "A Measure of the Goodness of Fit in Unbinned Likelihood Fits; End of\n  Bayesianism?",
        "authors": [
            "Rajendran Raja"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Maximum likelihood fits to data can be done using binned data (histograms)\nand unbinned data. With binned data, one gets not only the fitted parameters\nbut also a measure of the goodness of fit. With unbinned data, currently, the\nfitted parameters are obtained but no measure of goodness of fit is available.\nThis remains, to date, an unsolved problem in statistics. Using Bayes' theorem\nand likelihood ratios, we provide a method by which both the fitted quantities\nand a measure of the goodness of fit are obtained for unbinned likelihood fits,\nas well as errors in the fitted quantities. The quantity, conventionally\ninterpreted as a Bayesian prior, is seen in this scheme to be a number not a\ndistribution, that is determined from data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0401133v2"
    },
    {
        "title": "PhysicsGP: A Genetic Programming Approach to Event Selection",
        "authors": [
            "Kyle Cranmer",
            "R. Sean Bowman"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We present a novel multivariate classification technique based on Genetic\nProgramming. The technique is distinct from Genetic Algorithms and offers\nseveral advantages compared to Neural Networks and Support Vector Machines. The\ntechnique optimizes a set of human-readable classifiers with respect to some\nuser-defined performance measure. We calculate the Vapnik-Chervonenkis\ndimension of this class of learning machines and consider a practical example:\nthe search for the Standard Model Higgs Boson at the LHC. The resulting\nclassifier is very fast to evaluate, human-readable, and easily portable. The\nsoftware may be downloaded at: http://cern.ch/~cranmer/PhysicsGP.html\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0402030v1"
    },
    {
        "title": "A Neural Bayesian Estimator for Conditional Probability Densities",
        "authors": [
            "Michael Feindt"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  This article describes a robust algorithm to estimate a conditional\nprobability density f(t|x) as a non-parametric smooth regression function. It\nis based on a neural network and the Bayesian interpretation of the network\noutput as a posteriori probabability. The network is trained using example\nevents from history or simulation, which define the underlying probability\ndensity f(t,x).\n  Once trained, the network is applied on new, unknown examples x, for which it\ncan predict the probability distribution of the target variable t.\nEvent-by-event knowledge of the smooth function f(t|x) can be very useful, e.g.\nin maximum likelihood fits or for forecasting tasks. No assumptions are\nnecessary about the distribution, and non-Gaussian tails are accounted for\nautomatically. Important quantities like median, mean value, left and right\nstandard deviations, moments and expectation values of any function of t are\nreadily derived from it.\n  The algorithm can be considered as an event-by-event unfolding and leads to\nstatistically optimal reconstruction. The largest benefit of the method lies in\ncomplicated problems, when the measurements x are only relatively weakly\ncorrelated to the output t. As to assure optimal generalisation features and to\navoid overfitting, the networks are regularised by extended versions of weight\ndecay. The regularisation parameters are determined during the online-learning\nof the network by relations obtained from Bayesian statistics.\n  Some toy Monte Carlo tests and first real application examples from\nhigh-energy physics and econometry are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0402093v1"
    },
    {
        "title": "A Bayesian approach to change point analysis of discrete time series",
        "authors": [
            "Ali Mohammad-Djafari",
            "Olivier Feron"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  In this work we consider time series with a finite number of discrete point\nchanges. We assume that the data in each segment follows a different\nprobability density functions (pdf). We focus on the case where the data in all\nsegments are modeled by Gaussian probability density functions with different\nmeans, variances and correlation lengths. We put a prior law on the change\npoint instances (Poisson process) as well as on these different\nparameters(conjugate priors) and give the expression of the posterior probality\ndistributions of these change points. The computations are done by using an\nappropriate Markov Chain Monte Carlo (MCMC) technique.\n  The problem as we stated can also be considered as an unsupervised\nclassification and/or segmentation of the time serie. This analogy gives us the\npossibility to propose alternative modeling and computation of change points,\nwhich are more appropriate for multivariate signals, for example in image\nprocessing.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0403148v1"
    },
    {
        "title": "A Hidden Markov model for Bayesian data fusion of multivariate signals",
        "authors": [
            "Olivier Feron",
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  In this work we propose a Bayesian framework for data fusion of multivariate\nsignals which arises in imaging systems. More specifically, we consider the\ncase where we have observed two images of the same object through two different\nimaging processes. The objective of this work is then to propose a coherent\napproach to combine these data sets to obtain a segmented image which can be\nconsidered as the fusion result of these two images. The proposed approach is\nbased on a Hidden Markov Modeling (HMM) of the images with common segmentation,\nor equivalently, with common hidden classification label variables which is\nmodeled by the Potts Markov Random Field. We propose then an appropriate Markov\nChain Monte Carlo (MCMC) algorithm to implement the method and show some\nsimulation results and applications.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0403149v1"
    },
    {
        "title": "A hidden Markov Model for image fusion and their joint segmentation in\n  medical image computing",
        "authors": [
            "Olivier Feron",
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  In this work we propose a Bayesian framework for fully automated image fusion\nand their joint segmentation. More specifically, we consider the case where we\nhave observed images of the same object through different image processes or\nthrough different spectral bands. The objective of this work is then to propose\na coherent approach to combine these data sets and obtain a segmented image\nwhich can be considered as the fusion result of these observations. The\nproposed approach is based on a Hidden Markov Modeling (HMM) of the images with\ncommon segmentation, or equivalently, with common hidden classification label\nvariables which are modeled by the Potts Markov Random Field. We propose an\nappropriate Markov Chain Monte Carlo (MCMC) algorithm to implement the method\nand show some simulation results and applications.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0403150v1"
    },
    {
        "title": "Statistical randomization test for QCD intermittency in a single-event\n  distribution",
        "authors": [
            "Leif E. Peterson"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  A randomization test was developed to determine the statistical significance\nof QCD intermittency in single-event distributions. A total of 96 simulated\nintermittent distributions based on standard normal Gaussian distributions of\nsize N=500, 1000, 1500, 2000, 4000, 8000, 16000, and 32000 containing induced\nholes and spikes were tested for intermittency. Non-intermittent null\ndistributions were also simulated as part of the test. A log-linear model was\ndeveloped to simultaneously test the significance of fit coefficients for the\ny-intercept and slope contribution to ln(F_2) vs. ln(M) from both the\nintermittent and null distributions. Statistical power was also assessed for\neach fit coefficient to reflect the proportion of times out of 1000 tests each\ncoefficient was statistically significant, given the induced effect size and\nsample size of the Gaussians. Results indicate that the slope of ln(F_2) vs.\nln(M) for intermittent distributions increased with decreasing sample size, due\nto artificially-induced holes occurring in sparse histograms. For intermittent\nGaussians with 4000 variates, there was approximately 70% power to detect a\nslope difference of 0.02 between intermittent and null distributions. For\nsample sizes of 8000 and greater, there was more than 70% power to detect a\nslope difference of 0.01. The randomization test performed satisfactorily since\nthe power of the test for intermittency decreased with decreasing sample size.\nPower was near-zero when the test was applied to null distributions. The\nrandomization test can be used to establish the statistical significance of\nintermittency in empirical single-event Gaussian distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0404016v1"
    },
    {
        "title": "Signal processing and statistical methods in analysis of text and DNA",
        "authors": [
            "Matthew J. Berryman",
            "Andrew Allison",
            "Pedro Carpena",
            "Derek Abbott"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  A number of signal processing and statistical methods can be used in\nanalyzing either pieces of text or DNA sequences. These techniques can be used\nin a number of ways, such as determining authorship of documents, finding genes\nin DNA, and determining phylogenetic and linguistic trees. Signal processing\nmethods such as spectrograms provide useful new tools in the area of genomic\ninformation science. In particular, fractal analysis of DNA ``signals'' has\nprovided a new way of classifying organisms.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0404032v1"
    },
    {
        "title": "Techniques for noise removal from EEG, EOG and air flow signals in sleep\n  patients",
        "authors": [
            "Matthew J. Berryman",
            "Sheila Messer",
            "Andrew Allison",
            "Derek Abbott"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Noise is present in the wide variety of signals obtained from sleep patients.\nThis noise comes from a number of sources, from presence of extraneous signals\nto adjustments in signal amplification and shot noise in the circuits used for\ndata collection. The noise needs to be removed in order to maximize the\ninformation gained about the patient using both manual and automatic analysis\nof the signals. Here we evaluate a number of new techniques for removal of that\nnoise, and the associated problem of separating the original signal sources.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0404034v1"
    },
    {
        "title": "Entropy, Information Matrix and order statistics of Multivariate Pareto,\n  Burr and related distributions",
        "authors": [
            "Gholamhossein Yari",
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  In this paper we derive the exact analytical expressions for the information\nand covariance matrices of the multivariate Burr and related distributions.\nThese distributions arise as tractable parametric models in reliability,\nactuarial science, economics, finance and telecommunications. We show that all\nthe calculations can be obtained from one main moment multi dimensional\nintegral whose expression is obtained through some particular change of\nvariables.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0404062v1"
    },
    {
        "title": "Information and Covariance Matrices for Multivariate Burr III and\n  Logistic distributions",
        "authors": [
            "Gholamhossein Yari",
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Main result of this paper is to derive the exact analytical expressions of\ninformation and covariance matrices for multivariate Burr III and logistic\ndistributions. These distributions arise as tractable parametric models in\nprice and income distributions, reliability, economics, populations growth and\nsurvival data. We showed that all the calculations can be obtained from one\nmain moment multi dimensional integral whose expression is obtained through\nsome particular change of variables. Indeed, we consider that this calculus\ntechnique for improper integral has its own importance in applied probability\ncalculus.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0404063v1"
    },
    {
        "title": "MaxEnt assisted MaxLik tomography",
        "authors": [
            "J. Rehacek",
            "Z. Hradil"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Maximum likelihood estimation is a valuable tool often applied to inverse\nproblems in quantum theory. Estimation from small data sets can, however, have\nnon unique solutions. We discuss this problem and propose to use Jaynes maximum\nentropy principle to single out the most unbiased maximum-likelihood guess.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0404121v1"
    },
    {
        "title": "Statistical Analysis for Long Term Correlations in the Stress Time\n  Series of Jerky Flow",
        "authors": [
            "Dimitris Kugiumtzis",
            "Elias C. Aifantis"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Stress time series from the PLC effect typically exhibit stick-slips of\nupload and download type. These data contain strong short-term correlations of\na nonlinear type. We investigate whether there are also long term correlations,\ni.e. the successive up-down patterns are generated by a deterministic\nmechanism. A statistical test is conducted for the null hypothesis that the\nsequence of the up-down patterns is totally random. The test is constructed by\nmeans of surrogate data, suitably generated to represent the null hypothesis.\nLinear and nonlinear estimates are used as test statistics, namely\nautocorrelation, mutual information and Lyapunov exponents, which are found to\nhave proper performance for the test. The test is then applied to three stress\ntime series under different experimental conditions. Rejections are obtained\nfor one of them and not with all statistics. From the overall results we cannot\nconclude that the underlying mechanism to the PLC effect has long memory.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0405097v1"
    },
    {
        "title": "Detecting non-linearities in data sets. Characterization of Fourier\n  phase maps using the Weighted Scaling Indices",
        "authors": [
            "Roberto A. Monetti",
            "Wolfram Bunk",
            "Ferdinand Jamitzky",
            "Christoph Raeth",
            "Gregor Morfill"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We present a methodology for detecting non-linearities in data sets based on\nthe characterization of the structural features of the Fourier phase maps. A\nFourier phase map is a 2D set of points $M= \\{(\\phi_{\\vec{k}}, \\phi_{\\vec{k} +\n\\vec{\\Delta}})\\}$, where $ \\phi_{\\vec{k}}$ is the phase of the $k$-mode of the\nFourier transform of the data set and $\\vec{\\Delta}$ a phase shift. The\ninformation thus rendered on this space is analyzed using the spectrum of\nweighted scaling indices to detect phase coupling at any scale $\\vec{\\Delta}$.\nWe propose a statistical test of significance based on the comparison of the\nproperties of phase maps created from both the original data and surrogate\nrealizations. We have applied our method to the Lorenz system and the\nlogarithmic stock returns of the Dow Jones index. Applications to higher\ndimensional data are straightforward. The results indicate that both the Lorenz\nsystem and the Dow Jones time series exhibit significant signatures of\nnon-linear behavior.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0405130v1"
    },
    {
        "title": "Formalism for obtaining nuclear momentum distributions by the Deep\n  Inelastic Neutron Scattering technique",
        "authors": [
            "J. J. Blostein",
            "J. Dawidowski",
            "J. R. Granada"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We present a new formalism to obtain momentum distributions in condensed\nmatter from Neutron Compton Profiles measured by the Deep Inelastic Neutron\nScattering technique. The formalism describes exactly the Neutron Compton\nProfiles as an integral in the momentum variable $y$. As a result we obtain a\nVolterra equation of the first kind that relates the experimentally measured\nmagnitude with the momentum distributions of the nuclei in the sample. The\nintegration kernel is related with the incident neutron spectrum, the total\ncross section of the filter analyzer and the detectors efficiency function. A\ncomparison of the present formalism with the customarily employed approximation\nbased on a convolution of the momentum distribution with a resolution function\nis presented. We describe the inaccuracies that the use of this approximation\nproduces, and propose a new data treatment procedure based on the present\nformalism.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0406020v1"
    },
    {
        "title": "A Method to Separate Stochastic and Deterministic Information from\n  Electrocardiograms",
        "authors": [
            "R. M. Gutiérrez",
            "L. Sandoval"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  In this work we present a new idea to develop a method to separate stochastic\nand deterministic information contained in an electrocardiogram, ECG, which may\nprovide new sources of information with diagnostic purposes. We assume that the\nECG has information corresponding to many different processes related with the\ncardiac activity as well as contamination from different sources related with\nthe measurement procedure and the nature of the observed system itself. The\nmethod starts with the application of an improuved archetypal analysis to\nseparate the mentioned stochastic and deterministic information. From the\nstochastic point of view we analyze Renyi entropies, and with respect to the\ndeterministic perspective we calculate the autocorrelation function and the\ncorresponding correlation time. We show that healthy and pathologic information\nmay be stochastic and/or deterministic, can be identified by different measures\nand located in different parts of the ECG.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0408134v1"
    },
    {
        "title": "Astumian's Paradox revisited",
        "authors": [
            "R. Dean Astumian"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  I give a simple analysis of the game that I previously published in\nScientific American which shows the paradoxical behavior whereby two losing\ngames randomly combine to form a winning game. The game, modeled on a random\nwalk, requires only two states and is described by a first-order Markov\nprocess.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0409029v1"
    },
    {
        "title": "A Simple Model of Scale-free Networks Driven by both Randomness and\n  Adaptability",
        "authors": [
            "Dinghua Shi",
            "Xiang Zhu",
            "Liming Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  In this paper, we present a simple model of scale-free networks that\nincorporates both preferential & random attachment and anti-preferential &\nrandom deletion at each time step. We derive the degree distribution\nanalytically and show that it follows a power law with the degree exponent in\nthe range of (2,infinity). We also find a way to derive an expression of the\nclustering coefficient for growing networks and compute the average path length\nthrough simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0409061v2"
    },
    {
        "title": "The Inconstancy of the Fundamental Physical Constants: Computational\n  Status",
        "authors": [
            "V. V. Ezhela",
            "Yu. V. Kuyanov",
            "V. N. Larin",
            "A. S. Siver"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  It is argued that the CODATA recommended values of the fundamental physical\nconstants could not be used as the reference data in searching the hypothetical\nspace-time variations of the fundamental physical constants. It is shown that\nthe CODATA data permanently suffers a loss of self-consistency of the released\ndata due to unjustified over-rounding of their estimates. The simple estimates\nof the critical numbers of decimal digits that should be saved in the\nindependently rounded correlation coefficients, the average values and\nuncertainties to save the self-consistency is obtained. The set of high level\nquality requirements to the computerized presentation of the numerical data on\nthe jointly measured or estimated physical values are formulated. It is argued\n(once again) that the common standard for presentation of the numerical values\nof correlated quantities in publications and sites is urgently needed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0409117v4"
    },
    {
        "title": "Interval estimation in the presence of nuisance parameters. 1. Bayesian\n  approach",
        "authors": [
            "Joel Heinrich",
            "Craig Blocker",
            "John Conway",
            "Luc Demortier",
            "Louis Lyons",
            "Giovanni Punzi",
            "Pekka K. Sinervo"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We address the common problem of calculating intervals in the presence of\nsystematic uncertainties. We aim to investigate several approaches, but here\ndescribe just a Bayesian technique for setting upper limits. The particular\nexample we study is that of inferring the rate of a Poisson process when there\nare uncertainties on the acceptance and the background. Limit calculating\nsoftware associated with this work is available in the form of C functions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0409129v1"
    },
    {
        "title": "Discrete Scale Invariance in the Cascade Heart Rate Variability Of\n  Healthy Humans",
        "authors": [
            "Der Chyan Lin"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Evidence of discrete scale invariance (DSI) in daytime healthy heart rate\nvariability (HRV) is presented based on the log-periodic power law scaling of\nthe heart beat interval increment. Our analysis suggests multiple DSI groups\nand a dynamic cascading process. A cascade model is presented to simulate such\na property.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0411039v1"
    },
    {
        "title": "Scaling and wavelet-based analyses of the long-term heart rate\n  variability of the Eastern Oyster",
        "authors": [
            "P. A. Ritto",
            "J. G. Contreras",
            "J. J. Alvarado-Gil"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Characterisations of the long-term behaviour of heart rate variability in\nhumans have emerged in the last few years as promising candidates to became\nclinically significant tools. We present two different statistical analyses of\nlong time recordings of the heart rate variation in the Eastern Oyster. The\ncirculatory system of this marine mollusk has important anatomical and\nphysiological dissimilitudes in comparison to that of humans and it is exposed\nto dramatically different environmental influences. Our results resemble those\npreviously obtained in humans. This suggests that in spite of the\ndiscrepancies, the mechanisms of long--term cardiac control on both systems\nshare a common underlying dynamic.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0411087v2"
    },
    {
        "title": "On a possible dynamical scenario leading to a generalised Gamma\n  distribution",
        "authors": [
            "Silvio M. Duarte Queiros"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  In this report I present a possible scenario which can lead to the emergence\nof a generalised Gamma distribution first presented by R. Osorio et al. as the\ndistribution of traded volumes of stocks in financial markets. This propose is\nrelated with superstatics and the notion of moving average commonly used in\neconometrics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0411111v1"
    },
    {
        "title": "Evidence of crossover phenomena in wind speed data",
        "authors": [
            "Rajesh G. Kavasseri",
            "Radhakrishnan Nagarajan"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  In this report, a systematic analysis of hourly wind speed data obtained from\nthree potential wind generation sites (in North Dakota) is analyzed. The power\nspectra of the data exhibited a power-law decay characteristic of\n$1/f^{\\alpha}$ processes with possible long-range correlations. Conventional\nanalysis using Hurst exponent estimators proved to be inconclusive. Subsequent\nanalysis using detrended fluctuation analysis (DFA) revealed a crossover in the\nscaling exponent ($\\alpha$). At short time scales, a scaling exponent of\n$\\alpha \\sim 1.4$ indicated that the data resembled Brownian noise, whereas for\nlarger time scales the data exhibited long range correlations ($\\alpha \\sim\n0.7$). The scaling exponents obtained were similar across the three locations.\nOur findings suggest the possibility of multiple scaling exponents\ncharacteristic of multifractal signals.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0411194v1"
    },
    {
        "title": "Late-Night Thoughts About the Significance of a Small Count of Nuclear\n  or Particle Events",
        "authors": [
            "Ivan V. Anicin"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Reconciliation of frequentist and Bayesian approaches to elementary treatment\nof data in nuclear and particle physics is attempted. Unique procedure to\nexpress the significance of a small count in presence of background is\nhenceforth proposed and discussed in some detail.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0501108v3"
    },
    {
        "title": "Recurrence plot statistics and the effect of embedding",
        "authors": [
            "T. K. March",
            "S. C. Chapman",
            "R. O. Dendy"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Recurrence plots provide a graphical representation of the recurrent patterns\nin a timeseries, the quantification of which is a relatively new field. Here we\nderive analytical expressions which relate the values of key statistics,\nnotably determinism and entropy of line length distribution, to the correlation\nsum as a function of embedding dimension. These expressions are obtained by\nderiving the transformation which generates an embedded recurrence plot from an\nunembedded plot. A single unembedded recurrence plot thus provides the\nstatistics of all possible embedded recurrence plots. If the correlation sum\nscales exponentially with embedding dimension, we show that these statistics\nare determined entirely by the exponent of the exponential. This explains the\nresults of Iwanski and Bradley (Chaos 8 [1998] 861-871) who found that certain\nrecurrence plot statistics are apparently invariant to embedding dimension for\ncertain low-dimensional systems. We also examine the relationship between the\nmutual information content of two timeseries and the common recurrent structure\nseen in their recurrence plots. This allows time-localized contributions to\nmutual information to be visualized. This technique is demonstrated using\ngeomagnetic index data; we show that the AU and AL geomagnetic indices share\nhalf their information, and find the timescale on which mutual features appear.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0502042v1"
    },
    {
        "title": "A novel String Banana Template Method for Tracks Reconstruction in High\n  Multiplicity Events with significant Multiple Scattering and its Firmware\n  Implementation",
        "authors": [
            "P. Kulinich",
            "S. Basilev",
            "V. Krylov"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Novel String Banana Template Method (SBTM) for track reconstruction in\ndifficult conditions is proposed and implemented for off-line analysis of\nrelativistic heavy ion collision events.\n  The main idea of the method is in use of features of ensembles of tracks\nselected by 3-fold coincidence. Two steps model of track is used: the first one\n- averaged over selected ensemble and the second - per event dependent.\n  It takes into account Multiple Scattering (MS) for this particular track.\nSBTM relies on use of stored templates generated by precise Monte Carlo\nsimulation, so it's more time efficient for the case of 2D spectrometer.\n  All data required for track reconstruction in such difficult conditions could\nbe prepared in convenient format for fast use. Its template based nature and\nthe fact that the SBTM track model is actually very close to the hits implies\nthat it can be implemented in a firmware processor. In this report a block\ndiagram of firmware based pre-processor for track reconstruction in CMS-like Si\ntracker is proposed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0502068v2"
    },
    {
        "title": "An Iterative Procedure for the Estimation of Drift and Diffusion\n  Coefficients of Langevin Processes",
        "authors": [
            "D. Kleinhans",
            "R. Friedrich",
            "A. Nawroth",
            "J. Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  A general method is proposed which allows one to estimate drift and diffusion\ncoefficients of a stochastic process governed by a Langevin equation. It\nextends a previously devised approach [R. Friedrich et al., Physics Letters A\n271, 217 (2000)], which requires sufficiently high sampling rates. The analysis\nis based on an iterative procedure minimizing the Kullback-Leibler distance\nbetween measured and estimated two time joint probability distributions of the\nprocess.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0502152v2"
    },
    {
        "title": "Inference of a nonlinear stochastic model of the cardiorespiratory\n  interaction",
        "authors": [
            "V. N. Smelyanskiy",
            "D. G. Luchinsky",
            "A. Stefanovska",
            "P. V. E. McClintock"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  A new technique is introduced to reconstruct a nonlinear stochastic model of\nthe cardiorespiratory interaction. Its inferential framework uses a set of\npolynomial basis functions representing the nonlinear force governing the\nsystem oscillations. The strength and direction of coupling, and the noise\nintensity are simultaneously inferred from a univariate blood pressure signal,\nmonitored in a clinical environment. The technique does not require extensive\nglobal optimization and it is applicable to a wide range of complex dynamical\nsystems subject to noise.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0503040v1"
    },
    {
        "title": "Simultaneous Least Squares Treatment of Statistical and Systematic\n  Uncertainties",
        "authors": [
            "Werner M. Sun"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We present a least squares method for estimating parameters from measurements\nof event yields in the presence of background and crossfeed. We adopt a unified\napproach to incorporating the statistical and systematic uncertainties on the\nexperimental measurements input to the fit. We demonstrate this method with a\nfit for absolute hadronic D meson branching fractions, measured in e+e- ->\n\\psi(3770) -> D\\bar D$ transitions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0503050v2"
    },
    {
        "title": "Levy statistical fluctuations from a Random Amplifying Medium",
        "authors": [
            "Divya Sharma",
            "Hema Ramachandran",
            "N. Kumar"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We report the studies of emission from a novel random amplifying medium that\nwe term a ``Levy Laser'' due to the non-Gaussian statistical nature of its\nemission over the ensemble of random realizations. It is observed that the\namplification is dominated by certain improbable events that are ``larger than\nrare'', which give the intensity statistics a Levy like ``fat tail''. This, to\nthe best of our knowledge, provides the first experimental realization of Levy\nflight in optics in a random amplifying medium.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0503059v1"
    },
    {
        "title": "Modeling spatiotemporal noise covariance for MEG/EEG source analysis",
        "authors": [
            "S. M. Plis",
            "J. S. George",
            "S. C. Jun",
            "J. Pare-Blagoev",
            "D. M. Ranken",
            "D. M. Schmidt",
            "C. C. Wood"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We propose a new model for approximating spatiotemporal noise covariance for\nuse in MEG/EEG source analysis. Our model is an extension of an existing model\n[1,2] that uses a single Kronecker product of a pair of matrices - temporal and\nspatial covariance; we employ a series of Kronecker products in order to\nconstruct a better approximation of the full covariance. In contrast to the\nsingle-pair model that assumes the same temporal structure for all spatial\ncomponents, the proposed model allows for distinct, independent time courses at\neach spatial component. This model better describes spatially and temporally\ncorrelated background activity. At the same time, inversion of the model is\nfast which makes it useful in the inverse analysis. We have explored two\nversions of the model. One is based on orthogonal spatial components of the\nbackground. The other, more general model, is based on independent spatial\ncomponents. Performance of the new and previous models is compared in inverse\nsolutions to a large number of single dipole problems with simulated time\ncourses and background from authentic MEG data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0503063v2"
    },
    {
        "title": "Quantum Fluctuations of a Coulomb Potential as a Source of Flicker Noise",
        "authors": [
            "Kirill A. Kazakov"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  The power spectrum of quantum fluctuations of the electromagnetic field\nproduced by an elementary particle is determined. It is found that in a wide\nrange of practically important frequencies the power spectrum of fluctuations\nexhibits an inverse frequency dependence. The magnitude of fluctuations\nproduced by a conducting sample is shown to have a Gaussian distribution around\nits mean value, and its dependence on the sample geometry is determined. In\nparticular, it is demonstrated that for geometrically similar samples the power\nspectrum is inversely proportional to the sample volume. It is argued also that\nthe magnitude of fluctuations induced by external electric field is\nproportional to the field strength squared. A comparison with experimental data\non flicker noise measurements in continuous metal films is made.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0504175v2"
    },
    {
        "title": "No need to blur the picture",
        "authors": [
            "Axel G. Rossberg"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  A formalism specifying efficient, \"emergent\" descriptions of experimental\nsystems is developed. It does not depend on an a priori assumption of limited\navailable data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0505034v1"
    },
    {
        "title": "Deriving the sampling errors of correlograms for general white noise",
        "authors": [
            "T. D. Carozzi",
            "A. M. Buckley"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We derive the second-order sampling properties of certain autocovariance and\nautocorrelation estimators for sequences of independent and identically\ndistributed samples. Specifically, the estimators we consider are the classic\nlag windowed correlogram, the correlogram with subtracted sample mean, and the\nfixed-length summation correlogram. For each correlogram we derive explicit\nformulas for the bias, covariance, mean square error and consistency for\ngeneralised higher-order white noise sequences. In particular, this class of\nsequences may have non-zero means, be complexed valued and also includes\nnon-analytical noise signals. We find that these commonly used correlograms\nexhibit lag dependent covariance despite the fact that these processes are\nwhite and hence by definition do not depend on lag.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0505145v1"
    },
    {
        "title": "Reconstruction of an object from its symmetry-averaged diffraction\n  pattern",
        "authors": [
            "Veit Elser"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  By suitably generalizing the Fourier constraint projection in the difference\nmap phasing algorithm, an object can be reconstructed from its diffraction\npattern even when the latter has been incoherently averaged over a discrete\ngroup of symmetries. This resolves an ambiguity in the recent proposal for\naligning molecules by means of their anisotropic dielectric interaction with an\nintense light field. The algorithm is demonstrated with simulated data in two\nand three dimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0505174v1"
    },
    {
        "title": "Sampling errors of correlograms with and without sample mean removal for\n  higher-order complex white noise with arbitrary mean",
        "authors": [
            "T. D. Carozzi",
            "A. M. Buckley"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We derive the bias, variance, covariance, and mean square error of the\nstandard lag windowed correlogram estimator both with and without sample mean\nremoval for complex white noise with an arbitrary mean. We find that the\narbitrary mean introduces lag dependent covariance between different lags of\nthe correlogram estimates in spite of the lack of covariance in white noise for\nnon-zeros lags. We provide a heuristic rule for when the sample mean should be,\nand when it should not be, removed if the true mean is not known. The sampling\nproperties derived here are useful is assesing the general statistical\nperformance of autocovariance and autocorrelation estimators in different\nparameter regimes. Alternatively, the sampling properties could be used as\nbounds on the detection of a weak signal in general white noise.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0506030v1"
    },
    {
        "title": "The pulsing CPSD method for subcritical assemblies with pulsed sources",
        "authors": [
            "Daniel Ballester",
            "Jose L. Munoz-Cobo"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Stochastic neutron transport theory is applied to the derivation of the\ntwo-neutron-detectors cross power spectral density for subcritical assemblies\nwhen external pulsed sources are used. A general relationship between the\ntwo-detector probability generating functions of the kernel and the source is\nobtained considering the contribution to detectors statistics of both the\npulsed source and the intrinsic neutron source. An expansion in\nalpha-eigenvalues is derived for the final solution, which permits to take into\naccount the effect of higher harmonics in subcritical systems. Further,\nexpressions corresponding to the fundamental mode approximation are compared\nwith recent results from experiments performed under the MUSE-4 European\nresearch project.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0506166v1"
    },
    {
        "title": "StatPatternRecognition: A C++ Package for Statistical Analysis of High\n  Energy Physics Data",
        "authors": [
            "I. Narsky"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Modern analysis of high energy physics (HEP) data needs advanced statistical\ntools to separate signal from background. A C++ package has been implemented to\nprovide such tools for the HEP community. The package includes linear and\nquadratic discriminant analysis, decision trees, bump hunting (PRIM), boosting\n(AdaBoost), bagging and random forest algorithms, and interfaces to the\nstandard backpropagation neural net and radial basis function neural net\nimplemented in the Stuttgart Neural Network Simulator. Supplemental tools such\nas bootstrap, estimation of data moments, and a test of zero correlation\nbetween two variables with a joint elliptical distribution are also provided.\nThe package offers a convenient set of tools for imposing requirements on input\ndata and displaying output. Integrated in the BaBar computing environment, the\npackage maintains a minimal set of external dependencies and therefore can be\neasily adapted to any other environment. It has been tested on many idealistic\nand realistic examples.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0507143v1"
    },
    {
        "title": "On the Statistical Significance",
        "authors": [
            "Yongsheng Zhu"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  A definition for the statistical significance by constructing a correlation\nbetween the normal distribution integral probability and the p-value observed\nin an experiment is proposed, which is suitable for both counting experiment\nand continuous test statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0507145v2"
    },
    {
        "title": "Optimization of Signal Significance by Bagging Decision Trees",
        "authors": [
            "I. Narsky"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  An algorithm for optimization of signal significance or any other\nclassification figure of merit suited for analysis of high energy physics (HEP)\ndata is described. This algorithm trains decision trees on many bootstrap\nreplicas of training data with each tree required to optimize the signal\nsignificance or any other chosen figure of merit. New data are then classified\nby a simple majority vote of the built trees. The performance of this algorithm\nhas been studied using a search for the radiative leptonic decay B->gamma l nu\nat BaBar and shown to be superior to that of all other attempted classifiers\nincluding such powerful methods as boosted decision trees. In the B->gamma e nu\nchannel, the described algorithm increases the expected signal significance\nfrom 2.4 sigma obtained by an original method designed for the B->gamma l nu\nanalysis to 3.0 sigma.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0507157v1"
    },
    {
        "title": "Studies of Boosted Decision Trees for MiniBooNE Particle Identification",
        "authors": [
            "Hai-Jun Yang",
            "Byron P. Roe",
            "Ji Zhu"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Boosted decision trees are applied to particle identification in the\nMiniBooNE experiment operated at Fermi National Accelerator Laboratory\n(Fermilab) for neutrino oscillations. Numerous attempts are made to tune the\nboosted decision trees, to compare performance of various boosting algorithms,\nand to select input variables for optimal performance.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0508045v1"
    },
    {
        "title": "The network of concepts in written texts",
        "authors": [
            "Silvia M. G. Caldeira",
            "Thierry C. Petit Lobao",
            "R. F. S. Andrade",
            "Alexis Neme",
            "J. G. V. Miranda"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Complex network theory is used to investigate the structure of meaningful\nconcepts in written texts of individual authors. Networks have been constructed\nafter a two phase filtering, where words with less meaning contents are\neliminated, and all remaining words are set to their canonical form, without\nany number, gender or time flexion. Each sentence in the text is added to the\nnetwork as a clique. A large number of written texts have been scrutinized, and\nits found that texts have small-world as well as scale-free structures. The\ngrowth process of these networks has also been investigated, and a universal\nevolution of network quantifiers have been found among the set of texts written\nby distinct authors. Further analyzes, based on shufling procedures taken\neither on the texts or on the constructed networks, provide hints on the role\nplayed by the word frequency and sentence length distributions to the network\nstructure. Since the meaningful words are related to concepts in the author's\nmind, results for text networks may uncover patterns in communication and\nlanguage processes that occur in the mind.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0508066v1"
    },
    {
        "title": "The repository of physical models used for the CODATA-2002 FPC\n  (re-)evaluation",
        "authors": [
            "A. S. Siver"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We present PAREVAL package containing a repository of theoretical physical\nmodels used for (re-)evaluation of the fundamental physical constants (FPC). It\nholds all necessary data for building 105 (so called) observational equations\nand can be used in high precision calculations. Among repository models there\nare expressions for energy levels of hydrogen and deuterium (with 16 types of\ncontributions), electron and muon magnetic moment anomalies, muonium\nground-state hyperfine splitting, Zeeman energy levels in muonium. Each model\nis represented as {\\sl Mathematica} module with XML meta-data keeping\ninformation about the model (including data on dependence from others models).\nThere are also modules for working with the basic FPC.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0509007v1"
    },
    {
        "title": "An attempt to observe economy globalization: the cross correlation\n  distance evolution of the top 19 GDP's",
        "authors": [
            "Janusz Miskiewicz",
            "Marcel Ausloos"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Economy correlations between the 19 richest countries are investigated\nthrough their Gross Domestic Product increments. A distance is defined between\nincrement correlation matrix elements and their evolution studied as a function\nof time and time window size. Unidirectional and Bidirectional Minimal Length\nPaths are generated and analyzed for different time windows. A sort of critical\ncorrelation time window is found indicating a transition for best observations.\nThe mean length path decreases with time, indicating stronger correlations. A\nnew method for estimating a realistic minimal time window to observe\ncorrelations and deduce macroeconomy conclusions from such features is thus\nsuggested.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0509165v1"
    },
    {
        "title": "Model-based fit procedure for power-law-like spectra",
        "authors": [
            "Edoardo Milotti"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  $1/f^\\alpha$ noises are ubiquitous and affect many measurements. These noises\nare both a nuisance and a peculiarity of several physical systems; in\ndielectrics, glasses and networked liquids it is very common to study this\nnoise to gather useful information. Sometimes it happens that the noise has a\npower-law shape only in a certain frequency range, and contains other important\nfeatures, that are however difficult to study because simple fits often fail.\nHere I propose a model-based fit procedure that performs well on spectra\nobtained in a molecular dynamics simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0510011v1"
    },
    {
        "title": "Recognizing different types of stochastic processes",
        "authors": [
            "Jong U. Kim",
            "Laszlo B. Kish"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We propose a new cross-correlation method that can recognize independent\nrealizations of the same type of stochastic processes and can be used as a new\nkind of pattern recognition tool in biometrics, sensing, forensic, security and\nimage processing applications. The method, which we call bispectrum correlation\ncoefficient method, makes use of the cross-correlation of the bispectra. Three\nkinds of cross-correlation coefficients are introduced. To demonstrate the new\nmethod, six different random telegraph signals are tested, where four of them\nhave the same power density spectrum. It is shown that the three coefficients\ncan map the different stochastic processes to specific sub-volumes in a cube.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0510118v1"
    },
    {
        "title": "Note on clock synchronization and Edwards transformations",
        "authors": [
            "Piotr Kosinski"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Edwards transformations relating inertial frames with arbitrary clock\nsynchronization are reminded and put in more general setting. Their group\ntheoretical context is described.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0510260v2"
    },
    {
        "title": "Marked signal improvement by stochastic resonance for aperiodic signals\n  in the double-well system",
        "authors": [
            "R Mingesz",
            "Z Gingl",
            "P Makra"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  On the basis of our mixed-signal simulations we report significant stochastic\nresonance induced input-output signal improvement in the double-well system for\naperiodic input types. We used a pulse train with randomised pulse locations\nand a band-limited noise with low cut-off frequency as input signals, and\napplied a cross-spectral measure to quantify their noise content. We also\nsupplemented our examinations with simulations in the Schmitt trigger to show\nthat the signal improvement we obtained is not a result of a potential\nfiltering effect due to the limited response time of the double-well dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0511016v1"
    },
    {
        "title": "Statistical Challenges for Searches for New Physics at the LHC",
        "authors": [
            "Kyle Cranmer"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Because the emphasis of the LHC is on 5 sigma discoveries and the LHC\nenvironment induces high systematic errors, many of the common statistical\nprocedures used in High Energy Physics are not adequate. I review the basic\ningredients of LHC searches, the sources of systematics, and the performance of\nseveral methods. Finally, I indicate the methods that seem most promising for\nthe LHC and areas that are in need of further study.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0511028v2"
    },
    {
        "title": "Likelihood ratio intervals with Bayesian treatment of uncertainties:\n  coverage, power and combined experiments",
        "authors": [
            "Jan Conrad",
            "Fredrik Tegenfeldt"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  In this note we present studies of coverage and power for confidence\nintervals for a Poisson process with known background calculated using the\nLikelihood ratio (aka Feldman & Cousins) ordering with Bayesian treatment of\nuncertainties in nuisance parameters. We consider both the variant where the\nBayesian integration is done in both the numerator and the denominator and the\nmodification where the integration is done only in the numerator whereas in the\ndenominator the likelihood is taken at the maximum likelihood estimate of the\nparameters. Furthermore we discuss how measurements can be combined in this\nframework and give an illustration with limits on the branching ratio of a rare\nB-meson decay recently presented by CDF/D0. A set of C++ classes has been\ndeveloped which can be used to calculate confidence intervals for single or\ncombining multiple experiments using the above algorithms and considering a\nvariety of parameterizations to describe the uncertainties.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0511055v1"
    },
    {
        "title": "On the significance in signal search through the sliding window\n  algorithm",
        "authors": [
            "Gioacchino Ranucci"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  The experimental issue of the search for new particles of unknown mass poses\nthe challenge of exploring a wide interval to look for the usual signatures\nrepresented by excess of events above the background. A side effect of such a\nbroad range quest is that the traditional significance calculations valid for\nsignals of known location are no more applicable when such an information is\nmissing. In this note the specific signal search approach via observation\nwindows sliding over the range of interest is considered; in the assumptions of\nknown background and of fixed width of the exploring windows the statistical\nimplications of such a search scheme are described, with special emphasis on\nthe correct significance assessment for a claimed discovery.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0511118v2"
    },
    {
        "title": "Statistical considerations on safety analysis",
        "authors": [
            "L. Pal",
            "M. Makai"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Alerting experience with a well-acknowledged safety analysis code initiated\nthe authors to pay attention to safety issues of complex systems. Their first\nconcern was the statistical characteristics of such a code. We point out a\nremarkable weakness of the so called 0.95/0.95 methodology: when repeating the\nsearch for the tolerance limit, we get a higher value with non-negligible\nprobability. We propose the sign test as an alternative method. We point out\nthe correct form of Wilks' formula when the number of parameters subjected to\nlimitation is two or more.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0511140v1"
    },
    {
        "title": "Optimized Integration of the Equations of Motion of a Particle in the\n  HERA-B Magnet",
        "authors": [
            "Alexander Spiridonov"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  In this note we present a flexible approach to perform the propagation of\ntrack parameters and their derivatives in an inhomogeneous magnetic field,\nkeeping the computational effort small. We discuss also a Kalman filter\nimplementation using this optimized computation of the derivatives.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0511177v1"
    },
    {
        "title": "The Bayesian Effects in Measurement of the Asymmetry of Poisson Flows",
        "authors": [
            "S. I. Bityukov",
            "N. V. Krasnikov",
            "A. Kuznetsov",
            "V. V. Smirnova"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  As it follows from the interrelation of Gamma and Poisson distributions the\nobserved value of asymmetry for Poisson flows of events has the bias. The Monte\nCarlo experiment confirms the presence of this bias between the observed and\ninitial asymmetries. The obtained results allow to correct the estimation of\nthe asymmetry of flow rates.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0512056v1"
    },
    {
        "title": "The PAX Toolkit and its Applications at Tevatron and LHC",
        "authors": [
            "Steffen Kappler",
            "Martin Erdmann",
            "Ulrich Felzmann",
            "Dominic Hirschbuehl",
            "Matthias Kirsch",
            "Guenter Quast",
            "Alexander Schmidt",
            "Joanna Weng"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  At the CHEP03 conference we launched the Physics Analysis eXpert (PAX), a C++\ntoolkit released for the use in advanced high energy physics (HEP) analyses.\nThis toolkit allows to define a level of abstraction beyond detector\nreconstruction by providing a general, persistent container model for HEP\nevents. Physics objects such as particles, vertices and collisions can easily\nbe stored, accessed and manipulated. Bookkeeping of relations between these\nobjects (like decay trees, vertex and collision separation, etc.) including\ndeep copies is fully provided by the relation management. Event container and\nassociated objects represent a uniform interface for algorithms and facilitate\nthe parallel development and evaluation of different physics interpretations of\nindividual events. So-called analysis factories, which actively identify and\ndistinguish different physics processes and study systematic uncertainties, can\neasily be realized with the PAX toolkit.\n  PAX is officially released to experiments at Tevatron and LHC. Being explored\nby a growing user community, it is applied in a number of complex physics\nanalyses, two of which are presented here. We report the successful application\nin studies of t-tbar production at the Tevatron and Higgs searches in the\nchannel t-tbar-Higgs at the LHC and give a short outlook on further\ndevelopments.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0512232v1"
    },
    {
        "title": "Modular Implementation of Particle Flow Algorithm with Minimized\n  Dependence on the Detector Geometry",
        "authors": [
            "A. Raspereza"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A Particle Flow Algorithm (PFA) with the minimized dependence on the detector\ngeometry is presented. Current PFA implementation includes procedures of the\ntrack reconstruction, calorimeter clustering, and individual particle\nreconstruction and is meant as a tool for the optimization of the International\ne+e- Linear Collider detector.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0601069v1"
    },
    {
        "title": "Teaching statistics with Excel and R",
        "authors": [
            "Matteo Dell'Omodarme",
            "Giada Valle"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Despite several deficiencies, the use of spreadsheets in statistics courses\nis increasingly common. In this paper we discuss many shortcomings resulting\nfrom this approach. We suggest a technique integrating a spreadsheet and a\ndedicated software package (R), that takes advantage of the characteristics of\nboth programs. We also present a simple protocol of transferring data from the\nspreadsheet to R that can be safely used even in introductory courses.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0601083v1"
    },
    {
        "title": "Critique on the measurement of neutron cross-sections by the Deep\n  Inelastic Neutron Scattering technique",
        "authors": [
            "J. J. Blostein",
            "J. Dawidowski",
            "J. R. Granada"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We analyze a recent work of Mayers and Abdul-Redah,[J.Phys.: Condens. Matter\n{\\bf 16}, 4811 (2004)] in which the autors report the existence of anomalous\nneutron cross sections in several systems. In the present work we show that the\nDeep Inelastic Neutron Scattering (DINS) results presented by the authors are\naffected by an inaccurate formalism employed for obtaining nuclear momentum\ndistributions, and therefore definitive conclusions cannot be drawn on the\nsubject of anomalous cross sections. We also show the reasons why the exact\nformalism for obtaining momentum distributions that we recently published must\nbe employed for analysing the experimental data instead of the approximations\nperformed in the mentioned work. We also point out serious inconsistencies\nbetween different results reported in the mentioned work, as well as\nincompatibilities with previous results published by the authors. These\ninconsistencies, as well as experimental evidence against the existence of\nanomalous cross sections not considered by the authors, reinforce the need to\nrevise critically the procedures on which the usual DINS data analysis is based\nas well as the proper characterisation of the experimental set-up.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0601221v1"
    },
    {
        "title": "sPlot: A Quick Introduction",
        "authors": [
            "Muriel Pivk"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The paper advocates the use of a statistical tool dedicated to the\nexploration of data samples populated by several sources of events. This new\ntechnique, called sPlot, is able to unfold the contributions of the different\nsources to the distribution of a data sample in a given variable. The sPlot\ntool applies in the context of a Likelihood fit which is performed on the data\nsample to determine the yields of the various sources.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0602023v1"
    },
    {
        "title": "Stochasticity of Road Traffic Dynamics: Comprehensive Linear and\n  Nonlinear Time Series Analysis on High Resolution Freeway Traffic Records",
        "authors": [
            "Helge Siegel",
            "Dennis Belomestnyi"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The dynamical properties of road traffic time series from North-Rhine\nWestphalian motorways are investigated. The article shows that road traffic\ndynamics is well described as a persistent stochastic process with two fixed\npoints representing the freeflow (non-congested) and the congested state\nregime. These traffic states have different statistical properties, with\nrespect to waiting time distribution, velocity distribution and\nautocorrelation. Logdifferences of velocity records reveal non-normal,\nobviously leptocurtic distribution. Further, linear and nonlinear phase-plane\nbased analysis methods yield no evidence for any determinism or deterministic\nchaos to be involved in traffic dynamics on shorter than diurnal time scales.\nSeveral Hurst-exponent estimators indicate long-range dependence for the free\nflow state. Finally, our results are not in accordance to the typical heuristic\nfingerprints of self-organized criticality. We suggest the more simplistic\nassumption of a non-critical phase transition between freeflow and congested\ntraffic.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0602136v1"
    },
    {
        "title": "Tuning degree distributions of scale-free networks",
        "authors": [
            "C. C. Leary",
            "M. Schwehm",
            "M. Eichner",
            "H. P. Duerr"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Scale-free networks are characterized by a degree distribution with power-law\nbehavior and have been shown to arise in many areas, ranging from the World\nWide Web to transportation or social networks. Degree distributions of observed\nnetworks, however, often differ from the power-law type and data based\ninvestigations require modifications of the typical scale-free network.\n  We present an algorithm that generates networks in which the skewness of the\ndegree distribution is tuneable by modifying the preferential attachment step\nof the Barabasi-Albert construction algorithm. Skewness is linearly correlated\nwith the maximal degree of the network and, therefore, adequately represents\nthe influence of superspreaders or hubs. By combining our algorithm with work\nof Holme and Kim, we show how to generate networks with skewness gamma and\nclustering coefficient kappa, over a wide range of values.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0602152v2"
    },
    {
        "title": "The Application of Bayesian Technique for Particle Identification",
        "authors": [
            "Ding Tian"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The PID problem in high energy physics experiments is analysed with Bayesian\ntechnique. The corresponding applicable method is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0602170v2"
    },
    {
        "title": "Surrogate testing of volatility series from long-range correlated noise",
        "authors": [
            "Radhakrishnan Nagarajan"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Detrended fluctuation analysis (DFA) [1] of the volatility series has been\nfound to be useful in dentifying possible nonlinear/multifractal dynamics in\nthe empirical sample [2-4]. Long-range volatile correlation can be an outcome\nof static as well as dynamical nonlinearity. In order to argue in favor of\ndynamical nonlinearity, surrogate testing is used in conjunction with\nvolatility analysis [2-4]. In this brief communication, surrogate testing of\nvolatility series from long-range correlated noise and their static, invertible\nnonlinear transforms is investigated. Long-range correlated monofractal noise\nis generated using FARIMA (0, d, 0) with Gaussian and non-Gaussian innovations.\nWe show significant deviation in the scaling behavior between the empirical\nsample and the surrogate counterpart at large time-scales in the case of FARIMA\n(0, d, 0) with non-Gaussian innovations whereas no such discrepancy was\nobserved in the case of Gaussian innovations. The results encourage cautious\ninterpretation of surrogate testing in the presence of non-Gaussian\ninnovations.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0603126v2"
    },
    {
        "title": "Memory Effects and Scaling Properties of Traffic Flows",
        "authors": [
            "B. -S. Skagerstam",
            "A. Hansen"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Traffic flows are studied in terms of their noise of sound, which is an\neasily accessible experimental quantity. The sound noise data is studied making\nuse of scaling properties of wavelet transforms and Hurst exponents are\nextracted. The scaling behavior is used to characterize the traffic flows in\nterms of scaling properties of the memory function in Mori-Lee stochastic\ndifferential equations. The results obtained provides for a new theoretical as\nwell as experimental framework to characterize the large-time behavior of\ntraffic flows. The present paper outlines the procedure by making use of\none-lane computer simulations as well as sound-data measurements from a real\ntwo-lane traffic flow. We find the presence of conventional diffusion as well\nas 1/f-noise in real traffic flows at large time scales.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0603252v1"
    },
    {
        "title": "On Estimation of Hurst Scaling Exponent through Discrete Wavelets",
        "authors": [
            "P. Manimaran",
            "Prasanta K. Panigrahi",
            "Jitendra C. Parikh"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We study the scaling behavior of the fluctuations, as extracted through\nwavelet coefficients based on discrete wavelets. The analysis is carried out on\na variety of physical data sets, as well as Gaussian white noise and binomial\nmulti-fractal model time series and the results are compared with continuous\nwavelet based average wavelet coefficient method. It is found that high-pass\ncoefficients of wavelets, belonging to the Daubechies family are quite good in\nestimating the true power in the fluctuations in a non-stationary time series.\nHence, the fluctuation functions based on discrete wavelet coefficients find\nthe Hurst scaling exponents accurately.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0604004v2"
    },
    {
        "title": "Spurious detection of phase synchronization in coupled nonlinear\n  oscillators",
        "authors": [
            "Limei Xu",
            "Zhi Chen",
            "Kun Hu",
            "H. Eugene Stanley",
            "Plamen Ch. Ivanov"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Coupled nonlinear systems under certain conditions exhibit phase\nsynchronization, which may change for different frequency bands or with\npresence of additive system noise. In both cases, Fourier filtering is\ntraditionally used to preprocess data. We investigate to what extent the phase\nsynchronization of two coupled R\\\"{o}ssler oscillators depends on (1) the\nbroadness of their power spectrum, (2) the width of the band-pass filter, and\n(3) the level of added noise. We find that for identical coupling strengths,\noscillators with broader power spectra exhibit weaker synchronization. Further,\nwe find that within a broad band width range, band-pass filtering reduces the\neffect of noise but can lead to a spurious increase in the degree of\nsynchronization with narrowing band width, even when the coupling between the\ntwo oscillators remains the same.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0604039v1"
    },
    {
        "title": "Multiscale Entropy Analysis: A New Method to Detect Determinism in a\n  Time Series",
        "authors": [
            "A. Sarkar",
            "P. Barat"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In this letter we show that the Multiscale Entropy (MSE) analysis can detect\nthe determinism in a time series.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0604040v1"
    },
    {
        "title": "Bayesian analysis of signal deconvolution using measured instrument\n  response functions",
        "authors": [
            "Pascal Pernot"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Using measured instrumental response functions for data deconvolution is a\nknown source of uncertainty. This problem is revisited here with Bayesian data\nanalysis an Monte Carlo simulations. Noise correlation induced by the\nconvolution operator is identified as a major source of uncertainty which has\nbeen neglected in previous treatments of this problem. Application to a\nluminescence lifetime measurement setup shows that existing approximate\ntreatments are markedly defficient and that the correlation length of the noise\nis directly related to the lifetime to be estimated. Simple counteractive\ntreatments are proposed to increase the accuracy of this procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0604154v1"
    },
    {
        "title": "Precursors of extreme increments",
        "authors": [
            "Sarah Hallerberg",
            "Eduardo G. Altmann",
            "Detlef Holstein",
            "Holger Kantz"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We investigate precursors and predictability of extreme increments in a time\nseries. The events we are focusing on consist in large increments within\nsuccessive time steps. We are especially interested in understanding how the\nquality of the predictions depends on the strategy to choose precursors, on the\nsize of the event and on the correlation strength. We study the prediction of\nextreme increments analytically in an AR(1) process, and numerically in wind\nspeed recordings and long-range correlated ARMA data. We evaluate the success\nof predictions via receiver operator characteristics (ROC-curves). Furthermore,\nwe observe an increase of the quality of predictions with increasing event size\nand with decreasing correlation in all examples. Both effects can be understood\nby using the likelihood ratio as a summary index for smooth ROC-curves.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0604167v3"
    },
    {
        "title": "Space-Time Clustering and Correlations of Major Earthquakes",
        "authors": [
            "James R. Holliday",
            "John B. Rundle",
            "Donald L. Turcotte",
            "William Klein",
            "Kristy F. Tiampo"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Earthquake occurrence in nature is thought to result from correlated elastic\nstresses, leading to clustering in space and time. We show that occurrence of\nmajor earthquakes in California correlates with time intervals when\nfluctuations in small earthquakes are suppressed relative to the long term\naverage. We estimate a probability of less than 1% that this coincidence is due\nto random clustering.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605008v1"
    },
    {
        "title": "Concepts, Developments and Advanced Applications of the PAX Toolkit",
        "authors": [
            "S. Kappler",
            "M. Erdmann",
            "U. Felzmann",
            "A. Flossdorf",
            "M. Kirsch",
            "G. Mueller",
            "G. Quast",
            "C. Saout",
            "A. Schmidt",
            "J. Weng"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The Physics Analysis eXpert (PAX) is an open source toolkit for high energy\nphysics analysis. The C++ class collection provided by PAX is deployed in a\nnumber of analyses with complex event topologies at Tevatron and LHC. In this\narticle, we summarize basic concepts and class structure of the PAX kernel. We\nreport about the most recent developments of the kernel and introduce two new\nPAX accessories. The PaxFactory, that provides a class collection to facilitate\nevent hypothesis evolution, and VisualPax, a Graphical User Interface for PAX\nobjects.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605063v1"
    },
    {
        "title": "Bias-Free Estimation in Multicomponent Maximum Likelihood Fits with\n  Component-Dependent Templates",
        "authors": [
            "P. Catastini",
            "G. Punzi"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The possibility of strong biases in a multicomponent Maximum Likelihood fits\nwith component-dependent templates has been demonstrated in some toy problems.\nWe discuss here in detail a problem of practical interest, particle\nidentification based on time-of-flight or dE/dx information. We show that large\nbiases can occur in estimating particle fractions in a sample if differences\nbetween the momentum spectra of particles are ignored, and we present a more\nrobust fit technique, allowing bias-free estimation even when the particle\nspectra in the sample are unknown.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605130v1"
    },
    {
        "title": "Correlated Binomial Models and Correlation Structures",
        "authors": [
            "M. Hisakado",
            "K. Kitsukawa",
            "S. Mori"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We discuss a general method to construct correlated binomial distributions by\nimposing several consistent relations on the joint probability function. We\nobtain self-consistency relations for the conditional correlations and\nconditional probabilities. The beta-binomial distribution is derived by a\nstrong symmetric assumption on the conditional correlations. Our derivation\nclarifies the 'correlation' structure of the beta-binomial distribution. It is\nalso possible to study the correlation structures of other probability\ndistributions of exchangeable (homogeneous) correlated Bernoulli random\nvariables. We study some distribution functions and discuss their behaviors in\nterms of their correlation structures.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605189v1"
    },
    {
        "title": "A more exact solution for incorporating multiplicative systematic\n  uncertainties in branching ratio limits",
        "authors": [
            "K. Stenson"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A method to include multiplicative systematic uncertainties into branching\nratio limits was proposed by M. Convery. That solution used approximations\nwhich are not necessarily valid. This note provides a solution without\napproximations and compares the results.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605236v1"
    },
    {
        "title": "Relationship Between Structural Characters and Synchronizability of\n  Scale-free Networks",
        "authors": [
            "Jian-Guo Liu",
            "Yan-Zhong Dang",
            "Qiang Guo",
            "Zhong-Tuo Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Using Memory Tabu Search(MTS) algorithm, we investigate the relationship\nbetween structural characters and synchronizability of scale-free networks by\nmaximizing and minimizing the ratio $Q$ of the eigenvalues of the coupling\nmatrix by edge-intercrossing procedures. The numerical results indicate that\nclustering coefficient $C$, maximal betweenness $B_{max}$ are two most\nimportant factors to scale-free network synchronizability, and assortative\ncoefficient $r$ and average distance $D$ are the secondary ones. Moreover, the\naverage degree $<k>$ affects the relationship between above structural\ncharacters and synchronizability of scale-free networks, and the minimal $Q$\ndecreases when $<k>$ increases.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0606088v1"
    },
    {
        "title": "Comparison Tests of Variable-Stepsize Algorithms for Stochastic Ordinary\n  Differential Equations of Finance",
        "authors": [
            "Yin Mei Wong",
            "Joshua Wilkie"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Since the introduction of the Black-Scholes model stochastic processes have\nplayed an increasingly important role in mathematical finance. In many cases\nprices, volatility and other quantities can be modeled using stochastic\nordinary differential equations. Available methods for solving such equations\nhave until recently been markedly inferior to analogous methods for\ndeterministic ordinary differential equations. Recently, a number of methods\nwhich employ variable stepsizes to control local error have been developed\nwhich appear to offer greatly improved speed and accuracy. Here we conduct a\ncomparative study of the performance of these algorithms for problems taken\nfrom the mathematical finance literature.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0606095v1"
    },
    {
        "title": "Probability and Statistical Inference",
        "authors": [
            "Harrison B. Prosper"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  These lectures introduce key concepts in probability and statistical\ninference at a level suitable for graduate students in particle physics. Our\ngoal is to paint as vivid a picture as possible of the concepts covered.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0606179v1"
    },
    {
        "title": "Bayesian Comparison of GARCH Processes with Skewnes Mechanism in\n  Conditional Distributions",
        "authors": [
            "Mateusz Pipien"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The main goal of this paper is an application of Bayesian model comparison,\nbased on the posterior probabilities and posterior odds ratios, in testing the\nexplanatory power of the set of competing GARCH (ang. Generalised\nAutoregressive Conditionally Heteroscedastic) specifications, all with\nasymmetric and heavy tailed conditional distributions. In building competing\nvolatility models we consider, as an initial specification, GARCH process with\nconditional Student-t distribution with unknown degrees of freedom parameter,\nproposed by Bollerslev (1987). By introducing skewness into Student-t family\nand incorporating the resulting class as a conditional distribution we\ngenerated various GARCH models, which compete in explaining possible asymmetry\nof both conditional and unconditional distribution of financial data. Based on\nthe daily returns of hypothetical financial time series, we discuss the results\nof Bayesian comparison of alternative skewing mechanisms applied in the initial\nStudent-t GARCH framework. We also check the sensitivity of model ranking with\nrespect to the changes in prior distribution of model specific parameters.\nAdditionally, we present formal Bayesian inference about conditional asymmetry\nof the distribution of the daily returns in all competing specifications on the\nbasis of the skewness measure defined by Arnold and Groenveld (1995).\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0606253v4"
    },
    {
        "title": "1/f noise from the coincidences of similar single-sided random telegraph\n  signals",
        "authors": [
            "Giovanni Zanella"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In this paper it is demonstrated that 1/f power spectrum appears in the\nprocess originated by the superposition of many single-sided random telegraph\nsignals (RTS or RTN) with the same amplitude, probability and relaxation time.\nIndeed, the coincidences of these RTSs generate self-organizing fluctuations\nwhich are responsible for the generation of 1/f noise under various aspects.\nThe accord of the exposed model with other models and with various experimental\nresults is displayed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607044v1"
    },
    {
        "title": "A simple model of reactor cores for reactor neutrino flux calculations\n  for the KamLAND experiment",
        "authors": [
            "K. Nakajima",
            "K. Inoue",
            "K. Owada",
            "F. Suekane",
            "A. Suzuki",
            "G. Hirano",
            "S. Kosaka",
            "T. Ohta",
            "H. Tanaka"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  KamLAND is a reactor neutrino oscillation experiment with a very long\nbaseline. This experiment successfully measured oscillation phenomena of\nreactor antineutrinos coming mainly from 53 reactors in Japan. In order to\nextract the results, it is necessary to accurately calculate time-dependent\nantineutrino spectra from all the reactors. A simple model of reactor cores and\ncode implementing it were developed for this purpose. This paper describes the\nmodel of the reactor cores used in the KamLAND reactor analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607126v1"
    },
    {
        "title": "Effective memory of the minority game",
        "authors": [
            "C. H. Hung",
            "S. S. Liaw"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  It is known that the memory is relevant in the symmetric phase of the\nminority game. In our previous work we have successfully explained the\nquasi-periodic behavior of the game in the symmetric phase with the help of the\nprobability theory. Based on this explanation, we are able to determine how the\nmemory affects the variance of the system in this paper. By using some\nparticular types of fake history such as periodic type and random type, we\ndetermine how efficient the memory has been used in the standard game.\nFurthermore, the analysis on the effective memory strongly supports the result\nwe proposed previously that there are three distinct phases in the minority\ngame.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607206v1"
    },
    {
        "title": "Self-organization behavior in a constrained minority game",
        "authors": [
            "C. Liu",
            "S. S. Liaw"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In the standard minority game, every agent switches to his best strategy in\nhand at each time step. If only a small number of agents are allowed to switch\ntheir strategies at each time step, the population variance of the system\nplunges. The variance reaches a low value and remains steady for a period of\ntime. Then without any sign it starts to rise abruptly to a high value and then\ndecreases smoothly to the previous low value again. The process of an abrupt\nrise followed by a gentle decrease repeats again and again but without obvious\ncharacteristic length as time goes on. The phenomenon is similar to the\ncollapse of a sand pile with sands being added continuously from the top. We\ndefine the scale of collapse of the population variance by the difference of\nthe variance before and after an abrupt rise. We then find that the logarithmic\nplot of the frequency versus scale of the collapses follows a power law.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607207v1"
    },
    {
        "title": "Correlation of worldwide markets' entropies: time-scale approach",
        "authors": [
            "J. A. O. Matos",
            "S. M. A. Gama",
            "H. J. Ruskin",
            "A. Sharkasi",
            "M. Crane"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We use a new method of studying the Hurst exponent with time and scale\ndependency. This new approach allow us to recover the major events affecting\nworldwide markets (such as the September 11th terrorist attack) and analyze the\nway those effects propagate through the different scales. The time-scale\ndependence of the referred measures demonstrates the relevance of entropy\nmeasures in distinguishing the several characteristics of market indices:\n\"effects\" include early awareness, patterns of evolution as well as comparative\nbehaviour distinctions in emergent/established markets.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607296v1"
    },
    {
        "title": "Fractal Analysis of River Flow Fluctuations (with Erratum)",
        "authors": [
            "M. Sadegh Movahed",
            "Evalds Hermanis"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We use some fractal analysis methods to study river flow fluctuations.\n  The result of the Multifractal Detrended Fluctuation Analysis (MF-DFA) shows\nthat there are two crossover timescales at $s_{1\\times}\\sim12$ and\n$s_{2\\times}\\sim130$ months in the fluctuation function. We discuss how the\nexistence of the crossover timescales are related to a sinusoidal trend. The\nfirst crossover is due to the seasonal trend and the value of second ones is\napproximately equal to the well known cycle of sun activity. Using Fourier\ndetrended fluctuation analysis, the sinusoidal trend is eliminated. The value\nof Hurst exponent of the runoff water of rivers without the sinusoidal trend\nshows a long range correlation behavior. For the Daugava river the value of\nHurst exponent is $0.52\\pm0.01$ and also we find that these fluctuations have\nmultifractal nature. Comparing the MF-DFA results for the remaining data set of\nDaugava river to those for shuffled and surrogate series, we conclude that its\nmultifractal nature is almost entirely due to the broadness of probability\ndensity function.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608056v3"
    },
    {
        "title": "Multiscale reconstruction of time series",
        "authors": [
            "A. P. Nawroth",
            "J. Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A new method is proposed which allows a reconstruction of time series based\non higher order multiscale statistics given by a hierarchical process. This\nmethod is able to model the time series not only on a specific scale but for a\nrange of scales. It is possible to generate complete new time series, or to\nmodel the next steps for a given sequence of data. The method itself is based\non the joint probability density which can be extracted directly from given\ndata, thus no estimation of parameters is necessary. The results of this\napproach are shown for a real world dataset, namely for turbulence. The\nunconditional and conditional probability densities of the original and\nreconstructed time series are compared and the ability to reproduce both is\ndemonstrated. Therefore in the case of Markov properties the method proposed\nhere is able to generate artificial time series with correct n-point\nstatistics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608069v1"
    },
    {
        "title": "Medium and Small Scale Analysis of Financial Data",
        "authors": [
            "Andreas P. Nawroth",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A stochastic analysis of financial data is presented. In particular we\ninvestigate how the statistics of log returns change with different time delays\n$\\tau$. The scale dependent behaviour of financial data can be divided into two\nregions. The first time-range, the small-timescale region (in the range of\nseconds) seems to be characterized by universal features. The second\ntime-range, the medium-timescale range from several minutes upwards and can be\ncharacterized by a cascade process, which is given by a stochastic Markov\nprocess in the scale $\\tau$. A corresponding Fokker-Planck equation can be\nextracted from given data and provides a non equilibrium thermodynamical\ndescription of the complexity of financial data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608103v1"
    },
    {
        "title": "The dependence structure for PARMA models with alpha-stable innovations",
        "authors": [
            "Joanna Nowicka-Zagrajek",
            "Agnieszka Wylomanska"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In this paper we investigate the dependence structure for PARMA models (i.e.\nARMA models with periodic coefficients) with symmetric alpha-stable\ninnovations. In this case the covariance function is not defined and therefore\nother measures of dependence have to be used. After obtaining the form of the\nbounded solution of the PARMA system with symmetric alpha-stable innovations,\nwe study the codifference and the covariation -- the most popular measures of\ndependence defined for symmetric stable time series. We show that both\nconsidered measures are periodic. Moreover we determine the cases when the\ncodifference and the covariation are asymptotically proportional with the\ncoefficient of proportionality equal to alpha.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608137v1"
    },
    {
        "title": "Extract Signals by Fitting $χ^2$ Distribution of the Kinematic Fit",
        "authors": [
            "Ping Rong-Gang",
            "Mo Xiao-Hu",
            "Du Shu-Xian",
            "Liu Jing",
            "Ma Lian-Liang",
            "Shen Cheng-Ping",
            "Wang Zhi-Yong",
            "Yuan Chang-Zheng"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In measuring the $\\psip$ radiative decays at BESII, contribution of the\nbackground is serious in most of the final states. To extract the number of\nsignal events, a fit to the $\\chi^2$ distribution of kinematic fit using signal\nand background components is proposed. Extensive Monte-Carlo simulations (MCS)\nare performed, and the results show that the shape of $\\chi^2$ distribution of\nthe signal channel looks different from those of the background channels, thus\nit provides us by fitting the $\\chi^2$ distribution of the data to extract the\nnumber of signal events. An input-output test is performed using MCS, and the\nuncertainty of the fit method is found to be less than 2%.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608213v1"
    },
    {
        "title": "Signal discovery in sparse spectra: a Bayesian analysis",
        "authors": [
            "Allen Caldwell",
            "Kevin Kröninger"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A Bayesian analysis of the probability of a signal in the presence of\nbackground is developed, and criteria are proposed for claiming evidence for,\nor the discovery of a signal. The method is general and in particular\napplicable to sparsely populated spectra. Monte Carlo techniques to evaluate\nthe sensitivity of an experiment are described. As an example, the method is\nused to calculate the sensitivity of the GERDA experiment to neutrinoless\ndouble beta decay.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608249v1"
    },
    {
        "title": "Power-law distribution of individual Hirsch indices, the comparison of\n  merits in different fields, and the relation to a Pareto distribution",
        "authors": [
            "Pekka Pyykko"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A data set of Hirsch indices, $h$, for Finnish scientists in certain fields\nis statistically analyzed and fitted to $h(n) =Pn^p$ for the $n$-th most-quoted\nscientist. The precoefficient $P$ is characteristic for the field and the\nexponent $p$ is about -0.2 for all data sets considered. For Physics, Chemistry\nand Chemical Engineering, the $P$ are 49.7(8), 41.3(6), and 21.4(6),\nrespectively. These $p$ values correspond to Pareto exponents of about -7 for\nthe distribution of Hirsch indices $h$.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608282v1"
    },
    {
        "title": "Modeling, Performance Analysis and Comparison of Two Level Single Chain\n  Pointer Forwarding Strategy For Location Management in Wireless Mobile\n  Communication",
        "authors": [
            "Chhaya Ravi Kant",
            "P. Arun",
            "Nupur Prakash"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Global wireless networks enable mobile users to communicate regardless of\ntheir locations. Location management is an important part of the emerging\nwireless and mobile technology. A Personal Communication System (PCS) network\nmust have an efficient way to keep track of the mobile users to deliver\nservices effectively. Global System for Mobile Communication (GSM) is a\ncommonly accepted standard for mobility management of mobile users. Location\nmanagement involves location tracking, and location information storage.\nLocation management requires mobile users to register at various registration\nareas whenever they are on the move. The registration process may cause\nexcessive signaling traffic and long service delays. To improve the efficiency\nof location tracking and avoid call set up delays, several strategies such as\nlocal anchor scheme, per-user caching scheme and several pointer forwarding\nschemes have been proposed in the past. In this paper, we propose a new \"Two\nLevel Single Chain Pointer Forwarding (TLSCP) Strategy\" in which a two level\nhierarchy of level-1 and level-2 forwarding pointers reduced to single chain\nlength are used. Organizing the pointers in a two level hierarchy and further\nrestricting the pointer chain lengths at both the levels to single chain\nlocalizes the network signaling traffic and reduces the call set up delays. To\njustify the effectiveness of our proposed strategy, we develop an analytical\nmodel to evaluate the signaling cost. Our performance analysis shows that the\nproposed dynamic TLSCP scheme can significantly reduce the network signaling\ntraffic and cost for different categories of users under different network\nconditions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0609123v1"
    },
    {
        "title": "Phase transition and hysteresis in scale-free network traffic",
        "authors": [
            "Mao-Bin Hu",
            "Wen-Xu Wang",
            "Rui Jiang",
            "Qing-Song Wu",
            "Yong-Hong Wu"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We model information traffic on scale-free networks by introducing the node\nqueue length L proportional to the node degree and its delivering ability C\nproportional to L. The simulation gives the overall capacity of the traffic\nsystem, which is quantified by a phase transition from free flow to congestion.\nIt is found that the maximal capacity of the system results from the case of\nthe local routing coefficient \\phi slightly larger than zero, and we provide an\nanalysis for the optimal value of \\phi. In addition, we report for the first\ntime the fundamental diagram of flow against density, in which hysteresis is\nfound, and thus we can classify the traffic flow with four states: free flow,\nsaturated flow, bistable, and jammed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0609131v2"
    },
    {
        "title": "Burstiness and Memory in Complex Systems",
        "authors": [
            "Kwang-Il Goh",
            "Albert-Laszlo Barabasi"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The dynamics of a wide range of real systems, from email patterns to\nearthquakes, display a bursty, intermittent nature, characterized by short\ntimeframes of intensive activity followed by long times of no or reduced\nactivity. The understanding of the origin of such bursty patterns is hindered\nby the lack of tools to compare different systems using a common framework. We\nintroduce two measures to distinguish the mechanisms responsible for the bursty\nnature of real signals, changes in the interevent times and memory. We find\nthat while the burstiness of natural phenomena is rooted in both the interevent\ntime distribution and memory, for human dynamics memory is weak, and the bursty\ncharacter is due to changes in the interevent time distribution. Finally, we\nshow that current models lack in their ability to reproduce the activity\npattern observed in real systems, opening up new avenues for future work.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0610233v1"
    },
    {
        "title": "Studies of Stability and Robustness for Artificial Neural Networks and\n  Boosted Decision Trees",
        "authors": [
            "Hai-Jun Yang",
            "Byron P. Roe",
            "Ji Zhu"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In this paper, we compare the performance, stability and robustness of\nArtificial Neural Networks (ANN) and Boosted Decision Trees (BDT) using\nMiniBooNE Monte Carlo samples. These methods attempt to classify events given a\nnumber of identification variables. The BDT algorithm has been discussed by us\nin previous publications. Testing is done in this paper by smearing and\nshifting the input variables of testing samples. Based on these studies, BDT\nhas better particle identification performance than ANN. The degradation of the\nclassifications obtained by shifting or smearing variables of testing results\nis smaller for BDT than for ANN.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0610276v2"
    },
    {
        "title": "Statistical variances in traffic data",
        "authors": [
            "Milan Krbalek",
            "Petr Seba"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We perform statistical analysis of the single-vehicle data measured on the\nDutch freeway A9 and discussed in Ref. [2]. Using tools originating from the\nRandom Matrix Theory we show that the significant changes in the statistics of\nthe traffic data can be explained applying equilibrium statistical physics of\ninteracting particles.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0611049v2"
    },
    {
        "title": "Fatigue independent amplitude-frequency correlations in EMG signals",
        "authors": [
            "Adam Siemienski",
            "Alicja Kebel",
            "Piotr Klajner"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In order to assess fatigue independent amplitude-frequency correlations in\nEMG signals we asked nineteen male subjects to perform a series of isometric\nmuscular contractions by extensors of the knee joint. Different amplitudes of\nthe signal were due to randomly varying both the joint moment and the overall\namplification factor of the EMG apparatus. Mean and median frequency, RMS and\nmean absolute value were calculated for every combination of joint moment and\namplification at the original sampling rate of 5 kHz and at several simulated\nlower sampling rates. Negative Spearman and Kendall amplitude-frequency\ncorrelation coefficients were found, and they were more pronounced at high\nsampling rates.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0611120v1"
    },
    {
        "title": "The structure of the hydrated electron. Part 2. A mixed quantum\n  classical molecular dynamics - embedded cluster density functional theory:\n  single-excitation configuration interaction study",
        "authors": [
            "Ilya A. Shkrob",
            "William J. Glover",
            "Ross E. Larsen",
            "Benjamin J. Schwartz"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Adiabatic mixed quantum/classical molecular dynamics simulations were used to\ngenerate snapshots of the hydrated electron (e-) in liquid water at 300 K.\nWater cluster anions that include two complete solvation shells centered on the\ne- were extracted from these simulations and embedded in a matrix of fractional\npoint charges designed to represent the rest of the solvent. Density functional\ntheory and single-excitation configuration interaction methods were then\napplied to these embedded clusters. The salient feature of these hybrid\ncalculations is significant transfer (ca. 0.18) of the excess electron's charge\ndensity into the O 2p orbitals in OH groups forming the solvation cavity. We\nused the results of these calculations to examine the structure of the\nmolecular orbitals, the density of states, the absorption spectra in the\nvisible and ultraviolet, the hyperfine coupling (hfc) tensors, and the IR and\nRaman spectra of the e-. The calculated hfc tensors were used to compute the\nEPR and ESEEM spectra for the e- that compared favorably to the experimental\nspectra of trapped e- in alkaline ice. The calculated vibrational spectra of\nthe e- are consistent with the red-shifted bending and stretching frequencies\nobserved in resonance Raman experiments. The model also accounts for the VIS\nand 190-nm absorption bands of the e-. Thus, our study suggests that to explain\nseveral important experimentally observed properties of the e-, many-electron\neffects must be accounted for.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0611176v2"
    },
    {
        "title": "Ranking Spaces for Predicting Human Movement in an Urban Environment",
        "authors": [
            "Bin Jiang"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A city can be topologically represented as a connectivity graph, consisting\nof nodes representing individual spaces and links if the corresponding spaces\nare intersected. It turns out in the space syntax literature that some defined\ntopological metrics can capture human movement rates in individual spaces. In\nother words, the topological metrics are significantly correlated to human\nmovement rates, and individual spaces can be ranked by the metrics for\npredicting human movement. However, this correlation has never been well\njustified. In this paper, we study the same issue by applying the weighted\nPageRank algorithm to the connectivity graph or space-space topology for\nranking the individual spaces, and find surprisingly that (1) the PageRank\nscores are better correlated to human movement rates than the space syntax\nmetrics, and (2) the underlying space-space topology demonstrates small world\nand scale free properties. The findings provide a novel justification as to why\nspace syntax, or topological analysis in general, can be used to predict human\nmovement. We further conjecture that this kind of analysis is no more than\npredicting a drunkard's walking on a small world and scale free network.\n  Keywords: Space syntax, topological analysis of networks, small world, scale\nfree, human movement, and PageRank\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0612011v4"
    },
    {
        "title": "General radar transmission codes that minimize measurement error of a\n  static target",
        "authors": [
            "Juha Vierinen",
            "Markku Lehtinen",
            "Mikko Orispaa",
            "Baylie Damtie"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The variances of matched and sidelobe free mismatched filter estimators are\ngiven for arbitrary coherent targets in the case of aperiodic transmission. It\nis shown that mismatched filtering is often better than matched filtering in\nterms of estimation accuracy. A search strategy for finding general\ntransmission codes that minimize estimation error and satisfy constraints on\ncode power and amplitude range is then introduced. Results show that nearly\nperfect codes, with performance close to a single pulse with the same total\npower can be found. Also, finding these codes is not computationally expensive\nand such codes can be found for all practical code lengths. The estimation\naccuracy of the newly found codes are compared to binary phase codes of similar\nlength and found to be better in terms of estimator variance. Similar\ntransmission codes might be worth investigating also for sonar and\ntelecommunications applications.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0612040v1"
    },
    {
        "title": "A non-iterative algorithm to estimate the modes of univariate mixtures\n  with well separated components",
        "authors": [
            "Nicolas Paul",
            "Luc Fety",
            "Michel Terre"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  This paper deals with the estimation of the modes of an univariate mixture\nwhen the number of components is known and when the component density are well\nseparated. We propose an algorithm based on the minimization of the \"kp\"\ncriterion we introduced in a previous work. In this paper we show that the\nglobal minimum of this criterion can be reached with a linear least square\nminimization followed by a roots finding algorithm. This is a major advantage\ncompared to classical iterative algorithms such as K-means or EM which suffer\nfrom the potential convergence to some local extrema of the cost function they\nuse. Our algorithm performances are finally illustrated through simulations of\na five components mixture.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0612073v1"
    },
    {
        "title": "Limits on Relief through Constrained Exchange on Random Graphs",
        "authors": [
            "Randall A. LaViolette",
            "Lory A. Ellebracht",
            "Charles J. Gieseler"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Agents are represented by nodes on a random graph (e.g., small world or\ntruncated power law). Each agent is endowed with a zero-mean random value that\nmay be either positive or negative. All agents attempt to find relief, i.e., to\nreduce the magnitude of that initial value, to zero if possible, through\nexchanges. The exchange occurs only between agents that are linked, a\nconstraint that turns out to dominate the results. The exchange process\ncontinues until a Pareto equilibrium is achieved. Only 40%-90% of the agents\nachieved relief on small world graphs with mean degree between 2 and 40. Even\nfewer agents achieved relief on scale-free like graphs with a truncated power\nlaw degree distribution. The rate at which relief grew with increasing degree\nwas slow, only at most logarithmic for all of the graphs considered; viewed in\nreverse, relief is resilient to the removal of links.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0612085v2"
    },
    {
        "title": "Effect of meditation on scaling behavior and complexity of human heart\n  rate variability",
        "authors": [
            "A. Sarkar",
            "P. Barat"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The heart beat data recorded from samples before and during meditation are\nanalyzed using two different scaling analysis methods. These analyses revealed\nthat mediation severely affects the long range correlation of heart beat of a\nnormal heart. Moreover, it is found that meditation induces periodic behavior\nin the heart beat. The complexity of the heart rate variability is quantified\nusing multiscale entropy analysis and recurrence analysis. The complexity of\nthe heart beat during mediation is found to be more.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0612088v1"
    },
    {
        "title": "The use of cluster quality for track fitting in the CSC detector",
        "authors": [
            "Erez Etzion",
            "David Primor",
            "Giora Mikenberg",
            "Nir Amram",
            "Hagit Messer"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The new particle accelerators and its experiments create a challenging data\nprocessing environment, characterized by large amount of data where only small\nportion of it carry the expected new scientific information. Modern detectors,\nsuch as the Cathode Strip Chamber (CSC), achieve high accuracy of coordinate\nmeasurements (between 50 to 70 microns). However, heavy physical backgrounds\ncan decrease the accuracy significantly. In the presence of such background,\nthe charge induced over adjacent CSC strips (cluster) is different from the\nideal Matheison distribution. The traditional least squares method which takes\nthe same ideal position error for all clusters loses its optimal properties on\ncontaminated data. A new technique that calculates the cluster quality and uses\nit to improve the track fitting results is suggested. The algorithm is applied\non test beam data, and its performance is compared to other fitting methods. It\nis shown that the suggested algorithm improves the fitting performance\nsignificantly.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0612129v1"
    },
    {
        "title": "On the Combining Significances",
        "authors": [
            "Sergey Bityukov",
            "Nikolai Krasnikov",
            "Alexander Nikitenko"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We present the statistical approach to the combining of signal significances.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0612178v3"
    },
    {
        "title": "Operations structure for the management, control and support of the\n  INFN-GRID/Grid.It production infrastructure",
        "authors": [
            "Maria Cristina Vistoli",
            "Luciano Gaido",
            "Federico Calzolari"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Moving from a National Grid Testbed to a Production quality Grid service for\nthe HEP applications requires an effective operations structure and\norganization, proper user and operations support, flexible and efficient\nmanagement and monitoring tools. Moreover the middleware releases should be\neasily deployable using flexible configuration tools, suitable for various and\ndifferent local computing farms. The organizational model, the available tools\nand the agreed procedures for operating the national/regional grid\ninfrastructures that are part of the world-wide EGEE grid as well as the\ninterconnection of the regional operations structures with the global\nmanagement, control and support structure play a key role for the success of a\nreal production grid. In this paper we describe the operations structure that\nwe are currently using at the Italian Grid Operation and Support Center. The\nactivities described cover monitoring, management and support for the\nINFN-GRID/Grid.It production grid (spread over more than 30 sites) and its\ninterconnection with the EGEE/LCG structure as well as the roadmap to improve\nthe global support quality, stability and reliability of the Grid service.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0701067v1"
    },
    {
        "title": "Electron/pion separation with an Emulsion Cloud Chamber by using a\n  Neural Network",
        "authors": [
            "L. Arrabito",
            "D. Autiero",
            "C. Bozza",
            "S. Buontempo",
            "Y. Caffari",
            "L. Consiglio",
            "M. Cozzi",
            "N. D'Ambrosio",
            "G. De Lellis",
            "M. De Serio",
            "F. Di Capua",
            "D. Di Ferdinando",
            "N. Di Marco",
            "A. Ereditato",
            "L. S. Esposito",
            "S. Gagnebin",
            "G. Giacomelli",
            "M. Giorgini",
            "G. Grella",
            "M. Hauger",
            "M. Ieva",
            "J. Janicsko Csathy",
            "F. Juget",
            "I. Kreslo",
            "I. Laktineh",
            "A. Longhin",
            "G. Mandrioli",
            "A. Marotta",
            "J. Marteau",
            "P. Migliozzi",
            "P. Monacelli",
            "U. Moser",
            "M. T. Muciaccia",
            "A. Pastore",
            "L. Patrizii",
            "C. Pistillo",
            "M. Pozzato",
            "G. Romano",
            "G. Rosa",
            "A. Russo",
            "N. Savvinov",
            "A. Schembri",
            "L. Scotto Lavina",
            "S. Simone",
            "M. Sioli",
            "C. Sirignano",
            "G. Sirri",
            "P. Strolin",
            "V. Tioukov"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We have studied the performance of a new algorithm for electron/pion\nseparation in an Emulsion Cloud Chamber (ECC) made of lead and nuclear emulsion\nfilms. The software for separation consists of two parts: a shower\nreconstruction algorithm and a Neural Network that assigns to each\nreconstructed shower the probability to be an electron or a pion. The\nperformance has been studied for the ECC of the OPERA experiment [1].\n  The $e/\\pi$ separation algorithm has been optimized by using a detailed Monte\nCarlo simulation of the ECC and tested on real data taken at CERN (pion beams)\nand at DESY (electron beams). The algorithm allows to achieve a 90% electron\nidentification efficiency with a pion misidentification smaller than 1% for\nenergies higher than 2 GeV.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0701192v1"
    },
    {
        "title": "Treatment of Errors in Efficiency Calculations",
        "authors": [
            "T. Ullrich",
            "Z. Xu"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  In this report we discuss the treatment of statistical errors in cut\nefficiencies. The two commonly used methods for the calculation of the errors,\nPoissonian and Binomial, are shown to be defective. We derive the form of the\nunderlying probability density function and characterize its mean, mode, and\nvariance. A method for the calculation of errors based on the variance of the\ndistribution is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0701199v2"
    },
    {
        "title": "Deterministic Modularity Optimization",
        "authors": [
            "Sune Lehmann",
            "Lars Kai Hansen"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We study community structure of networks. We have developed a scheme for\nmaximizing the modularity Q based on mean field methods. Further, we have\ndefined a simple family of random networks with community structure; we\nunderstand the behavior of these networks analytically. Using these networks,\nwe show how the mean field methods display better performance than previously\nknown deterministic methods for optimization of Q.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0701348v2"
    },
    {
        "title": "Hilbert Space Becomes Ultrametric in the High Dimensional Limit:\n  Application to Very High Frequency Data Analysis",
        "authors": [
            "Fionn Murtagh"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  An ultrametric topology formalizes the notion of hierarchical structure. An\nultrametric embedding, referred to here as ultrametricity, is implied by a\nnatural hierarchical embedding. Such hierarchical structure can be global in\nthe data set, or local. By quantifying extent or degree of ultrametricity in a\ndata set, we show that ultrametricity becomes pervasive as dimensionality\nand/or spatial sparsity increases. This leads us to assert that very high\ndimensional data are of simple structure. We exemplify this finding through a\nrange of simulated data cases. We discuss also application to very high\nfrequency time series segmentation and modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0702064v1"
    },
    {
        "title": "A basic problem in the correlations between statistics and\n  thermodynamics",
        "authors": [
            "Congjie Ou",
            "Zhifu Huang",
            "Jincan Chen"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  By using a fonctionelle of probability distributions, several different\nstatistical physics including extensive and nonextensive statistics are unified\nin a general method. The essential equivalence between the MaxEnt process of\nthe most probable probility distribution in these statistics and the famous\nthermodynamical relation dU=TdS is strictly proved without any additional\nassumption. Moreover, it is expounded that all the conclusions of these\ndifferent statistics can be directly derived from the equivalent relation.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0702134v1"
    },
    {
        "title": "Evaluation of three methods for calculating statistical significance\n  when incorporating a systematic uncertainty into a test of the\n  background-only hypothesis for a Poisson process",
        "authors": [
            "Robert D. Cousins",
            "James T. Linnemann",
            "Jordan Tucker"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Hypothesis tests for the presence of new sources of Poisson counts amidst\nbackground processes are frequently performed in high energy physics (HEP),\ngamma ray astronomy (GRA), and other branches of science. While there are\nconceptual issues already when the mean rate of background is precisely known,\nthe issues are even more difficult when the mean background rate has\nnon-negligible uncertainty. After describing a variety of methods to be found\nin the HEP and GRA literature, we consider in detail three classes of\nalgorithms and evaluate them over a wide range of parameter space, by the\ncriterion of how close the ensemble-average Type I error rate (rejection of the\nbackground-only hypothesis when it is true) compares with the nominal\nsignificance level given by the algorithm. We recommend wider use of an\nalgorithm firmly grounded in frequentist tests of the ratio of Poisson means,\nalthough for very low counts the over-coverage can be severe due to the effect\nof discreteness. We extend the studies of Cranmer, who found that a popular\nBayesian-frequentist hybrid can undercover severely when taken to high Z\nvalues. We also examine the profile likelihood method, which has long been used\nin GRA and HEP; it provides an excellent approximation in much of the parameter\nspace, as previously studied by Rolke and collaborators.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0702156v4"
    },
    {
        "title": "TMVA - Toolkit for Multivariate Data Analysis",
        "authors": [
            "A. Hoecker",
            "P. Speckmayer",
            "J. Stelzer",
            "J. Therhaag",
            "E. von Toerne",
            "H. Voss",
            "M. Backes",
            "T. Carli",
            "O. Cohen",
            "A. Christov",
            "D. Dannheim",
            "K. Danielowski",
            "S. Henrot-Versille",
            "M. Jachowski",
            "K. Kraszewski",
            "A. Krasznahorkay Jr.",
            "M. Kruk",
            "Y. Mahalalel",
            "R. Ospanov",
            "X. Prudent",
            "A. Robert",
            "D. Schouten",
            "F. Tegenfeldt",
            "A. Voigt",
            "K. Voss",
            "M. Wolter",
            "A. Zemla"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  In high-energy physics, with the search for ever smaller signals in ever\nlarger data sets, it has become essential to extract a maximum of the available\ninformation from the data. Multivariate classification methods based on machine\nlearning techniques have become a fundamental ingredient to most analyses. Also\nthe multivariate classifiers themselves have significantly evolved in recent\nyears. Statisticians have found new ways to tune and to combine classifiers to\nfurther gain in performance. Integrated into the analysis framework ROOT, TMVA\nis a toolkit which hosts a large variety of multivariate classification\nalgorithms. Training, testing, performance evaluation and application of all\navailable classifiers is carried out simultaneously via user-friendly\ninterfaces. With version 4, TMVA has been extended to multivariate regression\nof a real-valued target vector. Regression is invoked through the same user\ninterfaces as classification. TMVA 4 also features more flexible data handling\nallowing one to arbitrarily form combined MVA methods. A generalised boosting\nmethod is the first realisation benefiting from the new framework.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0703039v5"
    },
    {
        "title": "Reduction of time-resolved space-based CCD photometry developed for MOST\n  Fabry Imaging data",
        "authors": [
            "P. Reegen",
            "T. Kallinger",
            "D. Frast",
            "M. Gruberbauer",
            "D. Huber",
            "J. M. Matthews",
            "D. Punz",
            "S. Schraml",
            "W. W. Weiss",
            "R. Kuschnig",
            "A. F. J. Moffat",
            "G. A. H. Walker",
            "D. B. Guenther",
            "S. M. Rucinski",
            "D. Sasselov"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The MOST (Microvariability & Oscillations of STars) satellite obtains\nultraprecise photometry from space with high sampling rates and duty cycles.\nAstronomical photometry or imaging missions in low Earth orbits, like MOST, are\nespecially sensitive to scattered light from Earthshine, and all these missions\nhave a common need to extract target information from voluminous data cubes.\nThey consist of upwards of hundreds of thousands of two-dimensional CCD frames\n(or sub-rasters) containing from hundreds to millions of pixels each, where the\ntarget information, superposed on background and instrumental effects, is\ncontained only in a subset of pixels (Fabry Images, defocussed images,\nmini-spectra). We describe a novel reduction technique for such data cubes:\nresolving linear correlations of target and background pixel intensities. This\nstepwise multiple linear regression removes only those target variations which\nare also detected in the background. The advantage of regression analysis\nversus background subtraction is the appropriate scaling, taking into account\nthat the amount of contamination may differ from pixel to pixel. The\nmultivariate solution for all pairs of target/background pixels is minimally\ninvasive of the raw photometry while being very effective in reducing\ncontamination due to, e.g., stray light. The technique is tested and\ndemonstrated with both simulated oscillation signals and real MOST photometry.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0703153v2"
    },
    {
        "title": "A Topological Pattern of Urban Street Networks: Universality and\n  Peculiarity",
        "authors": [
            "Bin Jiang"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  In this paper, we derive a topological pattern of urban street networks using\na large sample (the largest so far to the best of our knowledge) of 40 U.S.\ncities and a few more from elsewhere of different sizes. It is found that all\nthe topologies of urban street networks based on street-street intersection\ndemonstrate a small world structure, and a scale-free property for both street\nlength and connectivity degree. More specifically, for any street network,\nabout 80% of its streets have length or degrees less than its average value,\nwhile 20% of streets have length or degrees greater than the average. Out of\nthe 20%, there are less than 1% of streets which can form a backbone of the\nstreet network. Based on the finding, we conjecture that the 20% streets\naccount for 80% of traffic flow, and the 1% streets constitute a cognitive map\nof the urban street network. We illustrate further a peculiarity about the\nscale-free property.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0703223v3"
    },
    {
        "title": "A global algorithm for clustering univariate observations",
        "authors": [
            "Paul Terre Fety"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  This paper deals with the clustering of univariate observations: given a set\nof observations coming from $K$ possible clusters, one has to estimate the\ncluster means. We propose an algorithm based on the minimization of the \"KP\"\ncriterion we introduced in a previous work. In this paper, we show that the\nglobal minimum of this criterion can be reached by first solving a linear\nsystem then calculating the roots of some polynomial of order $K$. The KP\nglobal minimum provides a first raw estimate of the cluster means, and a final\nclustering step enables to recover the cluster means. Our method's relevance\nand superiority to the Expectation-Maximization algorithm is illustrated\nthrough simulations of various Gaussian mixtures. \\keywords{unsupervised\nclustering \\and non-iterative algorithm \\and optimization criterion \\and\nunivariate observations\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0703281v1"
    },
    {
        "title": "Experimental modeling of physical laws",
        "authors": [
            "I. Grabec"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  A physical law is represented by the probability distribution of a measured\nvariable. The probability density is described by measured data using an\nestimator whose kernel is the instrument scattering function. The experimental\ninformation and data redundancy are defined in terms of information entropy.\nThe model cost function, comprised of data redundancy and estimation error, is\nminimized by the creation-annihilation process.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0107v1"
    },
    {
        "title": "Long-range correlation and multifractality in Bach's Inventions pitches",
        "authors": [
            "G. R. Jafari",
            "P. Pedram",
            "L. Hedayatifar"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We show that it can be considered some of Bach pitches series as a stochastic\nprocess with scaling behavior. Using multifractal deterend fluctuation analysis\n(MF-DFA) method, frequency series of Bach pitches have been analyzed. In this\nview we find same second moment exponents (after double profiling) in ranges\n(1.7-1.8) in his works. Comparing MF-DFA results of original series to those\nfor shuffled and surrogate series we can distinguish multifractality due to\nlong-range correlations and a broad probability density function. Finally we\ndetermine the scaling exponents and singularity spectrum. We conclude fat tail\nhas more effect in its multifractality nature than long-range correlations.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0726v2"
    },
    {
        "title": "The Effect of Annealing Temperature on Statistical Properties of $WO_3$\n  Surface",
        "authors": [
            "G. R. Jafari",
            "A. A. Saberi",
            "R. Azimirad",
            "A. Z. Moshfegh",
            "S. Rouhani"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We have studied the effect of annealing temperature on the statistical\nproperties of $WO_3$ surface using atomic force microscopy techniques (AFM). We\nhave applied both level crossing and structure function methods. Level crossing\nanalysis indicates an optimum annealing temperature of around 400$^oC$ at which\nthe effective area of the $WO_3$ thin film is maximum, whereas composition of\nthe surface remains stoichiometric. The complexity of the height fluctuation of\nsurfaces was characterized by roughness, roughness exponent and lateral size of\nsurface features. We have found that there is a phase transition at around\n400$^oC$ from one set to two sets of roughness parameters. This happens due to\nmicrostructural changes from amorphous to crystalline structure in the samples\nthat has been already found experimentally.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.1023v1"
    },
    {
        "title": "Controlling surface statistical properties using bias voltage: Atomic\n  force microscopy and stochastic analysis",
        "authors": [
            "P. Sangpour",
            "G. R. Jafari",
            "O. Akhavan",
            "A. Z. Moshfegh",
            "M. Reza Rahimi Tabar"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The effect of bias voltages on the statistical properties of rough surfaces\nhas been studied using atomic force microscopy technique and its stochastic\nanalysis. We have characterized the complexity of the height fluctuation of a\nrough surface by the stochastic parameters such as roughness exponent, level\ncrossing, and drift and diffusion coefficients as a function of the applied\nbias voltage. It is shown that these statistical as well as microstructural\nparameters can also explain the macroscopic property of a surface. Furthermore,\nthe tip convolution effect on the stochastic parameters has been examined.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.1029v1"
    },
    {
        "title": "Etched Glass Surfaces, Atomic Force Microscopy and Stochastic Analysis",
        "authors": [
            "G. R. Jafari",
            "M. Reza Rahimi Tabar",
            "A. Iraji zad",
            "G. Kavei"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The effect of etching time scale of glass surface on its statistical\nproperties has been studied using atomic force microscopy technique. We have\ncharacterized the complexity of the height fluctuation of a etched surface by\nthe stochastic parameters such as intermittency exponents, roughness, roughness\nexponents, drift and diffusion coefficients and find their variations in terms\nof the etching time.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.1030v1"
    },
    {
        "title": "A multivariate approach to heavy flavour tagging with cascade training",
        "authors": [
            "J. Bastos",
            "Y. Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  This paper compares the performance of artificial neural networks and boosted\ndecision trees, with and without cascade training, for tagging b-jets in a\ncollider experiment. It is shown, using a Monte Carlo simulation of $WH \\to\nl\\nu q\\bar{q}$ events, that for a b-tagging efficiency of 50%, the light jet\nrejection power given by boosted decision trees without cascade training is\nabout 55% higher than that given by artificial neural networks. The cascade\ntraining technique can improve the performance of boosted decision trees and\nartificial neural networks at this b-tagging efficiency level by about 35% and\n80% respectively. We conclude that the cascade trained boosted decision trees\nmethod is the most promising technique for tagging heavy flavours at collider\nexperiments.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.3706v2"
    },
    {
        "title": "Markov properties in presence of measurement noise",
        "authors": [
            "David Kleinhans",
            "Rudolf Friedrich",
            "Matthias Waechter",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Recently, several powerful tools for the reconstruction of stochastic\ndifferential equations from measured data sets have been proposed [e.g. Siegert\net al., Physics Letters A 243, 275 (1998); Hurn et al., Journal of Time Series\nAnalysis 24, 45 (2003)]. Efficient application of the methods, however,\ngenerally requires Markov properties to be fulfilled. This constraint typically\nseems to be violated on small scales, which frequently is attributed to\nphysical effects. On the other hand, measurement noise such as uncorrelated\nmeasurement and discretization errors has large impacts on the statistics of\nmeasurements on small scales. We demonstrate, that the presence of measurement\nnoise, likewise, spoils Markov properties of an underlying Markov processes.\nThis fact is promising for the further development of techniques for the\nreconstruction of stochastic processes from measured data, since limitations at\nsmall scales might stem from artificial noise sources rather than from\nintrinsic properties of the dynamics of the underlying process. Measurement\nnoise, however, can be controlled much better than the intrinsic dynamics of\nthe underlying process.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.1222v2"
    },
    {
        "title": "Improved estimation of Fokker-Planck equations through optimisation",
        "authors": [
            "A. P. Nawroth",
            "J. Peinke",
            "D. Kleinhans",
            "R. Friedrich"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  An improved method for the description of hierarchical complex systems by\nmeans of a Fokker-Planck equation is presented. In particular the\nlimited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm for constraint\nproblems (L-BFGS-B) is used to minimize the distance between the numerical\nsolutions of the Fokker-Planck equation and the empirical probability density\nfunctions and thus to estimate properly the drift and diffusion term of the\nFokker-Planck equation. The optimisation routine is applied to a time series of\nvelocity measurements obtained from a turbulent helium gas jet in order to\ndemonstrate the benefits and to quantify the improvements of this new\noptimisation routine.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.1292v1"
    },
    {
        "title": "Annotated Bibliography of Some Papers on Combining Significances or\n  p-values",
        "authors": [
            "Robert D. Cousins"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  A question that comes up repeatedly is how to combine the results of two\nexperiments if all that is known is that one experiment had a n-sigma effect\nand another experiment had a m-sigma effect. This question is not well-posed:\ndepending on what additional assumptions are made, the preferred answer is\ndifferent. The note lists some of the more prominent papers on the topic, with\nsome brief comments and excerpts.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.2209v2"
    },
    {
        "title": "Dirichlet or Potts ?",
        "authors": [
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  When modeling the distribution of a set of data by a mixture of Gaussians,\nthere are two possibilities: i) the classical one is using a set of parameters\nwhich are the proportions, the means and the variances; ii) the second is to\nconsider the proportions as the probabilities of a discrete valued hidden\nvariable. In the first case a usual prior distribution for the proportions is\nthe Dirichlet which accounts for the fact that they have to sum up to one. In\nthe second case, to each data is associated a hidden variable for which we\nconsider two possibilities: a) assuming those variables to be i.i.d. We show\nthen that this scheme is equivalent to the classical mixture model with\nDirichlet prior; b) assuming a Markovian structure. Then we choose the simplest\nmarkovian model which is the Potts distribution. As we will see this model is\nmore appropriate for the case where the data represents the pixels of an image\nfor which the hidden variables represent a segmentation of that image. The main\nobject of this paper is to give some details on these models and different\nalgorithms used for their simulation and the estimation of their parameters.\n  Key Words: Mixture of Gaussians, Dirichlet, Potts, Classification,\nSegmentation.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.2450v1"
    },
    {
        "title": "Non Gaussianity and Non Stationarity modeled through Hidden Variables\n  and their use in ICA and Blind Source Separation",
        "authors": [
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Modeling non Gaussian and non stationary signals and images has always been\none of the most important part of signal and image processing methods. In this\npaper, first we propose a few new models, all based on using hidden variables\nfor modeling either stationary but non Gaussian or Gaussian but non stationary\nor non Gaussian and non stationary signals and images. Then, we will see how to\nuse these models in independent component analysis (ICA) or blind source\nseparation (BSS). The computational aspects of the Bayesian estimation\nframework associated with these prior models are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.2454v1"
    },
    {
        "title": "Hierarchical Markovian models for hyperspectral image segmentation",
        "authors": [
            "Ali Mohammad-Djafari",
            "Adel Mohammadpoor",
            "Nadia Bali"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Hyperspectral images can be represented either as a set of images or as a set\nof spectra. Spectral classification and segmentation and data reduction are the\nmain problems in hyperspectral image analysis. In this paper we propose a\nBayesian estimation approach with an appropriate hiearchical model with hidden\nmarkovian variables which gives the possibility to jointly do data reduction,\nspectral classification and image segmentation. In the proposed model, the\ndesired independent components are piecewise homogeneous images which share the\nsame common hidden segmentation variable. Thus, the joint Bayesian estimation\nof this hidden variable as well as the sources and the mixing matrix of the\nsource separation problem gives a solution for all the three problems of\ndimensionality reduction, spectra classification and segmentation of\nhyperspectral images. A few simulation results illustrate the performances of\nthe proposed method compared to other classical methods usually used in\nhyperspectral image processing.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.2459v1"
    },
    {
        "title": "Bayesian Separation of Document Images with Hidden Markov Model",
        "authors": [
            "Feng Su",
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  this paper we consider the problem of separating noisy instantaneous linear\nmixtures of document images in the Bayesian framework. The source image is\nmodeled hierarchically by a latent labeling process representing the common\nclassifications of document objects among different color channels and the\nintensity process of pixels given the class labels. A Potts Markov random field\nis used to model regional regularity of the classification labels inside object\nregions. Local dependency between neighboring pixels can also be accounted by\nsmoothness constraint on their intensities. Within the Bayesian approach, all\nunknowns including the source, the classification, the mixing coefficients and\nthe distribution parameters of these variables are estimated from their\nposterior laws. The corresponding Bayesian computations are done by MCMC\nsampling algorithm. Results from experiments on synthetic and real image\nmixtures are presented to illustrate the performance of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.2461v1"
    },
    {
        "title": "Noise characteristics of 3D time-of-flight cameras",
        "authors": [
            "Dragos Falie",
            "Vasile Buzuloiu"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Time-of-flight (TOF) cameras are based on a new technology that delivers\ndistance maps by the use of a modulated light source. In this paper we first\ndescribe a set of experiments that we performed with TOF cameras. We then\npropose a noise model which is able to explain some of the phenomena observed\nin the experiments. The model is based on assuming a noise source that is\ncorrelated with the light source (shot noise) and an additional additive noise\nsource (dark current noise). The model predicts well the dependency of the\ndistance errors on the image intensity and the true distance at an individual\npixel.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.2673v1"
    },
    {
        "title": "Non extensive statistic of Tsallis in the heartbeat of healthy humans",
        "authors": [
            "P. A. Ritto"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  It is studied the MIT-BIH Normal Sinus Rhythm Database using a statistical\ntechnique of analysis, that is based on the Wavelet and Hilbert Transforms.\nWith that technique, it was previously found, that there is a collective and\nintrinsic dynamical behavior up to a scale of 64 heartbeats. Now it is shown,\nthat using the Biorthogonal wavelet bior3.1 such a behavior reaches the scale\n1024. That result confirms, that the circulatory system is out of equilibrium.\nAccording to the Statistical Mechanics of Tsallis, and a recent interpretation\nof G. Wilk et al. respect to the non extensive parameter q, the healthy human\nbeing is characterized by q=1.70+/-0.01.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.3817v5"
    },
    {
        "title": "Optimal estimate of probability density functions from experimental data",
        "authors": [
            "R. Labbé"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  A method providing optimal estimate of probability density functions (PDFs)\nfrom time series is proposed. It allows almost arbitrary resolution PDFs when\napplied to either, sampled analytic functions or digitized data from\nexperiments. When results are compared with PDFs of the same data calculated\nusing the standard histogram method, a remarkable improvement is observed,\nespecially in far lateral regions of the PDF, where low probability events give\npoor statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.4278v1"
    },
    {
        "title": "An Algorithm for the Continuous Morlet Wavelet Transform",
        "authors": [
            "Richard Buessow"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  This article consists of a brief discussion of the energy density over time\nor frequency that is obtained with the wavelet transform. Also an efficient\nalgorithm is suggested to calculate the continuous transform with the Morlet\nwavelet. The energy values of the Wavelet transform are compared with the power\nspectrum of the Fourier transform. Useful definitions for power spectra are\ngiven. The focus of the work is on simple measures to evaluate the transform\nwith the Morlet wavelet in an efficient way. The use of the transform and the\ndefined values is shown in some examples.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.0099v2"
    },
    {
        "title": "Luminosity determination at HERA-B",
        "authors": [
            "The HERA-B Collaboration"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  A detailed description of an original method used to measure the luminosity\naccumulated by the HERA-B experiment for a data sample taken during the\n2002-2003 HERA running period is reported. We show that, with this method, a\ntotal luminosity measurement can be achieved with a typical precision,\nincluding overall systematic uncertainties, at a level of 5% or better. We also\nreport evidence for the detection of delta-rays generated in the target and\ncomment on the possible use of such delta rays to measure luminosity.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.0131v1"
    },
    {
        "title": "A discriminative method to conduct exploratory analyses in networks",
        "authors": [
            "Segismundo S. Izquierdo",
            "Luis R. Izquierdo"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  This paper, which commented on Newman and Leicht's \"Mixture models and\nexploratory analysis in networks\" (2007, PNAS 104, 9564-9569), has been\nwithdrawn. The reason for this removal is that we misinterpreted the conceptual\nframework that the authors of the original paper use. Specifically, it is\nassumed in our paper that the variable theta_ri denotes the *a priori*\nprobability that there exists an edge from group r to vertex i. The correct\ninterpretation is that theta_ri denotes the probability that a given edge from\ngroup r connects to vertex i.\n  We are very grateful to Mark Newman and Elizabeth Leicht not only for\npointing out our misinterpretation, but also for doing it so politely and\ngracefully.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.1356v2"
    },
    {
        "title": "Statistics of football dynamics",
        "authors": [
            "R. S. Mendes",
            "L. C. Malacarne",
            "C. Anteneodo"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We investigate the dynamics of football matches. Our goal is to characterize\nstatistically the temporal sequence of ball movements in this collective sport\ngame, searching for traits of complex behavior. Data were collected over a\nvariety of matches in South American, European and World championships\nthroughout 2005 and 2006. We show that the statistics of ball touches presents\npower-law tails and can be described by $q$-gamma distributions. To explain\nsuch behavior we propose a model that provides information on the\ncharacteristics of football dynamics. Furthermore, we discuss the statistics of\nduration of out-of-play intervals, not directly related to the previous\nscenario.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.1758v1"
    },
    {
        "title": "Eigenvalue Decomposition as a Generalized Synchronization Cluster\n  Analysis",
        "authors": [
            "Carsten Allefeld",
            "Markus Müller",
            "Jürgen Kurths"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Motivated by the recent demonstration of its use as a tool for the detection\nand characterization of phase-shape correlations in multivariate time series,\nwe show that eigenvalue decomposition can also be applied to a matrix of\nindices of bivariate phase synchronization strength. The resulting method is\nable to identify clusters of synchronized oscillators, and to quantify their\nstrength as well as the degree of involvement of an oscillator in a cluster.\nSince for the case of a single cluster the method gives similar results as our\nprevious approach, it can be seen as a generalized Synchronization Cluster\nAnalysis, extending its field of application to more complex situations. The\nperformance of the method is tested by applying it to simulation data.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.3375v3"
    },
    {
        "title": "Bayesian Learning of Neural Networks for Signal/Background\n  Discrimination in Particle Physics",
        "authors": [
            "Michael Pogwizd",
            "Laura Jane Elgass",
            "Pushpalatha C. Bhat"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Neural networks are used extensively in classification problems in particle\nphysics research. Since the training of neural networks can be viewed as a\nproblem of inference, Bayesian learning of neural networks can provide more\noptimal and robust results than conventional learning methods. We have\ninvestigated the use of Bayesian neural networks for signal/background\ndiscrimination in the search for second generation leptoquarks at the Tevatron,\nas an example. We present a comparison of the results obtained from the\nconventional training of feedforward neural networks and networks trained with\nBayesian methods.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.0930v1"
    },
    {
        "title": "Detecting synchronization clusters in multivariate time series via\n  coarse-graining of Markov chains",
        "authors": [
            "Carsten Allefeld",
            "Stephan Bialonski"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Synchronization cluster analysis is an approach to the detection of\nunderlying structures in data sets of multivariate time series, starting from a\nmatrix R of bivariate synchronization indices. A previous method utilized the\neigenvectors of R for cluster identification, analogous to several recent\nattempts at group identification using eigenvectors of the correlation matrix.\nAll of these approaches assumed a one-to-one correspondence of dominant\neigenvectors and clusters, which has however been shown to be wrong in\nimportant cases. We clarify the usefulness of eigenvalue decomposition for\nsynchronization cluster analysis by translating the problem into the language\nof stochastic processes, and derive an enhanced clustering method harnessing\nrecent insights from the coarse-graining of finite-state Markov processes. We\nillustrate the operation of our method using a simulated system of coupled\nLorenz oscillators, and we demonstrate its superior performance over the\nprevious approach. Finally we investigate the question of robustness of the\nalgorithm against small sample size, which is important with regard to field\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.2479v4"
    },
    {
        "title": "Non-independent continuous time random walks",
        "authors": [
            "Miquel Montero",
            "Jaume Masoliver"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The usual development of the continuous time random walk (CTRW) assumes that\njumps and time intervals are a two-dimensional set of independent and\nidentically distributed random variables. In this paper we address the\ntheoretical setting of non-independent CTRW's where consecutive jumps and/or\ntime intervals are correlated. An exact solution to the problem is obtained for\nthe special but relevant case in which the correlation solely depends on the\nsigns of consecutive jumps. Even in this simple case some interesting features\narise such as transitions from unimodal to bimodal distributions due to\ncorrelation. We also develop the necessary analytical techniques and\napproximations to handle more general situations that can appear in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.4087v2"
    },
    {
        "title": "Blind background prediction using a bifurcated analysis scheme",
        "authors": [
            "J. Nix",
            "J. Ma",
            "G. N. Perdue",
            "Y. W. Wah"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  A technique for background prediction using data, but maintaining a closed\nsignal box is described. The result is extended to two background sources.\nConditions on the applicability under correlated cuts are described. This\ntechnique is applied to both a toy model and an analysis of data from a rare\nneutral kaon decay experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.0318v1"
    },
    {
        "title": "On the estimation of a parameter with incomplete knowledge on a nuisance\n  parameter",
        "authors": [
            "Ali Mohammad-Djafari",
            "Adel Mohammadpour"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  In this paper we consider the problem of estimating a parameter of a\nprobability distribution when we have some prior information on a nuisance\nparameter. We start by the very simple case where we know perfectly the value\nof the nuisance parameter. The complete likelihood is the classical tool in\nthis case. Then, progressively, we consider the case where we are given a prior\nprobability distribution on this nuisance parameter. The marginal likelihood is\nthen the classical tool in this case. Then, we consider the case where we only\nhave a fixed number of its moments. Here, we may use the maximum entropy (ME)\nprinciple to assign a prior law and thus go back to the previous case. Finally,\nwe consider the case where we know only its median. In our knowledge, there is\nnot any classical tool for this case. We propose then a new tool for this case\nbased on a recently proposed alternative distribution to the marginal\nprobability distribution. This new criterion is obtained by first remarking\nthat the marginal distribution can be considered as the mean value of the\noriginal distribution over the prior probability law of the nuisance parameter,\nand then, by using the median in place of the mean. In this paper, we first\nsummarize the classical tools used for the three first cases, then we give the\nprecise definition of this new criterion and its properties and, finally,\npresent a few examples to show the differences of these cases.\n  Key Words: Nuisance parameter, Bayesian inference, Maximum Entropy,\nMarginalization, Incomplete knowledge, Mean and Median of the Likelihood over\nthe prior distribution\n",
        "pdf_link": "http://arxiv.org/pdf/0708.2999v1"
    },
    {
        "title": "Bayesian segmentation of hyperspectral images",
        "authors": [
            "Adel Mohammadpour",
            "Olivier Féron",
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  In this paper we consider the problem of joint segmentation of hyperspectral\nimages in the Bayesian framework. The proposed approach is based on a Hidden\nMarkov Modeling (HMM) of the images with common segmentation, or equivalently\nwith common hidden classification label variables which is modeled by a Potts\nMarkov Random Field. We introduce an appropriate Markov Chain Monte Carlo\n(MCMC) algorithm to implement the method and show some simulation results.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.3013v1"
    },
    {
        "title": "A Multivariate Training Technique with Event Reweighting",
        "authors": [
            "Hai-Jun Yang",
            "Tiesheng Dai",
            "Alan Wilson",
            "Zhengguo Zhao",
            "Bing Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  An event reweighting technique incorporated in multivariate training\nalgorithm has been developed and tested using the Artificial Neural Networks\n(ANN) and Boosted Decision Trees (BDT). The event reweighting training are\ncompared to that of the conventional equal event weighting based on the ANN and\nthe BDT performance. The comparison is performed in the context of the physics\nanalysis of the ATLAS experiment at the Large Hadron Collider (LHC), which will\nexplore the fundamental nature of matter and the basic forces that shape our\nuniverse. We demonstrate that the event reweighting technique provides an\nunbiased method of multivariate training for event pattern recognition.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.3635v4"
    },
    {
        "title": "Improving the consensus performance via predictive mechanisms",
        "authors": [
            "Hai-Tao Zhang",
            "Guy-Bart Stan",
            "Michael ZhiQiang Chen",
            "Jan M. Maciejowski",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Considering some predictive mechanisms, we show that ultrafast\naverage-consensus can be achieved in networks of interconnected agents. More\nspecifically, by predicting the dynamics of the network several steps ahead and\nusing this information in the design of the consensus protocol of each agent,\ndrastic improvements can be achieved in terms of the speed of consensus\nconvergence, without changing the topology of the network. Moreover, using\nthese predictive mechanisms, the range of sampling periods leading to consensus\nconvergence is greatly expanded compared with the routine consensus protocol.\nThis study provides a mathematical basis for the idea that some predictive\nmechanisms exist in widely-spread biological swarms, flocks, and networks. From\nthe industrial engineering point of view, inclusion of an efficient predictive\nmechanism allows for a significant increase in the speed of consensus\nconvergence and also a reduction of the communication energy required to\nachieve a predefined consensus performance.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.0172v2"
    },
    {
        "title": "Predictive protocol of flocks with small-world connection pattern",
        "authors": [
            "Hai-Tao Zhang",
            "Michael Z. Q. Chen",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  By introducing a predictive mechanism with small-world connections, we\npropose a new motion protocol for self-driven flocks. The small-world\nconnections are implemented by randomly adding long-range interactions from the\nleader to a few distant agents, namely pseudo-leaders. The leader can directly\naffect the pseudo-leaders, thereby influencing all the other agents through\nthem efficiently. Moreover, these pseudo-leaders are able to predict the\nleader's motion several steps ahead and use this information in decision making\ntowards coherent flocking with more stable formation. It is shown that drastic\nimprovement can be achieved in terms of both the consensus performance and the\ncommunication cost. From the industrial engineering point of view, the current\nprotocol allows for a significant improvement in the cohesion and rigidity of\nthe formation at a fairly low cost of adding a few long-range links embedded\nwith predictive capabilities. Significantly, this work uncovers an important\nfeature of flocks that predictive capability and long-range links can\ncompensate for the insufficiency of each other. These conclusions are valid for\nboth the attractive/repulsive swarm model and the Vicsek model.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.0380v3"
    },
    {
        "title": "Wrong Priors",
        "authors": [
            "Carlos C. Rodriguez"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  All priors are not created equal. There are right and there are wrong priors.\nThat is the main conclusion of this contribution. I use, a cooked-up example\ndesigned to create drama, and a typical textbook example to show the\npervasiveness of wrong priors in standard statistical practice.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.1067v1"
    },
    {
        "title": "Maximum Entropy, Time Series and Statistical Inference",
        "authors": [
            "Robert Kariotis"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  A brief discussion is given of the traditional version of the Maximum Entropy\nMethod, including a review of some of the criticism that has been made in\nregard to its use in statistical inference. Motivated by these questions, a\nmodified version of the method is proposed and applied to a simple problem,\ndemonstrating its use in inference.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.1504v1"
    },
    {
        "title": "Street-based Topological Representations and Analyses for Predicting\n  Traffic Flow in GIS",
        "authors": [
            "Bin Jiang",
            "Chengke Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  It is well received in the space syntax community that traffic flow is\nsignificantly correlated to a morphological property of streets, which are\nrepresented by axial lines, forming a so called axial map. The correlation\nco-efficient (R square value) approaches 0.8 and even a higher value according\nto the space syntax literature. In this paper, we study the same issue using\nthe Hong Kong street network and the Hong Kong Annual Average Daily Traffic\n(AADT) datasets, and find surprisingly that street-based topological\nrepresentations (or street-street topologies) tend to be better representations\nthan the axial map. In other words, vehicle flow is correlated to a\nmorphological property of streets better than that of axial lines. Based on the\nfinding, we suggest the street-based topological representations as an\nalternative GIS representation, and the topological analyses as a new\nanalytical means for geographic knowledge discovery.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.1981v1"
    },
    {
        "title": "Electron/Photon identification in ATLAS and CMS",
        "authors": [
            "C. Charlot"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Recent studies in ATLAS and CMS experiments for the reconstruction and\nidentification of electrons and photons using full Monte Carlo and testbeam\ndata are reported\n",
        "pdf_link": "http://arxiv.org/pdf/0709.2479v1"
    },
    {
        "title": "Order 1 autoregressive process of finite length",
        "authors": [
            "Calin Vamos",
            "Stefan M. Soltuz",
            "Maria Craciun"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The stochastic processes of finite length defined by recurrence relations\nrequest additional relations specifying the first terms of the process\nanalogously to the initial conditions for the differential equations. As a\ngeneral rule, in time series theory one analyzes only stochastic processes of\ninfinite length which need no such initial conditions and their properties are\nless difficult to be determined. In this paper we compare the properties of the\norder 1 autoregressive processes of finite and infinite length and we prove\nthat the time series length has an important influence mainly if the serial\ncorrelation is significant. These different properties can manifest themselves\nas transient effects produced when a time series is numerically generated. We\nshow that for an order 1 autoregressive process the transient behavior can be\navoided if the first term is a Gaussian random variable with standard deviation\nequal to that of the theoretical infinite process and not to that of the white\nnoise innovation.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.2963v1"
    },
    {
        "title": "Classification of interest rate curves using Self-Organising Maps",
        "authors": [
            "M. Kanevski",
            "M. Maignan",
            "V. Timonin",
            "A. Pozdnoukhov"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The present study deals with the analysis and classification of interest rate\ncurves. Interest rate curves (IRC) are the basic financial curves in many\ndifferent fields of economics and finance. They are extremely important tools\nin banking and financial risk management problems. Interest rates depend on\ntime and maturity which defines term structure of the interest rate curves. IRC\nare composed of interest rates at different maturities (usually fixed number)\nwhich move coherently in time. In the present study machine learning\nalgorithms, namely Self-Organising maps - SOM (Kohonen maps), are used to find\nclusters and to classify Swiss franc (CHF) interest rate curves.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.4401v1"
    },
    {
        "title": "Wavelet transform modulus maxima based fractal correlation analysis",
        "authors": [
            "D. C. Lin",
            "A. Sharif"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The wavelet transform modulus maxima (WTMM) used in the singularity analysis\nof one fractal function is extended to study the fractal correlation of two\nmultifractal functions. The technique is developed in the framework of joint\npartition function analysis (JPFA) proposed by Meneveau et al. [1] and is shown\nto be equally effective. In addition, we show that another leading approach\ndeveloped for the same purpose, namely, relative multifractal analysis, can be\nconsidered as a special case of JPFA at a particular parameter setting.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.4612v1"
    },
    {
        "title": "Statistical substantiation of introduction of the distributions\n  containing lifetime as thermodynamic parameter",
        "authors": [
            "V. V. Ryazanov"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  By means of an inequality of the information and parametrization of family of\ndistributions of the probabilities, supposing an effective estimation,\nintroduction of the distributions containing time of the first achievement of a\nlevel as internal thermodynamic parameter ground.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.2449v1"
    },
    {
        "title": "Modularity-Maximizing Network Communities via Mathematical Programming",
        "authors": [
            "Gaurav Agarwal",
            "David Kempe"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  In many networks, it is of great interest to identify \"communities\",\nunusually densely knit groups of individuals. Such communities often shed light\non the function of the networks or underlying properties of the individuals.\nRecently, Newman suggested \"modularity\" as a natural measure of the quality of\na network partitioning into communities. Since then, various algorithms have\nbeen proposed for (approximately) maximizing the modularity of the partitioning\ndetermined.\n  In this paper, we introduce the technique of rounding mathematical programs\nto the problem of modularity maximization, presenting two novel algorithms.\nMore specifically, the algorithms round solutions to linear and vector\nprograms. Importantly, the linear programing algorithm comes with an a\nposteriori approximation guarantee: by comparing the solution quality to the\nfractional solution of the linear program, a bound on the available \"room for\nimprovement\" can be obtained. The vector programming algorithm provides a\nsimilar bound for the best partition into two communities. We evaluate both\nalgorithms using experiments on several standard test cases for network\npartitioning algorithms, and find that they perform comparably or better than\npast algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.2533v3"
    },
    {
        "title": "On a Connection between Entropy, Extensive Measurement and Memoryless\n  Characterization",
        "authors": [
            "Peter Sunehag"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We define an entropy based on a chosen governing probability distribution. If\na certain kind of measurements follow such a distribution it also gives us a\nsuitable scale to study it with. This scale will appear as a link function that\nis applied to the measurements. A link function can also be used to define an\nalternative structure on a set. We will see that generalized entropies are\nequivalent to using a different scale for the phenomenon that is studied\ncompared to the scale the measurements arrive on. An extensive measurement\nscale is here a scale for which measurements fulfill a memoryless property. We\nconclude that the alternative algebraic structure defined by the link function\nmust be used if we continue to work on the original scale. We derive Tsallis\nentropy by using a generalized log-logistic governing distribution. Typical\napplications of Tsallis entropy are related to phenomena with power-law\nbehaviour.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4179v1"
    },
    {
        "title": "Generalization of Eight Methods for Determining R in the Ideal Gas Law",
        "authors": [
            "Donald B. Macnaughton"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The ideal gas law of physics and chemistry says that PV = nRT. This law is a\nstatement of the relationship between four variables (P, V, n, and T) that\nreflect properties of a quantity of gas in a container. The law enables us to\nmake accurate predictions of the value of any one of the four variables from\nthe values of the other three. The symbol R (called the \"molar gas constant\")\nis the sole parameter or constant of the law. R stands for a fixed number that\nhas been shown through experiments to equal approximately 8.314472. Eight\nmethods are available to analyze the data from a relevant experiment to\ndetermine the value of R. These methods are specific instances of eight general\nmethods that scientists use to determine the value(s) of the parameter(s) of a\nmodel equation of a relationship between variables. Parameter estimation is one\nstep in the study of a relationship between variables.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.0740v5"
    },
    {
        "title": "Identification of photon-tagged jets in the ALICE experiment",
        "authors": [
            "G. Conesa",
            "H. Delagrange",
            "J. Diaz",
            "Y. V. Kharlov",
            "Y. Schutz"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The ALICE experiment at LHC will detect and identify prompt photons and light\nneutral-mesons with the PHOS detector and the additional EMCal electromagnetic\ncalorimeter. Charged particles will be detected and identified by the central\ntracking system. In this article, the possibility of studying the interaction\nof jets with the nuclear medium, using prompt photons as a tool to tag jets, is\ninvestigated by simulations. New methods to identify prompt photon-jet events\nand to distinguish them from the jet-jet background are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.2431v1"
    },
    {
        "title": "Prior-predictive value from fast growth simulations",
        "authors": [
            "Holger Ahlers",
            "Andreas Engel"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Building on a variant of the Jarzynski equation we propose a new method to\nnumerically determine the prior-predictive value in a Bayesian inference\nproblem. The method generalizes thermodynamic integration and is not hampered\nby equilibration problems. We demonstrate its operation by applying it to two\nsimple examples and elucidate its performance. In the case of multi-modal\nposterior distributions the performance is superior to thermodynamic\nintegration.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.3630v1"
    },
    {
        "title": "Uncovering latent singularities from multifractal scaling laws in mixed\n  asymptotic regime. Application to turbulence",
        "authors": [
            "J. F. Muzy",
            "E. Bacry",
            "R. Baile",
            "P. Poggi"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  In this paper we revisit an idea originally proposed by Mandelbrot about the\npossibility to observe ``negative dimensions'' in random multifractals. For\nthat purpose, we define a new way to study scaling where the observation scale\n$\\tau$ and the total sample length $L$ are respectively going to zero and to\ninfinity. This ``mixed'' asymptotic regime is parametrized by an exponent\n$\\chi$ that corresponds to Mandelbrot ``supersampling exponent''. In order to\nstudy the scaling exponents in the mixed regime, we use a formalism introduced\nin the context of the physics of disordered systems relying upon traveling wave\nsolutions of some non-linear iteration equation. Within our approach, we show\nthat for random multiplicative cascade models, the parameter $\\chi$ can be\ninterpreted as a negative dimension and, as anticipated by Mandelbrot, allows\none to uncover the ``hidden'' negative part of the singularity spectrum,\ncorresponding to ``latent'' singularities. We illustrate our purpose on\nsynthetic cascade models. When applied to turbulence data, this formalism\nallows us to distinguish two popular phenomenological models of dissipation\nintermittency: We show that the mixed scaling exponents agree with a log-normal\nmodel and not with log-Poisson statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.4862v1"
    },
    {
        "title": "Analysis based on the Wavelet & Hilbert transforms applied to the full\n  time series of interbeats, for a triad of failures at the heart",
        "authors": [
            "P. A. Ritto"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  A tetra of sets which elements are time series of interbeats has been\nobtained from the databank Physionet-MIT-BIH, corresponding to the following\nfailures at the humans' heart: Obstructive Sleep Apnea, Congestive Heart\nFailure, and Atrial Fibrillation. Those times series has been analyzed\nstatistically using an already known technique based on the Wavelet and Hilbert\nTransforms. That technique has been applied to the time series of interbeats\nfor 87 patients, in order to find out the dynamics of the heart. The size of\nthe times series varies around 7 to 24 h. while the kind of wavelet selected\nfor this study has been any one of: Daubechies, Biortoghonal, and Gaussian. The\nanalysis has been done for the complet set of scales ranging from: 1-128\nheartbeats. Choosing the Biorthogonal wavelet: bior3.1, it is observed: (a)\nThat the time series hasn't to be cutted in shorter periods, with the purpose\nto obtain the collapsing of the data, (b) An analytical, universal behavior of\nthe data, for the first and second diseases, but not for the third.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.1001v4"
    },
    {
        "title": "Symmetry Breaking Study with Random Matrix Ensembles",
        "authors": [
            "M. S. Hussein",
            "J. X. de Carvalho",
            "M. P. Pato",
            "A. J. Sargeant"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  A random matrix model to describe the coupling of $m$-fold symmetry is\nconstructed. The particular threefold case is used to analyze data on\neigenfrequencies of elastomechanical vibration of an anisotropic quartz block.\nIt is suggested that such experimental/theoretical study may supply a powerful\nmeans to discern intrinsic symmetry of physical systems.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.3214v1"
    },
    {
        "title": "Mona Lisa, the stochastic view and fractality in color space",
        "authors": [
            "P. Pedram",
            "G. R. Jafari"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  A painting consists of objects which are arranged in specific ways. The art\nof painting is drawing the objects, which can be considered as known trends, in\nan expressive manner. Detrended methods are suitable for characterizing the\nartistic works of the painter by eliminating trends. It means that we study the\npaintings, regardless of its apparent purpose, as a stochastic process. We\napply multifractal detrended fluctuation analysis to characterize the\nstatistical properties of Mona Lisa, as an instance, to exhibit the fractality\nof the painting. Our results show that Mona Lisa is long range correlated and\nalmost behaves similar in various scales.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.2501v1"
    },
    {
        "title": "When are Extreme Events the better predictable, the larger they are?",
        "authors": [
            "S. Hallerberg",
            "H. Kantz"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We investigate the predictability of extreme events in time series. The focus\nof this work is to understand under which circumstances large events are better\npredictable than smaller events. Therefore we use a simple prediction algorithm\nbased on precursory structures which are identified using the maximum\nlikelihood principle. Using the receiver operator characteristic curve as a\nmeasure for the quality of predictions we find that the dependence on the event\nmagnitude is closely linked to the probability distribution function of the\nunderlying stochastic process. We evaluate this dependence on the probability\ndistribution function analytically and numerically. If we assume that the\noptimal precursory structures are used to make the predictions, we find that\nlarge increments are better predictable if the underlying stochastic process\nhas a Gaussian probability distribution function, whereas larger increments are\nharder to predict if the underlying probability distribution function has a\npower law tail. In the case of an exponential distribution function we find no\nsignificant dependence on the event magnitude. Furthermore we compare these\nresults with predictions of increments in correlated data, namely, velocity\nincrements of a free jet flow. The velocity increments in the free jet flow are\nin dependence on the time scale either asymptotically Gaussian or\nasymptotically exponential distributed. The numerical results for predictions\nwithin free jet data are in good agreement with the previous analytical\nconsiderations for random numbers.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.4525v1"
    },
    {
        "title": "Exact mean first-passage time on the T-graph",
        "authors": [
            "E. Agliari"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We consider a simple random walk on the T-fractal and we calculate the exact\nmean time $\\tau^g$ to first reach the central node $i_0$. The mean is performed\nover the set of possible walks from a given origin and over the set of starting\npoints uniformly distributed throughout the sites of the graph, except $i_0$.\nBy means of analytic techniques based on decimation procedures, we find the\nexplicit expression for $\\tau^g$ as a function of the generation $g$ and of the\nvolume $V$ of the underlying fractal. Our results agree with the asymptotic\nones already known for diffusion on the T-fractal and, more generally, they are\nconsistent with the standard laws describing diffusion on low-dimensional\nstructures.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.2248v1"
    },
    {
        "title": "Modeling Spacing Distribution of Queuing Vehicles in Front of a\n  Signalized Junction Using Random-Matrix Theory",
        "authors": [
            "Xuexiang Jin",
            "Yuelong Su",
            "Yi Zhang",
            "Li Li"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Modeling of headway/spacing between two consecutive vehicles has many\napplications in traffic flow theory and transport practice. Most known\napproaches only study the vehicles running on freeways. In this paper, we\npropose a model to explain the spacing distribution of queuing vehicles in\nfront of a signalized junction based on random-matrix theory. We show that the\nrecently measured spacing distribution data well fit the spacing distribution\nof a Gaussian symplectic ensemble (GSE). These results are also compared with\nthe spacing distribution observed for car parking problem. Why\nvehicle-stationary-queuing and vehicle-parking have different spacing\ndistributions (GSE vs GUE) seems to lie in the difference of driving patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.2619v1"
    },
    {
        "title": "A Markov Process Inspired Cellular Automata Model of Road Traffic",
        "authors": [
            "Fa Wang",
            "Li Li",
            "Jianming Hu",
            "Yan Ji",
            "Danya Yao",
            "Yi Zhang",
            "Xuexiang Jin",
            "Yuelong Su",
            "Zheng Wei"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  To provide a more accurate description of the driving behaviors in vehicle\nqueues, a namely Markov-Gap cellular automata model is proposed in this paper.\nIt views the variation of the gap between two consequent vehicles as a Markov\nprocess whose stationary distribution corresponds to the observed distribution\nof practical gaps. The multiformity of this Markov process provides the model\nenough flexibility to describe various driving behaviors. Two examples are\ngiven to show how to specialize it for different scenarios: usually mentioned\nflows on freeways and start-up flows at signalized intersections. The agreement\nbetween the empirical observations and the simulation results suggests the\nsoundness of this new approach.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.2624v2"
    },
    {
        "title": "On approximating the distributions of goodness-of-fit test statistics\n  based on the empirical distribution function: The case of unknown parameters",
        "authors": [
            "Marco Capasso",
            "Lucia Alessi",
            "Matteo Barigozzi",
            "Giorgio Fagiolo"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  This paper discusses some problems possibly arising when approximating via\nMonte-Carlo simulations the distributions of goodness-of-fit test statistics\nbased on the empirical distribution function. We argue that failing to\nre-estimate unknown parameters on each simulated Monte-Carlo sample -- and thus\navoiding to employ this information to build the test statistic -- may lead to\nwrong, overly-conservative testing. Furthermore, we present a simple example\nsuggesting that the impact of this possible mistake may turn out to be dramatic\nand does not vanish as the sample size increases.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.4322v1"
    },
    {
        "title": "Fractal and Multifractal Time Series",
        "authors": [
            "Jan W. Kantelhardt"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Data series generated by complex systems exhibit fluctuations on many time\nscales and/or broad distributions of the values. In both equilibrium and\nnon-equilibrium situations, the natural fluctuations are often found to follow\na scaling relation over several orders of magnitude, allowing for a\ncharacterisation of the data and the generating complex system by fractal (or\nmultifractal) scaling exponents. In addition, fractal and multifractal\napproaches can be used for modelling time series and deriving predictions\nregarding extreme events. This review article describes and exemplifies several\nmethods originating from Statistical Physics and Applied Mathematics, which\nhave been used for fractal and multifractal time series analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.0747v1"
    },
    {
        "title": "Bivariate phase-rectified signal averaging",
        "authors": [
            "Aicko Y. Schumann",
            "Jan W. Kantelhardt",
            "Axel Bauer",
            "Georg Schmidt"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Phase-Rectified Signal Averaging (PRSA) was shown to be a powerful tool for\nthe study of quasi-periodic oscillations and nonlinear effects in\nnon-stationary signals. Here we present a bivariate PRSA technique for the\nstudy of the inter-relationship between two simultaneous data recordings. Its\nperformance is compared with traditional cross-correlation analysis, which,\nhowever, does not work well for non-stationary data and cannot distinguish the\ncoupling directions in complex nonlinear situations. We show that bivariate\nPRSA allows the analysis of events in one signal at times where the other\nsignal is in a certain phase or state; it is stable in the presence of noise\nand impassible to non-stationarities.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.2798v1"
    },
    {
        "title": "Frequentist Coverage Properties of Uncertainty Intervals for Weak\n  Poisson Signals in the Presence of Background",
        "authors": [
            "K. J. Coakley",
            "J. D. Splett",
            "D. S. Simons"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We construct uncertainty intervals for weak Poisson signals in the presence\nof background. We consider the case where a primary experiment yields a\nrealization of the signal plus background, and a second experiment yields a\nrealization of the background. The data acquisitions times for the\nbackground-only experiment,T_bg, and the primary experiment,T, are selected so\nthat their ratio varies from 1 to 25. The expected number of background counts\nin the primary experiment varies from 0.2 to 2. We construct 90 and 95 percent\nconfidence intervals based on a propagation-of-errors method as well as two\nimplementations of a Neyman procedure where acceptance regions are constructed\nbased on a likelihood-ratio criterion that automatically determines whether the\nresulting confidence interval is one-sided or two-sided. The first Neyman\nprocedure (due to Feldman and Cousins) neglects uncertainty in the background.\nIn the other Neyman procedure, we account for uncertainty in the background\nwith a parametric bootstrap method. We also construct minimum length Bayesian\ncredibility intervals. For each method, we test for the presence of a signal\nbased on the value of the lower endpoint of the uncertainty interval. When\nT_bg/T is 5 or more and the expected background is 2 or less, the Feldman\nCousins method outperforms the other methods considered.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.4032v2"
    },
    {
        "title": "Characterizing Synchronization in Time Series using Information Measures\n  Extracted from Symbolic Representations",
        "authors": [
            "Roberto Monetti",
            "Wolfram Bunk",
            "Ferdinand Jamitzky"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We present a methodology to characterize synchronization in time series based\non symbolic representations. A symbol is linked to a sequence of numbers\nthrough the rank-order of its values. A representation of a time series results\nafter mapping all sequences into symbols. We propose a transcription scheme\nbetween symbolic representations to study the dynamics of coupled systems. This\nscheme allows us to use elements of group theory and to derive information\nmeasures to assess the degree of synchronization. We apply our method to a\nprototype non-linear system which displays a rich coupled dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.4634v1"
    },
    {
        "title": "Optimal Estimation of Several Linear Parameters in the Presence of\n  Lorentzian Thermal Noise",
        "authors": [
            "Jason H. Steffen",
            "Michael W. Moore",
            "Paul E. Boynton"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In a previous article we developed an approach to the optimal (minimum\nvariance, unbiased) statistical estimation technique for the equilibrium\ndisplacement of a damped, harmonic oscillator in the presence of thermal noise.\nHere, we expand that work to include the optimal estimation of several linear\nparameters from a continuous time series. We show that working in the basis of\nthe thermal driving force both simplifies the calculations and provides\nadditional insight to why various approximate (not optimal) estimation\ntechniques perform as they do. To illustrate this point, we compare the\nvariance in the optimal estimator that we derive for thermal noise with those\nof two approximate methods which, like the optimal estimator, suppress the\ncontribution to the variance that would come from the irrelevant, resonant\nmotion of the oscillator. We discuss how these methods fare when the dominant\nnoise process is either white displacement noise or noise with power spectral\ndensity that is inversely proportional to the frequency ($1/f$ noise). We also\nconstruct, in the basis of the driving force, an estimator that performs well\nfor a mixture of white noise and thermal noise. To find the optimal\nmulti-parameter estimators for thermal noise, we derive and illustrate a\ngeneralization of traditional matrix methods for parameter estimation that can\naccommodate continuous data. We discuss how this approach may help refine the\ndesign of experiments as they allow an exact, quantitative comparison of the\nprecision of estimated parameters under various data acquisition and data\nanalysis strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.0027v3"
    },
    {
        "title": "Accurate estimator of correlations between asynchronous signals",
        "authors": [
            "Bence Toth",
            "Janos Kertesz"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The estimation of the correlation between time series is often hampered by\nthe asynchronicity of the signals. Cumulating data within a time window\nsuppresses this source of noise but weakens the statistics. We present a method\nto estimate correlations without applying long time windows. We decompose the\ncorrelations of data cumulated over a long window using decay of lagged\ncorrelations as calculated from short window data. This increases the accuracy\nof the estimated correlation significantly and decreases the necessary efforts\nof calculations both in real and computer experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.2310v3"
    },
    {
        "title": "Bayesian approach to clustering real value, categorical and network\n  data: solution via variational methods",
        "authors": [
            "Alexei Vazquez"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Data clustering, including problems such as finding network communities, can\nbe put into a systematic framework by means of a Bayesian approach. The\napplication of Bayesian approaches to real problems can be, however, quite\nchallenging. In most cases the solution is explored via Monte Carlo sampling or\nvariational methods. Here we work further on the application of variational\nmethods to clustering problems. We introduce generative models based on a\nhidden group structure and prior distributions. We extend previous attends by\nJaynes, and derive the prior distributions based on symmetry arguments. As a\ncase study we address the problems of two-sides clustering real value data and\nclustering data represented by a hypergraph or bipartite graph. From the\nvariational calculations, and depending on the starting statistical model for\nthe data, we derive a variational Bayes algorithm, a generalized version of the\nexpectation maximization algorithm with a built in penalization for model\ncomplexity or bias. We demonstrate the good performance of the variational\nBayes algorithm using test examples.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.2689v3"
    },
    {
        "title": "Fluctuations and Pseudo Long Range Dependence in Network Flows: A\n  Non-Stationary Poisson Process Model",
        "authors": [
            "Yudong Chen",
            "Li Li",
            "Yi Zhang",
            "Jianming Hu"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In the study of complex networks (systems), the scaling phenomenon of flow\nfluctuations refers to a certain power-law between the mean flux (activity)\n$<F_i>$ of the $i$th node and its variance $\\sigma_i$ as $\\sigma_i \\propto <\nF_{i} > ^{\\alpha}$. Such scaling laws are found to be prevalent both in natural\nand man-made network systems, but our understanding of their origins still\nremains limited. In this paper, a non-stationary Poisson process model is\nproposed to give an analytical explanation of the non-universal scaling\nphenomenon: the exponent $\\alpha$ varies between 1/2 and 1 depending on the\nsize of sampling time window and the relative strength of the external/internal\ndriven forces of the systems. The crossover behavior and the relation of\nfluctuation scaling with pseudo long range dependence are also accounted for by\nthe model. Numerical experiments show that the proposed model can recover the\nmulti-scaling phenomenon.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.0206v2"
    },
    {
        "title": "Averaging Results with Theoretical Uncertainties",
        "authors": [
            "F. C. Porter"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Combining measurements which have \"theoretical uncertainties\" is a delicate\nmatter, due to an unclear statistical basis. We present an algorithm based on\nthe notion that a theoretical uncertainty represents an estimate of bias.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.0530v2"
    },
    {
        "title": "Maximise global gain in the minority game",
        "authors": [
            "Sy-Sang Liaw"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We find a simple, partially altruistic mechanism that can increase global\ngain for a community of selfish agents. The mechanism is implied in the\nphenomena found in the minority game. We apply the mechanism to a two-road\ntraffic system to maximise traffic flow.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.0698v1"
    },
    {
        "title": "Differential fast fixed-point algorithms for underdetermined\n  instantaneous and convolutive partial blind source separation",
        "authors": [
            "J. Thomas",
            "Y. Deville",
            "Shahram Hosseini"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  This paper concerns underdetermined linear instantaneous and convolutive\nblind source separation (BSS), i.e., the case when the number of observed mixed\nsignals is lower than the number of sources.We propose partial BSS methods,\nwhich separate supposedly nonstationary sources of interest (while keeping\nresidual components for the other, supposedly stationary, \"noise\" sources).\nThese methods are based on the general differential BSS concept that we\nintroduced before. In the instantaneous case, the approach proposed in this\npaper consists of a differential extension of the FastICA method (which does\nnot apply to underdetermined mixtures). In the convolutive case, we extend our\nrecent time-domain fast fixed-point C-FICA algorithm to underdetermined\nmixtures. Both proposed approaches thus keep the attractive features of the\nFastICA and C-FICA methods. Our approaches are based on differential sphering\nprocesses, followed by the optimization of the differential nonnormalized\nkurtosis that we introduce in this paper. Experimental tests show that these\ndifferential algorithms are much more robust to noise sources than the standard\nFastICA and C-FICA algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.3195v1"
    },
    {
        "title": "Universal, Continuous-Discrete Nonlinear Yau Filtering I: Affine, Linear\n  State Model with State-Independent Diffusion Matrix",
        "authors": [
            "Bhashyam Balaji"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The continuous-discrete filtering problem requires the solution of a partial\ndifferential equation known as the Fokker-Planck-Kolmogorov forward equation\n(FPKfe). In this paper, it is pointed out that for a state model with an\naffine, linear drift and state-independent diffusion matrix the fundamental\nsolution can be obtained using only linear algebra techniques. In particular,\nno differential equations need to be solved. Furthermore, there are no\nrestrictions on the size of the time step size, or on the measurement model.\nAlso discussed are important computational aspects that are crucial for\npotential real-time implementation for higher-dimensional problems. The\nsolution is universal in the sense that the initial distribution may be\narbitrary.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.1705v1"
    },
    {
        "title": "A Test for the Presence of a Signal, with Multiple Channels and Marked\n  Poisson",
        "authors": [
            "Wolfgang A. Rolke",
            "Angel M. Lopez"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We describe a statistical hypothesis test for the presence of a signal based\non the likelihood ratio statistic. We derive the test for a special case of\ninterest. We study extensions of the test to cases where there are multiple\nchannels and to marked Poisson distributions. We show the results of a number\nof performance studies which indicate that the test works very well, even far\nout in the tails of the distribution and with multiple channels and marked\nPoisson.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.2149v2"
    },
    {
        "title": "A New Car-Following Model Inspired by Galton Board",
        "authors": [
            "Fa Wang",
            "Li Li",
            "Xuexiang Jin",
            "Jianming Hu",
            "Yi Zhang",
            "Yan Ji"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Different from previous models based on scatter theory and random matrix\ntheory, a new interpretation of the observed log-normal type time-headway\ndistribution of vehicles is presented in this paper. Inspired by the well known\nGalton Board, this model views driver's velocity adjusting process similar to\nthe dynamics of a particle falling down a board and being deviated at decision\npoints. A new car-following model based on this idea is proposed to reproduce\nthe observed traffic flow phenomena. The agreement between the empirical\nobservations and the simulation results suggests the soundness of this new\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.3081v1"
    },
    {
        "title": "Universal analytic properties of noise. Introducing the J-Matrix\n  formalism",
        "authors": [
            "Daniel Bessis",
            "Luca Perotti"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We propose a new method in the spectral analysis of noisy time-series data\nfor damped oscillators. From the Jacobi three terms recursive relation for the\ndenominators of the Pad\\'e Approximations built on the well-known Z-transform\nof an infinite time-series, we build an Hilbert space operator, a J-Operator,\nwhere each bound state (inside the unit circle in the complex plane) is simply\nassociated to one damped oscillator while the continuous spectrum of the\nJ-Operator, which lies on the unit circle itself, is shown to represent the\nnoise. Signal and noise are thus clearly separated in the complex plane. For a\nfinite time series of length 2N, the J-operator is replaced by a finite order\nJ-Matrix J_N, having N eigenvalues which are time reversal covariant. Different\nclasses of input noise, such as blank (white and uniform), Gaussian and pink,\nare discussed in detail, the J-Matrix formalism allowing us to efficiently\ncalculate hundreds of poles of the Z-transform. Evidence of a universal\nbehaviour in the final statistical distribution of the associated poles and\nzeros of the Z-transform is shown. In particular the poles and zeros tend, when\nthe length of the time series goes to infinity, to a uniform angular\ndistribution on the unit circle. Therefore at finite order, the roots of unity\nin the complex plane appear to be noise attractors. We show that the\nZ-transform presents the exceptional feature of allowing lossless undersampling\nand how to make use of this property. A few basic examples are given to suggest\nthe power of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.3101v2"
    },
    {
        "title": "Instantaneous oscillatory direction and phase for multivariate\n  timeseries",
        "authors": [
            "Carsten Allefeld"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  This text describes a generalization of the analytic signal (Gabor, 1946)\napproach for the definition of instantaneous amplitude and phase to the case of\nmultivariate signals. It was originally written as an appendix for another\npaper, where the determination of the locally dominant oscillatory direction\n(the instantaneous amplitude) described here is used as a preprocessing step\nfor another kind of data analysis. The text is reproduced in a 'standalone'\nform because the procedure might prove useful in other contexts too, especially\nfor the purpose of phase synchronization analysis (Rosenblum et al., 1996)\nbetween two (or more) multivariate sets of time series (Pascual-Marqui, 2007).\n",
        "pdf_link": "http://arxiv.org/pdf/0807.3466v1"
    },
    {
        "title": "Resonant control of stochastic spatio-temporal dynamics in a tunnel\n  diode by multiple time delayed feedback",
        "authors": [
            "Niels Majer",
            "Eckehard Schöll"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We study the control of noise-induced spatio-temporal current density\npatterns in a semiconductor nanostructure (double barrier resonant tunnelling\ndiode) by multiple time-delayed feedback. We find much more pronounced resonant\nfeatures of noise-induced oscillations compared to single time feedback,\nrendering the system more sensitive to variations in the delay time $\\tau$. The\ncoherence of noise-induced oscillations measured by the correlation time\nexhibits sharp resonances as a function of $\\tau$, and can be strongly\nincreased by optimal choices of $\\tau$. Similarly, the peaks in the power\nspectral density are sharpened. We provide analytical insight into the control\nmechanism by relating the correlation times and mean frequencies of\nnoise-induced breathing oscillations to the stability properties of the\ndeterministic stationary current density filaments under the influence of the\ncontrol loop. Moreover, we demonstrate that the use of multiple time delays\nenlarges the regime in which the deterministic dynamical properties of the\nsystem are not changed by delay-induced bifurcations.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.1569v1"
    },
    {
        "title": "Optimal Adaptive Nonparametric Denoising of Multidimensional - Time\n  Signal",
        "authors": [
            "Eugene Ostrovsky",
            "Leonid Sirota"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We construct an adaptive asymptotically optimal in the classical norm of the\nspace L(2) of square integrable functions non - parametrical multidimensional\ntime defined signal regaining (adaptive filtration, noise canceller) on the\nbackground noise via multidimensional truncated Legendre expansion and optimal\nexperience design. The two - dimensional case is known as a picture processing,\npicture analysis or image processing. We offer a two version of an confidence\nregion building, also adaptive. Our estimates proposed by us have successfully\npassed experimental tests on problem by simulate of modeled with the use of\npseudo-random numbers as well as on real data (of seismic signals etc.) for\nwhich our estimations of the different signals were compared with classical\nestimates obtained by the kernel or wavelets estimations method. The precision\nof proposed here estimations is better. Our adaptive truncation may be used\nalso for the signal and image compression.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.3021v1"
    },
    {
        "title": "Sparse image reconstruction for molecular imaging",
        "authors": [
            "Michael Ting",
            "Raviv Raich",
            "Alfred O. Hero III"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The application that motivates this paper is molecular imaging at the atomic\nlevel. When discretized at sub-atomic distances, the volume is inherently\nsparse. Noiseless measurements from an imaging technology can be modeled by\nconvolution of the image with the system point spread function (psf). Such is\nthe case with magnetic resonance force microscopy (MRFM), an emerging\ntechnology where imaging of an individual tobacco mosaic virus was recently\ndemonstrated with nanometer resolution. We also consider additive white\nGaussian noise (AWGN) in the measurements. Many prior works of sparse\nestimators have focused on the case when H has low coherence; however, the\nsystem matrix H in our application is the convolution matrix for the system\npsf. A typical convolution matrix has high coherence. The paper therefore does\nnot assume a low coherence H. A discrete-continuous form of the Laplacian and\natom at zero (LAZE) p.d.f. used by Johnstone and Silverman is formulated, and\ntwo sparse estimators derived by maximizing the joint p.d.f. of the observation\nand image conditioned on the hyperparameters. A thresholding rule that\ngeneralizes the hard and soft thresholding rule appears in the course of the\nderivation. This so-called hybrid thresholding rule, when used in the iterative\nthresholding framework, gives rise to the hybrid estimator, a generalization of\nthe lasso. Unbiased estimates of the hyperparameters for the lasso and hybrid\nestimator are obtained via Stein's unbiased risk estimate (SURE). A numerical\nstudy with a Gaussian psf and two sparse images shows that the hybrid estimator\noutperforms the lasso.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.4079v1"
    },
    {
        "title": "Dynamic Computation of Network Statistics via Updating Schema",
        "authors": [
            "Jie Sun",
            "James P. Bagrow",
            "Erik M. Bollt",
            "Joesph D. Skufca"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In this paper we derive an updating scheme for calculating some important\nnetwork statistics such as degree, clustering coefficient, etc., aiming at\nreduce the amount of computation needed to track the evolving behavior of large\nnetworks; and more importantly, to provide efficient methods for potential use\nof modeling the evolution of networks. Using the updating scheme, the network\nstatistics can be computed and updated easily and much faster than\nre-calculating each time for large evolving networks. The update formula can\nalso be used to determine which edge/node will lead to the extremal change of\nnetwork statistics, providing a way of predicting or designing evolution rule\nof networks.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.4707v1"
    },
    {
        "title": "Characterizing Human Mobility Patterns in a Large Street Network",
        "authors": [
            "Bin Jiang",
            "Junjun Yin",
            "Sijian Zhao"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Previous studies demonstrated empirically that human mobility exhibits Levy\nflight behaviour. However, our knowledge of the mechanisms governing this Levy\nflight behaviour remains limited. Here we analyze over 72 000 people's moving\ntrajectories, obtained from 50 taxicabs during a six-month period in a large\nstreet network, and illustrate that the human mobility pattern, or the Levy\nflight behaviour, is mainly attributed to the underlying street network. In\nother words, the goal-directed nature of human movement has little effect on\nthe overall traffic distribution. We further simulate the mobility of a large\nnumber of random walkers, and find that (1) the simulated random walkers can\nreproduce the same human mobility pattern, and (2) the simulated mobility rate\nof the random walkers correlates pretty well (an R square up to 0.87) with the\nobserved human mobility rate.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.5001v2"
    },
    {
        "title": "Continuous-Discrete Path Integral Filtering",
        "authors": [
            "Bhashyam Balaji"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  A summary of the relationship between the Langevin equation,\nFokker-Planck-Kolmogorov forward equation (FPKfe) and the Feynman path integral\ndescriptions of stochastic processes relevant for the solution of the\ncontinuous-discrete filtering problem is provided in this paper. The practical\nutility of the path integral formula is demonstrated via some nontrivial\nexamples. Specifically, it is shown that the simplest approximation of the path\nintegral formula for the fundamental solution of the FPKfe can be applied to\nsolve nonlinear continuous-discrete filtering problems quite accurately. The\nDirac-Feynman path integral filtering algorithm is quite simple, and is\nsuitable for real-time implementation.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.0198v1"
    },
    {
        "title": "From time series to complex networks: the visibility graph",
        "authors": [
            "Lucas Lacasa",
            "Bartolo Luque",
            "Fernando Ballesteros",
            "Jordi Luque",
            "Juan Carlos Nuno"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In this work we present a simple and fast computational method, the\nvisibility algorithm, that converts a time series into a graph. The constructed\ngraph inherits several properties of the series in its structure. Thereby,\nperiodic series convert into regular graphs, and random series do so into\nrandom graphs. Moreover, fractal series convert into scale-free networks,\nenhancing the fact that power law degree distributions are related to\nfractality, something highly discussed recently. Some remarkable examples and\nanalytical tools are outlined in order to test the method's reliability. Many\ndifferent measures, recently developed in the complex network theory, could by\nmeans of this new approach characterize time series from a new point of view.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.0920v1"
    },
    {
        "title": "The infinite partition of a line segment and multifractal objects",
        "authors": [
            "A. I. L. de Araújo",
            "R. F. Soares",
            "J. P. de Oliveira",
            "G. Corso"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We report an algorithm for the partition of a line segment according to a\ngiven ratio $\\nu$. At each step the length distribution among sets of the\npartition follows a binomial distribution. We call $k$-set to the set of\nelements with the same length at the step $n$. The total number of elements is\n$2^n$ and the number of elements in a same $k$-set is $C_n^k$. In the limit of\nan infinite partion this object become a multifractal where each $k$-set\noriginate a fractal. We find the fractal spectrum $D_k$ and calculate where is\nits maximum. Finally we find the values of $D_k$ for the limits $k/n \\to 0$ and\n1.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.1130v1"
    },
    {
        "title": "Simultaneous Determination of Signal and Background Asymmetries",
        "authors": [
            "Jörg Pretz",
            "Jean-Marc Le Goff"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  This article discusses the determination of asymmetries. We consider a sample\nof events consisting of a peak of signal events on top of some background\nevents. Both signal and background have an unknown asymmetry, e.g. a spin or\nforward-backward asymmetry. A method is proposed which determines signal and\nbackground asymmetries simultaneously using event weighting. For vanishing\nasymmetries the statistical error of the asymmetries reaches the minimal\nvariance bound (MVB) given by the Cram\\'er-Rao inequality and it is very close\nto it for large asymmetries. The method thus provides a significant gain in\nstatistics compared to the classical method of side band subtraction of\nbackground asymmetries. It has the advantage with respect to the unbinned\nmaximum likelihood approach, reaching the MVB as well, that it does not require\nloops over the event sample in the minimization procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.1426v2"
    },
    {
        "title": "The solution classical and quantum feedback optimal control problem\n  without the Bellman Equation",
        "authors": [
            "Jaykov Foukzon"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  A new approach, which is proposed in this paper allows one to construct the\nBellman function V(t,x) and optimal control u(t) directly,i.e.,without any\nreference to the Bellman equation, by way of using strong large deviations\nprinciple for the solutions Colombeau-Ito's SDE.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.2170v4"
    },
    {
        "title": "The Correlation Function of Multiple Dependent Poisson Processes\n  Generated by the Alternating Renewal Process Method",
        "authors": [
            "Don H. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We derive conditions under which alternating renewal processes can be used to\nconstruct correlated Poisson processes. The pairwise correlation function is\nalso derived, showing that the resulting correlations can be negative. The\ntechnique and the analysis can be extended to the generation of two or more\ndependent renewal processes.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.3713v1"
    },
    {
        "title": "Introducing the q-Theil index",
        "authors": [
            "Marcel Ausloos",
            "Janusz Miskiewicz"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Starting from the idea of Tsallis on non-extensive statistical mechanics and\nthe {\\it q-entropy} notion, we recall the Theil index $Th$ and transform it\ninto the $Th_q$ index. Both indices can be used to map onto themselves any time\nseries in a non linear way. We develop an application of the $Th_q$ to the GDP\nevolution of 20 rich countries in the time interval [1950 - 2003] and search\nfor a proof of globalization of their economies. First we calculate the\ndistances between the \"new\" time series and to their mean, from which such data\nsimple networks are constructed. We emphasize that it is useful to, and we do,\ntake into account different time \"parameters\": (i) the moving average time\nwindow for the raw time series to calculate the $Th_q$ index; (ii) the moving\naverage time window for calculating the time series distances; (iii) a\ncorrelation time lag. This allows us to deduce optimal conditions to measure\nthe features of the network, i.e. the appearance in 1970 of a globalization\nprocess in the economy of such countries and the present beginning of\ndeviations. The $q$ value hereby used is that which measures the overall data\ndistribution and is equal to 1.8125.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.1018v1"
    },
    {
        "title": "Using the Notion of Copula in Tomography",
        "authors": [
            "Doriano-Boris Pougaza",
            "A. Mohammad-Djafari",
            "Jean-François Bercher"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In 1917 Johann Radon introduced the Radon transform which is used in 1963 by\nA. M. Cormack for application in the context of tomographic image\nreconstruction. He proposed to reconstruct the spatial variation of the\nmaterial density of the body from X-Ray images (radiographies) for different\ndirections. Independently G. N. Hounsfield derived an algorithm and built the\nfirst medical CT scanner. Basically the idea of the X-ray CT is to get an image\nof the interior structure of an object by X-raying the object from many\ndifferent directions. The mathematical problem is then estimating a\nmultivariate function from its line integrals.\n  Four year before Cormack's idea, Abe Sklar introduced a theory in the context\nof Statistics called copula. Shortly copulas are functions that link\nmultivariate distributions to theirs univariate marginal functions. It appeared\nthat copulas captivated all dependence structure concerning the marginal\nfunctions and offer a wide range of parametric family model which could be used\nas a model for the joint distribution function. This statistical problem is the\nsame as in Tomography, because a marginal density is obtained from a line\nintegral of its joint distribution. In the particular case of only given\nhorizontal and vertical projections corresponding to a given two marginal\nfunctions, we link the theory of copula to tomography via the Radon transform\nand Sklar's theorem. The result we propose seems to be new as mathematical\napproach to solve this tomographic inverse problem.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.1316v1"
    },
    {
        "title": "Surrogates with random Fourier Phases",
        "authors": [
            "Christoph Raeth",
            "Roberto A. Monetti"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The method of surrogates is widely used in the field of nonlinear data\nanalysis for testing for weak nonlinearities. The two most commonly used\nalgorithms for generating surrogates are the amplitude adjusted Fourier\ntransform (AAFT) and the iterated amplitude adjusted Fourier transfom (IAAFT)\nalgorithm. Both the AAFT and IAAFT algorithm conserve the amplitude\ndistribution in real space and reproduce the power spectrum (PS) of the\noriginal data set very accurately. The basic assumption in both algorithms is\nthat higher-order correlations can be wiped out using a Fourier phase\nrandomization procedure. In both cases, however, the randomness of the Fourier\nphases is only imposed before the (first) Fourier back tranformation. Until\nnow, it has not been studied how the subsequent remapping and iteration steps\nmay affect the randomness of the phases. Using the Lorenz system as an example,\nwe show that both algorithms may create surrogate realizations containing\nFourier phase correlations. We present two new iterative surrogate data\ngenerating methods being able to control the randomization of Fourier phases at\nevery iteration step. The resulting surrogate realizations which are truly\nlinear by construction display all properties needed for surrogate data.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2380v1"
    },
    {
        "title": "Bayesian credible interval construction for Poisson statistics",
        "authors": [
            "Yong-Sheng Zhu"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The construction of the Bayesian credible (confidence) interval for a Poisson\nobservable including both the signal and background with and without systematic\nuncertainties is presented. Introducing the conditional probability satisfying\nthe requirement of the background not larger than the observed events to\nconstruct the Bayesian credible interval is also discussed. A Fortran routine,\nBPOCI, has been developed to implement the calculation.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2705v1"
    },
    {
        "title": "Using the Memories of Multiscale Machines to Characterize Complex\n  Systems",
        "authors": [
            "Nick S. Jones"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  A scheme is presented to extract detailed dynamical signatures from\nsuccessive measurements of complex systems. Relative entropy based time series\ntools are used to quantify the gain in predictive power of increasing past\nknowledge. By lossy compression, data is represented by increasingly coarsened\nsymbolic strings. Each compression resolution is modeled by a machine: a finite\nmemory transition matrix. Applying the relative entropy tools to each machine's\nmemory exposes correlations within many time scales. Examples are given for\ncardiac arrhythmias and different heart conditions are distinguished.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.5079v1"
    },
    {
        "title": "Mathematical irrational numbers not so physically irrational",
        "authors": [
            "Y. J. Zhao",
            "Y. H. Gao",
            "J. P. Huang"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We investigate the topological structure of the decimal expansions of the\nthree famous naturally occurring irrational numbers, $\\pi$, $e$, and golden\nratio, by explicitly calculating the diversity of the pair distributions of the\nten digits ranging from 0 to 9. And we find that there is a universal two-phase\nbehavior, which collapses into a single curve with a power law phenomenon. We\nfurther reveal that the two-phase behavior is closely related to general\naspects of phase transitions in physical systems. It is then numerically shown\nthat such characteristics originate from an intrinsic property of genuine\nrandom distribution of the digits in decimal expansions. Thus, mathematical\nirrational numbers are not so physically irrational as long as they have such\nan intrinsic property.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.0768v1"
    },
    {
        "title": "The Visibility Graph: a new method for estimating the Hurst exponent of\n  fractional Brownian motion",
        "authors": [
            "Lucas Lacasa",
            "Bartolo Luque",
            "Jordi Luque",
            "Juan Carlos Nuno"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Fractional Brownian motion (fBm) has been used as a theoretical framework to\nstudy real time series appearing in diverse scientific fields. Because its\nintrinsic non-stationarity and long range dependence, its characterization via\nthe Hurst parameter H requires sophisticated techniques that often yield\nambiguous results. In this work we show that fBm series map into a scale free\nvisibility graph whose degree distribution is a function of H. Concretely, it\nis shown that the exponent of the power law degree distribution depends\nlinearly on H. This also applies to fractional Gaussian noises (fGn) and\ngeneric f^(-b) noises. Taking advantage of these facts, we propose a brand new\nmethodology to quantify long range dependence in these series. Its reliability\nis confirmed with extensive numerical simulations and analytical developments.\nFinally, we illustrate this method quantifying the persistent behavior of human\ngait dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.0888v1"
    },
    {
        "title": "On the use of continuous wavelet analysis for modal identification",
        "authors": [
            "Pierre Argoul",
            "Silvano Erlicher"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  This paper reviews two different uses of the continuous wavelet transform for\nmodal identification purposes. The properties of the wavelet transform, mainly\nenergetic, allow to emphasize or filter the main information within measured\nsignals and thus facilitate the modal parameter identification especially when\nmechanical systems exhibit modal coupling and/or relatively strong damping.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.2000v1"
    },
    {
        "title": "Maximum Entropy: The Universal Method for Inference",
        "authors": [
            "Adom Giffin"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this thesis we start by providing some detail regarding how we arrived at\nour present understanding of probabilities and how we manipulate them - the\nproduct and addition rules by Cox. We also discuss the modern view of entropy\nand how it relates to known entropies such as the thermodynamic entropy and the\ninformation entropy. Next, we show that Skilling's method of induction leads us\nto a unique general theory of inductive inference, the ME method and precisely\nhow it is that other entropies such as those of Renyi or Tsallis are ruled out\nfor problems of inference. We then explore the compatibility of Bayes and ME\nupdating. We show that ME is capable of producing every aspect of orthodox\nBayesian inference and proves the complete compatibility of Bayesian and\nentropy methods. The realization that the ME method incorporates Bayes' rule as\na special case allows us to go beyond Bayes' rule and to process both data and\nexpected value constraints simultaneously. We discuss the general problem of\nnon-commuting constraints, when they should be processed sequentially and when\nsimultaneously. The generic \"canonical\" form of the posterior distribution for\nthe problem of simultaneous updating with data and moments is obtained. This is\na major achievement since it shows that ME is not only capable of processing\ninformation in the form of constraints, like MaxEnt and information in the form\nof data, as in Bayes' Theorem, but also can process both forms simultaneously,\nwhich Bayes and MaxEnt cannot do alone. Finally, we illustrate some potential\napplications for this new method by applying ME to potential problems of\ninterest.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.2987v1"
    },
    {
        "title": "A New Look at an Old Tool-the Cumulative Spectral Power of Fast-Fourier\n  Transform Analysis",
        "authors": [
            "Sheng-Chiang Lee",
            "Randall D. Peters"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  As an old and widely used tool, it is still possible to find new insights and\napplications from Fast Fourier Transform (FFT)-based analyses. The FFT is\nfrequently used to generate the Power Spectral Density (PSD) function, by\nsquaring the spectral components that have been corrected for influence from\nthe instrument that generated the data. Although better than a raw-data\nspectrum, by removing influence of the instrument transfer function, the PSD is\nstill of limited value for time varying signals with noise, due to the very\nnature of the Fourier transform. The authors present here another way to treat\nthe FFT data, namely the Cumulative Spectral Power (CSP), as a promising means\nto overcome some of these limitations. As will be seen from the examples\nprovided, the CSP holds promise in a variety of different fields.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.3708v1"
    },
    {
        "title": "Implementation and application of kinematic vertex fitting in the\n  software environment of ILD",
        "authors": [
            "Fabian Moser",
            "Wolfgang Waltenberger",
            "Meinhard Regler",
            "Winfried Mitaroff"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  The vertex reconstruction toolkit RAVE has been extended by an option for the\ninclusion of kinematic constraints, and embedded into the ILD analysis\nframework Marlin. The new tools have been tested with an exemplary\nreconstruction of WW and ZZ decays. The presented results show the improvements\nachieved in precision of the fitted masses, and demonstrate the usage and\nfunctionality of the toolkit.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.4020v1"
    },
    {
        "title": "SAXSFit: A program for fitting small-angle x-ray and neutron scattering\n  data",
        "authors": [
            "Bridget Ingham",
            "Haiyong Li",
            "Emily L. Allen",
            "Michael F. Toney"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  SAXSFit is a computer analysis program that has been developed to assist in\nthe fitting of small-angle x-ray and neutron scattering spectra primarily from\nnanoparticles (nanopores). The fitting procedure yields the pore or particle\nsize distribution and eta parameter for one or two size distributions (which\ncan be log-normal, Schulz, or Gaussian). A power-law and/or constant background\ncan also be included. The program is written in Java so as to be stand-alone\nand platform-independent, and is designed to be easy for novices to use, with a\nuser-friendly graphical interface.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.4782v1"
    },
    {
        "title": "Binomial and ratio-of-Poisson-means frequentist confidence intervals\n  applied to the error evaluation of cut efficiencies",
        "authors": [
            "Gioacchino Ranucci"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  The evaluation of the error to be attributed to cut efficiencies is a common\nquestion in the practice of experimental particle physics. Specifically, the\nneed to evaluate the efficiency of the cuts for background removal, when they\nare tested in a signal-free-background-only energy window, originates a\nstatistical problem which finds its natural framework in the ample family of\nsolutions for two classical, and closely related, questions, i.e. the\ndetermination of confidence intervals for the parameter of a binomial\nproportion and for the ratio of Poisson means. In this paper the problem is\nfirst addressed from the traditional perspective, and afterwards naturally\nevolved towards the introduction of non standard confidence intervals both for\nthe binomial and Poisson cases; in particular, special emphasis is given to the\nintervals obtained through the application of the likelihood ratio ordering to\nthe traditional Neyman prescription for the confidence limits determination.\nDue to their attractiveness in term of reduced length and of coverage\nproperties, the new intervals are well suited as interesting alternative to the\nstandard Clopper-Pearson PDG intervals.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.4845v1"
    },
    {
        "title": "Dynamics of EEG Entropy: beyond signal plus noise",
        "authors": [
            "M. Ignaccolo",
            "M. Latka",
            "W. Jernajczyk",
            "P. Grigolini",
            "B. J. West"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  EEG time series are analyzed using the diffusion entropy method. The\nresulting EEG entropy manifests short-time scaling, asymptotic saturation and\nan attenuated alpha-rhythm modulation. These properties are faithfully modeled\nby a phenomenological Langevin equation interpreted within a neural network\ncontext. Detrended fluctuation analysis of the EEG data is compared with\ndiffusion entropy analysis and is found to suppress certain important\nproperties of the EEG time series.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.1113v1"
    },
    {
        "title": "About the parabolic relation existing between the skewness and the\n  kurtosis in time series of experimental data",
        "authors": [
            "F. Sattin",
            "M. Agostini",
            "R. Cavazzana",
            "G. Serianni",
            "P. Scarin",
            "N. Vianello"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this work we investigate the origin of the parabolic relation between\nskewness and kurtosis often encountered in the analysis of experimental\ntime-series. We argue that the numerical values of the coefficients of the\ncurve may provide informations about the specific physics of the system\nstudied, whereas the analytical curve per se is a fairly general consequence of\na few constraints expected to hold for most systems.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.2053v1"
    },
    {
        "title": "From Sigmoid Power Control Algorithm to Hopfield-like Neural Networks:\n  \"SIR\" (\"Signal\"-to-\"Interference\"-Ratio)-Balancing Sigmoid-Based Networks-\n  Part I: Continuous Time",
        "authors": [
            "Zekeriya Uykan"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Continuous-time Hopfield network has been an important focus of research area\nsince 1980s whose applications vary from image restoration to combinatorial\noptimization from control engineering to associative memory systems. On the\nother hand, in wireless communications systems literature, power control has\nbeen intensively studied as an essential mechanism for increasing the system\nperformance. A fully distributed power control algorithm (DPCA), called Sigmoid\nDPCA, is presented by Uykan in [10] and [11], which is obtained by discretizing\nthe continuous-time system. In this paper, we present a Sigmoid-based\n\"Signal-to-Interference Ratio, (SIR)\" balancing dynamic networks, called\nSgm\"SIR\"NN, which includes both the Sigmoid power control algorithm (SgmDPCA)\nand the Hopfield neural networks, two different areas whose scope of interest,\nmotivations and settings are completely different. It's shown that the\nSgm\"SIR\"N5C5C5C5C5CN exhibits features which are generally attributed to\nHopfield Networks. Computer simulations show the effectiveness of the proposed\nnetwork as compared to traditional Hopfield Network.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.2577v1"
    },
    {
        "title": "From Sigmoid Power Control Algorithm to Hopfield-like Neural Networks:\n  \"SIR\"-Balancing Sigmoid-Based Networks- Part II: Discrete Time",
        "authors": [
            "Zekeriya Uykan"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In the first part in [12], we present and analyse a Sigmoid-based\n\"Signal-to-Interference Ratio, (SIR)\" balancing dynamic network, called\nSgm\"SIR\"NN, which exhibits similar properties as traditional Hopfield NN does,\nin continuous time. In this second part, we present the corresponding network\nin discrete time: We show that in the proposed discrete-time network, called\nD-Sgm\"SIR\"NN, the defined error vector approaches to zero in a finite step in\nboth synchronous and asynchronous work modes. Our investigations show that i)\nEstablishing an analogy to the distributed (sigmoid) power control algorithm in\n[10] and [11] if the defined fictitious \"SIR\" is equal to 1 at the converged\neqiulibrium point, then it is one of the prototype vectors. ii) The\nD-Sgm\"SIR\"NN exhibits similar features as discrete-time Hopfield NN does. iii)\nEstablishing an analogy to the traditional 1-bit fixed-step power control\nalgorithm, the corresponding \"1-bit\" network, called Sign\"SIR\"NN network, is\nalso presented.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.2581v1"
    },
    {
        "title": "The relativity of theory",
        "authors": [
            "Nisheeth Srivastava"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  A general information-theoretic framework for deriving physical laws is\npresented and a principle of informational physics is enunciated within its\ncontext. Existing approaches intended to derive physical laws from\ninformation-theoretic first principles are unified as special cases of this\nframework with the introduction of constraints dependent on the physical\nprocess of observation. Some practical, theoretical and epistemological\nimplications of the validity of this approach are examined.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.3479v1"
    },
    {
        "title": "Ultimate \"SIR\" in Autonomous Linear Networks with Symmetric Weight\n  Matrices, and Its Use to Stabilize the Network - A Hopfield-like network",
        "authors": [
            "Zekeriya Uykan"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this paper, we present and analyse two Hopfield-like nonlinear networks,\nin continuous-time and discrete-time respectively. The proposed network is\nbased on an autonomous linear system with a symmetric weight matrix, which is\ndesigned to be unstable, and a nonlinear function stabilizing the whole network\nthanks to a manipulated state variable called``ultimate SIR''. This variable is\nobserved to be equal to the traditional Signal-to-Interference Ratio (SIR)\ndefinition in telecommunications engineering.\n  The underlying linear system of the proposed continuous-time network is\n$\\dot{{\\mathbf x}} = {\\mathbf B} {\\mathbf x}$ where {\\bf B} is a real symmetric\nmatrix whose diagonal elements are fixed to a constant. The nonlinear function,\non the other hand, is based on the defined system variables called ``SIR''s. We\nalso show that the ``SIR''s of all the states converge to a constant value,\ncalled ``system-specific Ultimate SIR''; which is equal to\n$\\frac{r}{\\lambda_{max}}$ where $r$ is the diagonal element of matrix ${\\bf B}$\nand $\\lambda_{max}$ is the maximum (positive) eigenvalue of diagonally-zero\nmatrix $({\\bf B} - r{\\bf I})$, where ${\\bf I}$ denotes the identity matrix. The\nsame result is obtained in its discrete-time version as well.\n  Computer simulations for binary associative memory design problem show the\neffectiveness of the proposed network as compared to the traditional Hopfield\nNetworks.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.3841v2"
    },
    {
        "title": "Analysis of \"SIR\" (\"Signal\"-to-\"Interference\"-Ratio) in Discrete-Time\n  Autonomous Linear Networks with Symmetric Weight Matrices",
        "authors": [
            "Zekeriya Uykan"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  It's well-known that in a traditional discrete-time autonomous linear\nsystems, the eigenvalues of the weigth (system) matrix solely determine the\nstability of the system. If the spectral radius of the system matrix is larger\nthan 1, then the system is unstable. In this paper, we examine the linear\nsystems with symmetric weight matrix whose spectral radius is larger than 1.\nThe author introduced a dynamic-system-version of \"Signal-to-Interference Ratio\n(SIR)\" in nonlinear networks in [7] and [8] and in continuous-time linear\nnetworks in [9]. Using the same \"SIR\" concept, we, in this paper, analyse the\n\"SIR\" of the states in the following two $N$-dimensional discrete-time\nautonomous linear systems: 1) The system ${\\mathbf x}(k+1) = \\big({\\bf I} +\n\\alpha (-r {\\bf I} + {\\bf W}) \\big) {\\mathbf x}(k)$ which is obtained by\ndiscretizing the autonomous continuous-time linear system in \\cite{Uykan09a}\nusing Euler method; where ${\\bf I}$ is the identity matrix, $r$ is a positive\nreal number, and $\\alpha >0$ is the step size. 2) A more general autonomous\nlinear system descibed by ${\\mathbf x}(k+1) = -\\rho {\\mathbf I + W} {\\mathbf\nx}(k)$, where ${\\mathbf W}$ is any real symmetric matrix whose diagonal\nelements are zero, and ${\\bf I}$ denotes the identity matrix and $\\rho$ is a\npositive real number. Our analysis shows that: 1) The \"SIR\" of any state\nconverges to a constant value, called \"Ultimate SIR\", in a finite time in the\nabove-mentioned discrete-time linear systems. 2) The \"Ultimate SIR\" in the\nfirst system above is equal to $\\frac{\\rho}{\\lambda_{max}}$ where\n$\\lambda_{max}$ is the maximum (positive) eigenvalue of the matrix ${\\bf W}$.\nThese results are in line with those of \\cite{Uykan09a} where corresponding\ncontinuous-time linear system is examined. 3) The \"Ultimate SIR\" ...\n",
        "pdf_link": "http://arxiv.org/pdf/0902.3844v1"
    },
    {
        "title": "On the SIRs (Signal-to-Interference-Ratio) in Discrete-Time Autonomous\n  Linear Networks",
        "authors": [
            "Zekeriya Uykan"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this letter, we improve the results in [5] by relaxing the symmetry\nassumption and also taking the noise term into account. The author examines two\ndiscrete-time autonomous linear systems whose motivation comes from a neural\nnetwork point of view in [5]. Here, we examine the following discrete-time\nautonomous linear system: ${\\mathbf x}(k+1) = {\\mathbf A} {\\mathbf x}(k) +\n{\\mathbf b}$ where ${\\mathbf A}$ is any real square matrix with linearly\nindependent eigenvectors whose largest eigenvalue is real and its norm is\nlarger than 1, and vector ${\\mathbf b}$ is constant. Using the same \"SIR\"\n(\"Signal\"-to-\"Interference\"-Ratio) concept as in [4] and [5], we show that the\nultimate \"SIR\" is equal to $\\frac{a_{ii}}{\\lambda_{max} - a_{ii}}$, $i=1, 2,\n>..., N$, where $N$ is the number of states, $a_{ii}$ is the diagonal elements\nof matrix ${\\bf A}$, and $\\lambda_{max}$ is the (single or multiple) eigenvalue\nwith maximum norm.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.2292v2"
    },
    {
        "title": "Joint Bayesian endmember extraction and linear unmixing for\n  hyperspectral imagery",
        "authors": [
            "Nicolas Dobigeon",
            "Said Moussaoui",
            "Martial Coulon",
            "Jean-Yves Tourneret",
            "Alfred O. Hero"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  This paper studies a fully Bayesian algorithm for endmember extraction and\nabundance estimation for hyperspectral imagery. Each pixel of the hyperspectral\nimage is decomposed as a linear combination of pure endmember spectra following\nthe linear mixing model. The estimation of the unknown endmember spectra is\nconducted in a unified manner by generating the posterior distribution of\nabundances and endmember parameters under a hierarchical Bayesian model. This\nmodel assumes conjugate prior distributions for these parameters, accounts for\nnon-negativity and full-additivity constraints, and exploits the fact that the\nendmember proportions lie on a lower dimensional simplex. A Gibbs sampler is\nproposed to overcome the complexity of evaluating the resulting posterior\ndistribution. This sampler generates samples distributed according to the\nposterior distribution and estimates the unknown parameters using these\ngenerated samples. The accuracy of the joint Bayesian estimator is illustrated\nby simulations conducted on synthetic and real AVIRIS images.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.3060v1"
    },
    {
        "title": "Nonparametric estimation of the heterogeneity of a random medium using\n  Compound Poisson Process modeling of wave multiple scattering",
        "authors": [
            "Nicolas Le Bihan",
            "Ludovic Margerin"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this paper, we present a nonparametric method to estimate the\nheterogeneity of a random medium from the angular distribution of intensity\ntransmitted through a slab of random material. Our approach is based on the\nmodeling of forward multiple scattering using Compound Poisson Processes on\ncompact Lie groups. The estimation technique is validated through numerical\nsimulations based on radiative transfer theory.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.4502v1"
    },
    {
        "title": "Generation-by-Generation Dissection of the Response Function in Long\n  Memory Epidemic Processes",
        "authors": [
            "A. Saichev",
            "D. Sornette"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In a number of natural and social systems, the response to an exogenous shock\nrelaxes back to the average level according to a long-memory kernel $\\sim\n1/t^{1+\\theta}$ with $0 \\leq \\theta <1$. In the presence of an epidemic-like\nprocess of triggered shocks developing in a cascade of generations at or close\nto criticality, this \"bare\" kernel is renormalized into an even slower decaying\nresponse function $\\sim 1/t^{1-\\theta}$. Surprisingly, this means that the\nshorter the memory of the bare kernel (the larger $1+\\theta$), the longer the\nmemory of the response function (the smaller $1-\\theta$). Here, we present a\ndetailed investigation of this paradoxical behavior based on a\ngeneration-by-generation decomposition of the total response function, the use\nof Laplace transforms and of \"anomalous\" scaling arguments. The paradox is\nexplained by the fact that the number of triggered generations grows\nanomalously with time at $\\sim t^\\theta$ so that the contributions of active\ngenerations up to time $t$ more than compensate the shorter memory associated\nwith a larger exponent $\\theta$. This anomalous scaling results fundamentally\nfrom the property that the expected waiting time is infinite for $0 \\leq \\theta\n\\leq 1$. The techniques developed here are also applied to the case $\\theta >1$\nand we find in this case that the total renormalized response is a {\\bf\nconstant} for $t < 1/(1-n)$ followed by a cross-over to $\\sim 1/t^{1+\\theta}$\nfor $t \\gg 1/(1-n)$.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.0872v1"
    },
    {
        "title": "Numerical Approach to Central Limit Theorem for Bifurcation Ratio of\n  Random Binary Tree",
        "authors": [
            "Ken Yamamoto",
            "Yoshihiro Yamazaki"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  A central limit theorem for binary tree is numerically examined. Two types of\ncentral limit theorem for higher-order branches are formulated. A topological\nstructure of a binary tree is expressed by a binary sequence, and the\nHorton-Strahler indices are calculated by using the sequence. By fitting the\nGaussian distribution function to our numerical data, the values of variances\nare determined and written in simple forms.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.2043v2"
    },
    {
        "title": "A reconstruction algorithm for single-particle diffraction imaging\n  experiments",
        "authors": [
            "Duane Ne-Te Loh",
            "Veit Elser"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We introduce the EMC algorithm for reconstructing a particle's 3D diffraction\nintensity from very many photon shot-noise limited 2D measurements, when the\nparticle orientation in each measurement is unknown. The algorithm combines a\nmaximization step (M) of the intensity's likelihood function, with expansion\n(E) and compression (C) steps that map the 3D intensity model to a redundant\ntomographic representation and back again. After a few iterations of the EMC\nupdate rule, the reconstructed intensity is given to the difference-map\nalgorithm for reconstruction of the particle contrast. We demonstrate\nreconstructions with simulated data and investigate the effects of particle\ncomplexity, number of measurements, and the number of photons per measurement.\nThe relatively transparent scaling behavior of our algorithm provides a first\nestimate of the data processing resources required for future single-particle\nimaging experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.2581v4"
    },
    {
        "title": "Evaluation of Mutual Information Estimators for Time Series",
        "authors": [
            "Angeliki Papana",
            "Dimitris Kugiumtzis"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We study some of the most commonly used mutual information estimators, based\non histograms of fixed or adaptive bin size, $k$-nearest neighbors and kernels,\nand focus on optimal selection of their free parameters. We examine the\nconsistency of the estimators (convergence to a stable value with the increase\nof time series length) and the degree of deviation among the estimators. The\noptimization of parameters is assessed by quantifying the deviation of the\nestimated mutual information from its true or asymptotic value as a function of\nthe free parameter. Moreover, some common-used criteria for parameter selection\nare evaluated for each estimator. The comparative study is based on Monte Carlo\nsimulations on time series from several linear and nonlinear systems of\ndifferent lengths and noise levels. The results show that the $k$-nearest\nneighbor is the most stable and less affected by the method-specific parameter.\nA data adaptive criterion for optimal binning is suggested for linear systems\nbut it is found to be rather conservative for nonlinear systems. It turns out\nthat the binning and kernel estimators give the least deviation in identifying\nthe lag of the first minimum of mutual information from nonlinear systems, and\nare stable in the presence of noise.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.4753v1"
    },
    {
        "title": "sFit: a method for background subtraction in maximum likelihood fit",
        "authors": [
            "Yuehong Xie"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  This paper presents a statistical method to subtract background in maximum\nlikelihood fit, without relying on any separate sideband or simulation for\nbackground modeling. The method, called sFit, is an extension to the sPlot\ntechnique originally developed to reconstruct true distribution for each date\ncomponent. The sWeights defined for the sPlot technique allow to construct a\nmodified likelihood function using only the signal probability density function\nand events in the signal region. Contribution of background events in the\nsignal region to the likelihood function cancels out on a statistical basis.\nMaximizing this likelihood function leads to unbiased estimates of the fit\nparameters in the signal probability density function.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.0724v1"
    },
    {
        "title": "A shift-optimized Hill-type estimator",
        "authors": [
            "Éva Rácz",
            "János Kertész",
            "Zoltán Eisler"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  A wide range of natural and social phenomena result in observables whose\ndistributions can be well approximated by a power-law decay. The well-known\nHill estimator of the tail exponent provides results which are in many respects\nsuperior to other estimators in case the asymptotics of the distribution is\nindeed a pure power-law, however,systematic errors occur if the distribution is\naltered by simply shifting it. We demonstrate some related problems which\ntypically emerge when dealing with empirical data and suggest a procedure\ndesigned to extend the applicability of the Hill estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.3096v1"
    },
    {
        "title": "Frequentist Evaluation of Intervals Estimated for a Binomial Parameter\n  and for the Ratio of Poisson Means",
        "authors": [
            "Robert D. Cousins",
            "Kathryn E. Hymes",
            "Jordan Tucker"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Confidence intervals for a binomial parameter or for the ratio of Poisson\nmeans are commonly desired in high energy physics (HEP) applications such as\nmeasuring a detection efficiency or branching ratio. Due to the discreteness of\nthe data, in both of these problems the frequentist coverage probability\nunfortunately depends on the unknown parameter. Trade-offs among desiderata\nhave led to numerous sets of intervals in the statistics literature, while in\nHEP one typically encounters only the classic intervals of Clopper-Pearson\n(central intervals with no undercoverage but substantial over-coverage) or a\nfew approximate methods which perform rather poorly. If strict coverage is\nrelaxed, some sort of averaging is needed to compare intervals. In most of the\nstatistics literature, this averaging is over different values of the unknown\nparameter, which is conceptually problematic from the frequentist point of view\nin which the unknown parameter is typically fixed. In contrast, we perform an\n(unconditional) {\\it average over observed data} in the ratio-of-Poisson-means\nproblem. If strict conditional coverage is desired, we recommend\nClopper-Pearson intervals and intervals from inverting the likelihood ratio\ntest (for central and non-central intervals, respectively). Lancaster's mid-$P$\nmodification to either provides excellent unconditional average coverage in the\nratio-of-Poisson-means problem.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.3831v2"
    },
    {
        "title": "Analysis of major failures in Europe's power grid",
        "authors": [
            "Martí Rosas-Casals",
            "Ricard Solé"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Power grids are prone to failure. Time series of reliability measures such as\ntotal power loss or energy not supplied can give significant account of the\nunderlying dynamical behavior of these systems, specially when the resulting\nprobability distributions present remarkable features such as an algebraic\ntail, for example. In this paper, seven years (from 2002 to 2008) of Europe's\ntransport of electricity network failure events have been analyzed and the best\nfit for this empirical data probability distribution is presented. With the\nactual span of available data and although there exists a moderate support for\nthe power law model, the relatively small amount of events contained in the\nfunction's tail suggests that other causal factors might be significantly\nruling the system's dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.1109v1"
    },
    {
        "title": "Sampling from naturally truncated power laws: The matchmaking paradox",
        "authors": [
            "I. M. Sokolov",
            "I. Eliazar"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Consider a network of M >> 1 nodes connected by N >> 1 links, in which the\ndistribution of the number of links per node follows a power law with exponent\n0<\\alpha <1. The power law is naturally truncated due to the fact that N is\nfinite. A subset of m << M nodes is sampled arbitrarily, yielding the sample\nmean \\eta : The average number of links per node, within the sampled subset. We\nexplore the statistics of the sample mean \\eta and show that its fluctuations\naround the population mean \\nu =N/M are extremely broad and strongly skewed --\nyielding typical values which are systematically and significantly smaller than\nthe population mean \\nu. Applying these results to the case of bipartite\nnetworks, we show that the sample means of the two parts of these networks\ngenerally differ -- the fact we call \"matchmaking paradox\" in the title.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.0078v1"
    },
    {
        "title": "Joint Image Restoration and Segmentation using Gauss-Markov-Potts Prior\n  Models and Variational Bayesian Computation: Technical Details",
        "authors": [
            "Hacheme Ayasso",
            "Ali Mohammad-Djafari"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We propose a method to restore and to segment simultaneously images degraded\nby a known point spread function (PSF) and additive white noise. For this\npurpose, we propose a joint Bayesian estimation framework, where a family of\nnon-homogeneous Gauss-Markov fields with Potts region labels models are chosen\nto serve as priors for images. Since neither the joint maximum a posteriori\nestimator nor posterior mean one are tractable, the joint posterior law of the\nimage, its segmentation and all the hyper-parameters, is approximated by a\nseparable probability laws using the Variational Bayes technique. This yields a\nknown probability laws of the posterior with mutually dependent shaping\nparameter, which aims to enhance the convergence speed of the estimator\ncompared to stochastic sampling based estimator. The main work is description\nis given in [1], while technical details of the variational calculations are\npresented in the current paper.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.1860v1"
    },
    {
        "title": "Utilisation de la notion de copule en tomographie",
        "authors": [
            "Doriano-Boris Pougaza",
            "Ali Mohammad-Djafari",
            "Jean-Francois Bercher"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Un probl\\`eme important en statistique est la d\\'etermination d'une loi de\nprobabilit\\'e jointe \\`a partir de ses lois marginales. Dans le cas\nbidimensionnel, les lois de probabilit\\'e marginales f1 (x) et f2(y) sont\nreli\\'ees \\`a la loi jointe f(x,y) par les int\\'egrales suivant les lignes\nhorizontale et verticale (les deux axes x et y). Ainsi, le probl\\`eme de la\nd\\'etermination de f(x,y) connaissant f1 (x) et f2(y) est un probl\\`eme inverse\nmal pos\\'e. En statistique la notion de copule est introduite pour obtenir une\nsolution \\`a ce probl\\`eme. Un probl\\`eme similaire en tomographie \\`a rayon X\nest la reconstruction d'une image f(x,y) repr\\'esentant la r\\'epartition de la\ndensit\\'e d'une quantit\\'e \\`a l'int\\'erieur de l'objet \\`a partir de ses deux\nprojections horizontale et verticale, f1 (x) et f2(y). Il existe aussi un grand\nnombre de m\\'ethodes pour de tels probl\\`emes fond\\'ees sur la transform\\'ee de\nRadon. Dans cet article, nous montrons les liens entre la notion de copule et\ncelle de la tomographie \\`a rayon X et voyons si on peut utiliser les\nm\\'ethodes d'un domaine \\`a l'autre.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.2532v2"
    },
    {
        "title": "Variable Metric Stochastic Approximation Theory",
        "authors": [
            "Peter Sunehag",
            "Jochen Trumpf",
            "S. V. N. Vishwanathan",
            "Nicol Schraudolph"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We provide a variable metric stochastic approximation theory. In doing so, we\nprovide a convergence theory for a large class of online variable metric\nmethods including the recently introduced online versions of the BFGS algorithm\nand its limited-memory LBFGS variant. We also discuss the implications of our\nresults for learning from expert advice.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.3529v1"
    },
    {
        "title": "High-resolution modal analysis",
        "authors": [
            "Kerem Ege",
            "Xavier Boutillon",
            "Bertrand David"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Usual modal analysis techniques are based on the Fourier transform. Due to\nthe Delta T . Delta f limitation, they perform poorly when the modal overlap mu\nexceeds 30%. A technique based on a high-resolution analysis algorithm and an\norder-detection method is presented here, with the aim of filling the gap\nbetween the low- and the high-frequency domains (30%<mu<100%). A pseudo-impulse\nforce is applied at points of interests of a structure and the response is\nmeasured at a given point. For each pair of measurements, the impulse response\nof the structure is retrieved by deconvolving the pseudo-impulse force and\nfiltering the response with the result. Following conditioning treatments, the\nreconstructed impulse response is analysed in different frequency-bands. In\neach frequency-band, the number of modes is evaluated, the frequencies and\ndamping factors are estimated, and the complex amplitudes are finally\nextracted. As examples of application, the separation of the twin modes of a\nsquare plate and the partial modal analyses of aluminium plates up to a modal\noverlap of 70% are presented. Results measured with this new method and those\ncalculated with an improved Rayleigh method match closely.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.0885v1"
    },
    {
        "title": "Reducing multi-dimensional information into a 1-d histogram",
        "authors": [
            "Mario Campanelli",
            "William Murray"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We present two methods for reducing multidimensional information to one\ndimension for ease of understand or analysis while maintaining statistical\npower. While not new, dimensional reduction is not greatly used in high-energy\nphysics and has applications whenever there is a distinctive feature (for\ninstance, a mass peak) in one variable but when signal purity depends on\nothers; so in practice in most of the areas of physics analysis. While both\nmethods presented here assume knowledge of the background, they differ in the\nfact that only one of the methods uses a model for the signal, trading some\nincrease in statistical power for this model dependence.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.0527v1"
    },
    {
        "title": "A Statistical Prescription to Estimate Properly Normalized Distributions\n  of Different Particle Species",
        "authors": [
            "Massimo Casarsa",
            "Pierluigi Catastini",
            "Giovanni Punzi",
            "Luciano Ristori"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We describe a statistical method to avoid biased estimation of the content of\ndifferent particle species. We consider the case when the particle\nidentification information strongly depends on some kinematical variables,\nwhose distributions are unknown and different for each particles species. We\nshow that the proposed procedure provides properly normalized and completely\ndata-driven estimation of the unknown distributions without any a priori\nassumption on their functional form. Moreover, we demonstrate that the method\ncan be generalized to any kinematical distribution of the particles.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.2266v1"
    },
    {
        "title": "Agent-based Simulation of Human Movement Shaped by the Underlying Street\n  Structure",
        "authors": [
            "Bin Jiang",
            "Tao Jia"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Relying on random and purposive moving agents, we simulated human movement in\nlarge street networks. We found that aggregate flow, assigned to individual\nstreets, is mainly shaped by the underlying street structure, and that human\nmoving behavior (either random or purposive) has little effect on the aggregate\nflow. This finding implies that given a street network, the movement patterns\ngenerated by purposive walkers (mostly human beings) and by random walkers are\nthe same. Based on the simulation and correlation analysis, we further found\nthat the closeness centrality is not a good indicator for human movement, in\ncontrast to a long standing view held by space syntax researchers. Instead we\nsuggest that Google's PageRank, and its modified version - weighted PageRank,\nbetweenness and degree centralities are all better indicators for predicting\naggregate flow.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.3055v4"
    },
    {
        "title": "Jointly Poisson processes",
        "authors": [
            "D. H. Johnson",
            "I. N. Goodman"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  What constitutes jointly Poisson processes remains an unresolved issue. This\nreport reviews the current state of the theory and indicates how the accepted\nbut unproven model equals that resulting from the small time-interval limit of\njointly Bernoulli processes. One intriguing consequence of these models is that\njointly Poisson processes can only be positively correlated as measured by the\ncorrelation coefficient defined by cumulants of the probability generating\nfunctional.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.2524v1"
    },
    {
        "title": "Optimal confidence intervals for bounded parameters (a correct\n  alternative to the recipe of Feldman and Cousins)",
        "authors": [
            "Fyodor V. Tkachov"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  A priori bound for the parameter to be estimated is incorporated into\nconfidence intervals within frequentistic approach in a straightforward and\noptimal fashion, ensuring the best resolution of non-boundary values as well as\nrobustness for non-physical values of the estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.4271v3"
    },
    {
        "title": "Symmetrization and enhancement of the continuous Morlet transform",
        "authors": [
            "Robert W. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  The forward and inverse wavelet transform using the continuous Morlet basis\nmay be symmetrized by using an appropriate normalization factor. The loss of\nresponse due to wavelet truncation is addressed through a renormalization of\nthe wavelet based on power. The spectral density has physical units which may\nbe related to the squared amplitude of the signal, as do its margins the mean\nwavelet power and the integrated instant power, giving a quantitative estimate\nof the power density with temporal resolution. Deconvolution with the wavelet\nresponse matrix reduces the spectral leakage and produces an enhanced wavelet\nspectrum providing maximum resolution of the harmonic content of a signal.\nApplications to data analysis are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.1126v2"
    },
    {
        "title": "Optimal upper bounds for non-negative parameters",
        "authors": [
            "Fyodor V. Tkachov"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Using the techniques of [arXiv:0911.4271], upper bounds for a given\nconfidence level are modified in an optimal fashion to incorporate the a priori\ninformation that the parameter being estimated is non-negative. A paradox with\ndifferent confidence intervals for the same confidence level is clarified. The\n\"lossy compression\" nature of the device of confidence intervals is discussed\nand a \"lossless\" option to present results is pointed out.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.1555v1"
    },
    {
        "title": "Symbolic Sequences and Tsallis Entropy",
        "authors": [
            "H. V. Ribeiro",
            "E. K. Lenzi",
            "R. S. Mendes",
            "G. A. Mendes",
            "L. R. da Silva"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We address this work to investigate symbolic sequences with long-range\ncorrelations by using computational simulation. We analyze sequences with two,\nthree and four symbols that could be repeated $l$ times, with the probability\ndistribution $p(l)\\propto 1/ l^{\\mu}$. For these sequences, we verified that\nthe usual entropy increases more slowly when the symbols are correlated and the\nTsallis entropy exhibits, for a suitable choice of $q$, a linear behavior. We\nalso study the chain as a random walk-like process and observe a nonusual\ndiffusive behavior depending on the values of the parameter $\\mu$.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.2855v1"
    },
    {
        "title": "Is the European Monetary System converging to integration?",
        "authors": [
            "Loet Leydesdorff",
            "Nienke Oomes"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The emerging system at the European level can be conceptualized as a pattern\nof relations among member states that tends to be reproduced despite\ndisturbances in individual trajectories. The Markov property is used as an\nindicator of systemness in the distribution. The individual trajectories of\nnations participating in the European Monetary System is assessed using an\ninformation theoretical model that is consistent with the Markov property in\nthe multivariate case. Economic and monetary integration are analyzed using\nindependent data sets. Increasing integration can be retrieved in both of these\ndimensions, notably since the currency crises of 1992 and 1993. However, the\ndynamics for countries which have strongly coupled their currency to the German\nMark are different from those which did not. Additionally, developments in\ninflation and exchange rates at the European level are assessed in relation to\nglobal developments.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.4424v1"
    },
    {
        "title": "Analysis of Birth weight using Singular Value Decomposition",
        "authors": [
            "D. Nagarajan",
            "P. Sunitha",
            "V. Nagarajan",
            "V. Seethalekshmi"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The researchers have drawn much attention about the birth weight of newborn\nbabies in the last three decades. The birth weight is one of the vital roles in\nthe babys health. So many researchers such as (2),(1) and (4) analyzed the\nbirth weight of babies. The aim of this paper is to analyze the birth weight\nand some other birth weight related variable, using singular value\ndecomposition and multiple linear regression.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.2059v1"
    },
    {
        "title": "Effects of coarse-graining on the scaling behavior of long-range\n  correlated and anti-correlated signals",
        "authors": [
            "Yinlin Xu",
            "Qianli D. Y. Ma",
            "Daniel T. Schmitt",
            "Pedro Bernaola-Galván",
            "Plamen Ch. Ivanov"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We investigate how various coarse-graining methods affect the scaling\nproperties of long-range power-law correlated and anti-correlated signals,\nquantified by the detrended fluctuation analysis. Specifically, for\ncoarse-graining in the magnitude of a signal, we consider (i) the Floor, (ii)\nthe Symmetry and (iii) the Centro-Symmetry coarse-graining methods. We find,\nthat for anti-correlated signals coarse-graining in the magnitude leads to a\ncrossover to random behavior at large scales, and that with increasing the\nwidth of the coarse-graining partition interval $\\Delta$ this crossover moves\nto intermediate and small scales. In contrast, the scaling of positively\ncorrelated signals is less affected by the coarse-graining, with no observable\nchanges when $\\Delta<1$, while for $\\Delta>1$ a crossover appears at small\nscales and moves to intermediate and large scales with increasing $\\Delta$. For\nvery rough coarse-graining ($\\Delta>3$) based on the Floor and Symmetry\nmethods, the position of the crossover stabilizes, in contrast to the\nCentro-Symmetry method where the crossover continuously moves across scales and\nleads to a random behavior at all scales, thus indicating a much stronger\neffect of the Centro-Symmetry compared to the Floor and the Symmetry methods.\nFor coarse-graining in time, where data points are averaged in non-overlapping\ntime windows, we find that the scaling for both anti-correlated and positively\ncorrelated signals is practically preserved. The results of our simulations are\nuseful for the correct interpretation of the correlation and scaling properties\nof symbolic sequences.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.3834v1"
    },
    {
        "title": "Anomalous diffusion as a stochastic component in the dynamics of complex\n  processes",
        "authors": [
            "Serge F. Timashev",
            "Yuriy S. Polyakov",
            "Pavel I. Misurkin",
            "Sergey G. Lakeev"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We propose an interpolation expression using the difference moment\n(Kolmogorov transient structural function) of the second order as the average\ncharacteristic of displacements for identifying the anomalous diffusion in\ncomplex processes when the stochastic dynamics of the system under study\nreaches a steady state (large time intervals). Our procedure based on this\nexpression for identifying anomalous diffusion and calculating its parameters\nin complex processes is applied to the analysis of the dynamics of blinking\nfluorescence of quantum dots, X-ray emission from accreting objects, fluid\nvelocity in Rayleigh-B\\'enard convection, and geoelectrical signal for a\nseismic area. For all four examples, the proposed interpolation is able to\nadequately describe the stochastic part of the experimental difference moment,\nwhich implies that anomalous diffusion manifests itself in these complex\nprocesses. The results of this study make it possible to broaden the range of\ncomplex natural processes in which anomalous diffusion can be identified.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.0235v1"
    },
    {
        "title": "Probability flux as a method for detecting scaling",
        "authors": [
            "M. Ignaccolo",
            "P. Grigolini",
            "B. J. West"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We introduce a new method for detecting scaling in time series. The method\nuses the properties of the probability flux for stochastic self-affine\nprocesses and is called the probability flux analysis (PFA). The advantages of\nthis method are: 1) it is independent of the finiteness of the moments of the\nself-affine process; 2) it does not require a binning procedure for numerical\nevaluation of the the probability density function. These properties make the\nmethod particularly efficient for heavy tailed distributions in which the\nvariance is not finite, for example, in Levy alpha-stable processes. This\nutility is established using a comparison with the diffusion entropy (DE)\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.0372v1"
    },
    {
        "title": "Random matrix route to image denoising",
        "authors": [
            "Gaurab Basu",
            "Kaushik Ray",
            "Prasanta K. Panigrahi"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We make use of recent results from random matrix theory to identify a derived\nthreshold, for isolating noise from image features. The procedure assumes the\nexistence of a set of noisy images, where denoising can be carried out on\nindividual rows or columns independently. The fact that these are guaranteed to\nbe correlated makes the correlation matrix an ideal tool for isolating noise.\nThe random matrix result provides lowest and highest eigenvalues for the\nGaussian random noise for which case, the eigenvalue distribution function is\nanalytically known. This provides an ideal threshold for removing Gaussian\nrandom noise and thereby separating the universal noisy features from the\nnon-universal components belonging to the specific image under consideration.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.1356v1"
    },
    {
        "title": "Optimal adaptive multidimensional-time signal energy estimation on the\n  background noise",
        "authors": [
            "Eugene Ostrovsky",
            "Eugene Rogover",
            "Leonid Sirota"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We construct an adaptive asymptotically optimal in the classical norm of the\nspace L(2,\\Omega) of square integrable random variables the Energy estimation\nof a signal (function) observed in some points (plan of experiment) on the\nbackground noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.4653v1"
    },
    {
        "title": "Nested Sampling with Constrained Hamiltonian Monte Carlo",
        "authors": [
            "M. J. Betancourt"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Nested sampling is a powerful approach to Bayesian inference ultimately\nlimited by the computationally demanding task of sampling from a heavily\nconstrained probability distribution. An effective algorithm in its own right,\nHamiltonian Monte Carlo is readily adapted to efficiently sample from any\nsmooth, constrained distribution. Utilizing this constrained Hamiltonian Monte\nCarlo, I introduce a general implementation of the nested sampling algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.0157v1"
    },
    {
        "title": "Reply to \"Comment on 'Ensemble Kalman filter with the unscented\n  transform'\"",
        "authors": [
            "Xiaodong Luo",
            "Irene M. Moroz",
            "Ibrahim Hoteit"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  This is a reply to the comment of Dr Sakov on the work \"Ensemble Kalman\nfilter with the unscented transform\" of Luo and Moroz (2009).\n",
        "pdf_link": "http://arxiv.org/pdf/1005.2664v1"
    },
    {
        "title": "Scaled unscented transform Gaussian sum filter: theory and application",
        "authors": [
            "Xiaodong Luo",
            "Irene M. Moroz",
            "Ibrahim Hoteit"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  In this work we consider the state estimation problem in\nnonlinear/non-Gaussian systems. We introduce a framework, called the scaled\nunscented transform Gaussian sum filter (SUT-GSF), which combines two ideas:\nthe scaled unscented Kalman filter (SUKF) based on the concept of scaled\nunscented transform (SUT), and the Gaussian mixture model (GMM). The SUT is\nused to approximate the mean and covariance of a Gaussian random variable which\nis transformed by a nonlinear function, while the GMM is adopted to approximate\nthe probability density function (pdf) of a random variable through a set of\nGaussian distributions. With these two tools, a framework can be set up to\nassimilate nonlinear systems in a recursive way. Within this framework, one can\ntreat a nonlinear stochastic system as a mixture model of a set of sub-systems,\neach of which takes the form of a nonlinear system driven by a known Gaussian\nrandom process. Then, for each sub-system, one applies the SUKF to estimate the\nmean and covariance of the underlying Gaussian random variable transformed by\nthe nonlinear governing equations of the sub-system. Incorporating the\nestimations of the sub-systems into the GMM gives an explicit (approximate)\nform of the pdf, which can be regarded as a \"complete\" solution to the state\nestimation problem, as all of the statistical information of interest can be\nobtained from the explicit form of the pdf ...\n  This work is on the construction of the Gaussian sum filter based on the\nscaled unscented transform.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.2665v1"
    },
    {
        "title": "Zipf's Law for All the Natural Cities in the United States: A Geospatial\n  Perspective",
        "authors": [
            "Bin Jiang",
            "Tao Jia"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  This paper provides a new geospatial perspective on whether or not Zipf's law\nholds for all cities or for the largest cities in the United States using a\nmassive dataset and its computing. A major problem around this issue is how to\ndefine cities or city boundaries. Most of the investigations of Zipf's law rely\non the demarcations of cities imposed by census data, e.g., metropolitan areas\nand census-designated places. These demarcations or definitions (of cities) are\ncriticized for being subjective or even arbitrary. Alternative solutions to\ndefining cities are suggested, but they still rely on census data for their\ndefinitions. In this paper we demarcate urban agglomerations by clustering\nstreet nodes (including intersections and ends), forming what we call natural\ncities. Based on the demarcation, we found that Zipf's law holds remarkably\nwell for all the natural cities (over 2-4 million in total) across the United\nStates. There is little sensitivity for the holding with respect to the\nclustering resolution used for demarcating the natural cities. This is a big\ncontrast to urban areas, as defined in the census data, which do not hold\nstable for Zipf's law.\n  Keywords: Natural cities, power law, data-intensive geospatial computing,\nscaling of geographic space\n",
        "pdf_link": "http://arxiv.org/pdf/1006.0814v2"
    },
    {
        "title": "Generation of Noise Time Series with arbitrary Power Spectrum",
        "authors": [
            "M. Carrettoni",
            "O. Cremonesi"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Noise simulation is a very powerful tool in signal analysis helping to\nforesee the system performance in real experimental situations. Time series\ngeneration is however a hard challenge when a robust model of the noise sources\nis missing. We present here a simple computational technique which allows the\ngeneration of noise samples of fixed length, given a desired power spectrum. A\nfew applications of the method are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.3289v2"
    },
    {
        "title": "Component Ratios of Independent and Herding Betters in a Racetrack\n  Betting Market",
        "authors": [
            "Shintaro Mori",
            "Masato Hisakado"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We study the time series data of the racetrack betting market in the Japan\nRacing Association (JRA). As the number of votes t increases, the win bet\nfraction x(t) converges to the final win bet fraction x_{f}. We observe the\npower law $(x(t)-x_{f})^{2} \\propto t^{-\\beta}$ with $\\beta\\simeq 0.488$. We\nmeasure the degree of the completeness of the ordering of the horses using an\nindex AR, the horses are ranked according to the size of the win bet fraction.\nAR(t) also obeys the power law and behaves as\n$\\mbox{AR}_{f}-\\mbox{AR}(t)\\propto t^{-\\gamma}$ with $\\gamma\\simeq 0.589$,\nwhere AR_{f} is the final value of AR. We introduce a simple voting model with\ntwo types of voters-independent and herding. Independent voters provide\ninformation about the winning probability of the horses and herding voters\ndecide their votes based on the popularities of the horses.This model can\nexplain two power laws of the betting process. The component ratio of the\nindependent voter to the herding voter is 1:3.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.4884v1"
    },
    {
        "title": "Calculating the Exact Pooled Variance",
        "authors": [
            "Joseph W. Rudmin"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  An exact method of calculating the variance of a pooled data set is\npresented. Its major advantages over the many other methods are that it is\nsimple, is easily derived and remembered, and requires no assumptions. The\nresult can be concisely summarized as follows: \"The Exact pooled variance is\nthe mean of the variances plus the variance of the means of the component data\nsets.\" The proof is so simple that it has certainly been done many times\nbefore, but it is absent in the textbooks. Its practical significance is\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.1012v1"
    },
    {
        "title": "Strategies for processing diffraction data from randomly oriented\n  particles",
        "authors": [
            "Veit Elser"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  This note compares the single-shot and intensity cross-correlation proposals\nfor x-ray imaging of randomly oriented particles and shows very directly that\nthe latter will usually not be feasible even when the former is.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.3777v1"
    },
    {
        "title": "Interplay between positive feedbacks in the generalized CEV process",
        "authors": [
            "St. Reimann",
            "V. Gontis",
            "M. Alaburda"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The dynamics of the {\\em generalized} CEV process $dX_t = aX_t^n dt+ bX_t^m\ndW_t$ $(gCEV)$ is due to an interplay of two feedback mechanisms:\nState-to-Drift and State-to-Diffusion, whose degrees are $n$ and $m$\nrespectively. We particularly show that the gCEV, in which both feedback\nmechanisms are {\\sc positive}, i.e. $n,m>1$, admits a stationary probability\ndistribution $P$ provided that $n<2m-1$, which asymptotically decays as a power\nlaw $P(x) \\sim \\frac{1}{x^\\mu}$ with tail exponent $\\mu = 2m > 2$. Furthermore\nthe power spectral density obeys $S(f) \\sim \\frac{1}{f^\\beta}$, where $\\beta =\n2 - \\:\\frac{1+\\epsilon}{2(m-1)}$, $\\epsilon>0$. Bursting behavior of the gCEV\nis investigated numerically. Burst intensity $S$ and burst duration $T$ are\nshown to be related by $S\\sim T^2$.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.0568v1"
    },
    {
        "title": "From Univariate to Multivariate Uncertainty Calculation",
        "authors": [
            "Michael P. Krystek"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The Guide to the Expression of Uncertainty in Measurement (GUM) mainly deals\nwith measurement models having only a single output quantity. However, in many\ncases more than one output quantity is required, where all of them are related\nto a common set of input quantities. In order to evaluate the measurement\nuncertainties associated with estimated expectations of these output\nquantities, the uncertainty propagation as treated in the GUM requires an\nappropriate extension. This article will introduce the concept of the\nmultivariate uncertainty calculation as an extension of the univariate\nuncertainty calculation.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.2700v2"
    },
    {
        "title": "The RooStats Project",
        "authors": [
            "Lorenzo Moneta",
            "Kevin Belasco",
            "Kyle Cranmer",
            "Sven Kreiss",
            "Alfio Lazzaro",
            "Danilo Piparo",
            "Gregory Schott",
            "Wouter Verkerke",
            "Matthias Wolf"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  RooStats is a project to create advanced statistical tools required for the\nanalysis of LHC data, with emphasis on discoveries, confidence intervals, and\ncombined measurements. The idea is to provide the major statistical techniques\nas a set of C++ classes with coherent interfaces, so that can be used on\narbitrary model and datasets in a common way. The classes are built on top of\nthe RooFit package, which provides functionality for easily creating\nprobability models, for analysis combinations and for digital publications of\nthe results. We will present in detail the design and the implementation of the\ndifferent statistical methods of RooStats. We will describe the various classes\nfor interval estimation and for hypothesis test depending on different\nstatistical techniques such as those based on the likelihood function, or on\nfrequentists or bayesian statistics. These methods can be applied in complex\nproblems, including cases with multiple parameters of interest and various\nnuisance parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.1003v2"
    },
    {
        "title": "Nonlinear Sampling and Lebesgue's Integral Sums",
        "authors": [
            "Emanuel Gluskin"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We consider nonlinear, or \"event-dependent\", sampling, i.e. such that the\nsampling instances {tk} depend on the function being sampled. The use of such\nsampling in the construction of Lebesgue's integral sums is noted and discussed\nas regards physical measurement and also possible nonlinearity of singular\nsystems. Though the limit of the sums, i.e. Lebesgue's integral, is linear with\nregard to the function being integrated, these sums are nonlinear in the sense\nof the sampling. A relevant method of frequency detection not using any clock,\nand using the nonlinear sampling, is considered. The mathematics and the\nrealization arguments essentially complete each other.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.1080v3"
    },
    {
        "title": "Removal of zero-point drift from AB data and the statistical cost",
        "authors": [
            "H Erik Swanson",
            "Stephan Schlamminger"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Often the result of a scientific experiment is given by the difference of\nmeasurements in two configurations, denoted by A and B. Since the measurements\nare not obtained simultaneously, drift of the zero-point can bias the result.\nIn practice measurement patterns are used to minimize this bias. The time\nsequence AB followed by BA, for example, would cancel a linear drift in the\naverage difference A-B. We propose taking data with an alternating series\nABAB.., and removing drift with a post-hoc analysis. We present an analysis\nmethod that removes bias from the result for drift up to polynomial order p. A\nstatistical cost function c(N) is introduced to compare the uncertainty in the\nend result with that from using a raw data average. For a data set size N>30\nthe statistical cost is negligible. For N<30 the cost is plotted as a function\nof N and filter order p and the trade off between the size of the data set and\np is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.1894v1"
    },
    {
        "title": "A Bayesian Approach To Histogram Comparison",
        "authors": [
            "M. J. Betancourt"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Determining if two histograms are consistent, whether they have been drawn\nfrom the same underlying distribution or not, is a common problem in physics.\nExisting approaches are not only limited in power but also inapplicable to\nhistograms filled with importance weights, a common feature of Monte Carlo\nsimulations. From a Bayesian perspective, the comparison between a single\nunderlying distribution and two underlying distributions is readily solved\nwithin the context of model comparison. I introduce an implementation of\nBayesian model comparison to the problem, including the extension to importance\nsampling.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.5604v1"
    },
    {
        "title": "Signal induced Symmetry Breaking in Noise Statistical Properties of Data\n  Analysis",
        "authors": [
            "Luca Perotti",
            "Daniel Vrinceanu",
            "Daniel Bessis"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  From a time series whose data are embedded in heavy noise, we construct an\nHilbert space operator (J-operator) whose discrete spectrum represents the\nsignal while the essential spectrum located on the unit circle, is associated\nwith the noise. Furthermore the essential spectrum, in the absence of signal,\nis built from roots of unity (\"clock\" distribution). These results are\nindependent of the statistical properties of the noise that can be Gaussian,\nnon-Gaussian, pink or even without second moment (Levy). The presence of the\nsignal has for effect to break the clock angular distribution of the essential\nspectrum on the unit circle. A discontinuity, proportional to the intensity of\nthe signal, appears in the angular distribution. The sensitivity of this method\nis definitely better than standard techniques. We build an example that\nsupports our claims.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.6021v1"
    },
    {
        "title": "Improved iterative Bayesian unfolding",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  This paper reviews the basic ideas behind a Bayesian unfolding published some\nyears ago and improves their implementation. In particular, uncertainties are\nnow treated at all levels by probability density functions and their\npropagation is performed by Monte Carlo integration. Thus, small numbers are\nbetter handled and the final uncertainty does not rely on the assumption of\nnormality. Theoretical and practical issues concerning the iterative use of the\nalgorithm are also discussed. The new program, implemented in the R language,\nis freely available, together with sample scripts to play with toy models.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.0632v1"
    },
    {
        "title": "A characterization of horizontal visibility graphs and combinatorics on\n  words",
        "authors": [
            "Gregory Gutin",
            "Toufik Mansour",
            "Simone Severini"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  An Horizontal Visibility Graph (for short, HVG) is defined in association\nwith an ordered set of non-negative reals. HVGs realize a methodology in the\nanalysis of time series, their degree distribution being a good discriminator\nbetween randomness and chaos [B. Luque, et al., Phys. Rev. E 80 (2009),\n046103]. We prove that a graph is an HVG if and only if outerplanar and has a\nHamilton path. Therefore, an HVG is a noncrossing graph, as defined in\nalgebraic combinatorics [P. Flajolet and M. Noy, Discrete Math., 204 (1999)\n203-229]. Our characterization of HVGs implies a linear time recognition\nalgorithm. Treating ordered sets as words, we characterize subfamilies of HVGs\nhighlighting various connections with combinatorial statistics and introducing\nthe notion of a visible pair. With this technique we determine asymptotically\nthe average number of edges of HVGs.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.1850v1"
    },
    {
        "title": "Cruising The Simplex: Hamiltonian Monte Carlo and the Dirichlet\n  Distribution",
        "authors": [
            "M. J. Betancourt"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Due to its constrained support, the Dirichlet distribution is uniquely suited\nto many applications. The constraints that make it powerful, however, can also\nhinder practical implementations, particularly those utilizing Markov Chain\nMonte Carlo (MCMC) techniques such as Hamiltonian Monte Carlo. I demonstrate a\nseries of transformations that reshape the canonical Dirichlet distribution\ninto a form much more amenable to MCMC algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.3436v4"
    },
    {
        "title": "On detecting determinism and nonlinearity in microelectrode recording\n  signals: Approach based on non-stationary surrogate data methods",
        "authors": [
            "D. Guarín",
            "A. Orozco",
            "E. Delgado",
            "E. Guijarro"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Two new surrogate methods, the Small Shuffle Surrogate (SSS) and the\nTruncated Fourier Transform Surrogate (TFTS), have been proposed to study\nwhether there are some kind of dynamics in irregular fluctuations and if so\nwhether these dynamics are linear or not, even if this fluctuations are\nmodulated by long term trends. This situation is theoretically incompatible\nwith the assumption underlying previously proposed surrogate methods. We apply\nthe SSS and TFTS methods to microelectrode recording (MER) signals from\ndifferent brain areas, in order to acquire a deeper understanding of them.\nThrough our methodology we conclude that the irregular fluctuations in MER\nsignals possess some determinism.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.4363v1"
    },
    {
        "title": "Analysis of stochastic time series in the presence of strong measurement\n  noise",
        "authors": [
            "Bernd Lehle"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  A new approach for the analysis of Langevin-type stochastic processes in the\npresence of strong measurement noise is presented. For the case of Gaussian\ndistributed, exponentially correlated, measurement noise it is possible to\nextract the strength and the correlation time of the noise as well as\npolynomial approximations of the drift and diffusion functions from the\nunderlying Langevin equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.5641v2"
    },
    {
        "title": "p-Values for Model Evaluation",
        "authors": [
            "Frederik Beaujean",
            "Allen Caldwell",
            "Daniel Kollar",
            "Kevin Kroeninger"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Deciding whether a model provides a good description of data is often based\non a goodness-of-fit criterion summarized by a p-value. Although there is\nconsiderable confusion concerning the meaning of p-values, leading to their\nmisuse, they are nevertheless of practical importance in common data analysis\ntasks. We motivate their application using a Bayesian argumentation. We then\ndescribe commonly and less commonly known discrepancy variables and how they\nare used to define p-values. The distribution of these are then extracted for\nexamples modeled on typical data analysis tasks, and comments on their\nusefulness for determining goodness-of-fit are given.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.1674v3"
    },
    {
        "title": "Super-extreme event's influence on a Weierstrass-Mandelbrot\n  Continuous-Time Random Walk",
        "authors": [
            "Tomasz Gubiec",
            "Tomasz R. Werner",
            "Ryszard Kutner",
            "Didier Sornette"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Two utmost cases of super-extreme event's influence on the velocity\nautocorrelation function (VAF) were considered. The VAF itself was derived\nwithin the hierarchical Weierstrass-Mandelbrot Continuous-Time Random Walk\n(WM-CTRW) formalism, which is able to cover a broad spectrum of continuous-time\nrandom walks. Firstly, we studied a super-extreme event in a form of a\nsustained drift, whose duration time is much longer than that of any other\nevent. Secondly, we considered a super-extreme event in the form of a shock\nwith the size and velocity much larger than those corresponding to any other\nevent. We found that the appearance of these super-extreme events substantially\nchanges the results determined by extreme events (the so called \"black swans\")\nthat are endogenous to the WM-CTRW process. For example, changes of the VAF in\nthe latter case are in the form of some instability and distinctly differ from\nthose caused in the former case. In each case these changes are quite different\ncompared to the situation without super-extreme events suggesting the\npossibility to detect them in natural system if they occur.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.6472v1"
    },
    {
        "title": "On combining significances. Some trivial examples",
        "authors": [
            "N. V. Krasnikov",
            "S. I. Bityukov"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  For Poisson distribution $Pois(n, \\lambda)$ with $\\lambda \\gg 1$, $n \\gg 1$\nwe propose to determine significance as $S =\n\\frac{n_{obs}-\\lambda}{\\sqrt{\\lambda}}$. The significance $S$ coincides up to\nsign with often used significance. For experiments which measure the same\nquantities the natural but not unique rule for significance combining is\n$S_{comb}(S_1, S_2) =\n\\frac{S_1\\sigma_1+S_2\\sigma_2}{\\sqrt{\\sigma^2_1+\\sigma^2_2}}$, where $\\sigma_1$\nand $\\sigma_2$ are variations. We also propose the rule for significances\ncombining for the case with systematic errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.0465v1"
    },
    {
        "title": "ISO Filters in Precision Engineering and Production Measurement",
        "authors": [
            "Michael P. Krystek"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Filters are used in precision engineering and production measurement to\nsuppress unwanted components in measured data, to reconstruct the sampled\nsurface in case of tactile measurements, or to separate portions of different\nscales. The application of such filters comprises a wide area, in essence,\ncoordinate measurement, form and contour measurement, as well as surface\ntexture measurement. So far, almost exclusively the Gaussian filter, as\nstandardised in ISO 11562, has been used in this area. But for some time past,\na new series of ISO documents (ISO/TS 16610) has been available which - besides\nthe Gaussian filter which is still available - specifies additional filters for\nthe field of geometrical product specification and verification. This\npublication gives an overview of these new ISO filters.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.0678v2"
    },
    {
        "title": "Monte Carlo uncertainty estimation for wavelength calibration",
        "authors": [
            "Thang H. L.",
            "Nguyen D. D.",
            "Dung D. N."
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  CIPM published the Supplement I for GUM in 2008 as not only an alternative\napproach to estimate the uncertainty for a given calibration measurement but\nalso as a proper uncertainty estimation one, whenever any of the conditions\nimposed in GUM which must be satisfied does not hold [1, 2]. Before the\nintroduction of the new approach in the Supplement I, namely Monte Carlo (MC)\nmethod, the GUM rules have been always applied even if in cases where the\nmentioned conditions were not fulfilled. After or even before the official\nintroduction of this MC method, a number of published papers in uncertainty\nestimation by using MC method had been shown up, giving more insight for the\nways the uncertainties estimated and also for the specific calibration\nmeasurements under investigation themselves [3 - 16]. However, in most of those\npublished papers, the application conditions required before a method selection\nshould be analyzed, in fact, has been ignored. In our uncertainty study for\nwavelength calibration by using beat frequency technique, the application\nconditions have not been ignored but analyzed carefully and it was found that a\nGUM application condition was unsatisfactory. However, all the MC method\nconditions for this calibration problem have been well fulfilled. A computer\nprogram was then coded [17] following the steps in the Supplement I. The\nresultant outputs showed difference in statistical nature and suggested to\nreplace the MC method to the GUM one as soon as possible for this case.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.2631v2"
    },
    {
        "title": "Unsupervised clustering analysis: a multiscale complex networks approach",
        "authors": [
            "Clara Granell",
            "Sergio Gomez",
            "Alex Arenas"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Unsupervised clustering, also known as natural clustering, stands for the\nclassification of data according to their similarities. Here we study this\nproblem from the perspective of complex networks. Mapping the description of\ndata similarities to graphs, we propose to extend two multiresolution\nmodularity based algorithms to the finding of modules (clusters) in general\ndata sets producing a multiscales' solution. We show the performance of these\nreported algorithms to the classification of a standard benchmark of data\nclustering and compare their performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.1890v1"
    },
    {
        "title": "Matrix Element Method in HEP: Transfer Functions, Efficiencies, and\n  Likelihood Normalization",
        "authors": [
            "Igor Volobouev"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  This article surveys the procedures used for deriving detector transfer\nfunctions and normalizing probability densities for the statistical analysis\ntechnique known as the \"matrix element method\" in the context of high energy\nphysics (HEP) data analysis. Common misconceptions about transfer functions and\nefficiencies are pointed out and clarified.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.2259v1"
    },
    {
        "title": "Robustness of Estimators of Long-Range Dependence and Self-Similarity\n  under non-Gaussianity",
        "authors": [
            "Christian L. E. Franzke",
            "Timothy Graves",
            "Nicholas W. Watkins",
            "Robert B. Gramacy",
            "Cecilia Hughes"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Long-range dependence and non-Gaussianity are ubiquitous in many natural\nsystems like ecosystems, biological systems and climate. However, it is not\nalways appreciated that both phenomena may occur together in natural systems\nand that self-similarity in a system can be a superposition of both phenomena.\nThese features, which are common in complex systems, impact the attribution of\ntrends and the occurrence and clustering of extremes. The risk assessment of\nsystems with these properties will lead to different outcomes (e.g. return\nperiods) than the more common assumption of independence of extremes. Two\nparadigmatic models are discussed which can simultaneously account for\nlong-range dependence and non-Gaussianity: Autoregressive Fractional Integrated\nMoving Average (ARFIMA) and Linear Fractional Stable Motion (LFSM). Statistical\nproperties of estimators for long-range dependence and self-similarity are\ncritically assessed. It is found that the most popular estimators can be biased\nin the presence of important features of many natural systems like trends and\nmultiplicative noise. Also the long-range dependence and non-Gaussianity of two\ntypical natural time series are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.5018v3"
    },
    {
        "title": "Scaling of Geographic Space as a Universal Rule for Map Generalization",
        "authors": [
            "Bin Jiang",
            "Xintao Liu",
            "Tao Jia"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Map generalization is a process of producing maps at different levels of\ndetail by retaining essential properties of the underlying geographic space. In\nthis paper, we explore how the map generalization process can be guided by the\nunderlying scaling of geographic space. The scaling of geographic space refers\nto the fact that in a geographic space small things are far more common than\nlarge ones. In the corresponding rank-size distribution, this scaling property\nis characterized by a heavy tailed distribution such as a power law, lognormal,\nor exponential function. In essence, any heavy tailed distribution consists of\nthe head of the distribution (with a low percentage of vital or large things)\nand the tail of the distribution (with a high percentage of trivial or small\nthings). Importantly, the low and high percentages constitute an imbalanced\ncontrast, e.g., 20 versus 80. We suggest that map generalization is to retain\nthe objects in the head and to eliminate or aggregate those in the tail. We\napplied this selection rule or principle to three generalization experiments,\nand found that the scaling of geographic space indeed underlies map\ngeneralization. We further relate the universal rule to T\\\"opfer's radical law\n(or trained cartographers' decision making in general), and illustrate several\nadvantages of the universal rule.\n  Keywords: Head/tail division rule, head/tail breaks, heavy tailed\ndistributions, power law, and principles of selection\n",
        "pdf_link": "http://arxiv.org/pdf/1102.1561v3"
    },
    {
        "title": "Estimation of Kramers-Moyal coefficients at low sampling rates",
        "authors": [
            "Christoph Honisch",
            "Rudolf Friedrich"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  A new optimization procedure for the estimation of Kramers-Moyal coefficients\nfrom stationary, one-dimensional, Markovian time series data is presented. The\nmethod takes advantage of a recently reported approach that allows to calculate\nexact finite sampling interval effects by solving the adjoint Fokker-Planck\nequation. Therefore it is well suited for the analysis of sparsely sampled time\nseries. The optimization can be performed either making a parametric ansatz for\ndrift and diffusion functions or also parameter free. We demonstrate the power\nof the method in several numerical examples with synthetic time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.5264v1"
    },
    {
        "title": "Discrete simulation of power law noise",
        "authors": [
            "Neil Ashby"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  A method for simulating power law noise in clocks and oscillators is\npresented based on modification of the spectrum of white phase noise, then\nFourier transforming to the time domain. Symmetric real matrices are introduced\nwhose traces--the sums of their eigenvalues--are equal to the Allan variances,\nin overlapping or non-overlapping forms, as well as for the corresponding forms\nof the modified Allan variance. Diagonalization of these matrices leads to\nexpressions for the probability distributions for observing a variance at an\narbitrary value of the sampling or averaging interval $\\tau$, and hence for\nestimating confidence in the measurements. A number of applications are\npresented for the common power-law noises.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5062v1"
    },
    {
        "title": "A precise method for visualizing dispersive features in image plots",
        "authors": [
            "P. Zhang",
            "P. Richard",
            "T. Qian",
            "Y. -M. Xu",
            "X. Dai",
            "H. Ding"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In order to improve the advantages and the reliability of the second\nderivative method in tracking the position of extrema from experimental curves,\nwe develop a novel analysis method based on the mathematical concept of\ncurvature. We derive the formulas for the curvature in one and two dimensions\nand demonstrate their applicability to simulated and experimental\nangle-resolved photoemission spectroscopy data. As compared to the second\nderivative, our new method improves the localization of the extrema and reduces\nthe peak broadness for a better visualization on intensity image plots.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.1524v2"
    },
    {
        "title": "Subgame perfect implementation: A new result",
        "authors": [
            "Haoyang Wu"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  This paper concerns what will happen if quantum mechanics is concerned in\nsubgame perfect implementation. The main result is: When additional conditions\nare satisfied, the traditional characterization on subgame perfect\nimplementation shall be amended by virtue of a quantum stage mechanism.\nFurthermore, by using an algorithmic stage mechanism, this amendment holds in\nthe macro world too.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.2479v2"
    },
    {
        "title": "Least-squares fit to a straight line when each variable contains all\n  equal errors",
        "authors": [
            "Alessandro Petrolini"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The least squares fit to a straight line, when both variables are affected by\nall equal uncorrelated errors, leads to very simple results for both the\nestimated parameters and their standard errors, of widespread applicability. In\nthis paper several formulas are derived, presenting a full set of results about\nthe estimated parameters and their standard errors. All the results have been\nvalidated with extensive Monte Carlo simulations. The emphasis of the paper is\non the calculation and properties of the best-fit parameters and their standard\nerrors.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.3132v3"
    },
    {
        "title": "Detection of trend changes in time series using Bayesian inference",
        "authors": [
            "Nadine Schütz",
            "Matthias Holschneider"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Change points in time series are perceived as isolated singularities where\ntwo regular trends of a given signal do not match. The detection of such\ntransitions is of fundamental interest for the understanding of the system's\ninternal dynamics. In practice observational noise makes it difficult to detect\nsuch change points in time series. In this work we elaborate a Bayesian method\nto estimate the location of the singularities and to produce some confidence\nintervals. We validate the ability and sensitivity of our inference method by\nestimating change points of synthetic data sets. As an application we use our\nalgorithm to analyze the annual flow volume of the Nile River at Aswan from\n1871 to 1970, where we confirm a well-established significant transition point\nwithin the time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.3448v1"
    },
    {
        "title": "Predicting catastrophes in nonlinear dynamical systems by compressive\n  sensing",
        "authors": [
            "Wen-Xu Wang",
            "Rui Yang",
            "Ying-Cheng Lai",
            "Vassilios Kovanis",
            "Celso Grebogi"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  An extremely challenging problem of significant interest is to predict\ncatastrophes in advance of their occurrences. We present a general approach to\npredicting catastrophes in nonlinear dynamical systems under the assumption\nthat the system equations are completely unknown and only time series\nreflecting the evolution of the dynamical variables of the system are\navailable. Our idea is to expand the vector field or map of the underlying\nsystem into a suitable function series and then to use the compressive-sensing\ntechnique to accurately estimate the various terms in the expansion. Examples\nusing paradigmatic chaotic systems are provided to demonstrate our idea.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.0462v1"
    },
    {
        "title": "Common Arc Method for Diffraction Pattern Orientation",
        "authors": [
            "Gábor Bortel",
            "Miklós Tegze"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Very short pulses of x-ray free-electron lasers opened the way to obtain\ndiffraction signal from single particles beyond the radiation dose limit. For\n3D structure reconstruction many patterns are recorded in the object's unknown\norientation. We describe a method for orientation of continuous diffraction\npatterns of non-periodic objects, utilizing intensity correlations in the\ncurved intersections of the corresponding Ewald spheres, hence named Common Arc\norientation. Present implementation of the algorithm optionally takes into\naccount the Friedel law, handles missing data and is capable to determine the\npoint group of symmetric objects. Its performance is demonstrated on simulated\ndiffraction datasets and verification of the results indicates high orientation\naccuracy even at low signal levels. The Common Arc method fills a gap in the\nwide palette of the orientation methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.1926v1"
    },
    {
        "title": "On the Four Types of Weight Functions for Spatial Contiguity Matrix",
        "authors": [
            "Yanguang Chen"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  This is a \"spatial autocorrelation analysis\" of spatial autocorrelation. I\nuse the 1-dimension spatial autocorrelation function (ACF) and partial\nautocorrelation function (PACF) to analyze four kinds of weight function in\ncommon use for the 2-dimensional spatial autocorrelation model. The aim of this\nstudy is at how to select a proper weight function to construct a spatial\ncontiguity matrix for spatial analysis. The scopes of application of different\nweight functions are defined in terms of the characters of their ACFs and\nPACFs.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.0824v1"
    },
    {
        "title": "Calculating a confidence interval on the sum of binned leakage",
        "authors": [
            "Ian Ruchlin",
            "Richard W. Schnee"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Calculating the expected number of misclassified outcomes is a standard\nproblem of particular interest for rare-event searches. The Clopper-Pearson\nmethod allows calculation of classical confidence intervals on the amount of\nmisclassification if data are all drawn from the same binomial probability\ndistribution. However, data is often better described by breaking it up into\nseveral bins, each represented by a different binomial distribution. We\ndescribe and provide an algorithm for calculating a classical confidence\ninterval on the expected total number of misclassified events from several\nbins, based on calibration data with the same probability of misclassification\non a bin-by-bin basis. Our method avoids a computationally intensive\nmultidimensional search by introducing a Lagrange multiplier and performing\nstandard root finding. This method has only quadratic time complexity as the\nnumber of bins, and produces confidence intervals that are only slightly\nconservative.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.6296v2"
    },
    {
        "title": "Likelihood based observability analysis and confidence intervals for\n  predictions of dynamic models",
        "authors": [
            "Clemens Kreutz",
            "Andreas Raue",
            "Jens Timmer"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Mechanistic dynamic models of biochemical networks such as Ordinary\nDifferential Equations (ODEs) contain unknown parameters like the reaction rate\nconstants and the initial concentrations of the compounds. The large number of\nparameters as well as their nonlinear impact on the model responses hamper the\ndetermination of confidence regions for parameter estimates. At the same time,\nclassical approaches translating the uncertainty of the parameters into\nconfidence intervals for model predictions are hardly feasible.\n  In this article it is shown that a so-called prediction profile likelihood\nyields reliable confidence intervals for model predictions, despite arbitrarily\ncomplex and high-dimensional shapes of the confidence regions for the estimated\nparameters. Prediction confidence intervals of the dynamic states allow a\ndata-based observability analysis. The approach renders the issue of sampling a\nhigh-dimensional parameter space into evaluating one-dimensional prediction\nspaces. The method is also applicable if there are non-identifiable parameters\nyielding to some insufficiently specified model predictions that can be\ninterpreted as non-observability. Moreover, a validation profile likelihood is\nintroduced that should be applied when noisy validation experiments are to be\ninterpreted.\n  The properties and applicability of the prediction and validation profile\nlikelihood approaches are demonstrated by two examples, a small and instructive\nODE model describing two consecutive reactions, and a realistic ODE model for\nthe MAP kinase signal transduction pathway. The presented general approach\nconstitutes a concept for observability analysis and for generating reliable\nconfidence intervals of model predictions, not only, but especially suitable\nfor mathematical models of biological systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.0013v1"
    },
    {
        "title": "Solution to Banff 2 Challenge Based on Likelihood Ratio Test",
        "authors": [
            "Wolfgang A Rolke"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We describe our solution to the Banff 2 challenge problems as well as the\noutcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.0458v1"
    },
    {
        "title": "Non-commutative tomography: A tool for data analysis and signal\n  processing",
        "authors": [
            "F. Briolle",
            "V. I. Man'ko",
            "B. Ricaud",
            "R. Vilela Mendes"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Tomograms, a generalization of the Radon transform to arbitrary pairs of\nnon-commuting operators, are positive bilinear transforms with a rigorous\nprobabilistic interpretation which provide a full characterization of the\nsignal and are robust in the presence of noise. We provide an explicit\nconstruction of tomogram transforms for many pairs of noncommuting operators in\none and two dimensions and illustrations of their use for denoising, detection\nof small signals and component separation.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.0929v1"
    },
    {
        "title": "How to produce discreet Gaussian sequences: Algorithm and code",
        "authors": [
            "Sparisoma Viridi",
            "Veinardi Suendo"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Algorithm and code to produce sequences whose members obey Gaussian\ndistribution function is reported. Discreet and limited number of groups are\ndefined in the distribution function, where each group is represented only with\none value instead of a range of value. The produced sequences are also checked\nback whether they still fit the discreet distribution function. Increasing of\nnumber of particles N increases the value of correlation coefficient R^2, but\nincreasing number of groups M reduces it. Value R^2 = 1 can be found for N =\n1000000 at least with M = 5000$ and for M = 10 at least with N = 1000.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.3291v1"
    },
    {
        "title": "Particle Kalman Filtering: A Nonlinear Bayesian Framework for Ensemble\n  Kalman Filters",
        "authors": [
            "Ibrahim Hoteit",
            "Xiaodong Luo",
            "Dinh-Tuan Pham"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  This paper investigates an approximation scheme of the optimal nonlinear\nBayesian filter based on the Gaussian mixture representation of the state\nprobability distribution function. The resulting filter is similar to the\nparticle filter, but is different from it in that, the standard weight-type\ncorrection in the particle filter is complemented by the Kalman-type correction\nwith the associated covariance matrices in the Gaussian mixture. We show that\nthis filter is an algorithm in between the Kalman filter and the particle\nfilter, and therefore is referred to as the particle Kalman filter (PKF). In\nthe PKF, the solution of a nonlinear filtering problem is expressed as the\nweighted average of an \"ensemble of Kalman filters\" operating in parallel.\nRunning an ensemble of Kalman filters is, however, computationally prohibitive\nfor realistic atmospheric and oceanic data assimilation problems. For this\nreason, we consider the construction of the PKF through an \"ensemble\" of\nensemble Kalman filters (EnKFs) instead, and call the implementation the\nparticle EnKF (PEnKF). We show that different types of the EnKFs can be\nconsidered as special cases of the PEnKF. Similar to the situation in the\nparticle filter, we also introduce a re-sampling step to the PEnKF in order to\nreduce the risk of weights collapse and improve the performance of the filter.\nNumerical experiments with the strongly nonlinear Lorenz-96 model are presented\nand discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.0168v1"
    },
    {
        "title": "Detecting series periodicity with horizontal visibility graphs",
        "authors": [
            "Angel M. Núñez",
            "Lucas Lacasa",
            "Eusebio Valero",
            "Jose Patricio Gómez",
            "Bartolo Luque"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The horizontal visibility algorithm has been recently introduced as a mapping\nbetween time series and networks. The challenge lies in characterizing the\nstructure of time series (and the processes that generated those series) using\nthe powerful tools of graph theory. Recent works have shown that the visibility\ngraphs inherit several degrees of correlations from their associated series,\nand therefore such graph theoretical characterization is in principle possible.\nHowever, both the mathematical grounding of this promising theory and its\napplications are on its infancy. Following this line, here we address the\nquestion of detecting hidden periodicity in series polluted with a certain\namount of noise. We first put forward some generic properties of horizontal\nvisibility graphs which allow us to define a (graph theoretical) noise\nreduction filter. Accordingly, we evaluate its performance for the task of\ncalculating the period of noisy periodic signals, and compare our results with\nstandard time domain (autocorrelation) methods. Finally, potentials,\nlimitations and applications are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.1693v1"
    },
    {
        "title": "Accuracy versus speed in fluctuation-enhanced sensing",
        "authors": [
            "P. Makra",
            "Z. Topalian",
            "C. G. Granqvist",
            "L. B. Kish",
            "C. Kwan"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Fluctuation-enhanced sensing comprises the analysis of the stochastic\ncomponent of the sensor signal and the utilization of the microscopic dynamics\nof the interaction between the agent and the sensor. We study the relationship\nbetween the measurement time window and the statistical error of the\nmeasurement data in the simplest case, when the output is the mean-square value\nof the stochastic signal. This situation is relevant at any practical case when\nthe time window is finite, for example, when a sampling of the output of a\nfluctuation-enhanced array takes place; or a single sensor's activation\n(temperature, etc) is stepped up; or a single sensor's output is monitored by\nsampling subsequently in different frequency windows. Our study provides a\nlower limit of the relative error versus data window size with different types\nof power density spectra: white noise, 1/f (flicker, pink) noise, and 1/f^2\n(red) noise spectra.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.1957v3"
    },
    {
        "title": "Failure of Numerical modeling of 3-D Position Reconstruction from\n  3-Axial Planar Spiral Coil Sensor Sensitivity due to Existence of Quadratic\n  Terms",
        "authors": [
            "Edi Sanjaya",
            "Mitra Djamal",
            "Sparisoma Viridi"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  A sensitivity profile of a planar spiral coil sensor (PSCS) is proposed and\nis used to generate the relation of 3-D position of object observed using three\nPSCSs, one in each x, y, and z axis to the sensors response. A numerical\nprocedure using self consistent field-like method to reconstruct the real\nposition of observed object from sensor sensitivity is presented and the\nresults are discussed. Unfortunately, the procedure fails to approach the\ndesired results due to the existence of quadratic terms.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.4185v2"
    },
    {
        "title": "Negatively Biased Relevant Subsets Induced by the Most-Powerful\n  One-Sided Upper Confidence Limits for a Bounded Physical Parameter",
        "authors": [
            "Robert D. Cousins"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Suppose an observable x is the measured value (negative or non-negative) of a\ntrue mean mu (physically non-negative) in an experiment with a Gaussian\nresolution function with known fixed rms deviation s. The most powerful\none-sided upper confidence limit at 95% C.L. is UL = x+1.64s, which I refer to\nas the \"original diagonal line\". Perceived problems in HEP with small or\nnon-physical upper limits for x<0 historically led, for example, to\nsubstitution of max(0,x) for x, and eventually to abandonment in the Particle\nData Group's Review of Particle Physics of this diagonal line relationship\nbetween UL and x. Recently Cowan, Cranmer, Gross, and Vitells (CCGV) have\nadvocated a concept of \"power constraint\" that when applied to this problem\nyields variants of diagonal line, including UL = max(-1,x)+1.64s. Thus it is\ntimely to consider again what is problematic about the original diagonal line,\nand whether or not modifications cure these defects. In a 2002 Comment,\nstatistician Leon Jay Gleser pointed to the literature on recognizable and\nrelevant subsets. For upper limits given by the original diagonal line, the\nsample space for x has recognizable relevant subsets in which the quoted 95%\nC.L. is known to be negatively biased (anti-conservative) by a finite amount\nfor all values of mu. This issue is at the heart of a dispute between Jerzy\nNeyman and Sir Ronald Fisher over fifty years ago, the crux of which is the\nrelevance of pre-data coverage probabilities when making post-data inferences.\nThe literature describes illuminating connections to Bayesian statistics as\nwell. Methods such as that advocated by CCGV have 100% unconditional coverage\nfor certain values of mu and hence formally evade the traditional criteria for\nnegatively biased relevant subsets; I argue that concerns remain. Comparison\nwith frequentist intervals advocated by Feldman and Cousins also sheds light on\nthe issues.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.2023v1"
    },
    {
        "title": "On the Performance of P2P Network: An Assortment Method",
        "authors": [
            "Yuqing Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  P2P systems have grown dramatically in recent years. The most popular type of\nP2P systems are file sharing networks, which are used to share various types of\ncontent over the Internet. Due to the increase in popularity of P2P systems,\nthe network performance of these systems has become a very important issue in\nthe design and realization of these networks. Hence, the performance of the P2P\nhas been improved. This paper will suggest the following methods for the\nimprovement of the P2P systems: Method-1: Improve the P2P routing by using a\nsandwich technique. Method-2: Improve the search performance by introducing a\nnew search based on the super peer. Method-3: Improving the search by\nintroducing a ranking algorithm based on the knowledge database. The system\ndemonstrates that the methods introduced here have the improved efficiency\ncompared to the previous methodologies. So, the results show that the\nperformance of the P2P systems have been improved by using the above methods,\nhence the traffic can be reduced.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.2611v1"
    },
    {
        "title": "On the average of inconsistent data",
        "authors": [
            "Giovanni Mana",
            "Maria Mirabela Predescu"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  When data do not conform to the hypothesis of a known sampling-variance, the\nfitting of a constant to the set of measured values is a long debated problem.\nGiven the data, the fitting would require to find which measurand value is most\nprobable. A fitting procedure is here reviewed which assigns probabilities to\nthe possible measurand values, on the assumption that the uncertainty\nassociated with each datum is the lower bound to the standard deviation. This\nprocedure is applied to derive an estimate of the Planck constant.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.5395v1"
    },
    {
        "title": "Does the measured value of the Planck constant depend on the energy of\n  measurements?",
        "authors": [
            "Enrico Massa",
            "Giovanni Mana",
            "Michael Jentschel"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The measurement of the Avogadro constant opened the way to a comparison of\nthe watt-balance measurements of the Planck constant with the values calculated\nfrom the quotients of the Planck constant and the mass of a particle or an\natom. Since the energy scales of these measurements span nine energy decades,\nthese data provide insight into the consistency of our understanding of\nphysics.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.0677v1"
    },
    {
        "title": "Estimation of drift and diffusion functions from time series data: A\n  maximum likelihood framework",
        "authors": [
            "David Kleinhans"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Complex systems are characterized by a huge number of degrees of freedom\noften interacting in a non-linear manner. In many cases macroscopic states,\nhowever, can be characterized by a small number of order parameters that obey\nstochastic dynamics in time. Recently techniques for the estimation of the\ncorresponding stochastic differential equations from measured data have been\nintroduced. This contribution develops a framework for the estimation of the\nfunctions and their respective (Bayesian posterior) confidence regions based on\nlikelihood estimators. In succession approximations are introduced that\nsignificantly improve the efficiency of the estimation procedure. While being\nconsistent with standard approaches to the problem this contribution solves\nimportant problems concerning the applicability and the accuracy of estimated\nparameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.1258v2"
    },
    {
        "title": "Fitting in a complex chi^2 landscape using an optimized hypersurface\n  sampling",
        "authors": [
            "L. C. Pardo",
            "M. Rovira-Esteva",
            "S. Busch",
            "J. -F. Moulin",
            "J. Ll. Tamarit"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Fitting a data set with a parametrized model can be seen geometrically as\nfinding the global minimum of the chi^2 hypersurface, depending on a set of\nparameters {P_i}. This is usually done using the Levenberg-Marquardt algorithm.\nThe main drawback of this algorithm is that despite of its fast convergence, it\ncan get stuck if the parameters are not initialized close to the final\nsolution. We propose a modification of the Metropolis algorithm introducing a\nparameter step tuning that optimizes the sampling of parameter space. The\nability of the parameter tuning algorithm together with simulated annealing to\nfind the global chi^2 hypersurface minimum, jumping across chi^2{P_i} barriers\nwhen necessary, is demonstrated with synthetic functions and with real data.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.2046v1"
    },
    {
        "title": "On computation of a common mean",
        "authors": [
            "Zinovy Malkin"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Combining several independent measurements of the same physical quantity is\none of the most important tasks in metrology. Small samples, biased input\nestimates, not always adequate reported uncertainties, and unknown error\ndistribution make a rigorous solution very difficult, if not impossible. For\nthis reason, many methods to compute a common mean and its uncertainty were\nproposed, each with own advantages and shortcomings. Most of them are variants\nof the weighted average (WA) approach with different strategies to compute WA\nand its standard deviation. Median estimate became also increasingly popular\nduring recent years. In this paper, these two methods in most widely used\nmodifications are compared using simulated and real data. To overcome some\nproblems of known approaches to compute the WA uncertainty, a new combined\nestimate has been proposed. It has been shown that the proposed method can help\nto obtain more robust and realistic estimate suitable for both consistent and\ndiscrepant measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.6639v1"
    },
    {
        "title": "musrfit: A free platform-independent framework for muSR data analysis",
        "authors": [
            "A. Suter",
            "B. M. Wojek"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  A free data-analysis framework for muSR has been developed. musrfit is fully\nwritten in C++, is running under GNU/Linux, Mac OS X, as well as Microsoft\nWindows, and is distributed under the terms of the GNU GPL. It is based on the\nCERN ROOT framework and is utilizing the Minuit optimization routines for\nfitting. It consists of a set of programs allowing the user to analyze and\nvisualize the data. The fitting process is controlled by an ascii-input file\nwith an extended syntax. A dedicated text editor is helping the user to create\nand handle these files in an efficient way, execute the fitting, show the data,\nget online help, and so on. A versatile tool for the generation of new input\nfiles and the extraction of fit parameters is provided as well. musrfit\nfacilitates a plugin mechanism allowing to invoke user-defined functions.\nHence, the functionality of the framework can be extended with a minimal amount\nof overhead for the user. Currently, musrfit can read the following facility\nraw-data files: PSI-BIN, MDU (PSI), ROOT (LEM/PSI), WKM (outdated ascii\nformat), MUD (TRIUMF), NeXus (ISIS).\n",
        "pdf_link": "http://arxiv.org/pdf/1111.1569v1"
    },
    {
        "title": "Plotting the Differences Between Data and Expectation",
        "authors": [
            "Georgios Choudalakis",
            "Diego Casadei"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  This article proposes a way to improve the presentation of histograms where\ndata are compared to expectation. Sometimes, it is difficult to judge by eye\nwhether the difference between the bin content and the theoretical expectation\n(provided by either a fitting function or another histogram) is just due to\nstatistical fluctuations. More importantly, there could be statistically\nsignificant deviations which are completely invisible in the plot. We propose\nto add a small inset at the bottom of the plot, in which the statistical\nsignificance of the deviation observed in each bin is shown. Even though the\nnumerical routines which we developed have only illustration purposes, it comes\nout that they are based on formulae which could be used to perform statistical\ninference in a proper way. An implementation of our computation is available at\nhttps://github.com/dcasadei/psde .\n",
        "pdf_link": "http://arxiv.org/pdf/1111.2062v6"
    },
    {
        "title": "Off-line data quality monitoring for the GERDA experiment",
        "authors": [
            "P. Zavarise",
            "M. Agostini",
            "A. A. Machado",
            "L. Pandola",
            "O. Volynets"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  GERDA is an experiment searching for the neutrinoless {\\beta}{\\beta} decay of\nGe-76. The experiment uses an array of high-purity germanium detectors,\nenriched in Ge-76, directly immersed in liquid argon. GERDA recently started\nthe physics data taking using eight enriched coaxial detectors. The status of\nthe experiment has to be closely monitored in order to promptly identify\npossible instabilities or problems. The on-line slow control system is\ncomplemented by a regular off-line monitoring of data quality. This ensures\nthat data are qualified to be used in the physics analysis and allows to reject\ndata sets which do not meet the minimum quality standards. The off-line data\nmonitoring is entirely performed within the software framework GELATIO. In\naddition, a relational database, complemented by a web-based interface, was\ndeveloped to support the off-line monitoring and to automatically provide\ninformation to daily assess data quality. The concept and the performance of\nthe off-line monitoring tools were tested and validated during the one-year\ncommissioning phase.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.7200v1"
    },
    {
        "title": "SVD-based unfolding: implementation and experience",
        "authors": [
            "Kerstin Tackmann",
            "Andreas Hoecker"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  With the first year of data taking at the LHC by the experiments, unfolding\nmethods for measured spectra are reconsidered with much interest. Here, we\npresent a novel ROOT-based implementation of the Singular Value Decomposition\napproach to data unfolding, and discuss concrete analysis experience with this\nalgorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2226v1"
    },
    {
        "title": "Estimating a Signal In the Presence of an Unknown Background",
        "authors": [
            "Wolfgang A. Rolke",
            "Angel M. López"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We describe a method for fitting distributions to data which only requires\nknowledge of the parametric form of either the signal or the background but not\nboth. The unknown distribution is fit using a non-parametric kernel density\nestimator. The method returns parameter estimates as well as errors on those\nestimates. Simulation studies show that these estimates are unbiased and that\nthe errors are correct.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2299v3"
    },
    {
        "title": "MaxEnt and dynamical information",
        "authors": [
            "A. Hernando",
            "A. Plastino",
            "A. R. Plastino"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The MaxEnt solutions are shown to display a variety of behaviors (beyond the\ntraditional and customary exponential one) if adequate dynamical information is\ninserted into the concomitant entropic-variational principle. In particular, we\nshow both theoretically and numerically that power laws and power laws with\nexponential cut-offs emerge as equilibrium densities in proportional and other\ndynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.0889v1"
    },
    {
        "title": "1/f Noise and the Infrared Catastrophe",
        "authors": [
            "Ferdinand Grueneis"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  It is generally assumed that stochastic processes exhibiting 1/f noise are\naffected with the so-called infrared catastrophe. We present an intermittent\nstochastic process generating 1/f noise which avoids this problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3338v1"
    },
    {
        "title": "The Quantum Mechanical Oscillator as a Possible Source of 1/f\n  Fluctuations",
        "authors": [
            "Ferdinand Grueneis"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We investigate consecutive absorption or emission of photons of the quantum\nmechanical harmonic oscillator as a possible source of 1/f fluctuations.\nSeparating the absorption and emission process, we show that consecutively\nabsorbed or emitted photons give rise to an intermittent stochastic process;\nthereby fluctuating clusters of photons are intermitted by distinct breaks. Let\nthe number of photons in a cluster be m and the cluster size distribution be\npm. We find that the intermittent process with a cluster size distribution pm\nproportional to m-2 generates a pure 1/f spectrum. We show that 1/f\nfluctuations are present in thermal equilibrium but average out to zero. As an\nexample we investigate phonons as a possible origin of 1/f fluctuations in an\nextrinsic semiconductor. Acoustic phonons always produce a change in the\nvolume; this affects the donor ionization energy modulating also the g-r\nprocess. We calculate the spectrum of such a modulated g-r process; thereby the\nintermittent character of phonon activity is identified as the origin of 1/f\nfluctuations. The Hooge parameter is found to depend on the mean number of\nphonons comprised in a cluster, on the modulation depth of the g-r process and\non a factor which describes the temperature dependence of 1/f noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.4162v2"
    },
    {
        "title": "Fully Bayesian Unfolding",
        "authors": [
            "Georgios Choudalakis"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Bayesian inference is applied directly to the problem of unfolding. The\noutcome is a posterior probability density for the spectrum before smearing,\ndefined in the multi-dimensional space of all possible spectra. Regularization\nconsists in choosing a non-constant prior. Despite some similarity, the fully\nbayesian unfolding (FBU) method, presented here, should not be confused with\nD'Agostini's iterative method.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.4612v4"
    },
    {
        "title": "Stochastic method for in-situ damage analysis",
        "authors": [
            "Philip Rinn",
            "Hendrik Heißelmann",
            "Matthias Wächter",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Based on the physics of stochastic processes we present a new approach for\nstructural health monitoring. We show that the new method allows for an in-situ\nanalysis of the elastic features of a mechanical structure even for realistic\nexcitations with correlated noise as it appears in real-world situations. In\nparticular an experimental set-up of undamaged and damaged beam structures was\nexposed to a noisy excitation under turbulent wind conditions. The method of\nreconstructing stochastic equations from measured data has been extended to\nrealistic noisy excitations like those given here. In our analysis the\ndeterministic part is separated from the stochastic dynamics of the system and\nwe show that the slope of the deterministic part, which is linked to mechanical\nfeatures of the material, changes sensitively with increasing damage. The\nresults are more significant than corresponding changes in eigenfrequencies, as\ncommonly used for structural health monitoring.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.0124v3"
    },
    {
        "title": "Joining Forces of Bayesian and Frequentist Methodology: A Study for\n  Inference in the Presence of Non-Identifiability",
        "authors": [
            "Andreas Raue",
            "Clemens Kreutz",
            "Fabian Joachim Theis",
            "Jens Timmer"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Increasingly complex applications involve large datasets in combination with\nnon-linear and high dimensional mathematical models. In this context,\nstatistical inference is a challenging issue that calls for pragmatic\napproaches that take advantage of both Bayesian and frequentist methods. The\nelegance of Bayesian methodology is founded in the propagation of information\ncontent provided by experimental data and prior assumptions to the posterior\nprobability distribution of model predictions. However, for complex\napplications experimental data and prior assumptions potentially constrain the\nposterior probability distribution insufficiently. In these situations Bayesian\nMarkov chain Monte Carlo sampling can be infeasible. From a frequentist point\nof view insufficient experimental data and prior assumptions can be interpreted\nas non-identifiability. The profile likelihood approach offers to detect and to\nresolve non-identifiability by experimental design iteratively. Therefore, it\nallows one to better constrain the posterior probability distribution until\nMarkov chain Monte Carlo sampling can be used securely. Using an application\nfrom cell biology we compare both methods and show that a successive\napplication of both methods facilitates a realistic assessment of uncertainty\nin model predictions.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.4605v1"
    },
    {
        "title": "Data assimilation in the low noise regime with application to the\n  Kuroshio",
        "authors": [
            "Eric Vanden-Eijnden",
            "Jonathan Weare"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  On-line data assimilation techniques such as ensemble Kalman filters and\nparticle filters lose accuracy dramatically when presented with an unlikely\nobservation. Such an observation may be caused by an unusually large\nmeasurement error or reflect a rare fluctuation in the dynamics of the system.\nOver a long enough span of time it becomes likely that one or several of these\nevents will occur. Often they are signatures of the most interesting features\nof the underlying system and their prediction becomes the primary focus of the\ndata assimilation procedure. The Kuroshio or Black Current that runs along the\neastern coast of Japan is an example of such a system. It undergoes infrequent\nbut dramatic changes of state between a small meander during which the current\nremains close to the coast of Japan, and a large meander during which it bulges\naway from the coast. Because of the important role that the Kuroshio plays in\ndistributing heat and salinity in the surrounding region, prediction of these\ntransitions is of acute interest. Here we focus on a regime in which both the\nstochastic forcing on the system and the observational noise are small. In this\nsetting large deviation theory can be used to understand why standard filtering\nmethods fail and guide the design of the more effective data assimilation\ntechniques. Motivated by our analysis we propose several data assimilation\nstrategies capable of efficiently handling rare events such as the transitions\nof the Kuroshio. These techniques are tested on a model of the Kuroshio and\nshown to perform much better than standard filtering methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.4952v2"
    },
    {
        "title": "Zero Mass Limit and Its Experimental Test",
        "authors": [
            "Ruoshi Yuan",
            "Ping Ao"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  J. M. Sancho [Phys. Rev. E 84, 062102 (2011)] analyzed two stochastic\ninterpretations on a recent experiment [Phys. Rev. Lett. 104, 170602 (2010)] of\nBrownian colloidal particles. The author asserted that the stochastic\ninterpretation \"obtained by simply setting the acceleration equal to zero\"\nshould not be taken and that the zero-mass limit interpretation of the\nexperimental data would not be physically correct. In this Comment we show that\nSancho's analysis is incomplete in that it pre-excludes zero mass limit and\nhence his assertions are incorrect. Our reasoning will be both mathematical and\nphysical.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.5765v1"
    },
    {
        "title": "Signal Recovery Using Splines",
        "authors": [
            "Oleksandr Shumeyko",
            "Ivan Devyatkin"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Practically, for all real measuring devices the result of a measurement is a\nconvolution of an input signal with a hardware function of a unit {\\phi}. We\ncall a spline to be {\\phi}-interpolating if the convolution of an input signal\nwith a hardware function of a unit {\\phi} coincides with the convolution of the\nspline with the hardware function. In the following article we consider\nconditions imposed on the hardware function {\\phi} under which a second- and\nthird-order {\\phi}-interpolating spline exists and is unique. Algorithms of\n{\\phi}-interpolating splines construction are written out.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.0528v1"
    },
    {
        "title": "Analysis of stochastic time series in N dimensions in the presence of\n  strong measurement noise",
        "authors": [
            "B. Lehle"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  An extension and generalization of a recently presented approach for the\nanalysis of Langevin-type stochastic processes in the presence of strong\nmeasurement noise is presented. For a stochastic process in N dimensions which\nis superimposed with strong, exponentially correlated, Gaussian distributed,\nmeasurement noise it is possible to extract the strength and the correlation\nfunctions of the noise as well as polynomial approximations of the drift and\ndiffusion functions of the underlying process.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.2334v2"
    },
    {
        "title": "About the probability distribution of a quantity with given mean and\n  variance",
        "authors": [
            "Stefano Olivares",
            "Matteo G. A. Paris"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Supplement 1 to GUM (GUM-S1) recommends the use of maximum entropy principle\n(MaxEnt) in determining the probability distribution of a quantity having\nspecified properties, e.g., specified central moments. When we only know the\nmean value and the variance of a variable, GUM-S1 prescribes a Gaussian\nprobability distribution for that variable. When further information is\navailable, in the form of a finite interval in which the variable is known to\nlie, we indicate how the distribution for the variable in this case can be\nobtained. A Gaussian distribution should only be used in this case when the\nstandard deviation is small compared to the range of variation (the length of\nthe interval). In general, when the interval is finite, the parameters of the\ndistribution should be evaluated numerically, as suggested by I. Lira,\nMetrologia, 46 L27 (2009). Here we note that the knowledge of the range of\nvariation is equivalent to a bias of the distribution toward a flat\ndistribution in that range, and the principle of minimum Kullback entropy (mKE)\nshould be used in the derivation of the probability distribution rather than\nthe MaxEnt, thus leading to an exponential distribution with non Gaussian\nfeatures. Furthermore, up to evaluating the distribution negentropy, we\nquantify the deviation of mKE distributions from MaxEnt ones and, thus, we\nrigorously justify the use of GUM-S1 recommendation also if we have further\ninformation on the range of variation of a quantity, namely, provided that its\nstandard uncertainty is sufficiently small compared to the range.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.2934v1"
    },
    {
        "title": "Particle filtering in high-dimensional chaotic systems",
        "authors": [
            "Nishanth Lingala",
            "N. Sri Namachchivaya",
            "Nicolas Perkowski",
            "Hoong C. Yeong"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We present an efficient particle filtering algorithm for multiscale systems,\nthat is adapted for simple atmospheric dynamics models which are inherently\nchaotic. Particle filters represent the posterior conditional distribution of\nthe state variables by a collection of particles, which evolves and adapts\nrecursively as new information becomes available. The difference between the\nestimated state and the true state of the system constitutes the error in\nspecifying or forecasting the state, which is amplified in chaotic systems that\nhave a number of positive Lyapunov exponents. The purpose of the present paper\nis to show that the homogenization method developed in Imkeller et al. (2011),\nwhich is applicable to high dimensional multi-scale filtering problems, along\nwith important sampling and control methods can be used as a basic and flexible\ntool for the construction of the proposal density inherent in particle\nfiltering. Finally, we apply the general homogenized particle filtering\nalgorithm developed here to the Lorenz'96 atmospheric model that mimics\nmid-latitude atmospheric dynamics with microscopic convective processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.1360v1"
    },
    {
        "title": "Atomic structure of a single large biomolecule from diffraction patterns\n  of random orientations",
        "authors": [
            "Miklós Tegze",
            "Gábor Bortel"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The short and intense pulses of the new X-ray free electron lasers, now\noperational or under construction, may make possible diffraction experiments on\nsingle molecule-sized objects with high resolution, before radiation damage\ndestroys the sample. In a single molecule imaging (SMI) experiment thousands of\ndiffraction patterns of single molecules with random orientations are recorded.\nOne of the most challenging problems of SMI is how to assemble these noisy\npatterns of unknown orientations into a consistent single set of diffraction\ndata. Here we present a new method which can solve the orientation problem of\nSMI efficiently even for large biological molecules and in the presence of\nnoise. We show on simulated diffraction patterns of a large protein molecule,\nhow the orientations of the patterns can be found and the structure to atomic\nresolution can be solved. The concept of our algorithm could be also applied to\nexperiments where images of an object are recorded in unknown orientations\nand/or positions like in cryoEM or tomography.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.2102v1"
    },
    {
        "title": "On Stochastic Error and Computational Efficiency of the Markov Chain\n  Monte Carlo Method",
        "authors": [
            "J. Li",
            "P. Vignal",
            "S. Sun",
            "V. M. Calo"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  In Markov Chain Monte Carlo (MCMC) simulations, the thermal equilibria\nquantities are estimated by ensemble average over a sample set containing a\nlarge number of correlated samples. These samples are selected in accordance\nwith the probability distribution function, known from the partition function\nof equilibrium state. As the stochastic error of the simulation results is\nsignificant, it is desirable to understand the variance of the estimation by\nensemble average, which depends on the sample size (i.e., the total number of\nsamples in the set) and the sampling interval (i.e., cycle number between two\nconsecutive samples). Although large sample sizes reduce the variance, they\nincrease the computational cost of the simulation. For a given CPU time, the\nsample size can be reduced greatly by increasing the sampling interval, while\nhaving the corresponding increase in variance be negligible if the original\nsampling interval is very small. In this work, we report a few general rules\nthat relate the variance with the sample size and the sampling interval. These\nresults are observed and confirmed numerically. These variance rules are\nderived for the MCMC method but are also valid for the correlated samples\nobtained using other Monte Carlo methods. The main contribution of this work\nincludes the theoretical proof of these numerical observations and the set of\nassumptions that lead to them.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.3176v3"
    },
    {
        "title": "Extended Kramers-Moyal analysis applied to optical trapping",
        "authors": [
            "Christoph Honisch",
            "Rudolf Friedrich",
            "Florian Hörner",
            "Cornelia Denz"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The Kramers-Moyal analysis is a well established approach to analyze\nstochastic time series from complex systems. If the sampling interval of a\nmeasured time series is too low, systematic errors occur in the analysis\nresults. These errors are labeled as finite time effects in the literature. In\nthe present article, we present some new insights about these effects and\ndiscuss the limitations of a previously published method to estimate\nKramers-Moyal coefficients at the presence of finite time effects. To increase\nthe reliability of this method and to avoid misinterpretations, we extend it by\nthe computation of error estimates for estimated parameters using a Monte Carlo\nerror propagation technique. Finally, the extended method is applied to a data\nset of an optical trapping experiment yielding estimations of the forces acting\non a Brownian particle trapped by optical tweezers. We find an increased\nMarkov-Einstein time scale of the order of the relaxation time of the process\nwhich can be traced back to memory effects caused by the interaction of the\nparticle and the fluid. Above the Markov-Einstein time scale, the process can\nbe very well described by the classical overdamped Markov model for Brownian\nmotion.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.1380v1"
    },
    {
        "title": "The two envelopes probability paradox: Much ado about nothing",
        "authors": [
            "R. A. Vazquez"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The two envelopes paradox is discussed. By calculating the conditional\nprobability, we arrive at a conditional expectations which differs from\nexisting results.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.4805v1"
    },
    {
        "title": "How does the past of a soccer match influence its future?",
        "authors": [
            "Andreas Heuer",
            "Oliver Rubner"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Scoring goals in a soccer match can be interpreted as a stochastic process.\nIn the most simple description of a soccer match one assumes that scoring goals\ncan be described by a constant goal rate for each team, implying simple\nPoissonian and Markovian behavior. Here a general framework for the\nidentification of deviations from this behavior is presented. For this endeavor\nit is essential to formulate an a priori estimate of the expected number of\ngoals per team in a specific match. The analysis scheme is applied to\napproximately 40 seasons of the German Bundesliga. It is possible to\ncharacterize the impact of the previous course of the match on the present\nmatch behavior. This allows one to identify interesting generic features about\nsoccer matches and thus to learn about the hidden complexities behind scoring\ngoals.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.4471v1"
    },
    {
        "title": "Nearfield Acoustic Holography using sparsity and compressive sampling\n  principles",
        "authors": [
            "Gilles Chardon",
            "Laurent Daudet",
            "Antoine Peillot",
            "François Ollivier",
            "Nancy Bertin",
            "Rémi Gribonval"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Regularization of the inverse problem is a complex issue when using\nNear-field Acoustic Holography (NAH) techniques to identify the vibrating\nsources. This paper shows that, for convex homogeneous plates with arbitrary\nboundary conditions, new regularization schemes can be developed, based on the\nsparsity of the normal velocity of the plate in a well-designed basis, i.e. the\npossibility to approximate it as a weighted sum of few elementary basis\nfunctions. In particular, these new techniques can handle discontinuities of\nthe velocity field at the boundaries, which can be problematic with standard\ntechniques. This comes at the cost of a higher computational complexity to\nsolve the associated optimization problem, though it remains easily tractable\nwith out-of-the-box software. Furthermore, this sparsity framework allows us to\ntake advantage of the concept of Compressive Sampling: under some conditions on\nthe sampling process (here, the design of a random array, which can be\nnumerically and experimentally validated), it is possible to reconstruct the\nsparse signals with significantly less measurements (i.e., microphones) than\nclassically required. After introducing the different concepts, this paper\npresents numerical and experimental results of NAH with two plate geometries,\nand compares the advantages and limitations of these sparsity-based techniques\nover standard Tikhonov regularization.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.5618v1"
    },
    {
        "title": "Basic Data Analysis and More - A Guided Tour Using Python",
        "authors": [
            "O. Melchert"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  In these lecture notes, a selection of frequently required statistical tools\nwill be introduced and illustrated. They allow to post-process data that stem\nfrom, e.g., large-scale numerical simulations (aka sequence of random\nexperiments). From a point of view of data analysis, the concepts and\ntechniques introduced here are of general interest and are, at best, employed\nby computational aid. Consequently, an exemplary implementation of the\npresented techniques using the Python programming language is provided. The\ncontents of these lecture notes is rather selective and represents a\ncomputational experimentalist's view on the subject of basic data analysis,\nranging from the simple computation of moments for distributions of random\nvariables to more involved topics such as hierarchical cluster analysis and the\nparallelization of Python code.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.6002v1"
    },
    {
        "title": "Prediction of particle type from measurements of particle location: A\n  physicist's approach to Bayesian classification",
        "authors": [
            "Robert W. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The Bayesian approach to the prediction of particle type given measurements\nof particle location is explored, using a parametric model whose prior is based\non the transformation group. Two types of particle are considered, and\nlocations are expressed in terms of a single spatial coordinate. Several cases\ncorresponding to different states of prior knowledge are evaluated, including\nthe effect of measurement uncertainty. Comparisons are made to nearest neighbor\nclassification and kernel density estimation. How one can evaluate the\nreliability of the prediction solely from the available data is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.3172v2"
    },
    {
        "title": "Randomness, Information, and Complexity",
        "authors": [
            "Peter Grassberger"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We review possible measures of complexity which might in particular be\napplicable to situations where the complexity seems to arise spontaneously. We\npoint out that not all of them correspond to the intuitive (or \"naive\") notion,\nand that one should not expect a unique observable of complexity. One of the\nmain problems is to distinguish complex from disordered systems. This and the\nfact that complexity is closely related to information requires that we also\ngive a review of information measures. We finally concentrate on quantities\nwhich measure in some way or other the difficulty of classifying and\nforecasting sequences of discrete symbols, and study them in simple examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.3459v1"
    },
    {
        "title": "An intermediate distribution between Gaussian and Cauchy distributions",
        "authors": [
            "Tong Liu",
            "Ping Zhang",
            "Wu-Sheng Dai",
            "Mi Xie"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  In this paper, we construct an intermediate distribution linking the Gaussian\nand the Cauchy distribution. We provide the probability density function and\nthe corresponding characteristic function of the intermediate distribution.\nBecause many kinds of distributions have no moment, we introduce weighted\nmoments. Specifically, we consider weighted moments under two types of weighted\nfunctions: the cut-off function and the exponential function. Through these two\ntypes of weighted functions, we can obtain weighted moments for almost all\ndistributions. We consider an application of the probability density function\nof the intermediate distribution on the spectral line broadening in laser\ntheory. Moreover, we utilize the intermediate distribution to the problem of\nthe stock market return in quantitative finance.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.5109v1"
    },
    {
        "title": "The Art of Probability Assignment",
        "authors": [
            "Vesselin I. Dimitrov"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The problem of assigning probabilities when little is known is analized in\nthe case where the quanities of interest are physical observables, i.e. can be\nmeasured and their values expressed by numbers. It is pointed out that the\nassignment of probabilities based on observation is a process of inference,\ninvolving the use of Bayes' theorem and the choice of a probability prior. When\na lot of data is available, the resulting probability are remarkably\ninsensitive to the form of the prior. In the oposite case of scarse data, it is\nsuggested that the probabilities are assigned such that they are the least\nsensitive to specific variations of the probability prior. In the continuous\ncase this results in a probability assignment rule wich calls for minimizing\nthe Fisher information subject to constraints reflecting all available\ninformation. In the discrete case, the corresponding quantity to be minimized\nturns out to be a Renyi distance between the original and the shifted\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.5276v2"
    },
    {
        "title": "Comments to the article \"Parametric fitting of data obtained from\n  detectors with finite resolution and limited acceptance\" [arXiv:1011.0662] by\n  Gagunashvili",
        "authors": [
            "G. Bohm",
            "G. Zech"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The publication by Gagunashvili [arXiv:1011.0662] suffers from several\ncaveats: i) The method is based upon the false assumption that the median of\nchi square distributed random variables is chi square distributed. ii) The\ninformation contained in the data is not fully used, iii) It is not clear how\nthe uncertainties associated to the fitted parameters can be evaluated. A\ncorrect solution of the problem is presented and results of the cited paper are\ncompared to results obtained using the approach described in the textbook by\nBohm and Zech. Finally, we correct false statements in the cited paper about a\nsection in our book.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.1830v1"
    },
    {
        "title": "Compressive Radar with Off-Grid Targets: A Perturbation Approach",
        "authors": [
            "Albert Fannjiang",
            "Hsiao-Chieh Tseng"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Compressed sensing (CS) schemes are proposed for monostatic as well as\nsynthetic aperture radar (SAR) imaging with chirped signals and\nUltra-Narrowband (UNB) continuous waveforms. In particular, a simple,\nperturbation method is developed to reduce the gridding error for off-grid\ntargets. A coherence bound is obtained for the resulting measurement matrix. A\ngreedy pursuit algorithm, Support-Constrained Orthogonal Matching Pursuit\n(SCOMP), is proposed to take advantage of the support constraint in the\nperturbation formulation and proved to have the capacity of determining the\noff-grid targets to the grid accuracy under favorable conditions.\nAlternatively, the Locally Optimized Thresholding (LOT) is proposed to enhance\nthe performance of the CS method, Basis Pursuit (BP). For the advantages of\nhigher signal-to-noise ratio and signal-to-interference ratio, it is proposed\nthat Spotlight SAR imaging be implemented with CS techniques and\nmulti-frequency UNB waveforms. Numerical simulations show promising results of\nthe proposed approach and algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.6399v2"
    },
    {
        "title": "On sampling and parametrization of discrete frequency distributions",
        "authors": [
            "Paolo Rossi"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The general relationship between an arbitrary frequency distribution and the\nexpectation value of the frequency distributions of its samples is esablished.\nA set of combinations of expectation values whose value does not in general\ndepend on the size of the sample is constructed. Distribution functions such\nthat the distribution of the expectation values of their samples is invariant\nin form are found and studied. The conditions under which the scaling limit of\nsuch distributions may exist are described.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.1410v1"
    },
    {
        "title": "Iterative unfolding with the Richardson-Lucy algorithm",
        "authors": [
            "Guenter Zech"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The Richardson-Lucy unfolding approach is simple and excellently performing.\nIt efficiently suppresses artificial high frequency contributions and permits\nto introduce known features of the true distribution. An algorithm to fix the\nnumber of iterations to an optimal value has been developed and tested with\nfive different types of distributions, with different event numbers and with\ndifferent binnings. The influence of the starting distribution has been\nstudied. A simple way to document the unfolding result such that it can be\ncompared to theoretical predictions is proposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.5177v2"
    },
    {
        "title": "Uncovering wind turbine properties through two-dimensional stochastic\n  modeling of wind dynamics",
        "authors": [
            "Frank Raischel",
            "Teresa Scholz",
            "Vitor V. Lopes",
            "Pedro G. Lind"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Using a method for stochastic data analysis, borrowed from statistical\nphysics, we analyze synthetic data from a Markov chain model that reproduces\nmeasurements of wind speed and power production in a wind park in Portugal. We\nfirst show that our analysis retrieves indeed the power performance curve,\nwhich yields the relationship between wind speed and power production and we\ndiscuss how this procedure can be extended for extracting unknown functional\nrelationships between pairs of physical variables in general. Second, we show\nhow specific features, such as the rated speed of the wind turbine or the\ndescriptive wind speed statistics, can be related with the equations describing\nthe evolution of power production and wind speed at single wind turbines.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.7161v3"
    },
    {
        "title": "Empirical validation of the thermal model of a passive solar cell test",
        "authors": [
            "T. A. Mara",
            "F. Garde",
            "H. Boyer",
            "M. Mamode"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The paper deals with an empirical validation of a building thermal model. We\nput the emphasis on sensitivity analysis and on research of inputs/residual\ncorrelation to improve our model. In this article, we apply a sensitivity\nanalysis technique in the frequency domain to point out the more important\nparameters of the model. Then, we compare measured and predicted data of indoor\ndry-air temperature. When the model is not accurate enough, recourse to\ntime-frequency analysis is of great help to identify the inputs responsible for\nthe major part of error. In our approach, two samples of experimental data are\nrequired. The first one is used to calibrate our model the second one to really\nvalidate the optimized model.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.3926v1"
    },
    {
        "title": "Identification of parameters in building concentration dispersion model",
        "authors": [
            "D. Calogine",
            "H. Boyer",
            "S. Ndoumbe",
            "C. Rivière",
            "F. Miranville"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The aim of this work is to simulate the pollutants transport in buildings.\nFocusing mainly on the presence of CO2, firstly we resolve the airflow\nequations for two typical validation cases, the Rao case and the IEA case.\nThese numerical results are compared to the most known software and they are\nused to evaluate of the evolution of CO2 concentration in the different rooms.\nIn order to obtain the different parameters and filters of the proposed model\nwe use a statistical method based on Bayesian inference. The final comparison\nof results is coherent but a complementary experimental procedure is necessary\nto calibrate and refine the model\n",
        "pdf_link": "http://arxiv.org/pdf/1212.3929v1"
    },
    {
        "title": "Parametric sensitivity analysis of a test cell thermal model using\n  spectral analysis",
        "authors": [
            "Thierry Alex Mara",
            "Harry Boyer",
            "François Garde"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The paper deals with an empirical validation of a building thermal model. We\nput the emphasis on sensitivity analysis and on research of inputs/residual\ncorrelation to improve our model. In this article, we apply a sensitivity\nanalysis technique in the frequency domain to point out the more important\nparameters of the model. Then, we compare measured and predicted data of indoor\ndry-air temperature. When the model is not accurate enough, recourse to\ntime-frequency analysis is of great help to identify the inputs responsible for\nthe major part of error. In our approach, two samples of experimental data are\nrequired. The first one is used to calibrate our model the second one to really\nvalidate the optimized model.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.5248v1"
    },
    {
        "title": "Evaluating strong measurement noise in data series with simulated\n  annealing method",
        "authors": [
            "J. Carvalho",
            "F. Raischel",
            "M. Haase",
            "P. G. Lind"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Many stochastic time series can be described by a Langevin equation composed\nof a deterministic and a stochastic dynamical part. Such a stochastic process\ncan be reconstructed by means of a recently introduced nonparametric method,\nthus increasing the predictability, i.e. knowledge of the macroscopic drift and\nthe microscopic diffusion functions. If the measurement of a stochastic process\nis affected by additional strong measurement noise, the reconstruction process\ncannot be applied. Here, we present a method for the reconstruction of\nstochastic processes in the presence of strong measurement noise, based on a\nsuitably parametrized ansatz. At the core of the process is the minimization of\nthe functional distance between terms containing the conditional moments taken\nfrom measurement data, and the corresponding ansatz functions. It is shown that\na minimization of the distance by means of a simulated annealing procedure\nyields better results than a previously used Levenberg-Marquardt algorithm,\nwhich permits a rapid and reliable reconstruction of the stochastic process.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.6356v1"
    },
    {
        "title": "Optimal estimation of free energies and stationary densities from\n  multiple biased simulations",
        "authors": [
            "Hao Wu",
            "Frank Noé"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  When studying high-dimensional dynamical systems such as macromolecules,\nquantum systems and polymers, a prime concern is the identification of the most\nprobable states and their stationary probabilities or free energies. Often,\nthese systems have metastable regions or phases, prohibiting to estimate the\nstationary probabilities by direct simulation. Efficient sampling methods such\nas umbrella sampling, metadynamics and conformational flooding have developed\nthat perform a number of simulations where the system's potential is biased\nsuch as to accelerate the rare barrier crossing events. A joint free energy\nprofile or stationary density can then be obtained from these biased\nsimulations with weighted histogram analysis method (WHAM). This approach (a)\nrequires a few essential order parameters to be defined in which the histogram\nis set up, and (b) assumes that each simulation is in global equilibrium. Both\nassumptions make the investigation of high-dimensional systems with previously\nunknown energy landscape difficult. Here, we introduce the transition matrix\nunweighting (TMU) method, a simple and efficient estimation method which\ndismisses both assumptions. The configuration space is discretized into sets,\nbut these sets are not only restricted to the selected slow coordinate but can\nbe clusters that form a partition of high-dimensional state space. The\nassumption of global equilibrium is replaced by requiring only local\nequilibrium within the discrete sets, and the stationary density or free energy\nis extracted from the transitions between clusters. We prove the asymptotic\nconvergence and normality of TMU, give an efficient approximate version of it\nand demonstrate its usefulness on numerical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.6711v1"
    },
    {
        "title": "The detection of signals buried in noise",
        "authors": [
            "Luigi Bergamaschi",
            "Giancarlo D'Agostino",
            "Laura Giordani",
            "Giovanni Mana",
            "Massimo Oddone"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  This paper examines signal detection in the presence of noise, with a\nparticular emphasis to the nuclear activation analysis. The problem is to\ndecide what between the signal-plus-background and no-signal hypotheses fits\nbetter the data and to quantify the relevant signal amplitude or detection\nlimit. Our solution is based on the use of Bayesian inferences to test the\ndifferent hypotheses.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.1528v1"
    },
    {
        "title": "Penalized Splines for Smooth Representation of High-dimensional Monte\n  Carlo Datasets",
        "authors": [
            "Nathan Whitehorn",
            "Jakob van Santen",
            "Sven Lafebre"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Detector response to a high-energy physics process is often estimated by\nMonte Carlo simulation. For purposes of data analysis, the results of this\nsimulation are typically stored in large multi-dimensional histograms, which\ncan quickly become both too large to easily store and manipulate and\nnumerically problematic due to unfilled bins or interpolation artifacts. We\ndescribe here an application of the penalized spline technique to efficiently\ncompute B-spline representations of such tables and discuss aspects of the\nresulting B-spline fits that simplify many common tasks in handling tabulated\nMonte Carlo data in high-energy physics analysis, in particular their use in\nmaximum-likelihood fitting.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2184v1"
    },
    {
        "title": "Extension of ITU IMT-A Channel Models for Elevation Domains and\n  Line-of-Sight Scenarios",
        "authors": [
            "Zhimeng Zhong",
            "Xuefeng Yin",
            "Xin Li",
            "Xue Li"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  In this contribution, the 3-dimensional (3D) channel characteristics,\nparticularly in the elevation domains, are extracted through measurements in\ntypical urban macro and micro environments in Xi'an China. Stochastic channel\nmodel parameters are obtained based on the high-resolution multi-path parameter\nestimates. In addition, a modified spatial channel model (SCM) for the\nline-of-sight (LoS) scenario is proposed where the LoS polarization matrix is\nparameterized in accordance with the reality. Measurement results justify the\nreasonability of the proposed model. These works significantly improve the\napplicability of the ITU SCM models in realistic 3D channel simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2518v2"
    },
    {
        "title": "Non-Gaussianity effect of petrophysical quantities by using q-entropy\n  and multi fractal random walk",
        "authors": [
            "Z. Koohi lai",
            "S. Vasheghani Farahani",
            "G. R. Jafari"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The geological systems such as petroleum reservoirs is investigated by the\nentropy introduced by Tsallis and multiplicative hierarchical cascade model.\nWhen non-Gaussianity appears, it is sign of uncertainty and phase transition,\nwhich could be sign of existence of petroleum reservoirs. Two important\nparameters which describe a system at any scale are determined; the\nnon-Gaussian degree, $q$, announced in entropy and the intermittency,\n$\\lambda^2$, which explains a critical behavior in the system. There exist some\npetrophysical indicators in order to characterize a reservoir, but there is\nvacancy to measure scaling information contain in comparison with together,\nyet. In this article, we compare the non-Gaussianity in three selected\nindicators in various scales. The quantities investigated in this article\nincludes Gamma emission (GR), sonic transient time (DT) and Neutron porosity\n(NPHI). It is observed that GR has a fat tailed PDF at all scales which is a\nsign of phase transition in the system which indicates high $q$ and\n$\\lambda^2$. This results in the availability of valuable information about\nthis quantity. NPHI displays a scale dependence of PDF which converges to a\nGaussian at large scales. This is a sign of a separated and uncorrelated\nporosity at large scales. For the DT series, small $\\lambda^2$ and $q$ at all\nscales are a hallmark of local correlations in this quantity.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.4648v1"
    },
    {
        "title": "Some Notes on the Use of the Windowed Fourier Transform for Spectral\n  Analysis of Discretely Sampled Data",
        "authors": [
            "Robert W. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The properties of the Gabor and Morlet transforms are examined with respect\nto the Fourier analysis of discretely sampled data. Forward and inverse\ntransform pairs based on a fixed window with uniform sampling of the frequency\naxis can satisfy numerically the energy and reconstruction theorems; however,\ntransform pairs based on a variable window or nonuniform frequency sampling in\ngeneral do not. Instead of selecting the shape of the window as some function\nof the central frequency, we propose constructing a single window with unit\nenergy from an arbitrary set of windows which is applied over the entire\nfrequency axis. By virtue of using a fixed window with uniform frequency\nsampling, such a transform satisfies the energy and reconstruction theorems.\nThe shape of the window can be tailored to meet the requirements of the\ninvestigator in terms of time/frequency resolution. The algorithm extends\nnaturally to the case of nonuniform signal sampling without modification beyond\nidentification of the Nyquist interval.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.0306v2"
    },
    {
        "title": "Automatic sorting of point pattern sets using Minkowski Functionals",
        "authors": [
            "Joshua Parker",
            "Eilon Sherman",
            "Matthias van de Raa",
            "Devaraj van der Meer",
            "Lawrence E. Samelson",
            "Wolfgang Losert"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Point pattern sets arise in many different areas of physical, biological, and\napplied research, representing many random realizations of underlying pattern\nformation mechanisms. These pattern sets can be heterogeneous with respect to\nunderlying spatial processes, which may not be visually distinguishable. This\nheterogeneity can be elucidated by looking at statistical measures of the\npatterns sets and using these measures to divide the pattern set into distinct\ngroups representing like spatial processes. We introduce here a numerical\nprocedure for sorting point pattern sets into spatially homogeneous groups\nusing Functional Principal Component Analysis (FPCA) applied to the\napproximated Minkowski functionals of each pattern. We demonstrate that this\nprocedure correctly sorts pattern sets into similar groups both when the\npatterns are drawn from similar processes and when the 2nd-order\ncharacteristics of the pattern are identical. We highlight this routine for\ndistinguishing the molecular patterning of fluorescently labeled cell membrane\nproteins, a subject of much interest in studies investigating complex spatial\nsignaling patterns involved in the human immune response.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.1121v1"
    },
    {
        "title": "2D anisotropic scattering pattern fitting using a novel Monte Carlo\n  method: Initial results",
        "authors": [
            "Brian R. Pauw",
            "Masato Ohnuma",
            "Kenji Sakurai",
            "Enno A. Klop"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Recently, a Monte Carlo method has been presented which allows for the\nform-free retrieval of size distributions from isotropic scattering patterns,\ncomplete with uncertainty estimates linked to the data quality. Here, we\npresent an adaptation to this method allowing for the fitting of anisotropic 2D\nscattering patterns. The model consists of a finite number of non-interacting\nellipsoids of revolution (but would work equally well for cylinders),\npolydisperse in both dimensions, and takes into account disorientation in the\nplane parallel to the detector plane. The method application results in three\nform-free distributions, two for the ellipsoid dimensions, and one for the\norientation distribution. It is furthermore shown that a morphological\nrestriction is needed to obtain a unique solution.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.2903v1"
    },
    {
        "title": "Object-based high contrast travel time tomography",
        "authors": [
            "Yenting Lin",
            "Antonio Ortega"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We consider travel time tomography problems involving detection of high\ncontrast, discrete high velocity structures. This results in a discrete\nnonlinear inverse problem, for which traditional grid-based models and\niterative linearized least-squares reconstruction algorithms are not suitable.\nThis is because travel paths change significantly near the high contrast\nvelocity structure, making it more difficult to inversely calculate the travel\npath and infer the velocity along the path. We propose a model-based approach\nto describe the high velocity structure using pre-defined elementary objects.\nCompared to a grid-based model, our approach has complexity that increases as a\nfunction of the number of objects, rather than increasing with the number of\ncells (usually very large). A new reconstruction algorithm is developed that\nprovides estimates of the probability that a high velocity structure appears at\nany point in the region of interest. Simulation results show that our method\ncan efficiently sample the model parameter space, and we map the model\nparameters into the high velocity structures in spatial domain to generate a\n\"probability map\", which represent the appearance of the high velocity\nstructure in different regions. We show the probability map not only gives the\nhighest probability to the optimal solution, but also includes other possible\nmodels as well.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.3052v2"
    },
    {
        "title": "Exact Fourier Spectrum Recovery",
        "authors": [
            "M. Andrecut"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Discrete Fourier Transform (DFT) is widely used in signal processing to\nanalyze the frequencies in a discrete signal. However, DFT fails to recover the\nexact Fourier spectrum, when the signal contains frequencies that do not\ncorrespond to the sampling grid. Here, we present an exact Fourier spectrum\nrecovery method and we provide an implementation algorithm. Also, we show\nnumerically that the proposed method is robust to noise perturbations.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.2043v1"
    },
    {
        "title": "Core-scale solute transport model selection using Monte Carlo analysis",
        "authors": [
            "Bwalya Malama",
            "Kristopher L. Kuhlman",
            "Scott C. James"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Model applicability to core-scale solute transport is evaluated using\nbreakthrough data from column experiments conducted with conservative tracers\ntritium (H-3) and sodium-22, and the retarding solute uranium-232. The three\nmodels considered are single-porosity, double-porosity with single-rate\nmobile-immobile mass-exchange, and the multirate model, which is a\ndeterministic model that admits the statistics of a random mobile-immobile\nmass-exchange rate coefficient. The experiments were conducted on intact\nCulebra Dolomite core samples. Previously, data were analyzed using single- and\ndouble-porosity models although the Culebra Dolomite is known to possess\nmultiple types and scales of porosity, and to exhibit multirate\nmobile-immobile-domain mass transfer characteristics at field scale. The data\nare reanalyzed here and null-space Monte Carlo analysis is used to facilitate\nobjective model selection. Prediction (or residual) bias is adopted as a\nmeasure of the model structural error. The analysis clearly shows single- and\ndouble-porosity models are structurally deficient, yielding late-time residual\nbias that grows with time. On the other hand, the multirate model yields\nunbiased predictions consistent with the late-time -5/2 slope diagnostic of\nmultirate mass transfer. The analysis indicates the multirate model is better\nsuited to describing core-scale solute breakthrough in the Culebra Dolomite\nthan the other two models.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.6738v1"
    },
    {
        "title": "Variational Assimilation for Xenon Dynamical Forecasts in Neutronic\n  using Advanced Background Error Covariance Matrix",
        "authors": [
            "Angélique Ponçot",
            "Jean-Philippe Argaud",
            "Bertrand Bouriquet",
            "Patrick Erhard",
            "Serge Gratton",
            "Olivier Thual"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Data assimilation method consists in combining all available pieces of\ninformation about a system to obtain optimal estimates of initial states. The\ndifferent sources of information are weighted according to their accuracy by\nthe means of error covariance matrices. Our purpose here is to evaluate the\nefficiency of variational data assimilation for the xenon induced oscillations\nforecasts in nuclear cores. In this paper we focus on the comparison between\n3DVAR schemes with optimised background error covariance matrix B and a 4DVAR\nscheme. Tests were made in twin experiments using a simulation code which\nimplements a mono-dimensional coupled model of xenon dynamics, thermal, and\nthermal-hydraulic processes. We enlighten the very good efficiency of the 4DVAR\nscheme as well as good results with the 3DVAR one using a careful multivariate\nmodelling of B.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.6836v1"
    },
    {
        "title": "Information directionality in coupled time series using transcripts",
        "authors": [
            "Roberto Monetti",
            "Wolfram Bunk",
            "Thomas Aschenbrenner",
            "Stephan Springer",
            "Jose Maria Amigo"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  In ordinal symbolic dynamics, transcripts describe the algebraic relationship\nbetween ordinal patterns. Using the concept of transcript, we exploit the\nmathematical structure of the group of permutations to derive properties and\nrelations among information measures of the symbolic representations of time\nseries. These theoretical results are then applied for the assessment of\ncoupling directionality in dynamical systems, where suitable coupling\ndirectionality measures are introduced depending only on transcripts. These\nnovel measures estimate information flow in lower space dimension and reduce to\nwell-established coupling directionality quantifiers when some general\nconditions are satisfied. Furthermore, by generalizing the definition of\ntranscript to ordinal patterns of different lengths, several of the commonly\nused information directionality measures can be encompassed within the same\nframework.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7613v1"
    },
    {
        "title": "Comment on Wibral et al. (2013): Measuring information-transfer delays,\n  PloS ONE 8(2): e55809",
        "authors": [
            "Jakob Runge"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Wibral et al. propose a measure of interaction delays rooted in an\ninformation-theoretic framework and contrast their measure with the bivariate\nmomentary information transfer (MIT), introduced in Pompe, B., & Runge, J.\n(2011). Momentary information transfer as a coupling measure of time series.\nPhys. Rev. E, 83(5), 1-12. In this version we correct an error in a previous\nversion of this comment.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7930v2"
    },
    {
        "title": "Powerlaw: a Python package for analysis of heavy-tailed distributions",
        "authors": [
            "Jeff Alstott",
            "Ed Bullmore",
            "Dietmar Plenz"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Power laws are theoretically interesting probability distributions that are\nalso frequently used to describe empirical data. In recent years effective\nstatistical methods for fitting power laws have been developed, but appropriate\nuse of these techniques requires significant programming and statistical\ninsight. In order to greatly decrease the barriers to using good statistical\nmethods for fitting power law distributions, we developed the powerlaw Python\npackage. This software package provides easy commands for basic fitting and\nstatistical analysis of distributions. Notably, it also seeks to support a\nvariety of user needs by being exhaustive in the options available to the user.\nThe source code is publicly available and easily extensible.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.0215v3"
    },
    {
        "title": "Invariant expectation values in the sampling of discrete frequency\n  distributions",
        "authors": [
            "Paolo Rossi"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The general relationship between an arbitrary frequency distribution and the\nexpectation value of the frequency distributions of its samples is discussed. A\nwide set of measurable quantities (\"invariant moments\") whose expectation value\ndoes not in general depend on the size of the sample is constructed and\nillustrated by applying the results to Ewens sampling formula. Invariant\nmoments are especially useful in the sampling of systems characterized by the\nabsence of an intrinsic scale. Distribution functions that may parametrize the\nsamples of scale-free distributions are considered and their invariant\nexpectation values are computed. The conditions under which the scaling limit\nof such distributions may exist are described.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.0397v1"
    },
    {
        "title": "Self-Similar Anisotropic Texture Analysis: the Hyperbolic Wavelet\n  Transform Contribution",
        "authors": [
            "S. G. Roux",
            "M. Clausel",
            "B. Vedel",
            "S. Jaffard",
            "P. Abry"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Textures in images can often be well modeled using self-similar processes\nwhile they may at the same time display anisotropy. The present contribution\nthus aims at studying jointly selfsimilarity and anisotropy by focusing on a\nspecific classical class of Gaussian anisotropic selfsimilar processes. It will\nfirst be shown that accurate joint estimates of the anisotropy and\nselfsimilarity parameters are performed by replacing the standard 2D-discrete\nwavelet transform by the hyperbolic wavelet transform, which permits the use of\ndifferent dilation factors along the horizontal and vertical axis. Defining\nanisotropy requires a reference direction that needs not a priori match the\nhorizontal and vertical axes according to which the images are digitized, this\ndiscrepancy defines a rotation angle. Second, we show that this rotation angle\ncan be jointly estimated. Third, a non parametric bootstrap based procedure is\ndescribed, that provides confidence interval in addition to the estimates\nthemselves and enables to construct an isotropy test procedure, that can be\napplied to a single texture image. Fourth, the robustness and versatility of\nthe proposed analysis is illustrated by being applied to a large variety of\ndifferent isotropic and anisotropic self-similar fields. As an illustration, we\nshow that a true anisotropy built-in self-similarity can be disentangled from\nan isotropic self-similarity to which an anisotropic trend has been\nsuperimposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.4384v1"
    },
    {
        "title": "Direct Phasing of Nanocrystal Diffraction",
        "authors": [
            "Veit Elser"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Recent experiments at free-electron laser x-ray sources have been able to\nresolve the intensity distributions about Bragg peaks in nanocrystals of large\nbiomolecules. Information derived from small shifts in the peak positions\naugment the Bragg samples of the particle intensity with samples of its\ngradients. Working on the assumption that the nanocrystal is entirely generated\nby lattice translations of a particle, we develop an algorithm that\nreconstructs the particle from intensities and intensity gradients. Unlike\ntraditional direct phasing methods that require very high resolution data in\norder to exploit sparsity of the electron density, our method imposes no\nconstraints on the contrast other than positivity and works well at low\nresolution. We demonstrate successful reconstructions with simulated P1\nlysozyme nanocrystal data down to a signal-to-noise ratio of 2 in the intensity\ngradients.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.0770v1"
    },
    {
        "title": "Remarks on the statistical aspects of the safety analysis",
        "authors": [
            "Lenard Pal",
            "Mihaly Makai"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We investigate the statistical methods applied throughout safety analysis of\ncomplex systems. The tolerance interval method implemented in the widely\nutilized 0.95|0.95 methodology is analyzed. We point out a remarkable weakness\nof the tolerance interval method concerning the principle of repeatability. It\nis proved that repeating twice the procedure, the probability that the second\nmaximum will be larger/smaller than the first one is 50\\%. This statement is\nnot surprising, it holds for any random variable with continuous distribution\nfunction. In order to demonstrate the undesirable consequences of the tolerance\ninterval method in the decision making, the results of the analysis of an\nelementary example are discussed. Instead of the tolerance interval method, we\nsuggest another method based on the sign test which has more encouraging\nfeatures, especially in the case of several output variables. The problematic\naspects of the method are also discussed. Finally, we suggest a simple test\ncase which is able to reveal if the tolerance interval method would not be\ncapable of determining the risky states of the system, when there are only a\nfew of them. If there are many, then the method may not explore each one in the\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.1336v1"
    },
    {
        "title": "Variational Data Assimilation via Sparse Regularization",
        "authors": [
            "A. M. Ebtehaj",
            "M. Zupanski",
            "G. Lerman",
            "E. Foufoula-Georgiou"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  This paper studies the role of sparse regularization in a properly chosen\nbasis for variational data assimilation (VDA) problems. Specifically, it\nfocuses on data assimilation of noisy and down-sampled observations while the\nstate variable of interest exhibits sparsity in the real or transformed domain.\nWe show that in the presence of sparsity, the $\\ell_{1}$-norm regularization\nproduces more accurate and stable solutions than the classic data assimilation\nmethods. To motivate further developments of the proposed methodology,\nassimilation experiments are conducted in the wavelet and spectral domain using\nthe linear advection-diffusion equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.1592v1"
    },
    {
        "title": "Scalar and vector Slepian functions, spherical signal estimation and\n  spectral analysis",
        "authors": [
            "Frederik J. Simons",
            "Alain Plattner"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  It is a well-known fact that mathematical functions that are timelimited (or\nspacelimited) cannot be simultaneously bandlimited (in frequency). Yet the\nfinite precision of measurement and computation unavoidably bandlimits our\nobservation and modeling scientific data, and we often only have access to, or\nare only interested in, a study area that is temporally or spatially bounded.\nIn the geosciences we may be interested in spectrally modeling a time series\ndefined only on a certain interval, or we may want to characterize a specific\ngeographical area observed using an effectively bandlimited measurement device.\nIt is clear that analyzing and representing scientific data of this kind will\nbe facilitated if a basis of functions can be found that are \"spatiospectrally\"\nconcentrated, i.e. \"localized\" in both domains at the same time. Here, we give\na theoretical overview of one particular approach to this \"concentration\"\nproblem, as originally proposed for time series by Slepian and coworkers, in\nthe 1960s. We show how this framework leads to practical algorithms and\nstatistically performant methods for the analysis of signals and their power\nspectra in one and two dimensions, and, particularly for applications in the\ngeosciences, for scalar and vectorial signals defined on the surface of a unit\nsphere.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3184v1"
    },
    {
        "title": "Effects of maximal fluctuation moment $q$ and detrending polynomial\n  orders on the observed multifractal features within MFDFA",
        "authors": [
            "Grzegorz Pamuła",
            "Dariusz Grech"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We focus on the importance of $q$ moments range used within multifractal\ndetrended fluctuation analysis (MFDFA) to calculate the generalized Hurst\nexponent spread and multifractal properties of signals. Different orders of\ndetrending polynomials are also discussed. In particular, we analyze\nquantitatively the corrections to the spread of generalized Hurst exponent\nprofile $\\Delta h$ allowing to extend the previously found by us formulas for\nlarge $q$, describing the level of artificial multiscaling in finite signals,\nto arbitrary narrower range of $q$ moments used in MFDFA technique in distinct\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.3653v2"
    },
    {
        "title": "The Multipoint Morisita Index for the Analysis of Spatial Patterns",
        "authors": [
            "J. Golay",
            "M. Kanevski",
            "C. Vega Orozco",
            "M. Leuenberger"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  In many fields, the spatial clustering of sampled data points has many\nconsequences. Therefore, several indices have been proposed to assess the level\nof clustering affecting datasets (e.g. the Morisita index, Ripley's K-function\nand R\\'enyi's generalized entropy). The classical Morisita index measures how\nmany times it is more likely to select two measurement points from the same\nquadrats (the data set is covered by a regular grid of changing size) than it\nwould be in the case of a random distribution generated from a Poisson process.\nThe multipoint version (k-Morisita) takes into account k points with k greater\nthan or equal to 2. The present research deals with a new development of the\nk-Morisita index for (1) monitoring network characterization and for (2) the\ndetection of patterns in monitored phenomena. From a theoretical perspective, a\nconnection between the k-Morisita index and multifractality has also been found\nand highlighted on a mathematical multifractal set.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.3756v3"
    },
    {
        "title": "Selection and orientation of different particles in single particle\n  imaging",
        "authors": [
            "Miklós Tegze",
            "Gábor Bortel"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The short pulses of X-ray free electron lasers can produce diffraction\npatterns with structural information before radiation damage destroys the\nparticle. The particles are injected into the beam in random orientations and\nthey should be identical. However, in real experimental conditions it is not\nalways possible to have identical particles. In this paper we show that the\ncorrelation maximization method, developed earlier, is able to select identical\nparticles from a mixture and find their orientations simultaneously.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.7889v1"
    },
    {
        "title": "Information geometric complexity of entropic motion on curved\n  statistical manifolds",
        "authors": [
            "Carlo Cafaro"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Physical systems behave according to their underlying dynamical equations\nwhich, in turn, can be identified from experimental data. Explaining data\nrequires selecting mathematical models that best capture the data regularities.\nIdentifying dynamical equations from the available data and statistical model\nselection are both very difficult tasks. Motivated by these fundamental links\namong physical systems, dynamical equations, experimental data and statistical\nmodeling, we discuss in this invited Contribution our information geometric\nmeasure of complexity of geodesic paths on curved statistical manifolds\nunderlying the entropic dynamics of classical physical systems described by\nprobability distributions. We also provide several illustrative examples of\nentropic dynamical models used to infer macroscopic predictions when only\npartial knowledge of the microscopic nature of the system is available.\nFinally, we present entropic arguments to briefly address complexity softening\neffects due to statistical embedding procedures.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.4867v2"
    },
    {
        "title": "Insight into the Properties of the UK Power Consumption Using a Linear\n  Regression and Wavelet Transform Approach",
        "authors": [
            "Samir Avdakovic",
            "Alma Ademovic",
            "Amir Nuhanovic"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  In this paper, the relationship between the Gross Domestic Product (GDP), air\ntemperature variations and power consumption is evaluated using the linear\nregression and Wavelet Coherence (WTC) approach on a 1971-2011 time series for\nthe United Kingdom (UK). The results based on the linear regression approach\nindicate that some 66% variability of the UK electricity demand can be\nexplained by the quarterly GDP variations, while only 11% of the quarterly\nchanges of the UK electricity demand are caused by seasonal air temperature\nvariations. WTC however, can detect the period of time when GDP and air\ntemperature significantly correlate with electricity demand and the results of\nthe wavelet correlation at different time scales indicate that a significant\ncorrelation is to be found on a long-term basis for GDP and on an annual basis\nfor seasonal air-temperature variations. This approach provides an insight into\nthe properties of the impact of the main factors on power consumption on the\nbasis of which the power system development or operation planning and\nforecasting the power consumption can be improved.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.5572v1"
    },
    {
        "title": "A Physically-Intuitive Method for Calculation of the Local Lattice\n  Constant from a High-Resolution Transmission Electron Microscopy Image by\n  Fourier Analysis",
        "authors": [
            "James T. Teherani",
            "Judy L. Hoyt"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We have developed a physically-intuitive method to calculate the local\nlattice constant as a function of position in a high-resolution transmission\nelectron microscopy image by performing a two-dimensional fast Fourier\ntransform. We apply a Gaussian filter with appropriate spatial\nfull-width-half-max (FWHM) bandwidth to the image centered at the desired\nlocation to calculate the local lattice constant (as opposed to the average\nlattice constant). Fourier analysis of the filtered image yields the vertical\nand horizontal lattice constants at this location. The process is repeated by\nstepping the Gaussian filter across the image to produce a set of local lattice\nconstants in the vertical and horizontal direction as a function of position in\nthe image. The method has been implemented in a freely available tool on\nnanoHUB.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.3155v1"
    },
    {
        "title": "Impact of scaling range on the effectiveness of detrending methods",
        "authors": [
            "Dariusz Grech",
            "Zygmunt Mazur"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We make the comparative study of scaling range properties for detrended\nfluctuation analysis (DFA), detrended moving average analysis (DMA) and\nrecently proposed new technique called modified detrended moving average\nanalysis (MDMA). Basic properties of scaling ranges for these techniques are\nreviewed. The efficiency and exactness of all three methods towards proper\ndetermination of scaling exponent $H$ is discussed, particularly for short\nseries of uncorrelated or persistent data. \\end{abstract}\n",
        "pdf_link": "http://arxiv.org/pdf/1309.5678v2"
    },
    {
        "title": "Agent-based modeling of zapping behavior of viewers, television\n  commercial allocation, and advertisement markets",
        "authors": [
            "Hiroyuki Kyan",
            "Jun-ichi Inoue"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We propose a simple probabilistic model of zapping behavior of television\nviewers. Our model might be regarded as a `theoretical platform' to investigate\nthe human collective behavior in the macroscopic scale through the zapping\naction of each viewer at the microscopic level. The stochastic process of\naudience measurements as macroscopic quantities such as television program\nrating point or the so-called gross rating point (GRP for short) are\nreconstructed using the microscopic modeling of each viewer's decision making.\nAssuming that each viewer decides the television station to watch by means of\nthree factors, namely, physical constraints on television controllers,\nexogenous information such as advertisement of program by television station,\nand endogenous information given by `word-of-mouth communication' through the\npast market history, we shall construct an aggregation probability of\nGibbs-Boltzmann-type with the energy function. We discuss the possibility for\nthe ingredients of the model system to exhibit the collective behavior due to\nnot exogenous but endogenous information.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.6217v1"
    },
    {
        "title": "Mixing Bandt-Pompe and Lempel-Ziv approaches: another way to analyze the\n  complexity of continuous-states sequences",
        "authors": [
            "S. Zozor",
            "D. Mateos",
            "P. W. Lamberti"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  In this paper, we propose to mix the approach underlying Bandt-Pompe\npermutation entropy with Lempel-Ziv complexity, to design what we call\nLempel-Ziv permutation complexity. The principle consists of two steps: (i)\ntransformation of a continuous-state series that is intrinsically multivariate\nor arises from embedding into a sequence of permutation vectors, where the\ncomponents are the positions of the components of the initial vector when\nre-arranged; (ii) performing the Lempel-Ziv complexity for this series of\n`symbols', as part of a discrete finite-size alphabet. On the one hand, the\npermutation entropy of Bandt-Pompe aims at the study of the entropy of such a\nsequence; i.e., the entropy of patterns in a sequence (e.g., local increases or\ndecreases). On the other hand, the Lempel-Ziv complexity of a discrete-state\nsequence aims at the study of the temporal organization of the symbols (i.e.,\nthe rate of compressibility of the sequence). Thus, the Lempel-Ziv permutation\ncomplexity aims to take advantage of both of these methods. The potential from\nsuch a combined approach - of a permutation procedure and a complexity analysis\n- is evaluated through the illustration of some simulated data and some real\ndata. In both cases, we compare the individual approaches and the combined\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.1379v3"
    },
    {
        "title": "Toward the estimation of background fluctuations under newly-observed\n  signals in particle physics",
        "authors": [
            "Federico Colecchia"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  When the number of events associated with a signal process is estimated in\nparticle physics, it is common practice to extrapolate background distributions\nfrom control regions to a predefined signal window. This allows accurate\nestimation of the expected, or average, number of background events under the\nsignal. However, in general, the actual number of background events can deviate\nfrom the average due to fluctuations in the data. Such a difference can be\nsizable when compared to the number of signal events in the early stages of\ndata analysis following the observation of a new particle, as well as in the\nanalysis of rare decay channels. We report on the development of a data-driven\ntechnique that aims to estimate the actual, as opposed to the expected, number\nof background events in a predefined signal window. We discuss results on toy\nMonte Carlo data and provide a preliminary estimate of systematic uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.2300v2"
    },
    {
        "title": "A complexity measure based method for studying the dependence of 222Rn\n  concentration time series on indoor air temperature and humidity",
        "authors": [
            "Dragutin T. Mihailovic",
            "Vladimir Udovičić",
            "Miodrag Krmar",
            "Ilija Arsenić"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We have suggested a complexity measure based method for studying the\ndependence of measured 222Rn concentration time series on indoor air\ntemperature and humidity. This method is based on the Kolmogorov complexity\n(KL). We have introduced (i) the sequence of the KL, (ii) the Kolmogorov\ncomplexity highest value in the sequence (KLM) and (iii) the KL of the product\nof time series. The noticed loss of the KLM complexity of 222Rn concentration\ntime series can be attributed to the indoor air humidity that keeps the radon\ndaughters in air.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.5766v1"
    },
    {
        "title": "The Transform between the space of observed values and the space of\n  possible values of the parameter",
        "authors": [
            "S. Bityukov",
            "N. Krasnikov",
            "V. Smirnova",
            "V. Taperechkina"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  In ref [math.ST/0411462] the notion of statistically dual distributions is\nintroduced. The reconstruction of confidence density [AIP Conference\nProceedings 803 (2005) 398] for the location parameter for several pairs of\nstatistically dual distributions (Poisson and Gamma, normal and normal, Cauchy\nand Cauchy, Laplace and Laplace) in the case of single observation of the\nrandom variable is a unique. It allows to introduce the Transform between the\nspace of observed values and the space of possible values of the parameter.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.6103v1"
    },
    {
        "title": "The autocorrelated noise filtering problem: the ISMC filter in a\n  specific case of distance measuring",
        "authors": [
            "Flavio Prattico"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  In a previous paper we were working on a electronic travel aid for blind\npeople based on infrared sensors. The signals coming from them are affected by\na great noise that also with the use of low pass filter cannot be clean well.\nMotivated by the improvement of the system, in this paper we show a novelty way\nto filter autocorrelated noise based on a probabilistic description of the\nprocess. We apply an indexed semi-Markov model in order to filter the signal\ncoming from the infrared sensor. We conduce first of all a data analysis on the\nnoise in order to understand well its form. We give the general formulation of\nthe new ISMC filter and at last we compare the results with a particular kind\nof Kalman filter for the specific stochastic application.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.7013v2"
    },
    {
        "title": "A comparison of nonlinear mixing models for vegetated areas using\n  simulated and real hyperspectral data",
        "authors": [
            "Nicolas Dobigeon",
            "Laurent Tits",
            "Ben Somers",
            "Yoann Altmann",
            "Pol Coppin"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Spectral unmixing is a crucial processing step when analyzing hyperspectral\ndata. In such analysis, most of the work in the literature relies on the widely\nacknowledged linear mixing model to describe the observed pixels.\nUnfortunately, this model has been shown to be of limited interest for specific\nscenes, in particular when acquired over vegetated areas. Consequently, in the\npast few years, several nonlinear mixing models have been introduced to take\nnonlinear effects into account while performing spectral unmixing. These models\nhave been proposed empirically, however without any thorough validation. In\nthis paper, the authors take advantage of two sets of real and physical-based\nsimulated data to validate the accuracy of various nonlinear models in\nvegetated areas. These physics-based and analysis models, and their\ncorresponding unmixing algorithms, are evaluated with respect to their ability\nof fitting the measured spectra and of providing an accurate estimation of the\nabundance coefficients, considered as the spatial distribution of the materials\nin each pixel.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.1270v2"
    },
    {
        "title": "Algorithm for spectral response analysis of superconducting microwave\n  transmission-line resonator",
        "authors": [
            "Muhammad Hanif"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  It has always been a challenge for researchers to efficiently and accurately\npost process experimental data which is distorted by the noise. Superconducting\nmicrowave devices e.g. resonators, directional filters, beam-splitters etc.\noperate at frequency of several GHz to THz and temperatures well below critical\ntemperature (Tc) with few exceptions like transition edge sensors where devices\nare operated at temperatures close to Tc. These devices are measured usually\nwith vector network analyser in terms of scattering parameters. Two kinds of\nerrors, systematic and drift can easily be removed from the measurements taken\nwith VNA. However, random errors are not easy to address and remove due to\ntheir unpredictability and randomness. In this manuscript we will present an\nalgorithm to post process experimental data to cope with measurements that have\nbeen corrupted or useful spectral response is buried in spurious signal. We\nhave developed a robust and efficient algorithm, implemented in MATLAB, to\ndetect peaks in spectral response, remove baseline and finally estimate\nparameters of two-port superconductor resonator using an Improved Nelder-Mead\nMethod for unconstrained multidimensional least square minimization. The\nalgorithm has been successfully tested and verified by processing spectral\nresponse of half wavelength microwave transmission-line resonator successfully\nisolating resonator response from noisy background. We were able to compute\nloaded quality factor, resonance frequency from response data with high\nreproducibility even from those experimental data sets where resonance spikes\nwere hardly visible.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.3145v1"
    },
    {
        "title": "Forecasting wind speed financial return",
        "authors": [
            "Guglielmo D'Amico",
            "Filippo Petroni",
            "Flavio Prattico"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The prediction of wind speed is very important when dealing with the\nproduction of energy through wind turbines. In this paper, we show a new\nnonparametric model, based on semi-Markov chains, to predict wind speed.\nParticularly we use an indexed semi-Markov model that has been shown to be able\nto reproduce accurately the statistical behavior of wind speed. The model is\nused to forecast, one step ahead, wind speed. In order to check the validity of\nthe model we show, as indicator of goodness, the root mean square error and\nmean absolute error between real data and predicted ones. We also compare our\nforecasting results with those of a persistence model. At last, we show an\napplication of the model to predict financial indicators like the Internal Rate\nof Return, Duration and Convexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.3895v1"
    },
    {
        "title": "Maximum-likelihood fits to histograms for improved parameter estimation",
        "authors": [
            "Joseph W. Fowler"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Straightforward methods for adapting the familiar chi^2 statistic to\nhistograms of discrete events and other Poisson distributed data generally\nyield biased estimates of the parameters of a model. The bias can be important\neven when the total number of events is large. For the case of estimating a\nmicrocalorimeter's energy resolution at 6 keV from the observed shape of the Mn\nK-alpha fluorescence spectrum, a poor choice of chi^2 can lead to biases of at\nleast 10% in the estimated resolution when up to thousands of photons are\nobserved. The best remedy is a Poisson maximum-likelihood fit, through a simple\nmodification of the standard Levenberg-Marquardt algorithm for chi^2\nminimization. Where the modification is not possible, another approach allows\niterative approximation of the maximum-likelihood fit.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.5622v1"
    },
    {
        "title": "Bayesian compressive sensing for ultrawideband inverse scattering in\n  random media",
        "authors": [
            "A. E. Fouda",
            "F. L. Teixeira"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We develop an ultrawideband (UWB) inverse scattering technique for\nreconstructing continuous random media based on Bayesian compressive sensing.\nIn addition to providing maximum a posteriori estimates of the unknown weights,\nBayesian inversion provides estimate of the confidence level of the solution,\nas well as a systematic approach for optimizing subsequent measurement(s) to\nmaximize information gain. We impose sparsity priors directly on spatial\nharmonics to exploit the spatial correlation exhibited by continuous media, and\nsolve for their posterior probability density functions efficiently using a\nfast relevance vector machine. We linearize the problem using the first-order\nBorn approximation which enables us to combine, in a single inversion,\nmeasurements from multiple transmitters and ultrawideband frequencies. We\nextend the method to high-contrast media using the distorted-Born iterative\nmethod. We apply time-reversal strategies to adaptively focus the inversion\neffort onto subdomains of interest, and hence reduce the overall inversion\ncost. The proposed techniques are illustrated in a number of canonical\nscenarios including crosshole and borehole sensing.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.1092v1"
    },
    {
        "title": "The Rényi entropy of Lévy distribution",
        "authors": [
            "Giorgio Sonnino",
            "György Steinbrecher",
            "Alberto Sonnino"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The equivalence between non-extensive C. Tsallis entropy and the extensive\nentropy introduced by Alfr\\'ed R\\'enyi is discussed. The R\\'enyi entropy is\nstudied from the perspective of the geometry of the Lebesgue and generalised,\nexotic Lebesgue spaces. A duality principle is established. The R\\'enyi entropy\nfor the L\\'evy distribution, in the domain when the nunerical methods fails, is\napproximated by asymptotic expansion for the large values of the R\\'enyi\nparameter.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.5909v1"
    },
    {
        "title": "The time series forecasting: from the aspect of network",
        "authors": [
            "S. Chen",
            "X. Lan",
            "Y. Hu",
            "Q. Liu",
            "Y. Deng"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Forecasting can estimate the statement of events according to the historical\ndata and it is considerably important in many disciplines. At present, time\nseries models have been utilized to solve forecasting problems in various\ndomains. In general, researchers use curve fitting and parameter estimation\nmethods (moment estimation, maximum likelihood estimation and least square\nmethod) to forecast. In this paper, a new sight is given to the forecasting and\na completely different method is proposed to forecast time series. Inspired by\nthe visibility graph and link prediction, this letter converts time series into\nnetwork and then finds the nodes which are mostly likelihood to link with the\npredicted node. Finally, the predicted value will be obtained according to the\nstate of the link. The TAIEX data set is used in the case study to illustrate\nthat the proposed method is effectiveness. Compared with ARIMA model, the\nproposed shows a good forecasting performance when there is a small amount of\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.1713v1"
    },
    {
        "title": "Confidence intervals with a priori parameter bounds",
        "authors": [
            "A. V. Lokhov",
            "F. V. Tkachov"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We review the methods of constructing confidence intervals that account for a\npriori information about one-sided constraints on the parameter being\nestimated. We show that the so-called method of sensitivity limit yields a\ncorrect solution of the problem. Derived are the solutions for the cases of a\ncontinuous distribution with non-negative estimated parameter and a discrete\ndistribution, specifically a Poisson process with background. For both cases,\nthe best upper limit is constructed that accounts for the a priori information.\nA table is provided with the confidence intervals for the parameter of Poisson\ndistribution that correctly accounts for the information on the known value of\nthe background along with the software for calculating the confidence intervals\nfor any confidence levels and magnitudes of the background (the software is\nfreely available for download via Internet).\n",
        "pdf_link": "http://arxiv.org/pdf/1403.5429v2"
    },
    {
        "title": "Modeling and analysis of cyclic inhomogeneous Markov processes: a wind\n  turbine case study",
        "authors": [
            "Teresa Scholz",
            "Vitor V. Lopes",
            "Pedro Lind",
            "Frank Raischel"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  A method is proposed to reconstruct a cyclic time-inhomogeneous Markov pro-\ncess from measured data. First, a time-inhomogeneous Markov model is fit to the\ndata, taken here from measurements on a wind turbine. From the time-dependent\ntransition matrices, the time-dependent Kramers-Moyal coefficients of the\ncorresponding stochastic process are computed. Further applications of this\nmethod are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.0203v2"
    },
    {
        "title": "Parameter estimation by fixed point of function of information\n  processing intensity",
        "authors": [
            "Rober Jankowski",
            "Marcin Makowski",
            "Edward W. Piotrowski"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We present a new method of estimating the dispersion of a distribution which\nis based on the surprising property of a function that measures information\nprocessing intensity. It turns out that this function has a maximum at its\nfixed point. We use a fixed-point equation to estimate the parameter of the\ndistribution that is of interest to us. We illustrate the estimation method by\nusing the example of an exponential distribution. The codes of programs that\ncalculate the experimental values of the information processing intensity are\npresented.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.0262v1"
    },
    {
        "title": "Toward fits to scaling-like data, but with inflection points &\n  generalized Lavalette function",
        "authors": [
            "Marcel Ausloos"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Experimental and empirical data are often analyzed on log-log plots in order\nto find some scaling argument for the observed/examined phenomenon at hands, in\nparticular for rank-size rule research, but also in critical phenomena in\nthermodynamics, and in fractal geometry. The fit to a straight line on such\nplots is not always satisfactory. Deviations occur at low, intermediate and\nhigh regimes along the log($x$)-axis. Several improvements of the mere power\nlaw fit are discussed, in particular through a Mandelbrot trick at low rank and\na Lavalette power law cut-off at high rank. In so doing, the number of free\nparameters increases. Their meaning is discussed, up to the 5 parameter free\nsuper-generalized Lavalette law and the 7-parameter free hyper-generalized\nLavalette law. It is emphasized that the interest of the basic 2-parameter free\nLavalette law and the subsequent generalizations resides in its \"noid\" (or\nsigmoid, depending on the sign of the exponents) form on a semi-log plot;\nsomething incapable to be found in other empirical law, like the\nZipf-Pareto-Mandelbrot law. It remained for completeness to invent a simple law\nshowing an inflection point on a \\underline{log-log plot}. Such a law can\nresult from a transformation of the Lavalette law through $x$ $\\rightarrow$\nlog($x$), but this meaning is theoretically unclear. However, a simple linear\ncombination of two basic Lavalette law is shown to provide the requested\nfeature. Generalizations taking into account two super-generalized or\nhyper-generalized Lavalette laws are suggested, but need to be fully considered\nat fit time on appropriate data.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.3605v1"
    },
    {
        "title": "Fractional Laplace Transforms - A Perspective",
        "authors": [
            "R. A. Treumann",
            "W. Baumjohann"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  A form of the Laplace transform is reviewed as a paradigm for an entire class\nof fractional functional transforms. Various of its properties are discussed.\nSuch transformations should be useful in application to differential/integral\nequations or problems in non-extensive statistical mechanics.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.5897v1"
    },
    {
        "title": "Use of Correlation Matrix to Assess the Stirring Performance of a\n  Reverberation Chamber: a Comparative Study",
        "authors": [
            "Gabriele Gradoni",
            "Valter Mariani Primiani",
            "Franco Moglie"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The use of correlation matrices to evaluate the number of uncorrelated\nstirrer positions of reverberation chamber has widespread applications in\nelectromagnetic compatibility. We present a comparative study of recent\ntechniques based on multivariate correlation functions aimed at relating\nspace-frequency inhomogeneities/anisotropies to the reduction of uncorrelated\npositions. Full wave finite-difference time domain simulations of an actual\nreverberation chamber are performed through an in-house parallel code. The\nefficiency of this code enables for capturing extensive\ninhomogeneous/anisotropic spatial volumes (frequency ranges). The concept of\nthreshold crossing is revised under the light of random field sampling, which\nis important to the performance of arbitrary reverberation chambers.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.6335v1"
    },
    {
        "title": "Binary versus non-binary information in real time series: empirical\n  results and maximum-entropy matrix models",
        "authors": [
            "Assaf Almog",
            "Diego Garlaschelli"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The dynamics of complex systems, from financial markets to the brain, can be\nmonitored in terms of multiple time series of activity of the constituent\nunits, such as stocks or neurons respectively. While the main focus of time\nseries analysis is on the magnitude of temporal increments, a significant piece\nof information is encoded into the binary projection (i.e. the sign) of such\nincrements. In this paper we provide further evidence of this by showing strong\nnonlinear relations between binary and non-binary properties of financial time\nseries. These relations are a novel quantification of the fact that extreme\nprice increments occur more often when most stocks move in the same direction.\nWe then introduce an information-theoretic approach to the analysis of the\nbinary signature of single and multiple time series. Through the definition of\nmaximum-entropy ensembles of binary matrices and their mapping to spin models\nin statistical physics, we quantify the information encoded into the simplest\nbinary properties of real time series and identify the most informative\nproperty given a set of measurements. Our formalism is able to accurately\nreplicate, and mathematically characterize, the observed binary/non-binary\nrelations. We also obtain a phase diagram allowing us to identify, based only\non the instantaneous aggregate return of a set of multiple time series, a\nregime where the so-called `market mode' has an optimal interpretation in terms\nof collective (endogenous) effects, a regime where it is parsimoniously\nexplained by pure noise, and a regime where it can be regarded as a combination\nof endogenous and exogenous factors. Our approach allows us to connect spin\nmodels, simple stochastic processes, and ensembles of time series inferred from\npartial information.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.7275v2"
    },
    {
        "title": "2-D Prony-Huang Transform: A New Tool for 2-D Spectral Analysis",
        "authors": [
            "Jérémy Schmitt",
            "Nelly Pustelnik",
            "Pierre Borgnat",
            "Patrick Flandrin",
            "Laurent Condat"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  This work proposes an extension of the 1-D Hilbert Huang transform for the\nanalysis of images. The proposed method consists in (i) adaptively decomposing\nan image into oscillating parts called intrinsic mode functions (IMFs) using a\nmode decomposition procedure, and (ii) providing a local spectral analysis of\nthe obtained IMFs in order to get the local amplitudes, frequencies, and\norientations. For the decomposition step, we propose two robust 2-D mode\ndecompositions based on non-smooth convex optimization: a \"Genuine 2-D\"\napproach, that constrains the local extrema of the IMFs, and a \"Pseudo 2-D\"\napproach, which constrains separately the extrema of lines, columns, and\ndiagonals. The spectral analysis step is based on Prony annihilation property\nthat is applied on small square patches of the IMFs. The resulting 2-D\nProny-Huang transform is validated on simulated and real data.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.7680v1"
    },
    {
        "title": "Finding metastable states in real-world time series with recurrence\n  networks",
        "authors": [
            "Iliusi Vega",
            "Christof Schütte",
            "Tim O. F. Conrad"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  In the framework of time series analysis with recurrence networks, we\nintroduce a self-adaptive method that determines the elusive recurrence\nthreshold and identifies metastable states in complex real-world time series.\nAs initial step, we introduce a way to set the embedding parameters used to\nreconstruct the state space from the time series. We set them as the ones\ngiving the maximum Shannon entropy for the first simultaneous minima of\nrecurrence rate and Shannon entropy. To identify metastable states, as well as\nthe transitions between them, we use a soft partitioning algorithm for module\nfinding which is specifically developed for the case in which a system shows\nmetastability. We illustrate our method with two complex time series examples.\nFinally, we show the robustness of our method for identifying metastable\nstates. Our results suggest that our method is robust for identifying\nmetastable states in complex time series, even when introducing considerable\nlevels of noise and missing data points.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.7807v3"
    },
    {
        "title": "Probability and Statistics for Particle Physicists",
        "authors": [
            "J. Ocariz"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Lectures presented at the 1st CERN Asia-Europe-Pacific School of High-Energy\nPhysics, Fukuoka, Japan, 14-27 October 2012. A pedagogical selection of topics\nin probability and statistics is presented. Choice and emphasis are driven by\nthe author's personal experience, predominantly in the context of physics\nanalyses using experimental data from high-energy physics detectors.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.3402v1"
    },
    {
        "title": "The bias of the unbiased estimator: a study of the iterative application\n  of the BLUE method",
        "authors": [
            "Luca Lista"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The best linear unbiased estimator (BLUE) is a popular statistical method\nadopted to combine multiple measurements of the same observable taking into\naccount individual uncertainties and their correlation. The method is unbiased\nby construction if the true uncertainties and their correlation are known, but\nit may exhibit a bias if uncertainty estimates are used in place of the true\nones, in particular if those estimated uncertainties depend on measured values.\nThis is the case for instance when contributions to the total uncertainty are\nknown as relative uncertainties. In those cases, an iterative application of\nthe BLUE method may reduce the bias of the combined measurement. The impact of\nthe iterative approach compared to the standard BLUE application is studied for\na wide range of possible values of uncertainties and their correlation in the\ncase of the combination of two measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.3425v4"
    },
    {
        "title": "Lagrangian Fuzzy Dynamics of Physical and Non-Physical Systems",
        "authors": [
            "Uziel Sandler"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  In this paper, we show how to study the evolution of a system, given\nimprecise knowledge about the state of the system and the dynamics laws. Our\napproach is based on Fuzzy Set Theory, and it will be shown that the\n\\emph{Fuzzy Dynamics} of a $n$-dimensional system is equivalent to Lagrangian\n(or Hamiltonian) mechanics in a $n+1$-dimensional space. In some cases,\nhowever, the corresponding Lagrangian is more general than the usual one and\ncould depend on the action. In this case, Lagrange's equations gain a non-zero\nright side proportional to the derivative of the Lagrangian with respect to the\naction. Examples of such systems are unstable systems, systems with dissipation\nand systems which can remember their history. Moreover, in certain situations,\nthe Lagrangian could be a set-valued function. The corresponding equations of\nmotion then become differential inclusions instead of differential equations.\nWe will also show that the principal of least action is a consequence of the\ncausality principle and the local topology of the state space and not an\nindependent axiom of classical mechanics.\n  We emphasize that our adaptation of Lagrangian mechanics does not use or\ndepend on specific properties of the physical system being modeled. Therefore,\nthis Lagrangian approach may be equally applied to \\emph{non-physical} systems.\nAn example of such an application is presented as well.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.3600v1"
    },
    {
        "title": "Prior-predictive value from fast-growth simulations: Error analysis and\n  bias estimation",
        "authors": [
            "Alberto Favaro",
            "Daniel Nickelsen",
            "Elena Barykina",
            "Andreas Engel"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Variants of fluctuation theorems recently discovered in the statistical\nmechanics of non-equilibrium processes may be used for the efficient\ndetermination of high-dimensional integrals as typically occurring in Bayesian\ndata analysis. In particular for multimodal distributions, Monte-Carlo\nprocedures not relying on perfect equilibration are advantageous. We provide a\ncomprehensive statistical error analysis for the determination of the\nprior-predictive value in a Bayes problem building on a variant of the\nJarzynski equation. Special care is devoted to the characterization of the bias\nintrinsic to the method. We also discuss the determination of averages over\nmultimodal posterior distributions with the help of a variant of the Crooks\ntheorem. All our findings are verified by extensive numerical simulations of\ntwo model systems with bimodal likelihoods.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.6108v2"
    },
    {
        "title": "Longest-path attacks on complex networks",
        "authors": [
            "Cunlai Pu",
            "Wei Cui"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We investigate the longest-path attacks on complex networks. Specifically, we\nremove approximately the longest simple path from a network iteratively until\nthere are no paths left in the network. We propose two algorithms, the random\naugmenting approach (RPA) and the Hamilton-path based approach (HPA), for\nfinding the approximately longest simple path in a network. Results demonstrate\nthat steps of longest-path attacks increase with network density linearly for\nrandom networks, while exponentially increasing for scale-free networks. The\nmore homogeneous the degree distribution is, the more fragile the network,\nwhich is totally different from the previous results of node or edge attacks.\nHPA is generally more efficient than RPA in the longest-path attacks of complex\nnetworks. These findings further help us understand the vulnerability of\ncomplex systems, better protect complex systems, and design more tolerant\ncomplex systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.7231v1"
    },
    {
        "title": "Rényi entropy and complexity measure for skew-gaussian distributions\n  and related families",
        "authors": [
            "Javier E. Contreras-Reyes"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  In this paper, we provide the R\\'enyi entropy and complexity measure for a\nnovel, flexible class of skew-gaussian distributions and their related\nfamilies, as a characteristic form of the skew-gaussian Shannon entropy. We\ngive closed expressions considering a more general class of closed\nskew-gaussian distributions and the weighted moments estimation method. In\naddition, closed expressions of R\\'enyi entropy are presented for extended\nskew-gaussian and truncated skew-gaussian distributions. Finally, additional\ninequalities for skew-gaussian and extended skew-gaussian R\\'enyi and Shannon\nentropies are reported.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.0111v2"
    },
    {
        "title": "Signal processing on graphs: Transforms and tomograms",
        "authors": [
            "R. Vilela Mendes",
            "Hugo C. Mendes",
            "Tanya Araújo"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Using projections on the (generalized) eigenvectors associated to matrices\nthat characterize the topological structure, several authors have constructed\ngeneralizations of the Fourier transform on graphs. By exploring mappings of\nthe spectrum of these matrices we show how to construct more general\ntransforms, in particular wavelet-like transforms on graphs. For time-series,\ntomograms, a generalization of the Radon transforms to arbitrary pairs of\nnon-commuting operators, are positive bilinear transforms with a rigorous\nprobabilistic interpretation which provide a full characterization of the\nsignals and are robust in the presence of noise. Here the notion of tomogram\ntransform is also extended to signals on arbitrary graphs\n",
        "pdf_link": "http://arxiv.org/pdf/1406.2185v1"
    },
    {
        "title": "Self-organized manifold learning and heuristic charting via adaptive\n  metrics",
        "authors": [
            "Denis Horvath",
            "Jozef Ulicny",
            "Branislav Brutovsky"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Classical metric and non-metric multidimensional scaling (MDS) variants are\nwidely known manifold learning (ML) methods which enable construction of low\ndimensional representation (projections) of high dimensional data inputs.\nHowever, their use is crucially limited to the cases when data are inherently\nreducible to low dimensionality. In general, drawbacks and limitations of\nthese, as well as pure, MDS variants become more apparent when the exploration\n(learning) is exposed to the structured data of high intrinsic dimension. As we\ndemonstrate on artificial and real-world datasets, the over-determination\nproblem can be solved by means of the hybrid and multi-component\ndiscrete-continuous multi-modal optimization heuristics. Its remarkable feature\nis, that projections onto 2D are constructed simultaneously with the data\ncategorization (classification) compensating in part for the loss of original\ninput information. We observed, that the optimization module integrated with ML\nmodeling, metric learning and categorization leads to a nontrivial mechanism\nresulting in generation of patterns of categorical variables which can be\ninterpreted as a heuristic charting. The method provides visual information in\nthe form of non-convex clusters or separated regions. Furthermore, the ability\nto categorize the surfaces into back and front parts of the analyzed 3D data\nobjects have been attained through self-organized structuring without\nsupervising.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.3440v1"
    },
    {
        "title": "Decay time integrals in neutral meson mixing and their efficient\n  evaluation",
        "authors": [
            "Till Moritz Karbach",
            "Gerhard Raven",
            "Manuel Schiller"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  In neutral meson mixing, a certain class of convolution integrals is required\nwhose solution involves the error function $\\mathrm{erf}(z)$ of a complex\nargument $z$. We show the the general shape of the analytic solution of these\nintegrals, and give expressions which allow the normalisation of these\nexpressions for use in probability density functions. Furthermore, we derive\nexpressions which allow a (decay time) acceptance to be included in these\nintegrals, or allow the calculation of moments.\n  We also describe the implementation of numerical routines which allow the\nnumerical evaluation of $w(z)=e^{-z^2}(1-\\mathrm{erf}(-iz))$, sometimes also\ncalled Faddeeva function, in C++. These new routines improve over the old\nCERNLIB routine(s) WWERF/CWERF in terms of both speed and accuracy. These new\nroutines are part of the RooFit package, and have been distributed with it\nsince ROOT version 5.34/08.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.0748v1"
    },
    {
        "title": "Alternative way to characterize a q-gaussian distribution by a robust\n  heavy tail measurement",
        "authors": [
            "E. L de Santa Helena",
            "C. M. Nascimento",
            "G. J. L. Gerhardt"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The q-Gaussians are a class of stable distributions which are present in many\nscientific fields, and that behave as heavy tailed distributions for an\nespecific range of q values. The identification of these values, which are used\nin the description of systems, is sometimes a hard task. In this work the\nidentification of a q-Gaussian distribution from empirical data was done by a\nmeasure of its tail weight using robust statistics. Numerical methods were used\nto generate artificial data, to find out the tail weight -- medcouple, and also\nto adjust the curve between medcouple and the q value. We showed that the\nmedcouple value remains unchanged when the calculation is applied to data which\nhave long memory. A routine was made to calculate the q value and its standard\ndeviation, when applied to empirical data. It is possible to identify a\nq-Gaussian by the proposed methods with higher precision than in the literature\nfor the same data sample, or as precise as found in the literature. However, in\nthis case, it is required a smaller sample of data. We hope that this method\nwill be able to open new ways for identifying physical phenomena that belongs\nto nonextensive frameworks.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.1287v2"
    },
    {
        "title": "Visualization of short-term heart period variability with network tools\n  as a method for quantifying autonomic drive",
        "authors": [
            "Danuta Makowiec",
            "Beata Graff",
            "Agnieszka Kaczkowska",
            "Grzegorz Graff",
            "Dorota Wejer",
            "Joanna Wdowczyk",
            "Marta Zarczynska-Buchowiecka",
            "Marcin Gruchala",
            "Zbigniew R. Struzik"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Signals from heart transplant recipients can be considered to be a natural\nsource of information for a better understanding of the impact of the autonomic\nnervous system on the complexity of heart rate variability. Beat-to-beat heart\nrate variability can be represented as a network of increments between\nsubsequent $RR$-intervals, which makes possible the visualization of short-term\nheart period fluctuations. A network is constructed of vertices representing\nincrements between subsequent $RR$-intervals, and edges which connect adjacent\n$RR$-increments. Two modes of visualization of such a network are proposed. The\nmethod described is applied to nocturnal Holter signals recorded from healthy\nyoung people and from cardiac transplant recipients. Additionally, the analysis\nis performed on surrogate data: shuffled RR-intervals (to display short-range\ndependence), and shuffled phases of the Fourier Transform of RR-intervals (to\nfilter out linear dependences). Important nonlinear properties of autonomic\nnocturnal regulation in short-term variability in healthy young persons are\nassociated with $RR$-increments: accelerations and decelerations of a size\ngreater than about 35 ms. They reveal that large accelerations are more likely\nantipersistent, while large decelerations are more likely persistent. Changes\nin $RR$-increments in a heart deprived of autonomic supervision are much lower\nthan in a healthy individual, and appear to be maintained around a homeostatic\nstate, but there are indications that this dynamics is nonlinear. The method is\nfruitful in the evaluation of the vagal activity - the quantity and quality of\nthe vagal tone - during the nocturnal rest of healthy young people. The method\nalso successfully extracts nonlinear effects related to intrinsic mechanisms of\nthe heart regulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.4921v1"
    },
    {
        "title": "Convergent Cross-Mapping and Pairwise Asymmetric Inference",
        "authors": [
            "James M. McCracken",
            "Robert S. Weigel"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Convergent Cross-Mapping (CCM) is a technique for computing specific kinds of\ncorrelations between sets of times series. It was introduced by Sugihara et al.\nand is reported to be \"a necessary condition for causation\" capable of\ndistinguishing causality from standard correlation. We show that the\nrelationships between CCM correlations proposed in \\cite{Sugihara2012} do not,\nin general, agree with intuitive concepts of \"driving\", and as such, should not\nbe considered indicative of causality. It is shown that CCM causality analysis\nimplies causality is a function of system parameters for simple linear and\nnonlinear systems. For example, in a RL circuit, both voltage and current can\nbe identified as the driver depending on the frequency of the source voltage.\nIt is shown that CCM causality analysis can, however, be modified to identify\nasymmetric relationships between pairs of time series that are consistent with\nintuition for the considered example systems for which CCM causality analysis\nprovided non-intuitive driver identifications. This modification of the CCM\ncausality analysis is introduced as \"pairwise asymmetric inference\" (PAI) and\nexamples of its use are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.5696v1"
    },
    {
        "title": "Reference analysis of the signal + background model in counting\n  experiments II. Approximate reference prior",
        "authors": [
            "Diego Casadei"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The objective Bayesian treatment of a model representing two independent\nPoisson processes, labelled as \"signal\" and \"background\" and both contributing\nadditively to the total number of counted events, is considered. It is shown\nthat the reference prior for the parameter of interest (the signal intensity)\ncan be well approximated by the widely (ab)used flat prior only when the\nexpected background is very high. On the other hand, a very simple\napproximation (the limiting form of the reference prior for perfect prior\nbackground knowledge) can be safely used over a large portion of the background\nparameters space. The resulting approximate reference posterior is a Gamma\ndensity whose parameters are related to the observed counts. This limiting form\nis simpler than the result obtained with a flat prior, with the additional\nadvantage of representing a much closer approximation to the reference\nposterior in all cases. Hence such limiting prior should be considered a better\ndefault or conventional prior than the uniform prior. On the computing side, it\nis shown that a 2-parameter fitting function is able to reproduce extremely\nwell the reference prior for any background prior. Thus, it can be useful in\napplications requiring the evaluation of the reference prior for a very large\nnumber of times. [The published version JINST 9 (2014) T10006 has a typo in the\nnormalization $N$ of eq.(2.6) that is fixed here.]\n",
        "pdf_link": "http://arxiv.org/pdf/1407.5893v6"
    },
    {
        "title": "Metrology and 1/f noise: linear regressions and confidence intervals in\n  flicker noise context",
        "authors": [
            "Francois Vernotte",
            "Eric Lantz"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  1/f noise is very common but is difficult to handle in a metrological way.\nAfter having recalled the main characteristics of stongly correlated noise,\nthis paper will determine relationships giving confidence intervals over the\narithmetic mean and the linear drift parameters. A complete example of\nprocessing of an actual measurement sequence affected by 1/f noise will be\ngiven.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.7760v6"
    },
    {
        "title": "Reply to \"Local Filtering Fundamentally Against Wide Spectrum\"",
        "authors": [
            "Jianwei Miao",
            "M. C. Scott",
            "Chien-Chun Chen",
            "Chun Zhu",
            "Edward R. White",
            "Chin-Yi Chiu",
            "B. C. Regan",
            "Yu Huang",
            "Laurence D. Marks"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  After carefully studying the comment by Wang et al. (arXiv:1408.6420), we\nfound it includes several mistakes and unjustified statements and Wang et al.\nlack very basic knowledge of dislocations. Moreover, there is clear evidence\nindicating that Wang et al. significantly misrepresented our method and claimed\nsomething that they actually did not implement.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.0774v1"
    },
    {
        "title": "Effect of Temperature on the Complexity of Solid Argon System",
        "authors": [
            "A Giri",
            "S Dey",
            "P Barat"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We study the measure of complexity in solid Argon system from the time series\ndata of kinetic energy of single Argon atoms at different equilibrated\ntemperatures. To account the inherent multi-scale dependence of the complexity,\nthe multi-scale entropy of the time series of kinetic energy of individual\nArgon atoms are computed at different equilibrated temperatures. The\nmulti-scale entropy study reveals that the dynamics of an atom becomes more\ncomplex at higher temperatures and the result corroborates well with the\nvariation of the pair correlation function of the atoms in the solid Argon\ncrystal. Also, we repeat the multi-scale entropy analysis for program generated\nLevy noise time series and for time series data obtained from the outcomes of\nexponential decay with noise dx(t) = -x(t) dt + sigma dB(t) (Langevin\nequation). Our study establishes that the scale dependence of sample entropy\nfor time series of kinetic energy of individual atoms in solid Argon system has\nsimilar tendency as that of Levy noise time series and the outcomes of\nexponential decay with noise (Langevin equation).\n",
        "pdf_link": "http://arxiv.org/pdf/1409.2761v2"
    },
    {
        "title": "Analyzing long-term correlated stochastic processes by means of\n  recurrence networks: Potentials and pitfalls",
        "authors": [
            "Yong Zou",
            "Reik V. Donner",
            "Jürgen Kurths"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Long-range correlated processes are ubiquitous, ranging from climate\nvariables to financial time series. One paradigmatic example for such processes\nis fractional Brownian motion (fBm). In this work, we highlight the potentials\nand conceptual as well as practical limitations when applying the recently\nproposed recurrence network (RN) approach to fBm and related stochastic\nprocesses. In particular, we demonstrate that the results of a previous\napplication of RN analysis to fBm (Liu \\textit{et al.,} Phys. Rev. E\n\\textbf{89}, 032814 (2014)) are mainly due to an inappropriate treatment\ndisregarding the intrinsic non-stationarity of such processes. Complementarily,\nwe analyze some RN properties of the closely related stationary fractional\nGaussian noise (fGn) processes and find that the resulting network properties\nare well-defined and behave as one would expect from basic conceptual\nconsiderations. Our results demonstrate that RN analysis can indeed provide\nmeaningful results for stationary stochastic processes, given a proper\nselection of its intrinsic methodological parameters, whereas it is prone to\nfail to uniquely retrieve RN properties for non-stationary stochastic processes\nlike fBm.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.3613v1"
    },
    {
        "title": "Monofractal nature of air temperature signals reveals their climate\n  variability",
        "authors": [
            "Adrien Deliège",
            "Samuel Nicolay"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We use the discrete \"wavelet transform microscope\" to show that the surface\nair temperature signals of weather stations selected in Europe are monofractal.\nThis study reveals that the information obtained in this way are richer than\nprevious works studying long range correlations in meteorological stations. The\napproach presented here allows to bind the H\\\"older exponents with the climate\nvariability. We also establish that such a link does not exist with methods\npreviously carried out.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.3774v1"
    },
    {
        "title": "Compressive Earth Observatory: An Insight from AIRS/AMSU Retrievals",
        "authors": [
            "Ardeshir Mohammad Ebtehaj",
            "Efi Foufoula-Georgiou",
            "Gilad Lerman",
            "Rafael Luis Bras"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We demonstrate that the global fields of temperature, humidity and\ngeopotential heights admit a nearly sparse representation in the wavelet\ndomain, offering a viable path forward to explore new paradigms of\nsparsity-promoting data assimilation and compressive recovery of land\nsurface-atmospheric states from space. We illustrate this idea using retrieval\nproducts of the Atmospheric Infrared Sounder (AIRS) and Advanced Microwave\nSounding Unit (AMSU) on board the Aqua satellite. The results reveal that the\nsparsity of the fields of temperature is relatively pressure-independent while\natmospheric humidity and geopotential heights are typically sparser at lower\nand higher pressure levels, respectively. We provide evidence that these\nland-atmospheric states can be accurately estimated using a small set of\nmeasurements by taking advantage of their sparsity prior.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.5068v2"
    },
    {
        "title": "Fluctuation scaling in neural spike trains",
        "authors": [
            "Shinsuke Koyama",
            "Ryota Kobayashi"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Fluctuation scaling has been observed universally in a wide variety of\nphenomena. In time series that describe sequences of events, fluctuation\nscaling is expressed as power function relationships between the mean and\nvariance of either inter-event intervals or counting statistics, depending on\nmeasurement variables. In this article, fluctuation scaling has been formulated\nfor a series of events in which scaling laws in the inter-event intervals and\ncounting statistics were related. We have considered the first-passage time of\nan Ornstein-Uhlenbeck process and used a conductance-based neuron model with\nexcitatory and inhibitory synaptic inputs to demonstrate the emergence of\nfluctuation scaling with various exponents, depending on the input regimes and\nthe ratio between excitation and inhibition. Furthermore, we have discussed the\npossible implication of these results in the context of neural coding.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.6800v3"
    },
    {
        "title": "Pedestrian Flow Simulation Validation and Verification Techniques",
        "authors": [
            "Mohamed H. Dridi"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  For the verification and validation of microscopic simulation models of\npedestrian flow, we have performed experiments for different kind of facilities\nand sites where most conflicts and congestion happens e.g. corridors, narrow\npassages, and crosswalks. The validity of the model should compare the\nexperimental conditions and simulation results with video recording carried out\nin the same condition like in real life e.g. pedestrian flux and density\ndistributions. The strategy in this technique is to achieve a certain amount of\naccuracy required in the simulation model. This method is good at detecting the\ncritical points in the pedestrians walking areas. For the calibration of\nsuitable models we use the results obtained from analyzing the video recordings\nin Hajj 2009 and these results can be used to check the design sections of\npedestrian facilities and exits. As practical examples, we present the\nsimulation of pilgrim streams on the Jamarat bridge.\n  The objectives of this study are twofold: first, to show through verification\nand validation that simulation tools can be used to reproduce realistic\nscenarios, and second, gather data for accurate predictions for designers and\ndecision makers.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.0603v1"
    },
    {
        "title": "AMGKQ: An Efficient Implementation of Adaptive Multivariate\n  Gauss-Kronrod Quadrature for Simultaneous Integrands in Octave/MATLAB",
        "authors": [
            "Robert W. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The algorithm AMGKQ for adaptive multivariate Gauss-Kronrod quadrature over\nhyper-rectangular regions of arbitrary dimensionality is proposed and\nimplemented in Octave/MATLAB. It can approximate numerically any number of\nintegrals over a common domain simultaneously. Improper integrals are addressed\nthrough singularity weakening coordinate transformations. Internal\nsingularities are addressed through the use of breakpoints. Its accuracy\nperformance is investigated thoroughly, and its running time is compared to\nother commonly available routines in two and three dimensions. Its running time\ncan be several orders of magnitude faster than recursively called quadrature\nroutines. Its performance is limited only by the memory structure of its\noperating environment. Included with the software are numerous examples of its\ninvocation.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.1064v1"
    },
    {
        "title": "Noise in oscillators: a review of state space decomposition approaches",
        "authors": [
            "Fabio L. Traversa",
            "Michele Bonnin",
            "Fernando Corinto",
            "Fabrizio Bonani"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We review the state space decomposition techniques for the assessment of the\nnoise properties of autonomous oscillators, a topic of great practical and\ntheoretical importance for many applications in many different fields, from\nelectronics, to optics, to biology. After presenting a rigorous definition of\nphase, given in terms of the autonomous system isochrons, we provide a\ngeneralized projection technique that allows to decompose the oscillator\nfluctuations in terms of phase and amplitude noise, pointing out that the very\ndefinition of phase (and orbital) deviations depends of the base chosen to\ndefine the aforementioned projection. After reviewing the most advanced\ntheories for phase noise, based on the use of the Floquet basis and of the\nreduction of the projected model by neglecting the orbital fluctuations, we\ndiscuss the intricacies of the phase reduction process pointing out the\npresence of possible variations of the noisy oscillator frequency due to\namplitude-related effects.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.1366v1"
    },
    {
        "title": "Fatigue Loads Estimation Through a Simple Stochastic Model",
        "authors": [
            "Pedro G. Lind",
            "Matthias Wächter",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We propose a procedure to estimate the fatigue loads on wind turbines, based\nin a recent framework used for reconstructing data series of stochastic\nproperties measured at wind turbines. Through a standard fatigue analysis, we\nshow that it is possible to accurately estimate fatigue loads in any wind\nturbine within one wind park, using only the load measurements at one single\nturbine and the set of wind speed measurements. Our framework consists of\nderiving a stochastic differential equation that describes the evolution of the\ntorque at one wind turbine driven by the wind speed. The stochastic equation is\nderived directly from the measurements and is afterwards used for predicting\nthe fatigue loads at neighboring turbines. Such a framework could be used to\nmitigate the financial efforts usually necessary for placing measurement\ndevices in all wind turbines within one wind farm. Finally, we also discuss the\nlimitations and possible improvements of the proposed procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.8005v1"
    },
    {
        "title": "Uncertainty Quantification for Airfoil Icing using Polynomial Chaos\n  Expansions",
        "authors": [
            "Anthony M. DeGennaro",
            "Clarence W. Rowley",
            "Luigi Martinelli"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The formation and accretion of ice on the leading edge of a wing can be\ndetrimental to airplane performance. Complicating this reality is the fact that\neven a small amount of uncertainty in the shape of the accreted ice may result\nin a large amount of uncertainty in aerodynamic performance metrics (e.g.,\nstall angle of attack). The main focus of this work concerns using the\ntechniques of Polynomial Chaos Expansions (PCE) to quantify icing uncertainty\nmuch more quickly than traditional methods (e.g., Monte Carlo). First, we\npresent a brief survey of the literature concerning the physics of wing icing,\nwith the intention of giving a certain amount of intuition for the physical\nprocess. Next, we give a brief overview of the background theory of PCE.\nFinally, we compare the results of Monte Carlo simulations to PCE-based\nuncertainty quantification for several different airfoil icing scenarios. The\nresults are in good agreement and confirm that PCE methods are much more\nefficient for the canonical airfoil icing uncertainty quantification problem\nthan Monte Carlo methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.3642v1"
    },
    {
        "title": "Linear processing of X-ray scattering patterns with missing pixels",
        "authors": [
            "Aliakbar Jafarpour"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  X-ray scattering patterns from emerging single particle experiments have\ncommonly many missing or contaminated pixels. This complicates different\nanalyses including projections on Fourier or other basis functions (for noise\nsuppression, compression, feature extraction, or retrieval of real-space\npatterns), as they require integration over all pixels. Here, we derive\nalternative formulations for Discrete Fourier Transform and a common orthogonal\nbasis by explicit consideration of missing pixels and finite size. Such linear\nformulations exclude the nonlinear distortion that would be caused by\nmultiplication of the complete scattering pattern with the mask function.\nContrary to nonlinear and non-convex phase retrieval optimizations, such\nreduced-dimension formulations can be used to fully enforce the constraints and\nto retrieve unknown intensities in a linear fashion. Applications are\ndemonstrated for some typical cases, and extensions to more general cases are\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.4966v1"
    },
    {
        "title": "Stable probability laws modeling random propagation times of waves\n  crossing different media",
        "authors": [
            "Bernard Lacaze"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  In a communication scheme, there exist points at the transmitter and at the\nreceiver where the wave is reduced to a finite set of functions of time which\ndescribe amplitudes and phases. For instance, the information is summarized in\nelectrical cables which preceed or follow antennas. In many cases, a random\npropagation time is sufficient to explain changes induced by the medium. In\nthis paper we study models based on stable probability laws which explain power\nspectra due to propagation of different kinds of waves in different media, for\ninstance, acoustics in quiet or turbulent atmosphere, ultrasonics in liquids or\ntissues, or electromagnetic waves in free space or in cables. Physical examples\nshow that a sub-class of probability laws appears in accordance with the\ncausality property of linear filters.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.5249v1"
    },
    {
        "title": "Statistical criteria for possible indications of new physics in tritium\n  $β$-decay spectrum",
        "authors": [
            "Aleksei Lokhov",
            "Fyodor Tkachov"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The method of quasi-optimal weights is applied to constructing\n(quasi-)optimal criteria for various anomalous contributions in experimental\nspectra. Anomalies in the spectra could indicate physics beyond the Standard\nModel (additional interactions and neutrino flavours, Lorenz violation etc.).\nIn particular the cumulative tritium $\\beta$-decay spectrum (for instance, in\nTroitsk-$\\nu$-mass, Mainz Neutrino Mass and KATRIN experiments) is analysed\nusing the derived special criteria. Using the power functions we show that the\nderived quasi-optimal criteria are efficient statistical instruments for\ndetecting the anomalous contributions in the spectra.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.6245v1"
    },
    {
        "title": "Big Data of Materials Science - Critical Role of the Descriptor",
        "authors": [
            "Luca M. Ghiringhelli",
            "Jan Vybiral",
            "Sergey V. Levchenko",
            "Claudia Draxl",
            "Matthias Scheffler"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Statistical learning of materials properties or functions so far starts with\na largely silent, non-challenged step: the choice of the set of descriptive\nparameters (termed descriptor). However, when the scientific connection between\nthe descriptor and the actuating mechanisms is unclear, causality of the\nlearned descriptor-property relation is uncertain. Thus, trustful prediction of\nnew promising materials, identification of anomalies, and scientific\nadvancement are doubtful. We analyse this issue and define requirements for a\nsuited descriptor. For a classical example, the energy difference of\nzincblende/wurtzite and rocksalt semiconductors, we demonstrate how a\nmeaningful descriptor can be found systematically.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.7437v2"
    },
    {
        "title": "How to analyze stochastic time series obeying a 2nd order differential\n  equation",
        "authors": [
            "Bernd Lehle",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The stochastic properties of a Langevin-type Markov process can be extracted\nfrom a given time series by a Markov analysis. Also processes that obey a\nstochastically forced second order differential equation can be analyzed this\nway by employing a particular embedding approach: To obtain a Markovian process\nin 2N dimensions from a non Markovian signal in N dimensions, the system is\ndescribed in a phase space that is extended by the temporal derivative of the\nsignal. For a discrete time series, however, this derivative can only be\ncalculated by a differencing scheme, which introduces an error. If the effects\nof this error are not accounted for, this leads to systematic errors in the\nestimation of the drift- and diffusion functions of the process. In this paper\nwe will analyze these errors and we will propose an approach that correctly\naccounts for them. This approach allows an accurate parameter estimation and,\nadditionally, is able to cope with weak measurement noise, which may be\nsuperimposed to a given time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.2238v1"
    },
    {
        "title": "Analog Forecasting with Dynamics-Adapted Kernels",
        "authors": [
            "Zhizhen Zhao",
            "Dimitrios Giannakis"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Analog forecasting is a nonparametric technique introduced by Lorenz in 1969\nwhich predicts the evolution of states of a dynamical system (or observables\ndefined on the states) by following the evolution of the sample in a historical\nrecord of observations which most closely resembles the current initial data.\nHere, we introduce a suite of forecasting methods which improve traditional\nanalog forecasting by combining ideas from kernel methods developed in harmonic\nanalysis and machine learning and state-space reconstruction for dynamical\nsystems. A key ingredient of our approach is to replace single-analog\nforecasting with weighted ensembles of analogs constructed using local\nsimilarity kernels. The kernels used here employ a number of dynamics-dependent\nfeatures designed to improve forecast skill, including Takens' delay-coordinate\nmaps (to recover information in the initial data lost through partial\nobservations) and a directional dependence on the dynamical vector field\ngenerating the data. Mathematically, our approach is closely related to kernel\nmethods for out-of-sample extension of functions, and we discuss alternative\nstrategies based on the Nystr\\\"om method and the multiscale Laplacian pyramids\ntechnique. We illustrate these techniques in applications to forecasting in a\nlow-order deterministic model for atmospheric dynamics with chaotic\nmetastability, and interannual-scale forecasting in the North Pacific sector of\na comprehensive climate model. We find that forecasts based on kernel-weighted\nensembles have significantly higher skill than the conventional approach\nfollowing a single analog.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.3831v3"
    },
    {
        "title": "The Basics of Information Geometry",
        "authors": [
            "Ariel Caticha"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  To what extent can we distinguish one probability distribution from another?\nAre there quantitative measures of distinguishability? The goal of this\ntutorial is to approach such questions by introducing the notion of the\n\"distance\" between two probability distributions and exploring some basic ideas\nof such an \"information geometry\".\n",
        "pdf_link": "http://arxiv.org/pdf/1412.5633v1"
    },
    {
        "title": "Big data need physical ideas and methods",
        "authors": [
            "J. P. Huang"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  If a person looks at WHITE paper through BLUE glasses, the paper will become\nBLUE in the eye of the person. Likewise, in the current study of big data which\nplay the same role as the white paper being looked at, various statistical\nmethods just serve as the blue glasses. That is, results obtained from big data\noften depend on the statistical methods in use, which may often defy reality.\nHere I suggest using physical ideas and methods to overcome this problem to the\ngreatest extent. This suggestion is helpful to development and application of\nbig data.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.6848v1"
    },
    {
        "title": "Point Information Gain and Multidimensional Data Analysis",
        "authors": [
            "Renata Rychtáriková",
            "Jan Korbel",
            "Petr Macháček",
            "Petr Císař",
            "Jan Urban",
            "Dmytro Soloviov",
            "Dalibor Štys"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We generalize the Point information gain (PIG) and derived quantities, i.e.\nPoint information entropy (PIE) and Point information entropy density (PIED),\nfor the case of R\\'enyi entropy and simulate the behavior of PIG for typical\ndistributions. We also use these methods for the analysis of multidimensional\ndatasets. We demonstrate the main properties of PIE/PIED spectra for the real\ndata on the example of several images, and discuss possible further utilization\nin other fields of data processing.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.02891v5"
    },
    {
        "title": "Can the bivariate Hurst exponent be higher than an average of the\n  separate Hurst exponents?",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  In this note, we investigate possible relationships between the bivariate\nHurst exponent $H_{xy}$ and an average of the separate Hurst exponents\n$\\frac{1}{2}(H_x+H_y)$. We show that two cases are well theoretically founded.\nThese are the cases when $H_{xy}=\\frac{1}{2}(H_x+H_y)$ and\n$H_{xy}<\\frac{1}{2}(H_x+H_y)$. However, we show that the case of\n$H_{xy}>\\frac{1}{2}(H_x+H_y)$ is not possible regardless of stationarity\nissues. Further discussion of the implications is provided as well together\nwith a note on the finite sample effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.02947v1"
    },
    {
        "title": "Magnetic Flux Leakage Method: Large-Scale Approximation",
        "authors": [
            "Anastasiya V. Pimenova",
            "Denis S. Goldobin",
            "Jeremy Levesley",
            "Andrey O. Ivantsov",
            "Peter Elkington",
            "Mark Bacciarelli"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We consider the application of the magnetic flux leakage (MFL) method to the\ndetection of defects in ferromagnetic (steel) tubulars. The problem setup\ncorresponds to the cases where the distance from the casing and the point where\nthe magnetic field is measured is small compared to the curvature radius of the\nundamaged casing and the scale of inhomogeneity of the magnetic field in the\ndefect-free case. Mathematically this corresponds to the planar ferromagnetic\nlayer in a uniform magnetic field oriented along this layer. Defects in the\nlayer surface result in a strong deformation of the magnetic field, which\nprovides opportunities for the reconstruction of the surface profile from\nmeasurements of the magnetic field. We deal with large-scale defects whose\ndepth is small compared to their longitudinal sizes---these being typical of\ncorrosive damage. Within the framework of large-scale approximation, analytical\nrelations between the casing thickness profile and the measured magnetic field\ncan be derived.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.03079v1"
    },
    {
        "title": "Normalizing the causality between time series",
        "authors": [
            "X. San Liang"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Recently, a rigorous yet concise formula has been derived to evaluate the\ninformation flow, and hence the causality in a quantitative sense, between time\nseries. To assess the importance of a resulting causality, it needs to be\nnormalized. The normalization is achieved through distinguishing three types of\nfundamental mechanisms that govern the marginal entropy change of the flow\nrecipient. A normalized or relative flow measures its importance relative to\nother mechanisms. In analyzing realistic series, both absolute and relative\ninformation flows need to be taken into account, since the normalizers for a\npair of reverse flows belong to two different entropy balances; it is quite\nnormal that two identical flows may differ a lot in relative importance in\ntheir respective balances. We have reproduced these results with several\nautoregressive models. We have also shown applications to a climate change\nproblem and a financial analysis problem. For the former, reconfirmed is the\nrole of the Indian Ocean Dipole as an uncertainty source to the El Ni\\~no\nprediction. This might partly account for the unpredictability of certain\naspects of El Ni\\~no that has led to the recent portentous but spurious\nforecasts of the 2014 \"Monster El Ni\\~no\". For the latter, an unusually strong\none-way causality has been identified from IBM (International Business Machines\nCorporation) to GE (General Electric Company) in their early era, revealing to\nus an old story, which has almost gone to oblivion, about \"Seven Dwarfs\"\ncompeting a giant for the mainframe computer market.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.03548v2"
    },
    {
        "title": "Criticality of mostly informative samples: A Bayesian model selection\n  approach",
        "authors": [
            "Ariel Haimovici",
            "Matteo Marsili"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We discuss a Bayesian model selection approach to high dimensional data in\nthe deep under sampling regime. The data is based on a representation of the\npossible discrete states $s$, as defined by the observer, and it consists of\n$M$ observations of the state. This approach shows that, for a given sample\nsize $M$, not all states observed in the sample can be distinguished. Rather,\nonly a partition of the sampled states $s$ can be resolved. Such partition\ndefines an {\\em emergent} classification $q_s$ of the states that becomes finer\nand finer as the sample size increases, through a process of {\\em symmetry\nbreaking} between states. This allows us to distinguish between the\n$resolution$ of a given representation of the observer defined states $s$,\nwhich is given by the entropy of $s$, and its $relevance$ which is defined by\nthe entropy of the partition $q_s$. Relevance has a non-monotonic dependence on\nresolution, for a given sample size. In addition, we characterise most relevant\nsamples and we show that they exhibit power law frequency distributions,\ngenerally taken as signatures of \"criticality\". This suggests that\n\"criticality\" reflects the relevance of a given representation of the states of\na complex system, and does not necessarily require a specific mechanism of\nself-organisation to a critical point.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.00356v3"
    },
    {
        "title": "Performance Indicators of Wind Energy Production",
        "authors": [
            "G. D'Amico",
            "F. Petroni",
            "F. Prattico"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Modeling wind speed is one of the key element when dealing with the\nproduction of energy through wind turbines. A good model can be used for\nforecasting, site evaluation, turbines design and many other purposes. In this\nwork we are interested in the analysis of the future financial cash flows\ngenerated by selling the electrical energy produced. We apply an indexed\nsemi-Markov model of wind speed that has been shown, in previous investigation,\nto reproduce accurately the statistical behavior of wind speed. The model is\napplied to the evaluation of financial indicators like the Internal Rate of\nReturn, semi-Elasticity and relative Convexity that are widely used for the\nassessment of the profitability of an investment and for the measurement and\nanalysis of interest rate risk. We compare the computation of these indicators\nfor real and synthetic data. Moreover, we propose a new indicator that can be\nused to compare the degree of utilization of different power plants.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.03205v1"
    },
    {
        "title": "The Langevin Approach: a simple stochastic method for complex phenomena",
        "authors": [
            "Nico Reinke",
            "André Fuchs",
            "Wided Medjroubi",
            "Pedro G. Lind",
            "Matthias Wächter",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We describe a simple stochastic method, so-called Langevin approach, which\nenables one to extract evolution equations of stochastic variables from a set\nof measurements. Our method is parameter-free and it is based on the nonlinear\nLangevin equation. Moreover, it can be applied not only to processes in time,\nbut also to processes in scale, given that the data available shows ergodicity.\nThis chapter introduces the mathematical foundations of the Langevin approach\nand describes how to implement it numerically. A specific application of the\nmethod is presented, namely to a turbulent velocity field measured in the\nlaboratory, retrieving the corresponding energy cascade and comparing with the\nresults from a computational simulation of that experiment. In addition, we\ndescribe a physical interpretation bridging between processes in time and in\nscale. Finally, we describe extensions of the method for time series\nreconstruction and applications to other fields such as finance, medicine,\ngeophysics and renewable energies.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.05253v1"
    },
    {
        "title": "Increment entropy as a measure of complexity for time series",
        "authors": [
            "Xiaofeng Liu",
            "Aimin Jiang",
            "Ning Xu",
            "Jianru Xue"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Entropy has been a common index to quantify the complexity of time series in\na variety of fields. Here, we introduce increment entropy to measure the\ncomplexity of time series in which each increment is mapped into a word of two\nletters, one letter corresponding to direction and the other corresponding to\nmagnitude. The Shannon entropy of the words is termed as increment entropy\n(IncrEn). Simulations on synthetic data and tests on epileptic EEG signals have\ndemonstrated its ability of detecting the abrupt change, regardless of\nenergetic (e.g. spikes or bursts) or structural changes. The computation of\nIncrEn does not make any assumption on time series and it can be applicable to\narbitrary real-world data.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.01053v1"
    },
    {
        "title": "On the long-term correlations and multifractal properties of electric\n  arc furnace time series",
        "authors": [
            "Lorenzo Livi",
            "Enrico Maiorino",
            "Antonello Rizzi",
            "Alireza Sadeghian"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  In this paper, we study long-term correlations and multifractal properties\nelaborated from time series of three-phase current signals coming from an\nindustrial electric arc furnace plant. Implicit sinusoidal trends are suitably\ndetected by considering the scaling of the fluctuation functions. Time series\nare then filtered via a Fourier-based analysis, removing hence such strong\nperiodicities. In the filtered time series we detected long-term, positive\ncorrelations. The presence of positive correlations is in agreement with the\ntypical V--I characteristic (hysteresis) of the electric arc furnace, providing\nthus a sound physical justification for the memory effects found in the current\ntime series. The multifractal signature is strong enough in the filtered time\nseries to be effectively classified as multifractal.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.03332v3"
    },
    {
        "title": "Exponential Family Models from Bayes' Theorem under Expectation\n  Constraints",
        "authors": [
            "Sergio Davis"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  It is shown that a consistent application of Bayesian updating from a prior\nprobability density to a posterior using evidence in the form of expectation\nconstraints leads to exactly the same results as the application of the maximum\nentropy principle, namely a posterior belonging to the exponential family. The\nBayesian updating procedure presented in this work is not expressed as a\nvariational principle, and does not involve the concept of entropy. Therefore\nit conceptually constitutes a complete alternative to entropic methods of\ninference.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.03451v2"
    },
    {
        "title": "Random matrix approach to multivariate categorical data analysis",
        "authors": [
            "Aashay Patil",
            "M. S. Santhanam"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Correlation and similarity measures are widely used in all the areas of\nsciences and social sciences. Often the variables are not numbers but are\ninstead qualitative descriptors called categorical data. We define and study\nsimilarity matrix, as a measure of similarity, for the case of categorical\ndata. This is of interest due to a deluge of categorical data, such as movie\nratings, top-10 rankings and data from social media, in the public domain that\nrequire analysis. We show that the statistical properties of the spectra of\nsimilarity matrices, constructed from categorical data, follow those from\nrandom matrix theory. We demonstrate this approach by applying it to the data\nof Indian general elections and sea level pressures in North Atlantic ocean.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.06559v1"
    },
    {
        "title": "Extended models of gravity in SNIa cosmological data using genetic\n  algorithms",
        "authors": [
            "O. López-Corona"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  In this talk I explained briefly the advantages of using genetic algorithms\non any measured data but specially astronomical ones. This kind of algorithms\nare not only a better computational paradigm, but they also allow for a more\nprofound data treatment enhancing theoretical developments. As an example, I\nwill use the SNIa cosmological data to fit the extended metric theories of\ngravity of Carranza et al. (2013, 2014) showing that the best parameters\ncombination deviate from theoretical predicted ones by a minimal amount. This\nmeans that these kind of gravitational extensions are statistically robust and\nshow that no dark matter and/or energy is required to explain the observations.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.08069v1"
    },
    {
        "title": "Equipartitions and a Distribution for Numbers: A Statistical Model for\n  Benford's Law",
        "authors": [
            "Joseph R. Iafrate",
            "Steven J. Miller",
            "Frederick W. Strauch"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  A statistical model for the fragmentation of a conserved quantity is\nanalyzed, using the principle of maximum entropy and the theory of partitions.\nUpper and lower bounds for the restricted partitioning problem are derived and\napplied to the distribution of fragments. The resulting power law directly\nleads to Benford's law for the first digits of the parts.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.08259v1"
    },
    {
        "title": "A rational approximation for efficient computation of the Voigt function\n  in quantitative spectroscopy",
        "authors": [
            "S. M. Abrarov",
            "B. M. Quine"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We present a rational approximation for rapid and accurate computation of the\nVoigt function, obtained by residue calculus. The computational test reveals\nthat with only $16$ summation terms this approximation provides average\naccuracy ${10^{- 14}}$ over a wide domain of practical interest $0 < x <\n40,000$ and ${10^{- 4}} < y < {10^2}$ for applications using the HITRAN\nmolecular spectroscopic database. The proposed rational approximation takes\nless than half the computation time of that required by Weideman's rational\napproximation. Algorithmic stability is achieved due to absence of the poles at\n$y \\geqslant 0$ and $ - \\infty < x < \\infty $.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.00322v1"
    },
    {
        "title": "On the relation between uncertainties of weighted frequency averages and\n  the various types of Allan deviations",
        "authors": [
            "Erik Benkler",
            "Christian Lisdat",
            "Uwe Sterr"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The power spectral density in Fourier frequency domain, and the different\nvariants of the Allan deviation (ADEV) in dependence on the averaging time are\nwell established tools to analyse the fluctuation properties and frequency\ninstability of an oscillatory signal. It is often supposed that the statistical\nuncertainty of a measured average frequency is given by the ADEV at a well\nconsidered averaging time. However, this approach requires further mathematical\njustification and refinement, which has already been done regarding the\noriginal ADEV for certain noise types. Here we provide the necessary background\nto use the modified Allan deviation (modADEV) and other two-sample deviations\nto determine the uncertainty of weighted frequency averages. The type of\ntwo-sample deviation used to determine the uncertainty depends on the method\nused for determination of the average. We find that the modADEV, which is\nconnected with $\\Lambda$-weighted averaging, and the two sample deviation\nassociated to a linear phase regression weighting (parADEV) are in particular\nadvantageous for measurements, in which white phase noise is dominating.\nFurthermore, we derive a procedure how to minimize the uncertainty of a\nmeasurement for a typical combination of white phase and frequency noise by\nadaptive averaging of the data set with different weighting functions. Finally,\nsome aspects of the theoretical considerations for real-world frequency\nmeasurement equipment are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.00466v3"
    },
    {
        "title": "Objective Bayesian analysis of counting experiments with correlated\n  sources of background",
        "authors": [
            "Diego Casadei",
            "Cornelius Grunwald",
            "Kevin Kröninger",
            "Florian Mentzel"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Searches for faint signals in counting experiments are often encountered in\nparticle physics and astrophysics, as well as in other fields. Many problems\ncan be reduced to the case of a model with independent and Poisson-distributed\nsignal and background. Often several background contributions are present at\nthe same time, possibly correlated. We provide the analytic solution of the\nstatistical inference problem of estimating the signal in the presence of\nmultiple backgrounds, in the framework of objective Bayes statistics. The model\ncan be written in the form of a product of a single Poisson distribution with a\nmultinomial distribution. The first is related to the total number of events,\nwhereas the latter describes the fraction of events coming from each individual\nsource. Correlations among different backgrounds can be included in the\ninference problem by a suitable choice of the priors.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.02566v4"
    },
    {
        "title": "Robust linear regression with broad distributions of errors",
        "authors": [
            "Eugene B. Postnikov",
            "Igor M. Sokolov"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We consider the problem of linear fitting of noisy data in the case of broad\n(say $\\alpha$-stable) distributions of random impacts (\"noise\"), which can lack\neven the first moment. This situation, common in statistical physics of small\nsystems, in Earth sciences, in network science or in econophysics, does not\nallow for application of conventional Gaussian maximum-likelihood estimators\nresulting in usual least-squares fits. Such fits lead to large deviations of\nfitted parameters from their true values due to the presence of outliers. The\napproaches discussed here aim onto the minimization of the width of the\ndistribution of residua. The corresponding width of the distribution can either\nbe defined via the interquantile distance of the corresponding distributions or\nvia the scale parameter in its characteristic function. The methods provide the\nrobust regression even in the case of short samples with large outliers, and\nare equivalent to the normal least squares fit for the Gaussian noises. Our\ndiscussion is illustrated by numerical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.03188v1"
    },
    {
        "title": "Category theoretic properties of the A. Rényi and C. Tsallis entropies",
        "authors": [
            "György Steinbrecher",
            "Alberto Sonnino",
            "Giorgio Sonnino"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The problem of embedding the Tsallis and R\\'{e}nyi entropies in the framework\nof category theory and their axiomatic foundation is studied. To this end, we\nconstruct a special category MES related to measured spaces. We prove that both\nof the R\\'{e}nyi and Tsallis entropies can be imbedded in the formalism of\ncategory theory by proving that the same basic functional that appears in their\ndefinitions, as well as in the associated Lebesgue space norms, has good\nalgebraic compatibility properties. We prove that this functional is both\nadditive and multiplicative with respect to the direct product and the disjoint\nsum (the coproduct) in the category MES, so it is a natural candidate for the\nmeasure of information or uncertainty. We prove that the category MES can be\nextended to monoidal category, both with respect to the direct product as well\nas to the coproduct. The basic axioms of the original R\\'{e}nyi entropy theory\nare generalized and reformulated in the framework of category MES and we prove\nthat these axioms foresee the existence of an universal exponent having the\nsame values for all the objects of the category MES. In addition, this\nuniversal exponent is the parameter, which appears in the definition of the\nTsallis and R\\'{e}nyi entropies.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.05552v3"
    },
    {
        "title": "Health risk modelling by transforming a multi-dimensional unknown\n  distribution to a multi-dimensional Gaussian",
        "authors": [
            "V. Kapoor"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The traditional approach of health risk modelling with multiple data sources\nproceeds via regression-based methods assuming a marginal distribution for the\noutcome variable. The data is collected for $N$ subjects over a $J$ time-period\nor from $J$ data sources. The response obtained from $i^{th}$ subject is\n$\\vec{Y}_i=({Y}_{i1},\\cdots, {Y}_{iJ})$. For $N$ subjects we obtain a $J$\ndimensional joint distribution for the subjects. In this work we propose a\nnovel approach of transforming any $J$ dimensional joint distribution to that\nof a $J$ dimensional Gaussian keeping the Shannon entropy constant. This is in\nstark contrast to the traditional approaches of assuming a marginal\ndistribution for each $Y_{ij}$ by treating the $Y_{ij}'$s as independent\nobservations. The said transformation is implemented in our computer package\ncalled ENTRA.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.05796v4"
    },
    {
        "title": "Hilbert transform based analyses on ship-rocking signals",
        "authors": [
            "Wei Huang",
            "Yu-jian Li",
            "Deyong Kang",
            "Zhi Chen"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The ship-rocking is a crucial factor which affects the accuracy of the\nocean-based flight vehicle measurement. Here we have analyzed four groups of\nship-rocking time series in horizontal and vertical directions utilizing a\nHilbert based method from statistical physics. Our method gives a way to\nconstruct an analytic signal on the two-dimensional plane from a\none-dimensional time series. The analytic signal share the complete property of\nthe original time series. From the analytic signal of a time series, we have\nfound some information of the original time series which are often hidden from\nthe view of the conventional methods. The analytic signals of interest usually\nevolve very smoothly on the complex plane. In addition, the phase of the\nanalytic signal is usually moves linearly in time. From the auto-correlation\nand cross-correlation functions of the original signals as well as the\ninstantaneous amplitudes and phase increments of the analytic signals we have\nfound that the ship-rocking in horizontal direction drives the ship-rocking in\nvertical direction when the ship navigates freely. And when the ship keeps a\nfixed navigation direction such relation disappears. Based on these resultswe\ncould predict certain amount of future values of the ship-rocking time series\nbased on the current and the previous values. Our predictions are as accurate\nas the conventional methods from stochastic processes and provide a much wider\nprediction time range.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.01117v1"
    },
    {
        "title": "Integration of in situ Imaging and Chord Length Distribution\n  Measurements for Estimation of Particle Size and Shape",
        "authors": [
            "Okpeafoh S. Agimelen",
            "Anna Jawor-Baczynska",
            "John McGinty",
            "Christos Tachtatzis",
            "Jerzy Dziewierz",
            "Ian Haley",
            "Jan Sefcik",
            "Anthony J. Mulholland"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Efficient processing of particulate products across various manufacturing\nsteps requires that particles possess desired attributes such as size and\nshape. Controlling the particle production process to obtain required\nattributes will be greatly facilitated using robust algorithms providing the\nsize and shape information of the particles from in situ measurements. However,\nobtaining particle size and shape information in situ during manufacturing has\nbeen a big challenge. This is because the problem of estimating particle size\nand shape (aspect ratio) from signals provided by in-line measuring tools is\noften ill posed, and therefore it calls for appropriate constraints to be\nimposed on the problem. One way to constrain uncertainty in estimation of\nparticle size and shape from in-line measurements is to combine data from\ndifferent measurements such as chord length distribution (CLD) and imaging.\nThis paper presents two different methods for combining imaging and CLD data\nobtained with in-line tools in order to get reliable estimates of particle size\ndistribution and aspect ratio, where the imaging data is used to constrain the\nsearch space for an aspect ratio from the CLD data.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.03320v2"
    },
    {
        "title": "Patterns for the waiting time in the context of discrete-time stochastic\n  processes",
        "authors": [
            "Tayeb Jamali",
            "G. R. Jafari",
            "S. Vasheghani Farahani"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The aim of this study is to extend the scope and applicability of the\nlevel-crossing method to discrete-time stochastic processes and generalize it\nto enable us to study multiple discrete-time stochastic processes. In previous\nversions of the level-crossing method, problems with it correspond to the fact\nthat this method had been developed for analyzing a continuous-time process or\nat most a multiple continuous-time process in an individual manner. However,\nsince all empirical processes are discrete in time, the already-established\nlevel-crossing method may not prove adequate for studying empirical processes.\nBeyond this, due to the fact that most empirical processes are coupled; their\nindividual study could lead to vague results. To achieve the objectives of this\nstudy, we first find an analytical expression for the average frequency of\ncrossing a level in a discrete-time process, giving the measure of the time\nexperienced for two consecutive crossings named as the \"waiting time\". We then\nintroduce the generalized level-crossing method by which the consideration of\ncoupling between the components of a multiple process becomes possible.\nFinally, we provide an analytic solution when the components of a multiple\nstochastic process are independent Gaussian white noises. The comparison of the\nresults obtained for coupled and uncoupled processes measures the strength and\nefficiency of the coupling, justifying our model and analysis. The advantage of\nthe proposed method is its sensitivity to the slightest coupling and shortest\ncorrelation length.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.03336v2"
    },
    {
        "title": "Comment on Dimensionless Units in the SI",
        "authors": [
            "Paul Quincey"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The recent paper by Mohr and Phillips (arXiv:1409.2794) describes several\nproblems relating to the treatment of angle measurement within SI, the unit\nhertz, and quantities that can be considered countable (rather than\nmeasureable). However, the proposals that they put forward bring new problems\nof their own. This paper proposes alternative suggestions that solve the\nproblems less painfully. Specifically, clarifying the text on angle in the SI\nbrochure; relegating the hertz to a \"Non-SI unit accepted for use with the\nInternational System of Units\", with specific application only for \"revolutions\nor cycles per second\"; and encouraging countable quantities to be presented as\npure numbers, while requiring that a sufficient description of the quantity\nbeing counted is given in the accompanying text.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.07230v1"
    },
    {
        "title": "Interplay between Detection Strategies and Stochastic Resonance\n  Properties",
        "authors": [
            "Paolo Addesso",
            "Vincenzo Pierro",
            "Giovanni Filatrella"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We discuss how to exploit stochastic resonance with the methods of\nstatistical theory of decisions. To do so, we evaluate two detection\nstrategies: escape time analysis and strobing. For a standard quartic bistable\nsystem with a periodic drive and disturbed by noise, we show that the detection\nstrategies and the physics of the double well are connected, inasmuch as one\n(the strobing strategy) is based on synchronization, while the other (escape\ntime analysis) is determined by the possibility to accumulate energy in the\noscillations. The analysis of the escape times best performs at the frequency\nof the geometric resonance, while strobing shows a peak of the performances at\na special noise level predicted by the stochastic resonance theory. We surmise\nthat the detection properties of the quartic potential are generic for\noverdamped and underdamped systems, in that the physical nature of resonance\ndecides the competition (in terms of performances) between different detection\nstrategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.07646v1"
    },
    {
        "title": "The Parabolic variance (PVAR), a wavelet variance based on least-square\n  fit",
        "authors": [
            "F. Vernotte",
            "M. Lenczner",
            "P. -Y. Bourgeois",
            "E. Rubiola"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  This article introduces the Parabolic Variance (PVAR), a wavelet variance\nsimilar to the Allan variance, based on the Linear Regression (LR) of phase\ndata. The companion article arXiv:1506.05009 [physics.ins-det] details the\n$\\Omega$ frequency counter, which implements the LR estimate.\n  The PVAR combines the advantages of AVAR and MVAR. PVAR is good for long-term\nanalysis because the wavelet spans over $2 \\tau$, the same of the AVAR wavelet;\nand good for short-term analysis because the response to white and flicker PM\nis $1/\\tau^3$ and $1/\\tau^2$, same as the MVAR.\n  After setting the theoretical framework, we study the degrees of freedom and\nthe confidence interval for the most common noise types. Then, we focus on the\ndetection of a weak noise process at the transition - or corner - where a\nfaster process rolls off. This new perspective raises the question of which\nvariance detects the weak process with the shortest data record. Our\nsimulations show that PVAR is a fortunate tradeoff. PVAR is superior to MVAR in\nall cases, exhibits the best ability to divide between fast noise phenomena (up\nto flicker FM), and is almost as good as AVAR for the detection of random walk\nand drift.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.00687v3"
    },
    {
        "title": "An algorithm for discovering Lagrangians automatically from data",
        "authors": [
            "D. J. A. Hills",
            "A. M. Grütter",
            "J. J. Hudson"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  An activity fundamental to science is building mathematical models. These\nmodels are used to both predict the results of future experiments and gain\ninsight into the structure of the system under study. We present an algorithm\nthat automates the model building process in a scientifically principled way.\nThe algorithm can take observed trajectories from a wide variety of mechanical\nsystems and, without any other prior knowledge or tuning of parameters, predict\nthe future evolution of the system. It does this by applying the principle of\nleast action and searching for the simplest Lagrangian that describes the\nsystem's behaviour. By generating this Lagrangian in a human interpretable\nform, it also provides insight into the working of the system.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.01293v1"
    },
    {
        "title": "SASfit: A comprehensive tool for small-angle scattering data analysis",
        "authors": [
            "Ingo Breßler",
            "Joachim Kohlbrecher",
            "Andreas F. Thünemann"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Small-angle X-ray and neutron scattering experiments are used in many fields\nof the life sciences and condensed matter research to obtain answers to\nquestions about the shape and size of nano-sized structures, typically in the\nrange of 1 to 100 nm. It provides good statistics for large numbers of\nstructural units for short measurement times. With the ever-increasing quantity\nand quality of data acquisition, the value of appropriate tools that are able\nto extract valuable information is steadily increasing. SASfit has been one of\nthe mature programs for small-angle scattering data analysis available for many\nyears. We describe the basic data processing and analysis work-flow along with\nrecent developments in the SASfit program package (version 0.94.6). They\ninclude (i) advanced algorithms for reduction of oversampled data sets (ii)\nimproved confidence assessment in the optimized model parameters and (iii) a\nflexible plug-in system for custom user-provided models. A scattering function\nof a mass fractal model of branched polymers in solution is provided as an\nexample for implementing a plug-in. The new SASfit release is available for\nmajor platforms such as Windows, Linux and Mac OS X. To facilitate\ndocumentation, it includes improved indexed user documentation as well as a\nweb-based wiki for peer collaboration and online videos for introduction of\nbasic usage. The usage of SASfit is illustrated by interpretation of the\nsmall-angle X-ray scattering curves of monomodal gold nanoparticles (NIST\nreference material 8011) and bimodal silica nanoparticles (EU reference\nmaterial ERM-FD-102).\n",
        "pdf_link": "http://arxiv.org/pdf/1506.02958v1"
    },
    {
        "title": "Neural Networks with Non-Uniform Embedding and Explicit Validation Phase\n  to Assess Granger Causality",
        "authors": [
            "Alessandro Montalto",
            "Sebastiano Stramaglia",
            "Luca Faes",
            "Giovanni Tessitore",
            "Roberto Prevete",
            "Daniele Marinazzo"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  A challenging problem when studying a dynamical system is to find the\ninterdependencies among its individual components. Several algorithms have been\nproposed to detect directed dynamical influences between time series. Two of\nthe most used approaches are a model-free one (transfer entropy) and a\nmodel-based one (Granger causality). Several pitfalls are related to the\npresence or absence of assumptions in modeling the relevant features of the\ndata. We tried to overcome those pitfalls using a neural network approach in\nwhich a model is built without any a priori assumptions. In this sense this\nmethod can be seen as a bridge between model-free and model-based approaches.\nThe experiments performed will show that the method presented in this work can\ndetect the correct dynamical information flows occurring in a system of time\nseries. Additionally we adopt a non-uniform embedding framework according to\nwhich only the past states that actually help the prediction are entered into\nthe model, improving the prediction and avoiding the risk of overfitting. This\nmethod also leads to a further improvement with respect to traditional Granger\ncausality approaches when redundant variables (i.e. variables sharing the same\ninformation about the future of the system) are involved. Neural networks are\nalso able to recognize dynamics in data sets completely different from the ones\nused during the training phase.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.00579v4"
    },
    {
        "title": "A More Accurate Fourier Transform",
        "authors": [
            "Elya Courtney",
            "Michael Courtney"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Fourier transform methods are used to analyze functions and data sets to\nprovide frequencies, amplitudes, and phases of underlying oscillatory\ncomponents. Fast Fourier transform (FFT) methods offer speed advantages over\nevaluation of explicit integrals (EI) that define Fourier transforms. This\npaper compares frequency, amplitude, and phase accuracy of the two methods for\nwell resolved peaks over a wide array of data sets including cosine series with\nand without random noise and a variety of physical data sets, including\natmospheric $\\mathrm{CO_2}$ concentrations, tides, temperatures, sound\nwaveforms, and atomic spectra. The FFT uses MIT's FFTW3 library. The EI method\nuses the rectangle method to compute the areas under the curve via complex\nmath. Results support the hypothesis that EI methods are more accurate than FFT\nmethods. Errors range from 5 to 10 times higher when determining peak frequency\nby FFT, 1.4 to 60 times higher for peak amplitude, and 6 to 10 times higher for\nphase under a peak. The ability to compute more accurate Fourier transforms has\npromise for improved data analysis in many fields, including more sensitive\nassessment of hypotheses in the environmental sciences related to\n$\\mathrm{CO_2}$ concentrations and temperature. Other methods are available to\naddress different weaknesses in FFTs; however, the EI method always produces\nthe most accurate output possible for a given data set. On the 2011 Lenovo\nThinkPad used in this study, an EI transform on a 10,000 point data set took 31\nseconds to complete. Source code (C) and Windows executable for the EI method\nare available at\nhttps://sourceforge.net/projects/amoreaccuratefouriertransform/.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.01832v1"
    },
    {
        "title": "Limitation of the Least Square Method in the Evaluation of Dimension of\n  Fractal Brownian Motions",
        "authors": [
            "Bingqiang Qiao",
            "Siming Liu",
            "Houdun Zeng",
            "Xiang Li",
            "Benzhong Dai"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  With the standard deviation for the logarithm of the re-scaled range $\\langle\n|F(t+\\tau)-F(t)|\\rangle$ of simulated fractal Brownian motions $F(t)$ given in\na previous paper \\cite{q14}, the method of least squares is adopted to\ndetermine the slope, $S$, and intercept, $I$, of the log$(\\langle\n|F(t+\\tau)-F(t)|\\rangle)$ vs $\\rm{log}(\\tau)$ plot to investigate the\nlimitation of this procedure. It is found that the reduced $\\chi^2$ of the\nfitting decreases with the increase of the Hurst index, $H$ (the expectation\nvalue of $S$), which may be attributed to the correlation among the re-scaled\nranges. Similarly, it is found that the errors of the fitting parameters $S$\nand $I$ are usually smaller than their corresponding standard deviations. These\nresults show the limitation of using the simple least square method to\ndetermine the dimension of a fractal time series. Nevertheless, they may be\nused to reinterpret the fitting results of the least square method to determine\nthe dimension of fractal Brownian motions more self-consistently. The currency\nexchange rate between Euro and Dollar is used as an example to demonstrate this\nprocedure and a fractal dimension of 1.511 is obtained for spans greater than\n30 transactions.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.03250v2"
    },
    {
        "title": "Causation entropy from symbolic representations of dynamical systems",
        "authors": [
            "Carlo Cafaro",
            "Warren M. Lord",
            "Jie Sun",
            "Erik M. Bollt"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Identification of causal structures and quantification of direct information\nflows in complex systems is a challenging yet important task, with practical\napplications in many fields. Data generated by dynamical processes or\nlarge-scale systems are often symbolized, either because of the finite\nresolution of the measurement apparatus, or because of the need of statistical\nestimation. By algorithmic application of causation entropy, we investigated\nthe effects of symbolization on important concepts such as Markov order and\ncausal structure of the tent map. We uncovered that these quantities depend\nnonmontonically and, most of all, sensitively on the choice of symbolization.\nIndeed, we show that Markov order and causal structure do not necessarily\nconverge to their original analog counterparts as the resolution of the\npartitioning becomes finer.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.07262v1"
    },
    {
        "title": "Detection of non-self-correcting nature of information cascade",
        "authors": [
            "Shintaro Mori",
            "Masafumi Hino",
            "Masato Hisakado",
            "Taiki Takahashi"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We propose a method of detecting non-self-correcting information cascades in\nexperiments in which subjects choose an option sequentially by observing the\nchoices of previous subjects. The method uses the correlation function $C(t)$\nbetween the first and the $t+1$-th subject's choices. $C(t)$ measures the\nstrength of the domino effect, and the limit value $c\\equiv \\lim_{t\\to\n\\infty}C(t)$ determines whether the domino effect lasts forever $(c>0)$ or not\n$(c=0)$. The condition $c>0$ is an adequate condition for a non-self-correcting\nsystem, and the probability that the majority's choice remains wrong in the\nlimit $t\\to \\infty$ is positive. We apply the method to data from two\nexperiments in which $T$ subjects answered two-choice questions: (i) general\nknowledge questions ($T_{avg}=60$) and (ii) urn-choice questions ($T=63$). We\nfind $c>0$ for difficult questions in (i) and all cases in (ii), and the\nsystems are not self-correcting.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.07265v1"
    },
    {
        "title": "Establishing a direct connection between detrended fluctuation analysis\n  and Fourier analysis",
        "authors": [
            "Ken Kiyono"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  To understand methodological features of the detrended fluctuation analysis\n(DFA) using a higher-order polynomial fitting, we establish the direct\nconnection between DFA and Fourier analysis. Based on an exact calculation of\nthe single-frequency response of the DFA, the following facts are shown\nanalytically: (1) in the analysis of stochastic processes exhibiting a\npower-law scaling of the power spectral density (PSD), $S(f) \\sim f^{-\\beta}$,\na higher-order detrending in the DFA has no adverse effect in the estimation of\nthe DFA scaling exponent $\\alpha$, which satisfies the scaling relation $\\alpha\n= (\\beta+1)/2$; (2) the upper limit of the scaling exponents detectable by the\nDFA depends on the order of polynomial fit used in the DFA, and is bounded by\n$m + 1$, where $m$ is the order of the polynomial fit; (3) the relation between\nthe time scale in the DFA and the corresponding frequency in the PSD are\ndistorted depending on both the order of the DFA and the frequency dependence\nof the PSD. We can improve the scale distortion by introducing the corrected\ntime scale in the DFA corresponding to the inverse of the frequency scale in\nthe PSD. In addition, our analytical approach makes it possible to characterize\nvariants of the DFA using different types of detrending. As an application,\nproperties of the DFA using moving average filtering are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.02204v4"
    },
    {
        "title": "Testing Universality in Critical Exponents: the Case of Rainfall",
        "authors": [
            "Anna Deluca",
            "Pedro Puig",
            "Alvaro Corral"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  One of the key clues to consider rainfall as a self-organized critical\nphenomenon is the existence of power-law distributions for rain-event sizes. We\nhave studied the problem of universality in the exponents of these\ndistributions by means of a suitable statistic whose distribution is inferred\nby several variations of a permutational test. In contrast to more common\napproaches, our procedure does not suffer from the difficulties of multiple\ntesting and does not require the precise knowledge of the uncertainties\nassociated to the power-law exponents. When applied to seven sites monitored by\nthe Atmospheric Radiation Measurement Program the test lead to the rejection of\nthe universality hypothesis, despite the fact that the exponents are rather\nclose to each other.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.06516v2"
    },
    {
        "title": "Reproducible Experiment Platform",
        "authors": [
            "Tatiana Likhomanenko",
            "Alex Rogozhnikov",
            "Alexander Baranov",
            "Egor Khairullin",
            "Andrey Ustyuzhanin"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Data analysis in fundamental sciences nowadays is an essential process that\npushes frontiers of our knowledge and leads to new discoveries. At the same\ntime we can see that complexity of those analyses increases fast due to\na)~enormous volumes of datasets being analyzed, b)~variety of techniques and\nalgorithms one have to check inside a single analysis, c)~distributed nature of\nresearch teams that requires special communication media for knowledge and\ninformation exchange between individual researchers. There is a lot of\nresemblance between techniques and problems arising in the areas of industrial\ninformation retrieval and particle physics. To address those problems we\npropose Reproducible Experiment Platform (REP), a software infrastructure to\nsupport collaborative ecosystem for computational science. It is a Python based\nsolution for research teams that allows running computational experiments on\nshared datasets, obtaining repeatable results, and consistent comparisons of\nthe obtained results. We present some key features of REP based on case studies\nwhich include trigger optimization and physics analysis studies at the LHCb\nexperiment.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.00624v1"
    },
    {
        "title": "Monotone measures of statistical complexity",
        "authors": [
            "Łukasz Rudnicki",
            "Irene V. Toranzo",
            "Pablo Sanchez-Moreno",
            "Jesus S. Dehesa"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We introduce and discuss the notion of monotonicity for the complexity\nmeasures of general probability distributions, patterned after the resource\ntheory of quantum entanglement. Then, we explore whether this property is\nsatisfied by the three main intrinsic measures of complexity (Cramer-Rao,\nFisher-Shannon, LMC) and some of their generalizations.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.01547v2"
    },
    {
        "title": "Reaction rate in an evanescent random walkers system",
        "authors": [
            "Miguel A. Ré",
            "Natalia C. Bustos"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Diffusion mediated reaction models are particularly ubiquitous in the\ndescription of physical, chemical or biological processes. The random walk\nschema is a useful tool for formulating these models. Recently, evanescent\nrandom walk models have received attention in order to include finite lifetime\nprocesses. For instance, activated chemical reactions, such as laser\nphotolysis, exhibit a different asymptotic limit when compared with immortal\nwalker models. A diffusion limited reaction model based on a one dimensional\ncontinuous time random walk on a lattice with evanescent walkers is presented\nhere. The absorption probability density and the reaction rate are analytically\ncalculated in the Laplace domain. A finite absorption rate is considered, a\nmodel usually referred to as imperfect trapping. Short and long time behaviors\nare analyzed.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.02069v1"
    },
    {
        "title": "Data-driven prediction and prevention of extreme events in a spatially\n  extended excitable system",
        "authors": [
            "Stephan Bialonski",
            "Gerrit Ansmann",
            "Holger Kantz"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Extreme events occur in many spatially extended dynamical systems, often\ndevastatingly affecting human life which makes their reliable prediction and\nefficient prevention highly desirable. We study the prediction and prevention\nof extreme events in a spatially extended system, a system of coupled\nFitzHugh-Nagumo units, in which extreme events occur in a spatially and\ntemporally irregular way. Mimicking typical constraints faced in field studies,\nwe assume not to know the governing equations of motion and to be able to\nobserve only a subset of all phase-space variables for a limited period of\ntime. Based on reconstructing the local dynamics from data and despite being\nchallenged by the rareness of events, we are able to predict extreme events\nremarkably well. With small, rare, and spatiotemporally localized perturbations\nwhich are guided by our predictions, we are able to completely suppress extreme\nevents in this system.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.02263v1"
    },
    {
        "title": "Structure of force networks in tapped particulate systems of disks and\n  pentagons (Part 2): Persistence analysis",
        "authors": [
            "L. Kondic",
            "M. Kramar",
            "L. A. Pangolin",
            "C. M. Carlevaro",
            "K. Mischaikow"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  In the companion paper~\\cite{paper1}, we use classical measures based on\nforce probability density functions (PDFs), as well as Betti numbers\n(quantifying the number of components, related to force chains, and loops), to\ndescribe the force networks in tapped systems of disks and pentagons. In the\npresent work, we focus on the use of persistence analysis, that allows to\ndescribe these networks in much more detail. This approach allows not only to\ndescribe, but also to quantify the differences between the force networks in\ndifferent realizations of a system, in different parts of the considered\ndomain, or in different systems. We show that persistence analysis clearly\ndistinguishes the systems that are very difficult or impossible to\ndifferentiate using other means. One important finding is that the differences\nin force networks between disks and pentagons are most apparent when loops are\nconsidered: the quantities describing properties of the loops may differ\nsignificantly even if other measures (properties of components, Betti numbers,\nor force PDFs) do not distinguish clearly the investigated systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.04345v1"
    },
    {
        "title": "From empirical data to continuous Markov processes: a systematic\n  approach",
        "authors": [
            "Pedro Lencastre",
            "Frank Raischel",
            "Tim Rogers",
            "Pedro G. Lind"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We present an approach for testing for the existence of continuous generators\nof discrete stochastic transition matrices. Typically, the known approaches to\nascertain the existence of continuous Markov processes are based in the\nassumption that only time-homogeneous generators exist. Here, a systematic\nextension to time-inhomogeneity is presented, based in new mathematical\npropositions incorporating necessary and sufficient conditions, which are then\nimplemented computationally and applied to numerical data. A discussion\nconcerning the bridging between rigorous mathematical results on the existence\nof generators to its computational implementation. Our detection algorithm\nshows to be effective in more than $80\\%$ of tested matrices, typically $90\\%$\nto $95\\%$, and for those an estimate of the (non-homogeneous) generator matrix\nfollows. We also solve the embedding problem analytically for the particular\ncase of three-dimensional circulant matrices. Finally, a discussion of possible\napplications of our framework to problems in different fields is briefly\naddressed.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.07282v1"
    },
    {
        "title": "Parameter-free resolution of the superposition of stochastic signals",
        "authors": [
            "Teresa Scholz",
            "Frank Raischel",
            "Vitor V. Lopes",
            "Bernd Lehle",
            "Matthias Wächter",
            "Joachim Peinke",
            "Pedro G. Lind"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  This paper presents a direct method to obtain the deterministic and\nstochastic contribution of the sum of two independent sets of stochastic\nprocesses, one of which is composed by Ornstein-Uhlenbeck processes and the\nother being a general (non-linear) Langevin process. The method is able to\ndistinguish between all stochastic process, retrieving their corresponding\nstochastic evolution equations. This framework is based on a recent approach\nfor the analysis of multidimensional Langevin-type stochastic processes in the\npresence of strong measurement (or observational) noise, which is here extended\nto impose neither constraints nor parameters and extract all coefficients\ndirectly from the empirical data sets. Using synthetic data, it is shown that\nthe method yields satisfactory results.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.07285v1"
    },
    {
        "title": "Real time Markov chains: Wind states in anemometric data",
        "authors": [
            "P. A. Sanchez",
            "M. Robles",
            "O. A. Jaramillo"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The description of wind phenomena is frequently based on data obtained from\nanemometers, which usually report the wind speed and direction only in a\nhorizontal plane. Such measurements are commonly used either to develop wind\ngeneration farms or to forecast weather conditions in a geographical region.\nBeyond these standard applications, the information contained in the data may\nbe richer than expected and may lead to a better understanding of the wind\ndynamics in a geographical area. In this work we propose a statistical analysis\nbased on the wind velocity vectors, which we propose may be grouped in \"wind\nstates\" associated to binormal distribution functions. We found that the\nvelocity plane defined by the anemometric velocity data may be used as a phase\nspace, where a finite number of states may be found and sorted using standard\nclustering methods. The main result is a discretization technique useful to\nmodel the wind with Markov chains. We applied such ideas in anemometric data\nfor two different sites in Mexico where the wind resource is considered\nreliable. The approximated Markov chains of both places give a set of values\nfor transition probabilities and residence times that may be regarded as a\nsignature of the dynamics of the site. We consider the results may impact\neither the technical decisions for the design of wind generators or the\ndetailed resource assessment in sites with non-standard features.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.08082v2"
    },
    {
        "title": "Coarse-grained sensitivity for multiscale data assimilation",
        "authors": [
            "Nozomi Sugiura"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We show that the effective average action and its gradient are useful for\nsolving multiscale data assimilation problems. We also present a procedure for\nnumerically evaluating the gradient of the effective average action, and\ndemonstrate that the variational problem for slow degrees of freedom can be\nsolved properly using the \"effective gradient.\"\n",
        "pdf_link": "http://arxiv.org/pdf/1510.09026v5"
    },
    {
        "title": "Prospects of ratio and differential (δ) ratio based\n  measurement-models: a case study for IRMS evaluation",
        "authors": [
            "B. P. Datta"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The suitability of a mathematical-model Y = f({Xi}) in serving a purpose\nwhatsoever (should be preset by the function f specific input-to-output\nvariation-rates, i.e.) can be judged beforehand. We thus evaluate here the two\napparently similar models: YA = fA(SRi,WRi) = (SRi/WRi) and: YD = fd(SRi,WRi) =\n([SRi,WRi] - 1) = (YA - 1), with SRi and WRi representing certain\nmeasurable-variables (e.g. the sample S and the working-lab-reference W\nspecific ith-isotopic-abundance-ratios, respectively, for a case as the isotope\nratio mass spectrometry (IRMS)). The idea is to ascertain whether fD should\nrepresent a better model than fA, specifically, for the well-known IRMS\nevaluation.\n  The study clarifies that fA and fD should really represent different\nmodel-families. For example, the possible variation, eA, of an absolute\nestimate as the yA (and/ or the risk of running a machine on the basis of the\nmeasurement-model fA) should be dictated by the possible\nRi-measurement-variations (u_S and u_W) only: eA = (u_S + u_W); i.e., at worst:\neA = 2ui. However, the variation, eD, of the corresponding differential (i.e.\nYD) estimate yd should largely be decided by SRi and WRi values: ed = 2(|m_i |x\nu_i) = (|m_i | x eA); with: mi = (SRi/[SRi - WRi]).\n  Thus, any IRMS measurement (i.e. for which |SRi - WRi| is nearly zero is a\nrequirement) should signify that |mi| tends to infinity. Clearly, yD should be\nless accurate than yA, and/ or even turn out to be highly erroneous (eD tends\nto infinity). Nevertheless, the evaluation as the absolute yA, and hence as the\nsample isotopic ratio Sri, is shown to be equivalent to our previously reported\nfinding that the conversion of a D-estimate (here, yD) into Sri should help to\nimprove the achievable output-accuracy and -comparability.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.01057v1"
    },
    {
        "title": "Langevin power curve analysis for numerical WEC models with new insights\n  on high frequency power performance",
        "authors": [
            "Tanja A. Mücke",
            "Matthias Wächter",
            "Patrick Milan",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Based on the Langevin equation it has been proposed to obtain power curves\nfor wind turbines from high frequency data of wind speed measurements u(t) and\npower output P (t). The two parts of the Langevin approach, power curve and\ndrift field, give a comprehensive description of the conversion dynamic over\nthe whole operating range of the wind turbine. The method deals with high\nfrequent data instead of 10 min means. It is therefore possible to gain a\nreliable power curve already from a small amount of data per wind speed.\nFurthermore, the method is able to visualize multiple fixed points, which is\ne.g. characteristic for the transition from partial to full load or in case the\nconversion process deviates from the standard procedures. In order to gain a\ndeeper knowledge it is essential that the method works not only for measured\ndata but also for numerical wind turbine models and synthetic wind fields.\nHere, we characterize the dynamics of a detailed numerical wind turbine model\nand calculate the Langevin power curve for different data samplings. We show,\nhow to get reliable results from synthetic data and verify the applicability of\nthe method for field measurements with ultra-sonic, cup and Lidar measurements.\nThe independence of the fixed points on site specific turbulence effects is\nalso confirmed with the numerical model. Furthermore, we demonstrate the\npotential of the Langevin approach to detect failures in the conversion process\nand thus show the potential of the Langevin approach for a condition monitoring\nsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.01765v1"
    },
    {
        "title": "A framework for interpreting regularized state estimation",
        "authors": [
            "Nozomi Sugiura",
            "Shuhei Masuda",
            "Yosuke Fujii",
            "Masafumi Kamachi",
            "Yoichi Ishikawa",
            "Toshiyuki Awaji"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Four-dimensional variational data assimilation (4D-Var) on a\nseasonal-to-interdecadal time scale under the existence of unstable modes can\nbe viewed as an optimization problem of synchronized, coupled chaotic systems.\nThe problem is tackled by adjusting initial conditions to bring all stable\nmodes closer to observations and by using a continuous guide to direct unstable\nmodes toward a reference time series. This interpretation provides a consistent\nand effective procedure for solving problems of long-term state estimation. By\napplying this approach to an ocean general circulation model with a\nparameterized vertical diffusion procedure, it is demonstrated that tangent\nlinear and adjoint models in this framework should have no unstable modes and\nhence be suitable for tracking persistent signals. This methodology is widely\napplicable to extend the assimilation period in 4D-Var.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.04790v1"
    },
    {
        "title": "On non-Poissonian Voronoi tessellations",
        "authors": [
            "M. Ferraro",
            "L. Zaninetti"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The Voronoi tessellation is the partition of space for a given seeds pattern\nand the result of the partition depends completely on the type of given pattern\n\"random\", Poisson-Voronoi tessellations (PVT), or \"non-random\", Non\nPoisson-Voronoi tessellations. In this note we shall consider properties of\nVoronoi tessellations with centers generated by Sobol quasi random sequences\nwhich produce a more ordered disposition of the centers with respect to the PVT\ncase. A probability density function for volumes of these Sobol Voronoi\ntessellations (SVT) will be proposed and compared with results of numerical\nsimulations. An application will be presented concerning the local structure of\ngas ($CO_2$) in the liquid-gas coexistence phase. Furthermore a probability\ndistribution will be computed for the length of chords resulting from the\nintersections of random lines with a three-dimensional SVT. The agreement of\nthe analytical formula with the results from a computer simulation will be also\ninvestigated. Finally a new type of Voronoi tessellation based on adjustable\npositions of seeds has been introduced which generalizes both PVT and SVT\ncases.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.06572v1"
    },
    {
        "title": "Non-Gaussian Parameter in k-Dimensional Euclidean Space",
        "authors": [
            "Zihan Huang",
            "Gaoming Wang",
            "Zhao Yu"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We generalize the non-Gaussian parameter, which is utilized to characterize\nthe distinction of dynamics between realistic and Gaussian Brownian diffusions,\nin k-dimensional Euclidean space.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.06672v1"
    },
    {
        "title": "Automodel solutions for Lévy flight-based transport on a uniform\n  background",
        "authors": [
            "A. B. Kukushkin",
            "P. A. Sdvizhenskii"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  A wide class of non-stationary superdiffusive transport on a uniform\nbackground with a power-law decay, at large distances, of the step-length\nprobability distribution function (PDF) is shown to possess an automodel\nsolution. The solution for Green function is constructed using the scaling laws\nfor the propagation front (relevant-to-superdiffusion average displacement) and\nasymptotic solutions far beyond and far in advance of the propagation front.\nThese scaling laws are determined essentially by the long-free-path carriers\n(L\\'evy flights). The validity of the suggested automodel solution is proved by\nits comparison with numerical solutions in the one-dimensional (1D) case of the\ntransport equation with a simple long-tailed PDF with various power-law\nexponents and in the 3D case of the Biberman-Holstein equation of the resonance\nradiation transfer for various (Doppler, Lorentz, Voight and Holtsmark)\nspectral line shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.08910v1"
    },
    {
        "title": "Non-judgemental Dynamic Fuel Cycle Benchmarking",
        "authors": [
            "Anthony Michael Scopatz"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  This paper presents a new fuel cycle benchmarking analysis methodology by\ncoupling Gaussian process regression, a popular technique in Machine Learning,\nto dynamic time warping, a mechanism widely used in speech recognition.\nTogether they generate figures-of-merit that are applicable to any time series\nmetric that a benchmark may study. The figures-of-merit account for uncertainty\nin the metric itself, utilize information across the whole time domain, and do\nnot require that the simulators use a common time grid. Here, a distance\nmeasure is defined that can be used to compare the performance of each\nsimulator for a given metric. Additionally, a contribution measure is derived\nfrom the distance measure that can be used to rank order the importance of fuel\ncycle metrics. Lastly, this paper warns against using standard signal\nprocessing techniques for error reduction. This is because it is found that\nerror reduction is better handled by the Gaussian process regression itself.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.09095v1"
    },
    {
        "title": "Development of methods of the Fractal Dimension estimation for the\n  ecological data analysis",
        "authors": [
            "Jakub Jura",
            "Aleš Antonín Kuběna",
            "Martina Mironovová"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  This paper deals with an estimating of the Fractal Dimension of a\nhydrometeorology variables like an Air temperature or humidity at a different\nsites in a landscape (and will be further evaluated from the land use point of\nview). Three algorithms and methods of an estimation of the Fractal Dimension\nof a hydrometeorology time series were developed. The first results indicate\nthat developed methods are usable for the analysis of a hydrometeorology\nvariables and for a testing of the relation with autoregulation functions of\necosystem\n",
        "pdf_link": "http://arxiv.org/pdf/1512.01208v1"
    },
    {
        "title": "1/f noise from the nonlinear transformations of the variables",
        "authors": [
            "B. Kaulakys",
            "M. Alaburda",
            "J. Ruseckas"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The origin of the low-frequency noise with power spectrum $1/f^\\beta$ (also\nknown as $1/f$ fluctuations or flicker noise) remains a challenge. Recently,\nthe nonlinear stochastic differential equations for modeling $1/f^\\beta$ noise\nhave been proposed and analyzed. Here we use the self-similarity properties of\nthis model with respect to the nonlinear transformations of the variable of\nthese equations and show that $1/f^\\beta$ noise of the observable may yield\nfrom the power-law transformations of well-known standard processes, like the\nBrownian motion, Bessel and similar stochastic processes. Analytical and\nnumerical investigations of such techniques for modeling processes with\n$1/f^\\beta$ fluctuations is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.04298v1"
    },
    {
        "title": "Generalized Rényi Entropy and Structure Detection of Complex Dynamical\n  Systems",
        "authors": [
            "György Steinbrecher",
            "Giorgio Sonnino"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We study the problem of detecting the structure of a complex dynamical system\ndescribed by a set of deterministic differential equation that contains a\nHamiltonian subsystem, without any information on the explicit form of\nevolution laws. We suppose that initial conditions are random and the initial\nconditions of the Hamiltonian subsystem are independent from the initial\nconditions of the rest of the system. The single numerical information is the\nprobability density function of the system at one or several, finite number of\ntime instants. In the framework of the formalism of the generalized R\\'{e}nyi\nentropy we find necessary and sufficient conditions that the back reaction of\nthe Hamiltonian subsystem to the rest of the system is negligible.The results\ncan be easily generalized to the case of general, measure preserving subsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.06108v1"
    },
    {
        "title": "On the super-resolution capacity of imagers using unknown speckle\n  illuminations",
        "authors": [
            "Jérôme Idier",
            "Simon Labouesse",
            "Marc Allain",
            "Penghuan Liu",
            "Sébastien Bourguignon",
            "Anne Sentenac"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Speckle based imaging consists of forming a super-resolved reconstruction of\nan unknown sample from low-resolution images obtained under random\ninhomogeneous illuminations (speckles). In a blind context where the\nilluminations are unknown, we study the intrinsic capacity of speckle-based\nimagers to recover spatial frequencies outside the frequency support of the\ndata, with minimal assumptions about the sample. We demonstrate that, under\nphysically realistic conditions, the covariance of the data has a\nsuper-resolution power corresponding to the squared magnitude of the imager\npoint spread function. This theoretical result is important for many practical\nimaging systems such as acoustic and electromagnetic tomographs, fluorescence\nand photoacoustic microscopes, or synthetic aperture radar imaging. A numerical\nvalidation is presented in the case of fluorescence microscopy.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.06260v3"
    },
    {
        "title": "Amplitude estimation of a sine function based on confidence intervals\n  and Bayes' theorem",
        "authors": [
            "Dennis Eversmann",
            "Jörg Pretz",
            "Marcel Rosenthal"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  This paper discusses the amplitude estimation using data originating from a\nsine-like function as probability density function. If a simple least squares\nfit is used, a significant bias is observed for small amplitudes. It is shown\nthat a proper treatment using the Feldman-Cousins algorithm of likelihood\nratios allows one to construct improved confidence intervals. Using Bayes'\ntheorem a probability density function is derived for the amplitude. It is used\nin an application to show that it leads to better estimates compared to a\nsimple least squares fit.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.08715v2"
    },
    {
        "title": "Nonparametric Maximum Entropy Estimation on Information Diagrams",
        "authors": [
            "Elliot A. Martin",
            "Jaroslav Hlinka",
            "Alexander Meinke",
            "Filip Děchtěrenko",
            "Jörn Davidsen"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Maximum entropy estimation is of broad interest for inferring properties of\nsystems across many different disciplines. In this work, we significantly\nextend a technique we previously introduced for estimating the maximum entropy\nof a set of random discrete variables when conditioning on bivariate mutual\ninformations and univariate entropies. Specifically, we show how to apply the\nconcept to continuous random variables and vastly expand the types of\ninformation-theoretic quantities one can condition on. This allows us to\nestablish a number of significant advantages of our approach over existing\nones. Not only does our method perform favorably in the undersampled regime,\nwhere existing methods fail, but it also can be dramatically less\ncomputationally expensive as the cardinality of the variables increases. In\naddition, we propose a nonparametric formulation of connected informations and\ngive an illustrative example showing how this agrees with the existing\nparametric formulation in cases of interest. We further demonstrate the\napplicability and advantages of our method to real world systems for the case\nof resting-state human brain networks. Finally, we show how our method can be\nused to estimate the structural network connectivity between interacting units\nfrom observed activity and establish the advantages over other approaches for\nthe case of phase oscillator networks as a generic example.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.00336v1"
    },
    {
        "title": "Model uncertainty and reference value of the Planck constant",
        "authors": [
            "Giovanni Mana"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Statistical parametric models are proposed to explain the values of the\nPlanck constant obtained by comparing electrical and mechanical powers and by\ncounting atoms in Si 28 enriched crystals. They assume that uncertainty\ncontributions -- having heterogeneous, datum-specific, variances -- might not\nbe included in the error budgets of some of the measured values. Model\nselection and model averaging are used to investigate data consistency, to\nidentify a reference value of the Planck constant, and to include the model\nuncertainty in the error budget.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.05765v1"
    },
    {
        "title": "On the use of financial analysis tools for the study of Dst time series\n  in the frame of complex systems",
        "authors": [
            "Stelios M. Potirakis",
            "Pavlos I. Zitis",
            "Georgios Balasis",
            "Konstantinos Eftaxias"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Technical analysis is considered the oldest, currently omnipresent, method\nfor financial markets analysis, which uses past prices aiming at the possible\nshort-term forecast of future prices. In the frame of complex systems, methods\nused to quantitatively analyze specific dynamic phenomena are often used to\nanalyze phenomena from other disciplines on the grounds that are governed by\nsimilar dynamics. An interesting task is the forecast of a magnetic storm. The\nhourly Dst is used as a global index for the monitoring of Earth's\nmagnetosphere, which could be either in quiet (normal) or in magnetic storm\n(pathological) state. This work is the first attempt to apply technical\nanalysis tools on Dst time series, aiming at the identification of indications\nwhich could be used for the study of the temporal evolution of Earth's\nmagnetosphere state. We focus on the analysis of Dst time series around the\noccurrence of magnetic storms, discussing the possible use of the resulting\ninformation in the frame of multidisciplinary efforts towards extreme events\nforecasting. We employ the following financial analysis tools: simple moving\naverage (SMA), Bollinger bands, and relative strength index (RSI). Using these\ntools, we formulate a methodology based on all indications that could be\nrevealed in order to infer the onset, duration and recovery phase of a magnetic\nstorm, focusing on the temporal sequence they occur. The applicability of the\nproposed methodology is examined on characteristic cases of magnetic storms\nwith encouraging results for space weather forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.07334v1"
    },
    {
        "title": "Blind fluorescence structured illumination microscopy: A new\n  reconstruction strategy",
        "authors": [
            "S. Labouesse",
            "M. Allain",
            "J. Idier",
            "S. Bourguignon",
            "A. Negash",
            "P. Liu",
            "A. Sentenac"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In this communication, a fast reconstruction algorithm is proposed for\nfluorescence \\textit{blind} structured illumination microscopy (SIM) under the\nsample positivity constraint. This new algorithm is by far simpler and faster\nthan existing solutions, paving the way to 3D and/or real-time 2D\nreconstruction.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.07851v3"
    },
    {
        "title": "Procedure to Approximately Estimate the Uncertainty of Material Ratio\n  Parameters due to Inhomogeneity of Surface Roughness",
        "authors": [
            "Dorothee Hüser",
            "Jonathan Hüser",
            "Sebastian Rief",
            "Jörg Seewig",
            "Peter Thomsen-Schmidt"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Roughness parameters that characterize contacting surfaces with regard to\nfriction and wear are commonly stated without uncertainties, or with an\nuncertainty only taking into account a very limited amount of aspects such as\nrepeatability of reproducibility (homogeneity) of the specimen. This makes it\ndifficult to discriminate between different values of single roughness\nparameters.\n  Therefore uncertainty assessment methods are required that take all relevant\naspects into account. In the literature this is scarcely performed and examples\nspecific for parameters used in friction and wear are not yet given.\n  We propose a procedure to derive the uncertainty from a single profile\nemploying a statistical method that is based on the statistical moments of the\namplitude distribution and the autocorrelation length of the profile. To show\nthe possibilities and the limitations of this method we compare the uncertainty\nderived from a single profile with that derived from a high statistics\nexperiment.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.00746v2"
    },
    {
        "title": "Variational estimation of the drift for stochastic differential\n  equations from the empirical density",
        "authors": [
            "Philipp Batz",
            "Andreas Ruttor",
            "Manfred Opper"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We present a method for the nonparametric estimation of the drift function of\ncertain types of stochastic differential equations from the empirical density.\nIt is based on a variational formulation of the Fokker-Planck equation. The\nminimization of an empirical estimate of the variational functional using\nkernel based regularization can be performed in closed form. We demonstrate the\nperformance of the method on second order, Langevin-type equations and show how\nthe method can be generalized to other noise models.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01159v1"
    },
    {
        "title": "The Langevin Approach: An R Package for Modeling Markov Processes",
        "authors": [
            "Philip Rinn",
            "Pedro G. Lind",
            "Matthias Wächter",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We describe an R package developed by the research group Turbulence, Wind\nenergy and Stochastics (TWiSt) at the Carl von Ossietzky University of\nOldenburg, which extracts the (stochastic) evolution equation underlying a set\nof data or measurements. The method can be directly applied to data sets with\none or two stochastic variables. Examples for the one-dimensional and\ntwo-dimensional cases are provided. This framework is valid under a small set\nof conditions which are explicitly presented and which imply simple preliminary\ntest procedures to the data. For Markovian processes involving Gaussian white\nnoise, a stochastic differential equation is derived straightforwardly from the\ntime series and captures the full dynamical properties of the underlying\nprocess. Still, even in the case such conditions are not fulfilled, there are\nalternative versions of this method which we discuss briefly and provide the\nuser with the necessary bibliography.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.02036v1"
    },
    {
        "title": "Numerical Stability of Generalized Entropies",
        "authors": [
            "György Steinbrecher",
            "Giorgio Sonnino"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In many applications, the probability density function is subject to\nexperimental errors. In this work the continuos dependence of a class of\ngeneralized entropies on the experimental errors is studied. This class\nincludes the C. Shannon, C. Tsallis, A. R\\'{e}nyi and generalized R\\'{e}nyi\nentropies. By using the connection between R\\'{e}nyi or Tsallis entropies, and\nthe \\textit{distance} in a the Lebesgue functional spaces, we introduce a\nfurther extensive generalizations of the R\\'{e}nyi entropy. In this work we\nsuppose that the experimental error is measured by some generalized $L^{p}$\ndistance. In line with the methodology normally used for treating the so called\n\\textit{ill-posed problems}, auxiliary stabilizing conditions are determined,\nsuch that small - in the sense of $L^{p}$ metric - experimental errors provoke\nsmall variations of the classical and generalized entropies. These stabilizing\nconditions are formulated in terms of $L^{p}$ metric in a class of generalized\n$L^{p}$ spaces of functions. Shannon's entropy requires, however, more\nrestrictive stabilizing conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.06240v3"
    },
    {
        "title": "A robust and passive method for geometric calibration of large arrays",
        "authors": [
            "Charles Vanwynsberghe",
            "Pascal Challande",
            "Jacques Marchal",
            "Régis Marchiano",
            "François Ollivier"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  This paper presents a complete strategy for the geometry estimation of large\nmicrophone arrays of arbitrary shape. Largeness is intended here in both number\nof microphones (hundreds) and size (few meters). Such arrays can be used for\nvarious applications in open or confined spaces like acoustical imaging, source\nidentification, or speech processing. For so large array systems,measuring the\ngeometry by hand is impractical. Therefore a blind passive method is proposed.\nIt is based on the analysis of the background acoustic noise, supposed to be a\ndiffuse field. The proposed strategy is a two-step process. First the pairwise\nmicrophone distances are identified by matching their measured coherence\nfunction to the one predicted by the diffuse field theory. Second, a robust\nmultidimensional scaling(MDS) algorithm is adapted and implemented. It takes\nadvantage of local characteristics to reduce the set of distances and infer the\ngeometry of the array. This work is an extension of previous studies, and it\novercomes unsolved drawbacks. In particular it deals efficiently with the\noutliers known to ruin standard MDS algorithms. Experimental proofs of this\nability are presented by treating the case of two arrays. They show that the\nproposed improvements manage large spatial arrays.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.07903v1"
    },
    {
        "title": "Least square estimation of phase, frequency and PDEV",
        "authors": [
            "Magnus Danielson",
            "Francois Vernotte",
            "Enrico Rubiola"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The Omega-preprocessing was introduced to improve phase noise rejection by\nusing a least square algorithm. The associated variance is the PVAR which is\nmore efficient than MVAR to separate the different noise types. However, unlike\nAVAR and MVAR, the decimation of PVAR estimates for multi-tau analysis is not\npossible if each counter measurement is a single scalar. This paper gives a\ndecimation rule based on two scalars, the processing blocks, for each\nmeasurement. For the Omega-preprocessing, this implies the definition of an\noutput standard as well as hardware requirements for performing high-speed\ncomputations of the blocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.01004v1"
    },
    {
        "title": "Modified cumulative distribution function in application to waiting time\n  analysis in CTRW scenario",
        "authors": [
            "Rafał Połoczański",
            "Agnieszka Wyłomańska",
            "Janusz Gajda",
            "Monika Maciejewska",
            "Andrzej Szczurek"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The continuous time random walk model plays an important role in modeling of\nso called anomalous diffusion behaviour. One of the specific property of such\nmodel are constant time periods visible in trajectory. In the continuous time\nrandom walk approach they are realizations of the sequence called waiting\ntimes. The main attention of the paper is paid on the analysis of waiting times\ndistribution. We introduce here novel methods of estimation and statistical\ninvestigation of such distribution. The methods are based on the modified\ncumulative distribution function. In this paper we consider three special cases\nof waiting time distributions, namely $\\alpha$-stable, tempered stable and\ngamma. However the proposed methodology can be applied to broad set of\ndistributions - in general it may serve as a method of fitting any distribution\nfunction if the observations are rounded. The new statistical techniques we\napply to the simulated data as well as to the real data describing $CO_2$\nconcentration in indoor air.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.02653v1"
    },
    {
        "title": "3D Image Reconstruction from Compton camera data",
        "authors": [
            "Peter Kuchment",
            "Fatma Terzioglu"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In this paper, we address analytically and numerically the inversion of the\nintegral transform (\\emph{cone} or \\emph{Compton} transform) that maps a\nfunction on $\\mathbb{R}^3$ to its integrals over conical surfaces. It arises in\na variety of imaging techniques, e.g. in astronomy, optical imaging, and\nhomeland security imaging, especially when the so called Compton cameras are\ninvolved.\n  Several inversion formulas are developed and implemented numerically in $3D$\n(the much simpler $2D$ case was considered in a previous publication). An\nadmissibility condition on detectors geometry is formulated, under which all\nthese inversion techniques will work.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.03805v3"
    },
    {
        "title": "A simple numeric algorithm for ancient coin dies identification",
        "authors": [
            "Luca Lista"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  A simple computer-based algorithm has been developed to identify pre-modern\ncoins minted from the same dies, intending mainly coins minted by hand-made\ndies designed to be applicable to images taken from auction websites or\ncatalogs. Though the method is not intended to perform a complete automatic\nclassification, which would require more complex and intensive algorithms\naccessible to experts of computer vision its simplicity of use and lack of\nspecific requirement about the quality of pictures can provide help and\ncomplementary information to the visual inspection, adding quantitative\nmeasurements of the \"distance\" between pairs of different coins. The distance\nmetric is based on a number of pre-defined reference points that mark key\nfeatures of the coin to identify the set of coins they have been minted from.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.04074v3"
    },
    {
        "title": "Thermodynamics of Benford's First Digit Law",
        "authors": [
            "Don S. Lemons"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Iafrate, Miller, and Strauch [Equipartition and a Distribution for Numbers: A\nStatistical Model for Benford's Law,\" arXiv:1503.08259] construct and test a\nstatistical model for partitioning a conserved quantity. One consequence of\ntheir model is Benford's law. This Comment amplifies their work by exploring\nits thermodynamic consequences.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.05715v1"
    },
    {
        "title": "The exact Laplacian spectrum for the Dyson hierarchical network",
        "authors": [
            "Elena Agliari",
            "Flavia Tavani"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We consider the Dyson hierarchical graph $\\mathcal{G}$, that is a weighted\nfully-connected graph, where the pattern of weights is ruled by the parameter\n$\\sigma \\in (1/2, 1]$. Exploiting the deterministic recursivity through which\n$\\mathcal{G}$ is built, we are able to derive explicitly the whole set of the\neigenvalues and the eigenvectors for its Laplacian matrix. Given that the\nLaplacian operator is intrinsically implied in the analysis of dynamic\nprocesses (e.g., random walks) occurring on the graph, as well as in the\ninvestigation of the dynamical properties of connected structures themselves\n(e.g., vibrational structures and the relaxation modes), this result allows\naddressing analytically a large class of problems. In particular, as examples\nof applications, we study the random walk and the continuous-time quantum walk\nembedded in $\\mathcal{G}$, and the relaxation times of a polymer whose\nstructure is described by $\\mathcal{G}$.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.05864v2"
    },
    {
        "title": "HORACE: software for the analysis of data from single crystal\n  spectroscopy experiments at time-of-flight neutron instruments",
        "authors": [
            "R. A. Ewings",
            "A. Buts",
            "M. D. Le",
            "J. van Duijn",
            "I. Bustinduy",
            "T. G. Perring"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The HORACE suite of programs has been developed to work with large\nmultiple-measurement data sets collected from time-of-flight neutron\nspectrometers equipped with arrays of position-sensitive detectors. The\nsoftware allows exploratory studies of the four dimensions of reciprocal space\nand excitation energy to be undertaken, enabling multi-dimensional subsets to\nbe visualized, algebraically manipulated, and models for the scattering to\nsimulated or fitted to the data. The software is designed to be an extensible\nframework, thus allowing user-customized operations to be performed on the\ndata. Examples of the use of its features are given for measurements exploring\nthe spin waves of the simple antiferromagnet RbMnF$_{3}$ and ferromagnetic\niron, and the phonons in URu$_{2}$Si$_{2}$.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.05895v1"
    },
    {
        "title": "On Causality in Dynamical Systems",
        "authors": [
            "Daniel Harnack",
            "Erik Laminski",
            "Klaus Richard Pawelzik"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Discovery of causal relations is fundamental for understanding the dynamics\nof complex systems. While causal interactions are well defined for acyclic\nsystems that can be separated into causally effective subsystems, a\nmathematical definition of gradual causal interaction is still lacking for\nnonseparable dynamical systems. The solution proposed here is analytically\ntractable for time discrete chaotic maps and is shown to fulfill basic\nrequirements for causality measures. It implies a method for determination of\ndirected effective influences using pairs of measurements from dynamical\nsystems. Applications to time series from systems of coupled differential\nequations and linear stochastic systems demonstrate its general utility.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.02570v3"
    },
    {
        "title": "Sequential motif profile of natural visibility graphs",
        "authors": [
            "Jacopo Iacovacci",
            "Lucas Lacasa"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The concept of sequential visibility graph motifs -subgraphs appearing with\ncharacteristic frequencies in the visibility graphs associated to time series-\nhas been advanced recently along with a theoretical framework to compute\nanalytically the motif profiles associated to Horizontal Visibility Graphs\n(HVGs). Here we develop a theory to compute the profile of sequential\nvisibility graph motifs in the context of Natural Visibility Graphs (VGs). This\ntheory gives exact results for deterministic aperiodic processes with a smooth\ninvariant density or stochastic processes that fulfil the Markov property and\nhave a continuous marginal distribution. The framework also allows for a linear\ntime numerical estimation in the case of empirical time series. A comparison\nbetween the HVG and the VG case (including evaluation of their robustness for\nshort series polluted with measurement noise) is also presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.02645v1"
    },
    {
        "title": "Distributed data processing and analysis environment for neutron\n  scattering experiments at CSNS",
        "authors": [
            "H. L. Tian",
            "J. R. Zhang",
            "L. L. Yan",
            "M. Tang",
            "L. Hu",
            "D. X. Zhao",
            "Y. X. Qiu",
            "H. Y. Zhang",
            "J. Zhuang",
            "R. Du"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  China Spallation Neutron Source (CSNS) is the first high-performance pulsed\nneutron source in China, which will meet the increasing fundamental research\nand technique applications demands domestically and overseas. A new distributed\ndata processing and analysis environment has been developed, which has generic\nfunctionalities for neutron scattering experiments. The environment consists of\nthree parts, an object-oriented data processing framework adopting a data\ncentered architecture, a communication and data caching system based on the C/S\nparadigm, and data analysis and visualization software providing the 2D/3D\nexperimental data display. This environment will be widely applied in CSNS for\nlive data processing.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.04053v2"
    },
    {
        "title": "Reconstruction of Ordinary Differential Equations From Time Series Data",
        "authors": [
            "Manuel Mai",
            "Mark D. Shattuck",
            "Corey S. O'Hern"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We develop a numerical method to reconstruct systems of ordinary differential\nequations (ODEs) from time series data without {\\it a priori} knowledge of the\nunderlying ODEs using sparse basis learning and sparse function reconstruction.\nWe show that employing sparse representations provides more accurate ODE\nreconstruction compared to least-squares reconstruction techniques for a given\namount of time series data. We test and validate the ODE reconstruction method\non known 1D, 2D, and 3D systems of ODEs. The 1D system possesses two stable\nfixed points; the 2D system possesses an oscillatory fixed point with closed\norbits; and the 3D system displays chaotic dynamics on a strange attractor. We\ndetermine the amount of data required to achieve an error in the reconstructed\nfunctions to less than $0.1\\%$. For the reconstructed 1D and 2D systems, we are\nable to match the trajectories from the original ODEs even at long times. For\nthe 3D system with chaotic dynamics, as expected, the trajectories from the\noriginal and reconstructed systems do not match at long times, but the\nreconstructed and original models possess similar Lyapunov exponents. Now that\nwe have validated this ODE reconstruction method on known models, it can be\nemployed in future studies to identify new systems of ODEs using time series\ndata from deterministic systems for which there is no currently known ODE\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.05420v1"
    },
    {
        "title": "Manifold boundaries give \"gray-box\" approximations of complex models",
        "authors": [
            "Mark K. Transtrum"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We discuss a method of parameter reduction in complex models known as the\nManifold Boundary Approximation Method (MBAM). This approach, based on a\ngeometric interpretation of statistics, maps the model reduction problem to a\ngeometric approximation problem. It operates iteratively, removing one\nparameter at a time, by approximating a high-dimension, but thin manifold by\nits boundary. Although the method makes no explicit assumption about the\nfunctional form of the model, it does require that the model manifold exhibit a\nhierarchy of boundaries, i.e., faces, edges, corners, hyper-corners, etc. We\nempirically show that a variety of model classes have this curious feature,\nmaking them amenable to MBAM. These model classes include models composed of\nelementary functions (e.g., rational functions, exponentials, and partition\nfunctions), a variety of dynamical system (e.g., chemical and biochemical\nkinetics, Linear Time Invariant (LTI) systems, and compartment models), network\nmodels (e.g., Bayesian networks, Markov chains, artificial neural networks, and\nMarkov random fields), log-linear probability distributions, and models with\nsymmetries. We discuss how MBAM recovers many common approximation methods for\neach model class and discuss potential pitfalls and limitations.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.08705v1"
    },
    {
        "title": "Joint reconstruction strategy for structured illumination microscopy\n  with unknown illuminations",
        "authors": [
            "Simon Labouesse",
            "Awoke Negash",
            "Jérôme Idier",
            "Sébastien Bourguignon",
            "Thomas Mangeat",
            "Penghuan Liu",
            "Anne Sentenac",
            "Marc Allain"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The blind structured illumination microscopy (SIM) strategy proposed in\n(Mudry et al., 1992) is fully re-founded in this paper, unveiling the central\nrole of the sparsity of the illumination patterns in the mechanism that drives\nsuper-resolution in the method. A numerical analysis shows that the resolving\npower of the method can be further enhanced with optimized one-photon or\ntwo-photon speckle illuminations. A much improved numerical implementation is\nprovided for the reconstruction problem under the image positivity constraint.\nThis algorithm rests on a new preconditioned proximal iteration faster than\nexisting solutions, paving the way to 3D and real-time 2D reconstruction\n",
        "pdf_link": "http://arxiv.org/pdf/1607.01980v4"
    },
    {
        "title": "A new model test in high energy physics in frequentist and Bayesian\n  statistical formalisms",
        "authors": [
            "Andrey Kamenshchikov"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  A problem of a new physical model test given observed experimental data is a\ntypical one for modern experiments of high energy physics (HEP). A solution of\nthe problem may be provided with two alternative statistical formalisms, namely\nfrequentist and Bayesian, which are widely spread in contemporary HEP searches.\nA characteristic experimental situation is modeled from general considerations\nand both the approaches are utilized in order to test a new model. The results\nare juxtaposed, what demonstrates their consistency in this work. An effect of\na systematic uncertainty treatment in the statistical analysis is also\nconsidered.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.04141v1"
    },
    {
        "title": "Analysis of distorted measurements -- parameter estimation and unfolding",
        "authors": [
            "Guenter Zech"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  1. Parameter inference from distorted measurements is discussed. 2. Smeared\nmeasurements are unfolded without explicit regularization. The corresponding\nresults are unbiased and permit to fit parameters and to apply quantitative\ngoodness-of-fit tests. 3. Common unfolding methods (iterative EM with early\nstopping, truncated SVD, ML fits with curvature, entropy and norm penalties)\nare tested and compared to each other with the regularization parameter\nadjusted to minimize the integrated square error (ISE) in all cases. Apart from\nhistogram representations, spline approximations are considered. All\nsimulations indicate that the EM method leads to smaller ISEs than the\ncompeting approaches. Especially promising is the EM unfolding to spline\napproximations.\n  The studies are based on different distributions, event numbers, resolutions\nand enough independent simulations to obtain conclusive results. It is proposed\nto unfold data with the EM method to b-spline approximations and to supplement\nthe results with histograms and error matrices obtained by unfolding into\nrelative wide bins without explicit regularization.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.06910v1"
    },
    {
        "title": "Practical Statistics for Particle Physicists",
        "authors": [
            "Harrison B. Prosper"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  These lectures introduce the basic ideas and practices of statistical\nanalysis for particle physicists, using a real-world example to illustrate how\nthe abstractions on which statistics is based are translated into practical\napplication.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.03201v1"
    },
    {
        "title": "Van Vleck correction generalization for complex correlators with\n  multilevel quantization",
        "authors": [
            "L. V. Benkevitch",
            "A. E. E. Rogers",
            "C. J. Lonsdale",
            "R. J. Cappallo",
            "D. Oberoi",
            "P. J. Erickson",
            "K. A. V. Baker"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Remote sensing with phased antenna arrays is based on measurement of the\ncross-correlations between the signals from each antenna pair. Digital\ncorrelators have systematic errors due to the quantization losses. The\ncorrelation errors allow substantial abatement based on the assumption that the\nanalog signals are stochastic processes sampled from a statistical distribution\n(usually the Gaussian). The correlation correction technique is named after Van\nVleck who was the first to apply it to two-level clipping quantizers. The\ncorrection is especially important for high correlation levels, e.g. in studies\nof solar radio emissions. We offer a generalized method that for every antenna\npair inputs the quantized signals' covariance and standard deviations, and\noutputs high-precision estimates of the analog correlation. Although\ncorrelation correction methods have been extensively investigated in the past,\nthere are several problems that, as far as we know, have not been published\nyet. We consider a very general quantization scheme with arbitrary set of\ntransition thresholds and output levels, and our correction method is designed\nfor correlations obtained from signals with generally unequal standard\ndeviations. We also provide a method for estimation of the analog standard\ndeviation from the quantized one for subsequent use in the correlation\ncorrection. We apply the correction to the the complex-valued analytic signals,\noverwhelmingly used in modern remote sensing systems with arrays of antennas.\nThe approach is valid not only for analytic signals with the imaginary part\nbeing the Hilbert transform of the real one, but also for more general,\ncircularly symmetric complex processes whose real and imaginary parts may have\narbitrary relationships to each other. This work was motivated by the need for\ngreater precision in analysis of data from the Murchison Widefield Array (MWA).\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04367v1"
    },
    {
        "title": "PALMA, an improved algorithm for DOSY signal processing",
        "authors": [
            "Afef Cherni",
            "Emilie Chouzenoux",
            "Marc-André Delsuc"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  NMR is a tool of choice for the measure of diffusion coefficients of species\nin solution. The DOSY experiment, a 2D implementation of this measure, has\nproven to be particularly useful for the study of complex mixtures, molecular\ninteractions, polymers, etc. However, DOSY data analysis requires to resort to\ninverse Laplace transform, in particular for polydisperse samples. This is a\nknown difficult numerical task, for which we present here a novel approach. A\nnew algorithm based on a splitting scheme and on the use of proximity operators\nis introduced. Used in conjunction with a Maximum Entropy and $\\ell_1$ hybrid\nregularisation, this algorithm converges rapidly and produces results robust\nagainst experimental noise. This method has been called PALMA. It is able to\nreproduce faithfully monodisperse as well as polydisperse systems, and numerous\nsimulated and experimental examples are presented. It has been implemented on\nthe server http://palma.labo.igbmc.fr where users can have their datasets\nprocessed automatically.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.07055v2"
    },
    {
        "title": "Improved Inference for the Signal Significance",
        "authors": [
            "Igor Volobouev",
            "A. Alexandre Trindade"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We study the properties of several likelihood-based statistics commonly used\nin testing for the presence of a known signal under a mixture model with known\nbackground, but unknown signal fraction. Under the null hypothesis of no\nsignal, all statistics follow a standard normal distribution in large samples,\nbut substantial deviations can occur at low sample sizes. Approximations for\nrespective $p$-values are derived to various orders of accuracy using the\nmethodology of Edgeworth expansions. Adherence to normality is studied, and the\nmagnitude of deviations is quantified according to resulting inflation or\ndeflation. We find that approximations to third-order accuracy are generally\nsufficient to guarantee $p$-values with nominal false positive error rates in\nthe five sigma range ($p$-value $= 2.87 \\times 10^{-7}$) for the classic Wald,\nscore, and likelihood ratio (LR) statistics at relatively low samples. Not only\ndoes LR have better adherence to normality, but it also consistently\noutperforms all other statistics in terms of false negative error rates. The\nreasons for this are shown to be connected with high-order cumulant behavior\ngleaned from fourth order Edgeworth expansions. Finally, a conservative\nprocedure is suggested for making finite sample adjustments while accounting\nfor the look elsewhere effect with the theory of random fields (a.k.a. the\nGross-Vitells method).\n",
        "pdf_link": "http://arxiv.org/pdf/1609.00752v2"
    },
    {
        "title": "Scale-Bridging Model Development for Coal Particle Devolatilization",
        "authors": [
            "Benjamin B Schroeder",
            "Sean T Smith",
            "Philip J Smith",
            "Thomas H Fletcher",
            "Andrew Packard",
            "Michael Frenklach",
            "Arun Hegde",
            "Wenyu Li",
            "James Oreluk"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  When performing large-scale, high-performance computations of multi-physics\napplications, it is common to limit the complexity of physics sub-models\ncomprising the simulation. For a hierarchical system of coal boiler simulations\na scale-bridging model is constructed to capture characteristics appropriate\nfor the application-scale from a detailed coal devolatilization model. Such\nscale-bridging allows full descriptions of scale-applicable physics, while\nfunctioning at reasonable computational costs. This study presents a variation\non multi-fidelity modeling with a detailed physics model, the chemical\npercolation devolatilization model, being used to calibrate a scale-briding\nmodel for the application of interest. The application space provides essential\ncontext for designing the scale-bridging model by defining scales, determining\nrequirements and weighting desired characteristics. A single kinetic reaction\nequation with functional yield model and distributed activation energy is\nimplemented to act as the scale-bridging model-form. Consistency constraints\nare used to locate regions of the scale-bridging model's parameter-space that\nare consistent with the uncertainty identified within the detailed model.\nUltimately, the performance of the scale-bridging model with consistent\nparameter-sets was assessed against desired characteristics of the detailed\nmodel and found to perform satisfactorily in capturing thermodynamic trends and\nkinetic timescales for the desired application-scale. Framing the process of\nmodel-form selection within the context of calibration and uncertainty\nquantification allows the credibility of the model to be established.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.00871v1"
    },
    {
        "title": "Combination of measurements and the BLUE method",
        "authors": [
            "Luca Lista"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The most accurate method to combine measurement from different experiments is\nto build a combined likelihood function and use it to perform the desired\ninference. This is not always possible for various reasons, hence approximate\nmethods are often convenient. Among those, the best linear unbiased estimator\n(BLUE) is the most popular, allowing to take into account individual\nuncertainties and their correlations. The method is unbiased by construction if\nthe true uncertainties and their correlations are known, but it may exhibit a\nbias if uncertainty estimates are used in place of the true ones, in particular\nif those estimated uncertainties depend on measured values. In those cases, an\niterative application of the BLUE method may reduce the bias of the combined\nmeasurement.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.00422v2"
    },
    {
        "title": "Three-dimensional thermographic imaging using a virtual wave concept",
        "authors": [
            "Peter Burgholzer",
            "Michael Thor",
            "Jürgen Gruber",
            "Günther Mayr"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In this work, it is shown that image reconstruction methods from ultrasonic\nimaging can be employed for thermographic signals. Before using these imaging\nmethods, a virtual signal is calculated by applying a local transformation to\nthe temperature evolution measured on a sample surface. The introduced\ntransformation describes all the irreversibility of the heat diffusion process\nand can be used for every sample shape. To date, one-dimensional methods have\nbeen primarily used in thermographic imaging. The proposed two-stage algorithm\nenables reconstruction in two and three dimensions. The feasibility of this\napproach is demonstrated through simulations and experiments. For the latter,\nsmall steel beads embedded in an epoxy resin are imaged. The resolution limit\nis found to be proportional to the depth of the structures and to be inversely\nproportional to the logarithm of the signal-to-noise ratio. Limited-view\nartefacts can arise if the measurement is performed on a single planar\ndetection surface. These artifacts can be reduced by measuring the\nthermographic signals from multiple planes, which is demonstrated by numerical\nsimulations and by experiments performed on an epoxy cube.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.00868v2"
    },
    {
        "title": "Design Analysis for Optimal Calibration of Diffusivity in Reactive\n  Multilayers",
        "authors": [
            "Manav Vohra",
            "Xun Huan",
            "Timothy P. Weihs",
            "Omar M. Knio"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Calibration of the uncertain Arrhenius diffusion parameters for quantifying\nmixing rates in Zr-Al nanolaminate foils was performed in a Bayesian setting\n[Vohra et al., 2014]. The parameters were inferred in a low temperature regime\ncharacterized by homogeneous ignition and a high temperature regime\ncharacterized by self-propagating reactions in the multilayers. In this work,\nwe extend the analysis to find optimal experimental designs that would provide\nthe best data for inference. We employ a rigorous framework that quantifies the\nexpected in- formation gain in an experiment, and find the optimal design\nconditions using numerical techniques of Monte Carlo, sparse quadrature, and\npolynomial chaos surrogates. For the low temperature regime, we find the\noptimal foil heating rate and pulse duration, and confirm through simulation\nthat the optimal design indeed leads to sharper posterior distributions of the\ndiffusion parameters. For the high temperature regime, we demonstrate potential\nfor increase in the expected information gain of the posteriors by increasing\nsample size and reducing uncertainty in measurements. Moreover, posterior\nmarginals are also produced to verify favorable experimental scenarios for this\nregime.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.02558v1"
    },
    {
        "title": "Three Lectures on Probability and Statistics",
        "authors": [
            "Carlos Mana"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Notes for a Course on Probability and Statistics: L1: Elements of\nProbability; L2: Bayesian Inference; L3: Monte Carlo Methods\n",
        "pdf_link": "http://arxiv.org/pdf/1610.05590v3"
    },
    {
        "title": "Uncertainty propagation with functionally correlated quantities",
        "authors": [
            "Mosè Giordano"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Many uncertainty propagation software exist, written in different programming\nlanguages, but not all of them are able to handle functional correlation\nbetween quantities. In this paper we review one strategy to deal with\nuncertainty propagation of quantities that are functionally correlated, and\nintroduce a new software offering this feature: the Julia package\nMeasurements.jl. It supports real and complex numbers with uncertainty,\narbitrary-precision calculations, mathematical and linear algebra operations\nwith matrices and arrays.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.08716v1"
    },
    {
        "title": "Machine learning methods for nanolaser characterization",
        "authors": [
            "Darko Zibar",
            "Molly Piels",
            "Ole Winther",
            "Jesper Moerk",
            "Christian Schaeffer"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Nanocavity lasers, which are an integral part of an on-chip integrated\nphotonic network, are setting stringent requirements on the sensitivity of the\ntechniques used to characterize the laser performance. Current characterization\ntools cannot provide detailed knowledge about nanolaser noise and dynamics. In\nthis progress article, we will present tools and concepts from the Bayesian\nmachine learning and digital coherent detection that offer novel approaches for\nhighly-sensitive laser noise characterization and inference of laser dynamics.\nThe goal of the paper is to trigger new research directions that combine the\nfields of machine learning and nanophotonics for characterizing nanolasers and\neventually integrated photonic networks\n",
        "pdf_link": "http://arxiv.org/pdf/1611.03335v1"
    },
    {
        "title": "Vector Nonlocal Euclidean Median: Principal Bundle Captures The Nature\n  of Patch Space",
        "authors": [
            "Chen-Yun Lin",
            "Arin Minasian",
            "Xin Jessica Qi",
            "Hau-Tieng Wu"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We extensively study the rotational group structure inside the patch space by\nintroducing the fiber bundle structure. The rotational group structure leads to\na new image denoising algorithm called the \\textit{vector non-local Euclidean\nmedian} (VNLEM). The theoretical aspect of VNLEM is studied, which explains why\nthe VNLEM and traditional non-local mean/non-local Euclidean median (NLEM)\nalgorithm work. The numerical issue of the VNLEM is improved by taking the\norientation feature in the commonly applied scale-invariant feature transform\n(SIFT), and a theoretical analysis of the robustness of the orientation feature\nin the SIFT is provided. The VNLEM is applied to an image database of 1,361\nimages and compared with the NLEM. Different image quality assessments based on\nthe error-sensitivity or the human visual system are applied to evaluate the\nperformance. The results confirmed the potential of the VNLEM algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.05073v2"
    },
    {
        "title": "Properties of frequentist confidence levels derivatives",
        "authors": [
            "Miriam Lucio Martínez",
            "Diego Martínez Santos",
            "Francesco Dettori"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In high energy physics, results from searches for new particles or rare\nprocesses are often reported using a modified frequentist approach, known as\n$\\rm{CL_s}$ method. In this paper, we study the properties of the derivatives\nof $\\rm{CL_s}$ and $\\rm{CL_{s+b}}$ as signal strength estimators if the\nconfidence levels are interpreted as credible intervals. Our approach allows\nobtaining best fit points and $\\chi^2$ functions which can be used for\nphenomenology studies. In addition, this approach can be used to incorporate\n$\\rm{CL_s}$ results into Bayesian combinations.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.06293v1"
    },
    {
        "title": "Probabilistic prediction of the AL index with the diffusion forecasting\n  model",
        "authors": [
            "Dimitrios Giannakis",
            "Matina Gkioulidou",
            "John Harlim"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We propose a nonparametric approach for probabilistic prediction of the AL\nindex trained with AL and solar wind ($v B_z$) data. Our framework relies on\nthe diffusion forecasting technique, which views AL and $ v B_z $ data as\nobservables of an autonomous, ergodic, stochastic dynamical system operating on\na manifold. Diffusion forecasting builds a data-driven representation of the\nMarkov semigroup governing the evolution of probability measures of the\ndynamical system. In particular, the Markov semigroup operator is represented\nin an orthonormal basis acquired from data using the diffusion maps algorithm\nand Takens delay embeddings. This representation of the evolution semigroup is\nused in conjunction with a Bayesian filtering algorithm for forecast\ninitialization to predict the probability that the AL index is less than a\nuser-selected threshold over arbitrary lead times and without requiring\nexogenous inputs. We find that the model produces skillful forecasts out to at\nleast two-hour leads despite gaps in the training data.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07272v2"
    },
    {
        "title": "A Neural Network Approach for the Peak Profile Characterization",
        "authors": [
            "Ruben A. Dilanian"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The neural network-based approach, presented in this paper, was developed for\nthe analysis of peak profiles and for the prediction of base profile\ncharacteristics, such as width, asymmetry, asymptotic (\"peak tales\"), etc. of\nthe observed distributions. The obtained parameters can be used as the initial\nparameters in the peak decomposition applications. The neural network\narchitecture, presented here, was designed for the analysis of one particular\ntype of peak profiles, the Voigt type distributions (symmetrical and\nasymmetrical), and is suitable for a variety of applications, such as x-ray and\nneutron powder diffraction, x-ray spectroscopy, etc. The approach itself,\nhowever, is not limited to the demonstrated case, but is applicable to other\ntypes of peak profile distributions. The approach was successfully tested on\nexperimentally collected x-ray powder diffraction data.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07466v1"
    },
    {
        "title": "Bayesian regression of piecewise homogeneous Poisson processes",
        "authors": [
            "Diego Sevilla"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  In this paper, a Bayesian method for piecewise regression is adapted to\nhandle counting processes data distributed as Poisson. A numerical code in\nMathematica is developed and tested analyzing simulated data. The resulting\nmethod is valuable for detecting breaking points in the count rate of time\nseries for Poisson processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.06029v1"
    },
    {
        "title": "Visibility graphs of random scalar fields and spatial data",
        "authors": [
            "Lucas Lacasa",
            "Jacopo Iacovacci"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The family of visibility algorithms were recently introduced as mappings\nbetween time series and graphs. Here we extend this method to characterize\nspatially extended data structures by mapping scalar fields of arbitrary\ndimension into graphs. After introducing several possible extensions, we\nprovide analytical results on some topological properties of these graphs\nassociated to some types of real-valued matrices, which can be understood as\nthe high and low disorder limits of real-valued scalar fields. In particular,\nwe find a closed expression for the degree distribution of these graphs\nassociated to uncorrelated random fields of generic dimension, extending a well\nknown result in one-dimensional time series. As this result holds independently\nof the field's marginal distribution, we show that it directly yields a\nstatistical randomness test, applicable in any dimension. We showcase its\nusefulness by discriminating spatial snapshots of two-dimensional white noise\nfrom snapshots of a two-dimensional lattice of diffusively coupled chaotic\nmaps, a system that generates high dimensional spatio-temporal chaos. We\nfinally discuss the range of potential applications of this combinatorial\nframework, which include image processing in engineering, the description of\nsurface growth in material science, soft matter or medicine and the\ncharacterization of potential energy surfaces in chemistry, disordered systems\nand high energy physics. An illustration on the applicability of this method\nfor the classification of the different stages involved in carcinogenesis is\nbriefly discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.07813v1"
    },
    {
        "title": "Dynamic principle for ensemble control tools",
        "authors": [
            "A. Samoletov",
            "B. Vasiev"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Dynamical equations describing physical systems at statistical equilibrium\nare commonly extended by mathematical tools called \"thermostats\". These tools\nare designed for sampling ensembles of statistical mechanics. We propose a\ndynamic principle for derivation of stochastic and deterministic thermostats.\nIt is based on fundamental physical assumptions such that the canonical measure\nis invariant for the thermostat dynamics. This is a clear advantage over a\nrange of recently proposed and widely discussed in the literature mathematical\nthermostat schemes. Following justification of the proposed principle we show\nits generality and usefulness for modeling a wide range of natural systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.08399v3"
    },
    {
        "title": "The ITS-90 after definition of neon isotopic reference composition.\n  Extent of the isotopic effect on previous inter-comparison results",
        "authors": [
            "Franco Pavese",
            "Anna Szmyrka-Grzebyk",
            "Peter P. M. Steur"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Starting from the end of the past century, the importance has been recognized\nof the effect of isotopic composition on some of the temperature fixed points\nfor the most accurate realizations of the ITS-90. In the original definition of\nthe latter, dating back to 1990, only a generic reference was made to natural\ncomposition of the substances used for the realization of the fixed points,\nexcept for helium. The definition of a reference isotopic composition for three\nfixed points, e-H2, Ne and H2O, while eliminating the non-uniqueness of the\nScale in this respect, induced detectable differences in the present and future\nrealizations of the Scale, at the highest accuracy level, with respect to the\nprevious realizations, when they affected the results of past key comparisons,\nnamely the K1 and K1.1, and K2 and K2.1 to K2.5 and the related regional and\nsupplementary ones. The paper provides evidence of the extent of this effect by\nusing the results of the relevant key comparisons for Neon archived in the BIPM\nKCDB, and of other comparisons existing in the literature: 1979-1984, 2007-2012\nand 2009-2010 sealed cell comparisons; and discusses the meaning and the\noutcomes of this evaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.05054v3"
    },
    {
        "title": "Improvement of PolSAR Decomposition Scattering Powers Using a Relative\n  Decorrelation Measure",
        "authors": [
            "D. Ratha",
            "M. Surendar",
            "A. Bhattacharya"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  In this letter, a methodology is proposed to improve the scattering powers\nobtained from model-based decomposition using Polarimetric Synthetic Aperture\nRadar (PolSAR) data. The novelty of this approach lies in utilizing the\nintrinsic information in the off-diagonal elements of the 3$\\times$3 coherency\nmatrix $\\mathbf{T}$ represented in the form of complex correlation\ncoefficients. Two complex correlation coefficients are computed between\nco-polarization and cross-polarization components of the Pauli scattering\nvector. The difference between modulus of complex correlation coefficients\ncorresponding to $\\mathbf{T}^{\\mathrm{opt}}$ (i.e. the degree of polarization\n(DOP) optimized coherency matrix), and $\\mathbf{T}$ (original) matrices is\nobtained. Then a suitable scaling is performed using fractions \\emph{i.e.,}\n$(T_{ii}^{\\mathrm{opt}}/\\sum\\limits_{i=1}^{3}T_{ii}^{\\mathrm{opt}})$ obtained\nfrom the diagonal elements of the $\\mathbf{T}^{\\mathrm{opt}}$ matrix.\nThereafter, these new quantities are used in modifying the Yamaguchi\n4-component scattering powers obtained from $\\mathbf{T}^{\\mathrm{opt}}$. To\ncorroborate the fact that these quantities have physical relevance, a\nquantitative analysis of these for the L-band AIRSAR San Francisco and the\nL-band Kyoto images is illustrated. Finally, the scattering powers obtained\nfrom the proposed methodology are compared with the corresponding powers\nobtained from the Yamaguchi \\emph{et. al.,} 4-component (Y4O) decomposition and\nthe Yamaguchi \\emph{et. al.,} 4-component Rotated (Y4R) decomposition for the\nsame data sets. The proportion of negative power pixels is also computed. The\nresults show an improvement on all these attributes by using the proposed\nmethodology.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.06054v1"
    },
    {
        "title": "Multi-Fidelity Surrogate Based on Single Linear Regression",
        "authors": [
            "Yiming Zhang",
            "Nam-Ho Kim",
            "Chanyoung Park",
            "Raphael T. Haftka"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Various frameworks have been proposed to predict mechanical system responses\nby combining data from different fidelities for design optimization and\nuncertainty quantification as reviewed by Fern\\'andez-Godino et al. and\nPeherstorfer et al.. Among all frameworks, the Bayesian framework based on\nGaussian processes has the potential of highest accuracy. However, the Bayesian\nframework requires optimization for estimating hyper-parameters, and there is a\nrisk of estimating inappropriate hyper-parameters as Kriging surrogate often\ndoes, especially in the presence of noisy data. We propose an easy and yet\npowerful framework for practical design and applications. In this technical\nnote, we revised a heuristic framework which minimizes the prediction errors at\nhigh-fidelity samples using optimization. The system behavior (high-fidelity\nbehavior) is approximated by a linear combination of the low-fidelity\npredictions and a polynomial-based discrepancy function. The key idea is to\nconsider the low-fidelity model as a basis function in the multi-fidelity model\nwith the scale factor as a regression coefficient. The design matrix for\nleast-square estimation consists of both the low-fidelity model and discrepancy\nfunction. Then the scale factor and coefficients of the basis functions are\nobtained simultaneously using linear regression, which guarantees the\nuniqueness of fitting process. Besides enabling efficient estimation of the\nparameters, the proposed least-squares multi-fidelity surrogate (LS-MFS) can be\napplicable to other regression models by simply replacing the design matrix.\nTherefore, the LS-MFS is expected to be easily applied to various applications\nsuch as prediction variance, D-optimal designs, uncertainty propagation and\ndesign optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.02956v1"
    },
    {
        "title": "Berthil Cepstrum: a Novel Vibration Analysis Method based on Marginal\n  Hilbert Spectrum Applied to Artificial Motor Aging",
        "authors": [
            "Harun Siljak",
            "Abdulhamit Subasi"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Motor age determination as a part of condition monitoring heavily employs\nvibration analysis. This study introduces a new method for such analysis, based\non concepts of cepstrum and marginal Hilbert spectrum. This new method, named\nBerthil cepstrum may be applied in general signal processing, not only when\nvibration signals are concerned. Classical marginal Hilbert spectrum has also\nbeen applied to the artificial motor aging data with excellent results.\nFurthermore, a ranking of known spectrum-based methods for determination of\nmotor age together with the new methods introduced in this study has been made\nbased on SVM and RELIEF attribute ranking, showing quality of the new methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.05075v1"
    },
    {
        "title": "Experiment Software and Projects on the Web with VISPA",
        "authors": [
            "Martin Erdmann",
            "Benjamin Fischer",
            "Robert Fischer",
            "Erik Geiser",
            "Christian Glaser",
            "Gero Mueller",
            "Marcel Rieger",
            "Martin Urban",
            "Ralf Florian von Cube",
            "Christoph Welling"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The Visual Physics Analysis (VISPA) project defines a toolbox for accessing\nsoftware via the web. It is based on latest web technologies and provides a\npowerful extension mechanism that enables to interface a wide range of\napplications. Beyond basic applications such as a code editor, a file browser,\nor a terminal, it meets the demands of sophisticated experiment-specific use\ncases that focus on physics data analyses and typically require a high degree\nof interactivity. As an example, we developed a data inspector that is capable\nof browsing interactively through event content of several data formats, e.g.,\n\"MiniAOD\" which is utilized by the CMS collaboration. The VISPA extension\nmechanism can also be used to embed external web-based applications that\nbenefit from dynamic allocation of user-defined computing resources via SSH.\nFor example, by wrapping the \"JSROOT\" project, ROOT files located on any remote\nmachine can be inspected directly through a VISPA server instance. We\nintroduced domains that combine groups of users and role-based permissions.\nThereby, tailored projects are enabled, e.g. for teaching where access to\nstudent's homework is restricted to a team of tutors, or for\nexperiment-specific data that may only be accessible for members of the\ncollaboration. We present the extension mechanism including corresponding\napplications and give an outlook onto the new permission system.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.00954v1"
    },
    {
        "title": "Minimal Dominating Set problem studied by simulated annealing and cavity\n  method: Analytics and population dynamics",
        "authors": [
            "Yusupjan Habibulla"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The minimal dominating Set (MDS) problem is a prototypical hard combinatorial\noptimization problem. Two years ago we studied this problem by cavity method.\nAlthough we get the solution of a given graph, which gives very good estimation\nof minimal dominating size, but we don't know whether we get the ground state\nsolution and how many solutions exist in the ground state. For this purpose,\nlast year we continue to develop the one step replica symmetry breaking (RSB)\ntheory to find the ground state energy of the MDS problem. Finally we find that\n1) The MDS problem solution space has both condensation transition and cluster\ntransition on regular Random (RR) graph and we prove this by simulated\nannealing dynamical process. 2) We developed zero temperature Survey\nPropagation (SP) algorithm on ER graph to estimate the ground state energy and\nto get Survey Propagation Decimation (SPD) algorithm with good results same as\nBPD algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01064v1"
    },
    {
        "title": "Relative distance between tracers as a measure of diffusivity within\n  moving aggregates",
        "authors": [
            "Wolfram Pönisch",
            "Vasily Zaburdaev"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Tracking of particles, be it a passive tracer or an actively moving bacterium\nin the growing bacterial colony, is a powerful technique to probe the physical\nproperties of the environment of the particles. One of the most common measures\nof particle motion driven by fluctuations and random forces is its diffusivity,\nwhich is routinely obtained by measuring the mean squared displacement of the\nparticles. However, often the tracer particles may be moving in a domain or an\naggregate which itself experiences some regular or random motion and thus masks\nthe diffusivity of tracers. Here we provide a method for assessing the\ndiffusivity of tracer particles within mobile aggregates by measuring the\nso-called mean squared relative distance (MSRD) between two tracers. We provide\nanalytical expressions for both the ensemble and time averaged MSRD allowing\nfor direct identification of diffusivities from experimental data.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.06533v2"
    },
    {
        "title": "A recipe for irreproducible results",
        "authors": [
            "Ole Peters",
            "Maximilian Werner"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Recent studies have shown that many results published in peer-reviewed\nscientific journals are not reproducible. This raises the following question:\nwhy is it so easy to fool myself into believing that a result is reliable when\nin fact it is not? Using Brownian motion as a toy model, we show how this can\nhappen if ergodicity is assumed where it is unwarranted. A measured value can\nappear stable when judged over time, although it is not stable across the\nensemble: a different result will be obtained each time the experiment is run.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.07773v1"
    },
    {
        "title": "Statistical tests for MIXMAX pseudorandom number generator",
        "authors": [
            "Narek H. Martirosyan",
            "Gevorg A. Karyan",
            "Norayr Z. Akopov"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The Pseudo-Random Number Generators (PRNGs) are key tools in Monte Carlo\nsimulations. More recently, the MIXMAX PRNG has been included in ROOT and Class\nLibrary for High Energy Physics (CLHEP) software packages and claims to be a\nstate of art generator due to its long period, high performance and good\nstatistical properties. In this paper the various statistical tests for MIXMAX\nare performed. The results compared with those obtained from other PRNGs, e.g.\nMersenne Twister, Ranlux, LCG reveal better qualities for MIXMAX in generating\nrandom numbers. The Mersenne Twister is by far the most widely used PRNG in\nmany software packages including packages in High Energy Physics (HEP), however\nthe results show that MIXMAX is not inferior to Mersenne Twister.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.00466v2"
    },
    {
        "title": "Uncertainty and auto-correlation in Measurement",
        "authors": [
            "Markus Schiebl"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Although a system is described by a well-known set of equations leading to a\ndeterministic behavior, in the real world the value of a measurand obtained by\nan experiment will mostly scatter. Accordingly, an uncertainty is associated\nwith that value of the measurand due to apparently random fluctuation. This\npapers deals with the question why this discrepancy exist. Furthermore it will\nbe shown how the uncertainty of one individual observation is calculated and\nconsequently how the best estimate and its corresponding uncertainty\nconsiderering auto-correlstions is determined.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.03276v3"
    },
    {
        "title": "Practical Statistics",
        "authors": [
            "L. Lyons"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Accelerators and detectors are expensive, both in terms of money and human\neffort. It is thus important to invest effort in performing a good statistical\nanalysis of the data, in order to extract the best information from it. This\nseries of five lectures deals with practical aspects of statistical issues that\narise in typical High Energy Physics analyses.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.01007v1"
    },
    {
        "title": "Reliable uncertainties in indirect measurements",
        "authors": [
            "Marek W. Gutowski"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  In this article we present very intuitive, easy to follow, yet mathematically\nrigorous, approach to the so called data fitting process. Rather than\nminimizing the distance between measured and simulated data points, we prefer\nto find such an area in searched parameters' space that generates simulated\ncurve crossing as many acquired experimental points as possible, but at least\nhalf of them. Such a task is pretty easy to attack with interval calculations.\nThe problem is, however, that interval calculations operate on guaranteed\nintervals, that is on pairs of numbers determining minimal and maximal values\nof measured quantity while in vast majority of cases our measured quantities\nare expressed rather as a pair of two other numbers: the average value and its\nstandard deviation. Here we propose the combination of interval calculus with\nbasic notions from probability and statistics. This approach makes possible to\nobtain the results in familiar form as reliable values of searched parameters,\ntheir standard deviations, and their correlations as well. There are no\nassumptions concerning the probability density distributions of experimental\nvalues besides the obvious one that their variances are finite. Neither the\nsymmetry of uncertainties of experimental distributions is required (assumed)\nnor those uncertainties have to be `small.' As a side effect, outliers are\nquietly and safely ignored, even if numerous.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.01452v1"
    },
    {
        "title": "Structural Damage Identification Using Piezoelectric Impedance\n  Measurement with Sparse Inverse Analysis",
        "authors": [
            "Pei Cao",
            "Qi Shuai",
            "Jiong Tang"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The impedance/admittance measurements of a piezoelectric transducer bonded to\nor embedded in a host structure can be used as damage indicator. When a\ncredible model of the healthy structure, such as the finite element model, is\navailable, using the impedance/admittance change information as input, it is\npossible to identify both the location and severity of damage. The inverse\nanalysis, however, may be under-determined as the number of unknowns in\nhigh-frequency analysis is usually large while available input information is\nlimited. The fundamental challenge thus is how to find a small set of solutions\nthat cover the true damage scenario. In this research we cast the damage\nidentification problem into a multi-objective optimization framework to tackle\nthis challenge. With damage locations and severities as unknown variables, one\nof the objective functions is the difference between impedance-based model\nprediction in the parametric space and the actual measurements. Considering\nthat damage occurrence generally affects only a small number of elements, we\nchoose the sparsity of the unknown variables as another objective function,\ndeliberately, the l0 norm. Subsequently, a multi-objective Dividing RECTangles\n(DIRECT) algorithm is developed to facilitate the inverse analysis where the\nsparsity is further emphasized by sigmoid transformation. As a deterministic\ntechnique, this approach yields results that are repeatable and conclusive. In\naddition, only one algorithmic parameter, the number of function evaluations,\nis needed. Numerical and experimental case studies demonstrate that the\nproposed framework is capable of obtaining high-quality damage identification\nsolutions with limited measurement information.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.02968v3"
    },
    {
        "title": "An Emergent Space for Distributed Data with Hidden Internal Order\n  through Manifold Learning",
        "authors": [
            "Felix P. Kemeth",
            "Sindre W. Haugland",
            "Felix Dietrich",
            "Tom Bertalan",
            "Kevin Höhlein",
            "Qianxiao Li",
            "Erik M. Bollt",
            "Ronen Talmon",
            "Katharina Krischer",
            "Ioannis G. Kevrekidis"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Manifold-learning techniques are routinely used in mining complex\nspatiotemporal data to extract useful, parsimonious data\nrepresentations/parametrizations; these are, in turn, useful in nonlinear model\nidentification tasks. We focus here on the case of time series data that can\nultimately be modelled as a spatially distributed system (e.g. a partial\ndifferential equation, PDE), but where we do not know the space in which this\nPDE should be formulated. Hence, even the spatial coordinates for the\ndistributed system themselves need to be identified - to emerge from - the data\nmining process. We will first validate this emergent space reconstruction for\ntime series sampled without space labels in known PDEs; this brings up the\nissue of observability of physical space from temporal observation data, and\nthe transition from spatially resolved to lumped (order-parameter-based)\nrepresentations by tuning the scale of the data mining kernels. We will then\npresent actual emergent space discovery illustrations. Our illustrative\nexamples include chimera states (states of coexisting coherent and incoherent\ndynamics), and chaotic as well as quasiperiodic spatiotemporal dynamics,\narising in partial differential equations and/or in heterogeneous networks. We\nalso discuss how data-driven spatial coordinates can be extracted in ways\ninvariant to the nature of the measuring instrument. Such gauge-invariant data\nmining can go beyond the fusion of heterogeneous observations of the same\nsystem, to the possible matching of apparently different systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.05406v2"
    },
    {
        "title": "A Statistical Distance Derived From The Kolmogorov-Smirnov Test:\n  specification, reference measures (benchmarks) and example uses",
        "authors": [
            "Renato Fabbri",
            "Fernando Gularte De León"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Statistical distances quantifies the difference between two statistical\nconstructs. In this article, we describe reference values for a distance\nbetween samples derived from the Kolmogorov-Smirnov statistic $D_{F,F'}$. Each\nmeasure of the $D_{F,F'}$ is a measure of difference between two samples. This\ndistance is normalized by the number of observations in each sample to yield\nthe $c'=D_{F,F'}\\sqrt{\\frac{n n'}{n+n'}}$ statistic, for which high levels\nfavor the rejection of the null hypothesis (that the samples are drawn from the\nsame distribution). One great feature of $c'$ is that it inherits the\nrobustness of $D_{F,F'}$ and is thus suitable for use in settings where the\nunderlying distributions are not known. Benchmarks are obtained by comparing\nsamples derived from standard distributions. The supplied example applications\nof the $c'$ statistic for the distinction of samples in real data enables\nfurther insights about the robustness and power of such statistical distance.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.00761v1"
    },
    {
        "title": "A Blade Tip-Timing method based on Periodic Nonuniform Sampling of order\n  2",
        "authors": [
            "Bernard Lacaze"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Vibrations are among main causes of fatigue and damages leading to\ndestruction of rotating blades. Consequently, motions of blades have to be\ncarefully studied, and in particular, periodic components. Blade Tip-Timing\n(BTT) methods achieve non-intrusive measurements which is a great advantage\nwith respect to classical methods. They can be modelled by a Periodic\nNonuniform Sampling of order L equal to the number of probes (PNSL). In this\npaper, we develop the case L=2, and we prove that this model is sufficient for\nfinding the places of asynchronous frequency lines, with their amplitudes and\ntheir phases. To increase L will increase the redundance and then the accuracy\nof the method.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.06135v1"
    },
    {
        "title": "The footprint of atmospheric turbulence in power grid frequency\n  measurements",
        "authors": [
            "Hauke Haehne",
            "Jannik Schottler",
            "Matthias Waechter",
            "Joachim Peinke",
            "Oliver Kamps"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Fluctuating wind energy makes a stable grid operation challenging. Due to the\ndirect contact with atmospheric turbulence, intermittent short-term variations\nin the wind speed are converted to power fluctuations that cause transient\nimbalances in the grid. We investigate the impact of wind energy feed-in on\nshort-term fluctuations in the frequency of the public power grid, which we\nhave measured in our local distribution grid. By conditioning on wind power\nproduction data, provided by the ENTSO-E transparency platform, we demonstrate\nthat wind energy feed-in has a measurable effect on frequency increment\nstatistics for short time scales (< 1 sec) that are below the activation time\nof frequency control. Our results are in accordance with previous numerical\nstudies of self-organized synchronization in power grids under intermittent\nperturbation and rise new challenges for a stable operation of future power\ngrids fed by a high share of renewable generation.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.00628v2"
    },
    {
        "title": "Probabilistic modelling and reconstruction of strain",
        "authors": [
            "Carl Jidling",
            "Johannes Hendriks",
            "Niklas Wahlström",
            "Alexander Gregg",
            "Thomas B. Schön",
            "Christopher Wensrich",
            "Adrian Wills"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  This paper deals with modelling and reconstruction of strain fields, relying\nupon data generated from neutron Bragg-edge measurements. We propose a\nprobabilistic approach in which the strain field is modelled as a Gaussian\nprocess, assigned a covariance structure customised by incorporation of the\nso-called equilibrium constraints. The computational complexity is\nsignificantly reduced by utilising an approximation scheme well suited for the\nproblem. We illustrate the method on simulations and real data. The results\nindicate a high potential and can hopefully inspire the concept of\nprobabilistic modelling to be used within other tomographic applications as\nwell.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.03636v3"
    },
    {
        "title": "Gamma-ray intensities in multi-gated spectra",
        "authors": [
            "Camille Ducoin",
            "Guillaume Maquart",
            "Olivier Stézowski"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The level structure of nuclei offers a large amount and variety of\ninformation to improve our knowledge of the strong interaction and of\nmesoscopic quantum systems. Gamma spectroscopy is a powerful tool to perform\nsuch studies: modern gamma multi-detectors present increasing performances in\nterms of sensitivity and efficiency, allowing to extend ever more our ability\nto observe and characterize abundant nuclear states. For instance, the\nhigh-spin part of level schemes often reflects intriguing nuclear shape\nphenomena: this behaviour is unveiled by high-fold experimental data analysed\nthrough multi-coincidence spectra, in which long deexcitation cascades become\nobservable. Determining the intensity of newly discovered transitions is\nimportant to characterize the nuclear structure and formation mechanism of the\nemitting levels. However, it is not trivial to relate the apparent intensity\nobserved in multi-gated spectra to the actual transition intensity. In this\nwork, we introduce the basis of a formalism affiliated with graph theory: we\nhave obtained analytic expressions from which data-analysis methods can\neventually be derived to recover this link in a rigorous way.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.06076v1"
    },
    {
        "title": "Quantifying Acoustophoretic Separation of Microparticle Populations by\n  Mean-and-Covariance Dynamics for Gaussians in Mixture Models",
        "authors": [
            "Fabio Garofalo"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  A method for the quantification of acoustophoretic separation and dispersion\nfor microparticle populations featuring continuously distributed physical\nparameters is presented. The derivation of the method starts by (i)~considering\nthe equation of motion for a particle ensemble in the coordinate+parameter\nspace, (ii)~performing moment analysis on the transport equation for the\nprobability density function (PDF), and (iii)~expanding up to the first-order\nthe drift (and the diffusion coefficient) around the mean of the PDF. Following\nthese steps, a system of ordinary differential equations for the evolution of\nthe mean and the covariance in the coordinate+parameter space is derived. These\ndifferential equations enable for the approximation of the acoustophoretic\nseparation dynamics of particle ensembles by using a gaussian mixture for which\nthe mean and the covariance of each gaussian evolve according to the\nmean-and-covariance dynamics. The approximation property of this method is\nshown by comparison with direct numerical simulations of particle ensembles in\nthe cases of prototypical models of acoustophoretic and free-flow\nacoustophoretic separations for which the particle populations are distributed\naccording to the radius. Furthermore, the indicators for quantifying free-flow\nacoustophoretic separation performance are introduced, and a method for the\ninference of particle-histogram parameters is illustrated.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.09790v1"
    },
    {
        "title": "Topological time-series analysis with delay-variant embedding",
        "authors": [
            "Quoc Hoan Tran",
            "Yoshihiko Hasegawa"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Identifying the qualitative changes in time-series data provides insights\ninto the dynamics associated with such data. Such qualitative changes can be\ndetected through topological approaches, which first embed the data into a\nhigh-dimensional space using a time-delay parameter and subsequently extract\ntopological features describing the shape of the data from the embedded points.\nHowever, the essential topological features that are extracted using a single\ntime delay are considered to be insufficient for evaluating the aforementioned\nqualitative changes, even when a well-selected time delay is used. We therefore\npropose a delay-variant embedding method that constructs the extended\ntopological features by considering the time delay as a variable parameter\ninstead of considering it as a single fixed value. This delay-variant embedding\nmethod reveals multiple-time-scale patterns in a time series by allowing the\nobservation of the variations in topological features, with the time delay\nserving as an additional dimension in the topological feature space. We\ntheoretically prove that the constructed topological features are robust when\nthe time series is perturbed by noise. Furthermore, we combine these features\nwith the kernel technique in machine learning algorithms to classify the\ngeneral time-series data. We demonstrate the effectiveness of our method for\nclassifying the synthetic noisy biological and real time-series data. Our\nmethod outperforms a method that is based on a single time delay and,\nsurprisingly, achieves the highest classification accuracy on an average among\nthe standard time-series analysis techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00208v4"
    },
    {
        "title": "On the Study of Chaos and Memory Effects in the Bonhoeffer-van der Pol\n  Oscillator with a Non-Ideal Capacitor",
        "authors": [
            "Jamieson Brechtl",
            "Xie Xie",
            "Karin A. Dahmen",
            "Peter K. Liaw",
            "Steven J. Zinkle"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  In this paper, the voltage fluctuations of the Bonhoeffer van der pol\noscillator system with a non-ideal capacitor were investigated. Here, the\ncapacitor was modeled, using a fractional differential equation in which the\norder of the fractional derivative is also a measure of the memory in the\ndielectric. The governing fractional differential equation was derived using\ntwo methods, namely a differential and integral approach. The former method\nutilized a hierarchical resistor-capacitor (RC) ladder model while the latter\nutilized the theory of the universal dielectric-response. The dynamical\nbehavior of the potential across the capacitor was found to be affected by this\nparameter, and, therefore, the memory of the system. Additionally, findings\nindicate that an increase in the memory parameter was associated with an\nincrease in the energy stored in the dielectric. It was found that oscillation\ndeath resulted in a higher amount of stored energy in the dielectric over time,\nas compared to behavior, which displayed relaxation oscillations or chaotic\nfluctuations. The relatively-lower stored energy resulting from the latter\ntypes of dynamical behavior appeared to be a consequence of the memory effect,\nwhere present accumulations of energy in the capacitor are affected by previous\ndecreases in the potential. Hence, in this type of scenario, the dielectric\nmaterial can be thought of as remembering the past behavior of the voltage,\nwhich leads to either a decrease, or an enhancement in the stored energy. The\nnon-ideal capacitor was also found to have a transitory nature, where it\nbehaves more like a resistor as {\\alpha} approaches 0, and conversely, more\nlike a capacitor as {\\alpha} goes to 1. Here, a decrease in {\\alpha} was linked\nto an enhanced metallic character of the dielectric.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.01108v1"
    },
    {
        "title": "A new quantity for statistical analysis: \"Scaling invariable Benford\n  distance\"",
        "authors": [
            "Peiyan Luo",
            "Yongqing Li"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  For the first time, we introduce \"Scaling invariable Benford distance\" and\n\"Benford cyclic graph\", which can be used to analyze any data set. Using the\nquantity and the graph, we analyze some date sets with common distributions,\nsuch as normal, exponent, etc., find that different data set has a much\ndifferent value of \"Scaling invariable Benford distance\" and different figure\nfeature of \"Benford cyclic graph\". We also explore the influence of data size\non \"Scaling invariable Benford distance\", and find that it firstly reduces with\ndata size increasing, then approximate to a fixed value when the size is large\nenough.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.01117v2"
    },
    {
        "title": "Method of fractal diversity in data science problems",
        "authors": [
            "Vitalii Vladimirov",
            "Elena Vladimirova"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The parameter (SNR) is obtained for distinguishing the Gaussian function, the\ndistribution of random variables in the absence of cross correlation, from\nother functions, which makes it possible to describe collective states with\nstrong cross-correlation of data. The signal-to-noise ratio (SNR) in\none-dimensional space is determined and a calculation algorithm based on the\nfractal variety of the Cantor dust in a closed loop is given. The algorithm is\ninvariant for linear transformations of the initial data set, has\nrenormalization-group invariance, and determines the intensity of\ncross-correlation (collective effect) of the data. The description of the\ncollective state is universal and does not depend on the nature of the\ncorrelation of data, nor is the universality of the distribution of random\nvariables in the absence of data correlation. The method is applicable for\nlarge sets of non-Gaussian or strange data obtained in information technology.\nIn confirming the hypothesis of Koshland, the application of the method to the\nintensity data of digital X-ray diffraction spectra with the calculation of the\ncollective effect makes it possible to identify a conformer exhibiting\nbiological activity.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.02663v1"
    },
    {
        "title": "Machine Learning Peeling and Loss Modelling of Time-Domain Reflectometry",
        "authors": [
            "J. R. Rinehart",
            "J. H. Béjanin",
            "T. C. Fraser",
            "M. Mariantoni"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  A fundamental pursuit of microwave metrology is the determination of the\ncharacteristic impedance profile of microwave systems. Among other methods,\nthis can be practically achieved by means of time-domain reflectometry (TDR)\nthat measures the reflections from a device due to an applied stimulus.\nConventional TDR allows for the measurement of systems comprising a single\nimpedance. However, real systems typically feature impedance variations that\nobscure the determination of all impedances subsequent to the first one. This\nproblem has been studied previously and is generally known as scattering\ninversion or, in the context of microwave metrology, time-domain \"peeling\". In\nthis article, we demonstrate the implementation of a space-time efficient\npeeling algorithm that corrects for the effect of prior impedance mismatch in a\nnonuniform lossless transmission line, regardless of the nature of the\nstimulus. We generalize TDR measurement analysis by introducing two tools: A\nstochastic machine learning clustering tool and an arbitrary lossy transmission\nline modeling tool. The former mitigates many of the imperfections typically\nplaguing TDR measurements (except for dispersion) and allows for an efficient\nprocessing of large datasets; the latter allows for a complete transmission\nline characterization including both conductor and dielectric loss.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.04756v1"
    },
    {
        "title": "Fusion of laser diffraction and chord length distribution data for\n  estimation of particle size distribution using multi-objective optimisation",
        "authors": [
            "Okpeafoh S. Agimelen",
            "Carla Ferreira",
            "Bilal Ahmed",
            "Javier Cardona",
            "Yi-chieh Chen",
            "Alastair J. Florence",
            "Ivan Andonovic",
            "Jan Sefcik",
            "Anthony J. Mulholland"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The in situ measurement of the particle size distribution (PSD) of a\nsuspension of particles presents huge challenges. Various effects from the\nprocess could introduce noise to the data from which the PSD is estimated. This\nin turn could lead to the occurrence of artificial peaks in the estimated PSD.\nLimitations in the models used in the PSD estimation could also lead to the\noccurrence of these artificial peaks. This could pose a significant challenge\nto in situ monitoring of particulate processes, as there will be no independent\nestimate of the PSD to allow a discrimination of the artificial peaks to be\ncarried out.\n  Here, we present an algorithm which is capable of discriminating between\nartificial and true peaks in PSD estimates based on fusion of multiple data\nstreams. In this case, chord length distribution and laser diffraction data\nhave been used. The data fusion is done by means of multi-objective\noptimisation using the weighted sum approach. The algorithm is applied to two\ndifferent particle suspensions. The estimated PSDs from the algorithm are\ncompared with offline estimates of PSD from the Malvern Mastersizer and\nMorphologi G3. The results show that the algorithm is capable of eliminating an\nartificial peak in a PSD estimate when this artificial peak is sufficiently\ndisplaced from the true peak. However, when the artificial peak is too close to\nthe true peak, it is only suppressed but not completely eliminated.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.05633v1"
    },
    {
        "title": "Modelling Reservoir Computing with the Discrete Nonlinear Schrödinger\n  Equation",
        "authors": [
            "Simone Borlenghi",
            "Magnus Boman",
            "Anna Delin"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We formulate, using the discrete nonlinear Schroedinger equation (DNLS), a\ngeneral approach to encode and process information based on reservoir\ncomputing. Reservoir computing is a promising avenue for realizing neuromorphic\ncomputing devices. In such computing systems, training is performed only at the\noutput level, by adjusting the output from the reservoir with respect to a\ntarget signal. In our formulation, the reservoir can be an arbitrary physical\nsystem, driven out of thermal equilibrium by an external driving. The DNLS is a\ngeneral oscillator model with broad application in physics and we argue that\nour approach is completely general and does not depend on the physical\nrealisation of the reservoir. The driving, which encodes the object to be\nrecognised, acts as a thermodynamical force, one for each node in the\nreservoir. Currents associated to these thermodynamical forces in turn encode\nthe output signal from the reservoir. As an example, we consider numerically\nthe problem of supervised learning for pattern recognition, using as reservoir\na network of nonlinear oscillators.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.09048v2"
    },
    {
        "title": "Scaling property of the statistical Two-Sample Energy Test",
        "authors": [
            "G. Zech"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The energy test is a powerful binning-free, multi-dimensional and\ndistribution-free tool that can be applied to compare a measurement to a given\nprediction (goodness-of-fit) or to check whether two data samples originate\nfrom the same population (two-sample test). In both cases the distribution of\nthe test statistic under the null hypothesis H_0, (correct prediction, same\npopulation) has to be obtained by simulation. This poses computational problems\nif the data samples are large, but the difficulty can be overcome with the help\nof a scaling property which relates the distribution of small samples to the\ndistribution of large samples. Scaling has been made plausible in Ref. W.\nBarter et al. JINST 13 P04011 by extensive simulations. In this article an\nanalytic proof is presented which makes the calculation of p-values obtained by\nscaling more reliable.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.10599v1"
    },
    {
        "title": "reductus: a stateless Python data-reduction service with a browser\n  frontend",
        "authors": [
            "Brian Maranville",
            "William Ratcliff II",
            "Paul Kienzle"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The online data reduction service reductus transforms measurements in\nexperimental science from laboratory coordinates into physically meaningful\nquantities with accurate estimation of uncertainties based on instrumental\nsettings and properties. This reduction process is based on a few well-known\ntransformations, but flexibility in the application of the transforms and\nalgorithms supports flexibility in experiment design, supporting a broader\nrange of measurements than a rigid reduction scheme for data. The user\ninterface allows easy construction of arbitrary pipelines from well-known data\ntransforms using a visual dataflow diagram. Source data is drawn from a\nnetworked, open data repository. The Python backend uses intelligent caching to\nstore intermediate results of calculations for a highly responsive user\nexperience. The reference implementation allows immediate reduction of\nmeasurements as they are recorded for the three neutron reflectometry\ninstruments at the NIST Center for Neutron Research (NCNR), without the need\nfor visiting scientists to install additional software on their own computers.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.11248v1"
    },
    {
        "title": "Fast Entropy Estimation for Natural Sequences",
        "authors": [
            "Andrew D. Back",
            "Daniel Angus",
            "Janet Wiles"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  It is well known that to estimate the Shannon entropy for symbolic sequences\naccurately requires a large number of samples. When some aspects of the data\nare known it is plausible to attempt to use this to more efficiently compute\nentropy. A number of methods having various assumptions have been proposed\nwhich can be used to calculate entropy for small sample sizes. In this paper,\nwe examine this problem and propose a method for estimating the Shannon entropy\nfor a set of ranked symbolic natural events. Using a modified\nZipf-Mandelbrot-Li law and a new rank-based coincidence counting method, we\npropose an efficient algorithm which enables the entropy to be estimated with\nsurprising accuracy using only a small number of samples. The algorithm is\ntested on some natural sequences and shown to yield accurate results with very\nsmall amounts of data.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06630v1"
    },
    {
        "title": "Refining the Experimental Extraction of the Number of Independent\n  Samples in a Mode-Stirred Reverberation Chamber",
        "authors": [
            "Khalid Oubaha",
            "Martin Richter",
            "Ulrich Kuhl",
            "Fabrice Mortessagne",
            "Olivier Legrand"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We investigate the number of independent samples in a chaotic reverberation\nchamber. Its evaluation as defined by the IEC standard can be made more precise\nwhen using not the index of the first value larger then the correlation length\nbut using the value obtained by a linear interpolation instead. The results are\nvalidated by a juxtaposition with values from a measurement using a high\nstirrer-angle resolution. A comparison with estimates known from the literature\nvalidates our findings. An alternative approach using the local maxima of the\nparametric dependence of the transmission is presented in order to show the\napplicability of the extracted correlation length over a large range of\nfrequencies.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.07214v1"
    },
    {
        "title": "Vector-Valued Spectral Analysis of Space-Time Data",
        "authors": [
            "Dimitrios Giannakis",
            "Joanna Slawinska",
            "Abbas Ourmazd",
            "Zhizhen Zhao"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Identifying coherent spatiotemporal patterns generated by complex dynamical\nsystems is a central problem in many science and engineering disciplines. Here,\nwe combine ideas from the theory of operator-valued kernels with\ndelay-embedding techniques in dynamical systems to develop of a method for\nobjective identification of spatiotemporal coherent patterns, without prior\nknowledge of the state space and/or the dynamical laws of the system generating\nthe data. A key aspect of this method is that it operates on a space of\nvector-valued observables using a kernel measure of similarity that takes into\naccount both temporal and spatial degrees of freedom (in contrast, classical\nkernel techniques such as PCA utilize aggregate measures of similarity between\n'snapshots'). As a result, spectral decomposition of data via our approach\nyields a significantly more efficient and physically meaningful representation\nof complex spatiotemporal signals than conventional methods based on\nscalar-valued kernels. We demonstrate this behavior with applications to an\noscillator model and sea surface temperature data from a climate model.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09134v1"
    },
    {
        "title": "A New Approach for 4DVar Data Assimilation",
        "authors": [
            "Xiangjun Tian",
            "Aiguo Dai",
            "Xiaobing Feng",
            "Hongqin Zhang",
            "Rui Han",
            "Lu Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Four-dimensional variational data assimilation (4DVar) has become an\nincreasingly important tool in data science with wide applications in many\nengineering and scientific fields such as geoscience1-12, biology13 and the\nfinancial industry14. The 4DVar seeks a solution that minimizes the departure\nfrom the background field and the mismatch between the forecast trajectory and\nthe observations within an assimilation window. The current state-of-the-art\n4DVar offers only two choices by using different forms of the forecast model:\nthe strong- and weak-constrained 4DVar approaches15-16. The former ignores the\nmodel error and only corrects the initial condition error at the expense of\nreduced accuracy; while the latter accounts for both the initial and model\nerrors and corrects them separately, which increases computational costs and\nuncertainty. To overcome these limitations, here we develop an integral\ncorrecting 4DVar (i4DVar) approach by treating all errors as a whole and\ncorrecting them simultaneously and indiscriminately. To achieve that, a novel\nexponentially decaying function is proposed to characterize the error evolution\nand correct it at each time step in the i4DVar. As a result, the i4DVar greatly\nenhances the capability of the strong-constrained 4DVar for correcting the\nmodel error while also overcomes the limitation of the weak-constrained 4DVar\nfor being prohibitively expensive with added uncertainty. Numerical experiments\nwith the Lorenz model show that the i4DVar significantly outperforms the\nexisting 4DVar approaches. It has the potential to be applied in many\nscientific and engineering fields and industrial sectors in the big data era\nbecause of its ease of implementation and superior performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09320v2"
    },
    {
        "title": "An Accurate Data Cleaning Procedure for Electron Cyclotron Emission\n  Imaging on EAST Tokamak Based on Methodology of Machine Learning",
        "authors": [
            "C. Li",
            "T. Lan",
            "Y. Wang",
            "J. Liu",
            "J. Xie",
            "T. Lan",
            "H. Li",
            "H. Qin"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  A new data cleaning procedure for electron cyclotron emission imaging (ECEI)\nof EAST tokamak is constructed. Machine learning techniques, including SVM and\nDecision tree, are applied to identifying saturated, zero, and weak signals of\nECEI raw data, which not only reduces the effort of researchers for data\nanalysis, but also improves the accuracy of data preprocessing. To enhance the\nreliability of the procedure, proper training sets are sampled based on massive\nraw data from the experiments of ECEI on EAST tokamak. Window size of temporal\nsignal, kernel function, and other model parameters are obtained after model\ntraining. Consequently, the recognition rates of saturated, zero, and weak\nsignals in raw data are 99.4%, 99.86%, and 99.9%, respectively, which proves\nthe accuracy of this procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09540v1"
    },
    {
        "title": "$β$-Decay Spectrum, Response Function and Statistical Model for\n  Neutrino Mass Measurements with the KATRIN Experiment",
        "authors": [
            "M. Kleesiek",
            "J. Behrens",
            "G. Drexlin",
            "K. Eitel",
            "M. Erhard",
            "J. A. Formaggio",
            "F. Glück",
            "S. Groh",
            "M. Hötzel",
            "S. Mertens",
            "A. W. P. Poon",
            "C. Weinheimer",
            "K. Valerius"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The objective of the Karlsruhe Tritium Neutrino (KATRIN) experiment is to\ndetermine the effective electron neutrino mass $m(\\nu_\\text{e})$ with an\nunprecedented sensitivity of $0.2\\,\\text{eV}$ (90\\% C.L.) by precision electron\nspectroscopy close to the endpoint of the $\\beta$ decay of tritium. We present\na consistent theoretical description of the $\\beta$ electron energy spectrum in\nthe endpoint region, an accurate model of the apparatus response function, and\nthe statistical approaches suited to interpret and analyze tritium $\\beta$\ndecay data observed with KATRIN with the envisaged precision. In addition to\nproviding detailed analytical expressions for all formulae used in the\npresented model framework with the necessary detail of derivation, we discuss\nand quantify the impact of theoretical and experimental corrections on the\nmeasured $m(\\nu_\\text{e})$. Finally, we outline the statistical methods for\nparameter inference and the construction of confidence intervals that are\nappropriate for a neutrino mass measurement with KATRIN. In this context, we\nbriefly discuss the choice of the $\\beta$ energy analysis interval and the\ndistribution of measuring time within that range.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.00369v2"
    },
    {
        "title": "Boundary conditions for similarity Index",
        "authors": [
            "Madhu Kashyap Jagadeesh",
            "Purusharth Saxena"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The recent development, shows that the Bray-Curtis's formula for similarity\nIndex (1957), has been applied in various fields like Ecology, Astrophysics,\netc. In this paper, we found the possible boundary conditions for this evolved\nformula (i.e. the numerical range in which the formula becomes in-effective to\ngive the expected result). Here we have simulated the real world data in the\nform of normally distributed random numbers, that directly shows the range (or\nconditions) at which this formula gives unambiguous similarity result.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.02263v2"
    },
    {
        "title": "Lectures on Statistics in Theory: Prelude to Statistics in Practice",
        "authors": [
            "Robert D. Cousins"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  This is a writeup of lectures on \"statistics\" that have evolved from the\ninitial version for the 2009 Hadron Collider Physics Summer School at CERN to\nversions for other venues and, most recently, for the African School of\nFundamental Physics and Applications in 2024. The emphasis is on foundations,\nusing simple examples to illustrate the points that are still debated in the\nprofessional statistics literature. The three main approaches to interval\nestimation (Neyman confidence, Bayesian, likelihood ratio) are discussed and\ncompared in detail, with and without nuisance parameters. Hypothesis testing is\ndiscussed mainly from the frequentist point of view, with pointers to the\nBayesian literature. Various foundational issues are emphasized, including the\nconditionality principle and the likelihood principle.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.05996v4"
    },
    {
        "title": "Simplicial complexes and complex systems",
        "authors": [
            "Vsevolod Salnikov",
            "Daniele Cassese",
            "Renaud Lambiotte"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We provide a short introduction to the field of topological data analysis and\ndiscuss its possible relevance for the study of complex systems. Topological\ndata analysis provides a set of tools to characterise the shape of data, in\nterms of the presence of holes or cavities between the points. The methods,\nbased on notion of simplicial complexes, generalise standard network tools by\nnaturally allowing for many-body interactions and providing results robust\nunder continuous deformations of the data. We present strengths and weaknesses\nof current methods, as well as a range of empirical studies relevant to the\nfield of complex systems, before identifying future methodological challenges\nto help understand the emergence of collective phenomena.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07747v1"
    },
    {
        "title": "Statistical Criticality arises in Most Informative Representations",
        "authors": [
            "Ryan John Cubero",
            "Junghyo Jo",
            "Matteo Marsili",
            "Yasser Roudi",
            "Juyong Song"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We show that statistical criticality, i.e. the occurrence of power law\nfrequency distributions, arises in samples that are maximally informative about\nthe underlying generating process. In order to reach this conclusion, we first\nidentify the frequency with which different outcomes occur in a sample, as the\nvariable carrying useful information on the generative process. The entropy of\nthe frequency, that we call relevance, provides an upper bound to the number of\ninformative bits. This differs from the entropy of the data, that we take as a\nmeasure of resolution. Samples that maximise relevance at a given resolution -\nthat we call maximally informative samples - exhibit statistical criticality.\nIn particular, Zipf's law arises at the optimal trade-off between resolution\n(i.e. compression) and relevance. As a byproduct, we derive a bound of the\nmaximal number of parameters that can be estimated from a dataset, in the\nabsence of prior knowledge on the generative model.\n  Furthermore, we relate criticality to the statistical properties of the\nrepresentation of the data generating process. We show that, as a consequence\nof the concentration property of the Asymptotic Equipartition Property,\nrepresentations that are maximally informative about the data generating\nprocess are characterised by an exponential distribution of energy levels. This\narises from a principle of minimal entropy, that is conjugate of the maximum\nentropy principle in statistical mechanics. This explains why statistical\ncriticality requires no parameter fine tuning in maximally informative samples.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.00249v5"
    },
    {
        "title": "Optimal parameters for anomalous diffusion exponent estimation from\n  noisy data",
        "authors": [
            "Yann Lanoiselée",
            "Denis S. Grebenkov",
            "Grzegorz Sikora",
            "Aleksandra Grzesiek",
            "Agnieszka Wyłomańska"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The most common way of estimating the anomalous diffusion exponent from\nsingle-particle trajectories consists in a linear fitting of the dependence of\nthe time averaged mean square displacement on the lag time at the log-log\nscale. However, various measurement noises that are unavoidably present in\nexperimental data, can strongly deteriorate the quality of this estimation\nprocedure and bias the estimated exponent. To investigate the impact of noises\nand to improve the estimation quality, we compare three approaches for\nestimating the anomalous diffusion exponent and check their efficiency on\nfractional Brownian motion corrupted by Gaussian noise. We discuss how the\nparameters of this anomalous diffusion model and the parameters of the\nestimation techniques influence the estimated exponent. We show that the\nconventional linear fitting is the least optimal method for the analysis of\nnoisy data.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.00412v1"
    },
    {
        "title": "Reducing model bias in a deep learning classifier using domain\n  adversarial neural networks in the MINERvA experiment",
        "authors": [
            "G. N. Perdue",
            "A. Ghosh",
            "M. Wospakrik",
            "F. Akbar",
            "D. A. Andrade",
            "M. Ascencio",
            "L. Bellantoni",
            "A. Bercellie",
            "M. Betancourt",
            "G. F. R. Caceres Vera",
            "T. Cai",
            "M. F. Carneiro",
            "J. Chaves",
            "D. Coplowe",
            "H. da Motta",
            "G. A. Díaz",
            "J. Felix",
            "L. Fields",
            "R. Fine",
            "A. M. Gago",
            "R. Galindo",
            "T. Golan",
            "R. Gran",
            "J. Y. Han",
            "D. A. Harris",
            "D. Jena",
            "J. Kleykamp",
            "M. Kordosky",
            "X. G. Lu",
            "E. Maher",
            "W. A. Mann",
            "C. M. Marshall",
            "K. S. McFarland",
            "A. M. McGowan",
            "B. Messerly",
            "J. Miller",
            "J. K. Nelson",
            "C. Nguyen",
            "A. Norrick",
            " Nuruzzaman",
            "A. Olivier",
            "R. Patton",
            "M. A. Ramírez",
            "R. D. Ransome",
            "H. Ray",
            "L. Ren",
            "D. Rimal",
            "D. Ruterbories",
            "H. Schellman",
            "C. J. Solano Salinas",
            "H. Su",
            "S. Upadhyay",
            "E. Valencia",
            "J. Wolcott",
            "B. Yaeggy",
            "S. Young"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We present a simulation-based study using deep convolutional neural networks\n(DCNNs) to identify neutrino interaction vertices in the MINERvA passive\ntargets region, and illustrate the application of domain adversarial neural\nnetworks (DANNs) in this context. DANNs are designed to be trained in one\ndomain (simulated data) but tested in a second domain (physics data) and\nutilize unlabeled data from the second domain so that during training only\nfeatures which are unable to discriminate between the domains are promoted.\nMINERvA is a neutrino-nucleus scattering experiment using the NuMI beamline at\nFermilab. $A$-dependent cross sections are an important part of the physics\nprogram, and these measurements require vertex finding in complicated events.\nTo illustrate the impact of the DANN we used a modified set of simulation in\nplace of physics data during the training of the DANN and then used the label\nof the modified simulation during the evaluation of the DANN. We find that deep\nlearning based methods offer significant advantages over our prior track-based\nreconstruction for the task of vertex finding, and that DANNs are able to\nimprove the performance of deep networks by leveraging available unlabeled data\nand by mitigating network performance degradation rooted in biases in the\nphysics models used for training.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.08332v4"
    },
    {
        "title": "Bayesian Modeling of Inconsistent Plastic Response due to Material\n  Variability",
        "authors": [
            "Francesco Rizzi",
            "Mohammad Khalil",
            "Reese E. Jones",
            "Jeremy A. Templeton",
            "Jakob T. Ostien",
            "Brad L. Boyce"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The advent of fabrication techniques such as additive manufacturing has\nfocused attention on the considerable variability of material response due to\ndefects and other microstructural aspects. This variability motivates the\ndevelopment of an enhanced design methodology that incorporates inherent\nmaterial variability to provide robust predictions of performance. In this\nwork, we develop plasticity models capable of representing the distribution of\nmechanical responses observed in experiments using traditional plasticity\nmodels of the mean response and recently developed uncertainty quantification\n(UQ) techniques. We demonstrate that the new method provides predictive\nrealizations that are superior to more traditional ones, and how these UQ\ntechniques can be used in model selection and assessing the quality of\ncalibrated physical parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.01009v1"
    },
    {
        "title": "Detection of time reversibility in time series by ordinal patterns\n  analysis",
        "authors": [
            "Johann H. Martínez",
            "José L. Herrera-Diestra",
            "Mario Chavez"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Time irreversibility is a common signature of nonlinear processes, and a\nfundamental property of non-equilibrium systems driven by non-conservative\nforces. A time series is said to be reversible if its statistical properties\nare invariant regardless of the direction of time. Here we propose the Time\nReversibility from Ordinal Patterns method (TiROP) to assess time-reversibility\nfrom an observed finite time series. TiROP captures the information of scalar\nobservations in time forward, as well as its time-reversed counterpart by means\nof ordinal patterns. The method compares both underlying information contents\nby quantifying its (dis)-similarity via Jensen-Shannon divergence. The\nstatistic is contrasted with a population of divergences coming from a set of\nsurrogates to unveil the temporal nature and its involved time scales. We\ntested TiROP in different synthetic and real, linear and non linear time\nseries, juxtaposed with results from the classical Ramsey's time reversibility\ntest. Our results depict a novel, fast-computation, and fully data-driven\nmethodology to assess time-reversibility at different time scales with no\nfurther assumptions over data. This approach adds new insights about the\ncurrent non-linear analysis techniques, and also could shed light on\ndetermining new physiological biomarkers of high reliability and computational\nefficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.04377v1"
    },
    {
        "title": "Image registration and super resolution from first principles",
        "authors": [
            "Colin B. Clement",
            "Matthew Bierbaum",
            "James P. Sethna"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Image registration is the inference of transformations relating noisy and\ndistorted images. It is fundamental in computer vision, experimental physics,\nand medical imaging. Many algorithms and analyses exist for inferring shift,\nrotation, and nonlinear transformations between image coordinates. Even in the\nsimplest case of translation, however, all known algorithms are biased and none\nhave achieved the precision limit of the Cramer Rao bound (CRB). Following\nBayesian inference, we prove that the standard method of shifting one image to\nmatch another cannot reach the CRB. We show that the bias can be cured and the\nCRB reached if, instead, we use Super Registration: learning an optimal model\nfor the underlying image and shifting that to match the data. Our theory shows\nthat coarse-graining oversampled images can improve registration precision of\nthe standard method. For oversampled data, our method does not yield striking\nimprovements as measured by eye. In these cases, however, we show our new\nregistration method can lead to dramatic improvements in extractable\ninformation, for example, inferring $10\\times$ more precise particle positions.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.05583v2"
    },
    {
        "title": "Statistical Models with Uncertain Error Parameters",
        "authors": [
            "Glen Cowan"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  In a statistical analysis in Particle Physics, nuisance parameters can be\nintroduced to take into account various types of systematic uncertainties. The\nbest estimate of such a parameter is often modeled as a Gaussian distributed\nvariable with a given standard deviation (the corresponding \"systematic\nerror\"). Although the assigned systematic errors are usually treated as\nconstants, in general they are themselves uncertain. A type of model is\npresented where the uncertainty in the assigned systematic errors is taken into\naccount. Estimates of the systematic variances are modeled as gamma distributed\nrandom variables. The resulting confidence intervals show interesting and\nuseful properties. For example, when averaging measurements to estimate their\nmean, the size of the confidence interval increases for decreasing\ngoodness-of-fit, and averages have reduced sensitivity to outliers. The basic\nproperties of the model are presented and several examples relevant for\nParticle Physics are explored.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.05778v3"
    },
    {
        "title": "Analysis of Daily Streamflow Complexity by Kolmogorov Measures and\n  Lyapunov Exponent",
        "authors": [
            "Dragutin T. Mihailović",
            "Emilija Nikolić-Đorić",
            "Ilija Arsenić",
            "Slavica Malinović-Milićević",
            "Vijay P. Singh",
            "Tatijana Stošić",
            "Borko Stošić"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Analysis of daily streamflow variability in space and time is important for\nwater resources planning, development, and management. The natural variability\nof streamflow is being complicated by anthropogenic influences and climate\nchange, which may introduce additional complexity into the phenomenological\nrecords. To address this question for daily discharge data recorded during the\nperiod 1989-2016 at twelve gauging stations on Brazos River in Texas (USA), we\nuse a set of novel quantitative tools: Kolmogorov complexity (KC) with its\nderivative associated measures to assess complexity, and Lyapunov time (LT) to\nassess predictability. We find that all daily discharge series exhibit long\nmemory with an increasing downflow tendency, while the randomness of the series\nat individual sites cannot be definitively concluded. All Kolmogorov complexity\nmeasures have relatively small values with the exception of the USGS (United\nStates Geological Survey) 08088610 station at Graford, Texas, which exhibits\nthe highest values of these complexity measures. This finding may be attributed\nto the elevated effect of human activities at Graford, and proportionally\nlesser effect at other stations. In addition, complexity tends to decrease\ndownflow, meaning that larger catchments are generally less influenced by\nanthropogenic activity. The correction on randomness of Lyapunov time\n(quantifying predictability) is found to be inversely proportional to the\nKolmogorov complexity, which strengthens our conclusion regarding the effect of\nanthropogenic activities, considering that KC and LT are distinct measures,\nbased on rather different techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.08633v1"
    },
    {
        "title": "Inference of the Kinetic Ising Model with Heterogeneous Missing Data",
        "authors": [
            "Carlo Campajola",
            "Fabrizio Lillo",
            "Daniele Tantari"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We consider the problem of inferring a causality structure from multiple\nbinary time series by using the Kinetic Ising Model in datasets where a\nfraction of observations is missing. We take our steps from a recent work on\nMean Field methods for the inference of the model with hidden spins and develop\na pseudo-Expectation-Maximization algorithm that is able to work even in\nconditions of severe data sparsity. The methodology relies on the\nMartin-Siggia-Rose path integral method with second order saddle-point solution\nto make it possible to calculate the log-likelihood in polynomial time, giving\nas output a maximum likelihood estimate of the couplings matrix and of the\nmissing observations. We also propose a recursive version of the algorithm,\nwhere at every iteration some missing values are substituted by their maximum\nlikelihood estimate, showing that the method can be used together with\nsparsification schemes like LASSO regularization or decimation. We test the\nperformance of the algorithm on synthetic data and find interesting properties\nwhen it comes to the dependency on heterogeneity of the observation frequency\nof spins and when some of the hypotheses that are necessary to the saddle-point\napproximation are violated, such as the small couplings limit and the\nassumption of statistical independence between couplings.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.08843v3"
    },
    {
        "title": "Contamination Source Detection in Water Distribution Networks using\n  Belief Propagation",
        "authors": [
            "Ernesto Ortega",
            "Alfredo Braunstein",
            "Alejandro Lage-Castellanos"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We present a Bayesian approach for the Contamination Source Detection problem\nin Water Distribution Networks. Given an observation of contaminants in one or\nmore nodes in the network, we try to give probable explanation for it assuming\nthat contamination is a rare event. We introduce extra variables to\ncharacterize the place and pattern of the first contamination event. Then we\nwrite down the posterior distribution for these extra variables given the\nobservation obtained by the sensors. Our method relies on Belief Propagation\nfor the evaluation of the marginals of this posterior distribution and the\ndetermination of the most likely origin. The method is implemented on a\nsimplified binary forward-in-time dynamics. Simulations on data coming from the\nrealistic simulation software EPANET on two networks show that the simplified\nmodel is nevertheless flexible enough to capture crucial information about\ncontaminant sources.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.10626v1"
    },
    {
        "title": "Three-Cornered Hat and Groslambert Covariance: A first attempt to assess\n  the uncertainty domains",
        "authors": [
            "François Vernotte",
            "Éric Lantz"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The three-cornered hat method and the Groslambert Covariance are very often\nused to estimate the frequency stability of each individual oscillator in a set\nof three oscillators by comparing them in pairs. However, no rigorous method to\nassess the uncertainties over their estimates has yet been formulated. In order\nto overcome this lack, this paper will first study the direct problem, i.e. the\ncalculation of the statistics of the clock stability estimates by assuming\nknown values of the true clock stabilities and then will propose a first\nattempt to solve the inverse problem, i.e. the assessment of a confidence\ninterval over the true clock stabilities by assuming known values of the clock\nstability estimates. We show that this method is reliable from 5 Equivalent\nDegrees of Freedom (EDF) and beyond.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.01530v2"
    },
    {
        "title": "Manifold Learning for Organizing Unstructured Sets of Process\n  Observations",
        "authors": [
            "Felix Dietrich",
            "Mahdi Kooshkbaghi",
            "Erik M. Bollt",
            "Ioannis G. Kevrekidis"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Data mining is routinely used to organize ensembles of short temporal\nobservations so as to reconstruct useful, low-dimensional realizations of an\nunderlying dynamical system. In this paper, we use manifold learning to\norganize unstructured ensembles of observations (\"trials\") of a system's\nresponse surface. We have no control over where every trial starts; and during\neach trial operating conditions are varied by turning \"agnostic\" knobs, which\nchange system parameters in a systematic but unknown way. As one (or more)\nknobs \"turn\" we record (possibly partial) observations of the system response.\nWe demonstrate how such partial and disorganized observation ensembles can be\nintegrated into coherent response surfaces whose dimension and parametrization\ncan be systematically recovered in a data-driven fashion. The approach can be\njustified through the Whitney and Takens embedding theorems, allowing\nreconstruction of manifolds/attractors through different types of observations.\nWe demonstrate our approach by organizing unstructured observations of response\nsurfaces, including the reconstruction of a cusp bifurcation surface for\nHydrogen combustion in a Continuous Stirred Tank Reactor. Finally, we\ndemonstrate how this observation-based reconstruction naturally leads to\ninformative transport maps between input parameter space and output/state\nvariable spaces.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.12952v3"
    },
    {
        "title": "Symmetry-guided nonrigid registration: the case for distortion\n  correction in multidimensional photoemission spectroscopy",
        "authors": [
            "Rui Patrick Xian",
            "Laurenz Rettig",
            "Ralph Ernstorfer"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Image symmetrization is an effective strategy to correct symmetry distortion\nin experimental data for which symmetry is essential in the subsequent\nanalysis. In the process, a coordinate transform, the symmetrization transform,\nis required to undo the distortion. The transform may be determined by image\nregistration (i.e. alignment) with symmetry constraints imposed in the\nregistration target and in the iterative parameter tuning, which we call\nsymmetry-guided registration. An example use case of image symmetrization is\nfound in electronic band structure mapping by multidimensional photoemission\nspectroscopy, which employs a 3D time-of-flight detector to measure electrons\nsorted into the momentum ($k_x$, $k_y$) and energy ($E$) coordinates. In\nreality, imperfect instrument design, sample geometry and experimental settings\ncause distortion of the photoelectron trajectories and, therefore, the symmetry\nin the measured band structure, which hinders the full understanding and use of\nthe volumetric datasets. We demonstrate that symmetry-guided registration can\ncorrect the symmetry distortion in the momentum-resolved photoemission\npatterns. Using proposed symmetry metrics, we show quantitatively that the\niterative approach to symmetrization outperforms its non-iterative counterpart\nin the restored symmetry of the outcome while preserving the average shape of\nthe photoemission pattern. Our approach is generalizable to distortion\ncorrections in different types of symmetries and should also find applications\nin other experimental methods that produce images with similar features.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.00312v2"
    },
    {
        "title": "Revising the stochastic iterative ensemble smoother",
        "authors": [
            "Patrick N. Raanes",
            "Geir Evensen",
            "Andreas S. Stordal"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Ensemble randomized maximum likelihood (EnRML) is an iterative (stochastic)\nensemble smoother, used for large and nonlinear inverse problems, such as\nhistory matching and data assimilation. Its current formulation is overly\ncomplicated and has issues with computational costs, noise, and covariance\nlocalization, even causing some practitioners to omit crucial prior\ninformation. This paper resolves these difficulties and streamlines the\nalgorithm, without changing its output. These simplifications are achieved\nthrough the careful treatment of the linearizations and subspaces. For example,\nit is shown (a) how ensemble linearizations relate to average sensitivity, and\n(b) that the ensemble does not lose rank during updates. The paper also draws\nsignificantly on the theory of the (deterministic) iterative ensemble Kalman\nsmoother (IEnKS). Comparative benchmarks are obtained with the Lorenz-96 model\nwith these two smoothers and the ensemble smoother using multiple data\nassimilation (ES-MDA).\n",
        "pdf_link": "http://arxiv.org/pdf/1901.06570v3"
    },
    {
        "title": "Timing and characterization of shaped pulses with MHz ADCs in a detector\n  system: a comparative study and deep learning approach",
        "authors": [
            "Pengcheng Ai",
            "Dong Wang",
            "Guangming Huang",
            "Ni Fang",
            "Deli Xu",
            "Fan Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Timing systems based on Analog-to-Digital Converters are widely used in the\ndesign of previous high energy physics detectors. In this paper, we propose a\nnew method based on deep learning to extract the time information from a finite\nset of ADC samples. Firstly, a quantitative analysis of the traditional curve\nfitting method regarding three kinds of variations (long-term drift, short-term\nchange and random noise) is presented with simulation illustrations. Next, a\ncomparative study between curve fitting and the neural networks is made to\ndemonstrate the potential of deep learning in this problem. Simulations show\nthat the dedicated network architecture can greatly suppress the noise RMS and\nimprove timing resolution in non-ideal conditions. Finally, experiments are\nperformed with the ALICE PHOS FEE card. The performance of our method is more\nthan 20% better than curve fitting in the experimental condition.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.07836v3"
    },
    {
        "title": "readPTU: a Python Library to Analyse Time Tagged Time Resolved Data",
        "authors": [
            "Guillem Ballesteros",
            "Raphael Proux",
            "Cristian Bonato",
            "Brian D. Gerardot"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  readPTU is a python package designed to analyze time-correlated single-photon\ncounting data. The use of the library promotes the storage of the complete time\narrival information of the photons and full flexibility in post-processing data\nfor analysis. The library supports the computation of time resolved signal with\nexternal triggers and second order autocorrelation function analysis can be\nperformed using multiple algorithms that provide the user with different\ntrade-offs with regards to speed and accuracy. Additionally, a thresholding\nalgorithm to perform time post-selection is also available. The library has\nbeen designed with performance and extensibility in mind to allow future users\nto implement support for additional file extensions and algorithms without\nhaving to deal with low level details. We demonstrate the performance of\nreadPTU by analyzing the second-order autocorrelation function of the resonance\nfluorescence from a single quantum dot in a two-dimensional semiconductor.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.07112v2"
    },
    {
        "title": "USID and Pycroscopy -- Open frameworks for storing and analyzing\n  spectroscopic and imaging data",
        "authors": [
            "Suhas Somnath",
            "Chris R. Smith",
            "Nouamane Laanait",
            "Rama K. Vasudevan",
            "Anton Ievlev",
            "Alex Belianinov",
            "Andrew R. Lupini",
            "Mallikarjun Shankar",
            "Sergei V. Kalinin",
            "Stephen Jesse"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Materials science is undergoing profound changes due to advances in\ncharacterization instrumentation that have resulted in an explosion of data in\nterms of volume, velocity, variety and complexity. Harnessing these data for\nscientific research requires an evolution of the associated computing and data\ninfrastructure, bridging scientific instrumentation with super- and cloud-\ncomputing. Here, we describe Universal Spectroscopy and Imaging Data (USID), a\ndata model capable of representing data from most common instruments,\nmodalities, dimensionalities, and sizes. We pair this schema with the\nhierarchical data file format (HDF5) to maximize compatibility,\nexchangeability, traceability, and reproducibility. We discuss a family of\ncommunity-driven, open-source, and free python software packages for storing,\nprocessing and visualizing data. The first is pyUSID which provides the tools\nto read and write USID HDF5 files in addition to a scalable framework for\nparallelizing data analysis. The second is Pycroscopy, which provides\nalgorithms for scientific analysis of nanoscale imaging and spectroscopy\nmodalities and is built on top of pyUSID and USID. The instrument-agnostic\nnature of USID facilitates the development of analysis code independent of\ninstrumentation and task in Pycroscopy which in turn can bring scientific\ncommunities together and break down barriers in the age of open-science. The\ninterested reader is encouraged to be a part of this ongoing community-driven\neffort to collectively accelerate materials research and discovery through the\nrealms of big data.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.09515v2"
    },
    {
        "title": "Using the singular value decomposition to extract 2D correlation\n  functions from scattering patterns",
        "authors": [
            "Philipp Bender",
            "Dominika Zákutná",
            "Sabrina Disch",
            "Lourdes Marcano",
            "Diego Alba Venero",
            "Dirk Honecker"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We apply the truncated singular value decomposition (SVD) to extract the\nunderlying 2D correlation functions from small-angle scattering patterns. We\ntest the approach by transforming the simulated data of ellipsoidal particles\nand show that also in case of anisotropic patterns (i.e. aligned ellipsoids)\nthe derived correlation functions correspond to the theoretically predicted\nprofiles. Furthermore, we use the truncated SVD to analyze the small-angle\nx-ray scattering patterns of colloidal dispersions of hematite spindles and\nmagnetotactic bacteria in presence of magnetic fields, to verify that this\napproach can be applied to extract model-free the scattering profiles of\nanisotropic scatterers from noisy data.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.10802v2"
    },
    {
        "title": "KLTS: A rigorous method to compute the confidence intervals for the\n  Three-Cornered Hat and for Groslambert Covariance",
        "authors": [
            "Éric Lantz",
            "Claudio E. Calosso",
            "Enrico Rubiola",
            "Vincent Giordano",
            "Christophe Fluhr",
            "Benoît Dubois",
            "François Vernotte"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  The three-cornered hat / Groslambert Covariance methods are widely used to\nestimate the stability of each individual clock in a set of three, but no\nmethod gives reliable confidence intervals for large integration times.\n  We propose a new KLTS (Karhunen-Lo\\`eve Tansform using Sufficient statistics)\nmethod which uses these estimators to take into account the statistics of all\nthe measurements between the pairs of clocks in a Bayesian way. The resulting\nCumulative Density Function (CDF) yields confidence intervals for each clock\nAVAR. This CDF provides also a stability estimator which is always positive.\n  Checked by massive Monte-Carlo simulations, KLTS proves to be perfectly\nreliable even for one degree of freedom. An example of experimental measurement\nis given.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.05849v4"
    },
    {
        "title": "On the methodologies for the assessment of the impact of parameters in\n  acoustophoretic separation devices",
        "authors": [
            "Fabio Garofalo"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  In this communication I reconcile the kinematic method illustrated by some\nauthors~\\cite{yang2018,vitali2018} in studying the impact of system and\nsuspension parameters on acoustophoretic separations with the statistical\nmethod formerly proposed by Garofalo~\\cite{garofalo2014,garofalo2014_2} and\nlately extended to particle populations by the same\nauthor~\\cite{garofalo2017,garofalo2018}. The connection between these two\nmethods is established by (i)~reinterpreting the kinematic method in terms of\ntangent space dynamics, and (ii)~transforming the dynamics in the tangent space\ninto the dynamics of the area elements. The dynamics of the area elements is\nequivalent to the dynamics of the covariance matrix derived by moment analysis\nand associated with the dispersion problem during microparticle\nacoustophoresis. The similarities and the differences between the kinematic\nbased method and the stochastic method proposed by the present auuthor are\nillustrated and discussed in the light of the numerical results for a\nprototypical model of acoustophoretic separation. \\keywords{Acoustofl\n",
        "pdf_link": "http://arxiv.org/pdf/1904.09976v1"
    },
    {
        "title": "Change detection in SAR time-series based on the coefficient of\n  variation",
        "authors": [
            "Elise Colin Koeniguer",
            "Jean-Marie Nicolas"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  This paper discusses change detection in SAR time-series. Firstly, several\nstatistical properties of the coefficient of variation highlight its pertinence\nfor change detection. Then several criteria are proposed. The coefficient of\nvariation is suggested to detect any kind of change.\n  Then other criteria based on ratios of coefficients of variations are\nproposed to detect long events such as construction test sites, or point-event\nsuch as vehicles.\n  These detection methods are evaluated first on theoretical statistical\nsimulations to determine the scenarios where they can deliver the best results.\nThen detection performance is assessed on real data for different types of\nscenes and sensors (Sentinel-1, UAVSAR). In particular, a quantitative\nevaluation is performed with a comparison of our solutions with\nstate-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.11335v2"
    },
    {
        "title": "Testing statistical laws in complex systems",
        "authors": [
            "Martin Gerlach",
            "Eduardo G. Altmann"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  The availability of large datasets requires an improved view on statistical\nlaws in complex systems, such as Zipf's law of word frequencies, the\nGutenberg-Richter law of earthquake magnitudes, or scale-free degree\ndistribution in networks. In this paper we discuss how the statistical analysis\nof these laws are affected by correlations present in the observations, the\ntypical scenario for data from complex systems. We first show how standard\nmaximum-likelihood recipes lead to false rejections of statistical laws in the\npresence of correlations. We then propose a conservative method (based on\nshuffling and under-sampling the data) to test statistical laws and find that\naccounting for correlations leads to smaller rejection rates and larger\nconfidence intervals on estimated parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.11624v1"
    },
    {
        "title": "Comment on \"Rényi entropy yields artifficial biases not in the data\n  and incorrect updating due to the infinite-size data\"",
        "authors": [
            "Petr Jizba",
            "Jan Korbel"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  In their recent paper [Phys. Rev. E 99 (2019) 032134], T. Oikinomou and B.\nBagci have argued that R\\'enyi entropy is ill-suited for inference purposes\nbecause it is not consistent with the Shore{ Johnson axioms of statistical\nestimation theory. In this Comment we seek to clarify the latter statement by\nshowing that there are several issues in Oikinomou{Bagci reasonings which lead\nto erroneous conclusions. When all these issues are properly accounted for, no\nviolation of Shore- Johnson axioms is found.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.00729v1"
    },
    {
        "title": "Data Processing Protocol for Regression of Geothermal Times Series with\n  Uneven Intervals",
        "authors": [
            "Palash Panja",
            "Pranay Asai",
            "Raul Velasco",
            "Milind Deo"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Regression of data generated in simulations or experiments has important\nimplications in sensitivity studies, uncertainty analysis, and prediction\naccuracy. Depending on the nature of the physical model, data points may not be\nevenly distributed. It is not often practical to choose all points for\nregression of a model because it doesn't always guarantee a better fit. Fitness\nof the model is highly dependent on the number of data points and the\ndistribution of the data along the curve. In this study, the effect of the\nnumber of points selected for regression is investigated and various schemes\naimed to process regression data points are explored. Time series data i.e.,\noutput varying with time, is our prime interest mainly the temperature profile\nfrom enhanced geothermal system. The objective of the research is to find a\nbetter scheme for choosing a fraction of data points from the entire set to\nfind a better fitness of the model without losing any features or trends in the\ndata. A workflow is provided to summarize the entire protocol of data\npreprocessing, regression of mathematical model using training data, model\ntesting, and error analysis. Six different schemes are developed to process\ndata by setting criteria such as equal spacing along axes (X and Y), equal\ndistance between two consecutive points on the curve, constraint in the angle\nof curvature, etc. As an example for the application of the proposed schemes, 1\nto 20% of the data generated from the temperature change of a typical\ngeothermal system is chosen from a total of 9939 points. It is shown that the\nnumber of data points, to a degree, has negligible effect on the fitted model\ndepending on the scheme. The proposed data processing schemes are ranked in\nterms of R2 and NRMSE values.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.06531v1"
    },
    {
        "title": "From periodic sampling to irregular sampling through PNS (Periodic\n  Nonuniform Sampling)",
        "authors": [
            "Bernard Lacaze"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Resampling is an operation costly in calculation time and accuracy. It\nregularizes irregular sampling, replacing N data by N periodic estimations.\nThis stage can be suppressed, using formulas built with incoming data and\ncompleted by sequences of elements which influence decreases when the number of\ndata increases. Obviously, some spectral properties (for processes) and some\nasymptotic properties (for the sampling sequence) have to be fulfilled. In this\npaper, we explain that it is possible to elaborate a logical theory, starting\nfrom the ordinary periodic sampling, and generalized by the PNS (Periodic\nNonuniform Sampling), to treat more general irregular samplings. The \"baseband\nspectrum\" hypothesis linked to the \"Nyquist bound\" (or Shannon bound) is\ngeneralized to spectra in a finite number of intervals, suited to the \"Landau\ncondition\".\n",
        "pdf_link": "http://arxiv.org/pdf/1905.11101v1"
    },
    {
        "title": "A practical way to regularize unfolding of sharply varying spectra with\n  low data statistics",
        "authors": [
            "Andrei Gaponenko"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Unfolding is a well-established tool in particle physics. However, a naive\napplication of the standard regularization techniques to unfold the momentum\nspectrum of protons ejected in the process of negative muon nuclear capture led\nto a result exhibiting unphysical artifacts. A finite data sample limited the\nrange in which unfolding can be performed, thus introducing a cutoff. A sharply\nfalling \"true\" distribution led to low data statistics near the cutoff, which\nexacerbated the regularization bias and produced an unphysical spike in the\nresulting spectrum. An improved approach has been developed to address these\nissues and is illustrated using a toy model. The approach uses full Poisson\nlikelihood of data, and produces a continuous, physically plausible, unfolded\ndistribution. The new technique has a broad applicability since spectra with\nsimilar features, such as sharply falling spectra, are common.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.07918v1"
    },
    {
        "title": "A universal rank-order transform to extract signals from noisy data",
        "authors": [
            "Glenn Ierley",
            "Alex Kostinski"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We introduce an ordinate method for noisy data analysis, based solely on rank\ninformation and thus insensitive to outliers. The method is nonparametric,\nobjective, and the required data processing is parsimonious. Main ingredients\nare a rank-order data matrix and its transform to a stable form, which provide\nlinear trends in excellent agreement with least squares regression, despite the\nloss of magnitude information. A group symmetry orthogonal decomposition of the\n2D rank-order transform for iid (white) noise is further ordered by principal\ncomponent analysis. This two-step procedure provides a noise \"etalon\" used to\ncharacterize arbitrary stationary stochastic processes. The method readily\ndistinguishes both the Ornstein-Uhlenbeck process and chaos generated by the\nlogistic map from white noise. Ranking within randomness differs fundamentally\nfrom that in deterministic chaos and signals, thus forming the basis for signal\ndetection. To further illustrate the breadth of applications, we apply this\nordinate method to the canonical nonlinear parameter estimation problem of\ntwo-species radioactive decay, outperforming special-purpose least square\nsoftware. It is demonstrated that the method excels when extracting trends in\nheavy-tailed noise and, unlike the Thiele-Sen estimator, is not limited to\nlinear regression. Lastly, a simple expression is given that yields a close\napproximation for signal extraction of an underlying generally nonlinear\nsignal.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.08729v1"
    },
    {
        "title": "Super-resolution energy spectra from neutron direct-geometry\n  spectrometers",
        "authors": [
            "Fahima Islam",
            "Jiao Y. Y. Lin",
            "Richard Archibald",
            "Douglas L. Abernathy",
            "Iyad Al-Qasir",
            "Anne A. Campbell",
            "Matthew B. Stone",
            "Garrett E. Granroth"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Neutron direct-geometry time-of-flight chopper spectroscopy is instrumental\nin studying fundamental excitations of vibrational and/or magnetic origin. We\nreport here that techniques in super-resolution optical imagery (which is in\nreal-space) can be adapted to enhance resolution and reduce noise for a neutron\nspectroscopy (an instrument for mapping excitations in reciprocal space). The\nprocedure to reconstruct super-resolution energy spectra of phonon density of\nstates relies on a realization of multi-frame registration, accurate\ndetermination of the energy-dependent point spread function, asymmetric nature\nof instrument resolution broadening, and iterative reconstructions. Applying\nthese methods to phonon density of states data for a graphite sample\ndemonstrates contrast enhancement, noise reduction, and ~5-fold improvement\nover nominal energy resolution. The data were collected at three different\nincident energies measured at the Wide Angular-Range Chopper Spectrometer at\nthe Spallation Neutron Source.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.09482v1"
    },
    {
        "title": "Precursors to Rare Events in Stochastic Resonance",
        "authors": [
            "L. T. Giorgini",
            "S. H. Lim",
            "W. Moon",
            "J. S. Wettlaufer"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  In stochastic resonance, a periodically forced Brownian particle in a\ndouble-well potential jumps between minima at rare increments, the prediction\nof which poses a major theoretical challenge. Here, we use a path-integral\nmethod to find a precursor to these transitions by determining the most\nprobable (or \"{optimal}\") space-time path of a particle. We characterize the\noptimal path using a direct comparison principle between the Langevin and\nHamiltonian dynamical descriptions, allowing us to express the jump condition\nin terms of the accumulation of noise around the stable periodic path. In\nconsequence, as a system approaches a rare event these fluctuations approach\none of the deterministic minimizers, thereby providing a precursor for\npredicting a stochastic transition. We demonstrate the method numerically,\nwhich allows us to determine whether a state is following a stable periodic\npath or will experience an incipient jump with a high probability. The vast\nrange of systems that exhibit stochastic resonance behavior insures broad\nrelevance of our framework, which allows one to extract precursor fluctuations\nfrom data.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.10469v4"
    },
    {
        "title": "Representing Model Discrepancy in Bound-to-Bound Data Collaboration",
        "authors": [
            "Wenyu Li",
            "Arun Hegde",
            "James Oreluk",
            "Andrew Packard",
            "Michael Frenklach"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We extended the existing methodology in Bound-to-Bound Data Collaboration\n(B2BDC), an optimization-based deterministic uncertainty quantification (UQ)\nframework, to explicitly take into account model discrepancy. The discrepancy\nwas represented as a linear combination of finite basis functions and the\nfeasible set was constructed according to a collection of modified model-data\nconstraints. Formulas for making predictions were also modified to include the\nmodel discrepancy function. Prior information about the model discrepancy can\nbe added to the framework as additional constraints. Dataset consistency, a\ncentral feature of B2BDC, was generalized based on the extended framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.00886v2"
    },
    {
        "title": "Reducing the dependence of the neural network function to systematic\n  uncertainties in the input space",
        "authors": [
            "Stefan Wunsch",
            "Simon Jörger",
            "Roger Wolf",
            "Günter Quast"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Applications of neural networks to data analyses in natural sciences are\ncomplicated by the fact that many inputs are subject to systematic\nuncertainties. To control the dependence of the neural network function to\nvariations of the input space within these systematic uncertainties, several\nmethods have been proposed. In this work, we propose a new approach of training\nthe neural network by introducing penalties on the variation of the neural\nnetwork output directly in the loss function. This is achieved at the cost of\nonly a small number of additional hyperparameters. It can also be pursued by\ntreating all systematic variations in the form of statistical weights. The\nproposed method is demonstrated with a simple example, based on\npseudo-experiments, and by a more complex example from high-energy particle\nphysics.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11674v2"
    },
    {
        "title": "Burst-tree decomposition of time series reveals the structure of\n  temporal correlations",
        "authors": [
            "Hang-Hyun Jo",
            "Takayuki Hiraoka",
            "Mikko Kivelä"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Comprehensive characterization of non-Poissonian, bursty temporal patterns\nobserved in various natural and social processes is crucial to understand the\nunderlying mechanisms behind such temporal patterns. Among them bursty event\nsequences have been studied mostly in terms of interevent times (IETs), while\nthe higher-order correlation structure between IETs has gained very little\nattention due to the lack of a proper characterization method. In this paper we\npropose a method of decomposing an event sequence into a set of IETs and a\nburst tree, which exactly captures the structure of temporal correlations that\nis entirely missing in the analysis of IET distributions. We apply the\nburst-tree decomposition method to various datasets and analyze the structure\nof the revealed burst trees. In particular, we observe that event sequences\nshow similar burst-tree structure, such as heavy-tailed burst size\ndistributions, despite of very different IET distributions. The burst trees\nallow us to directly characterize the preferential and assortative mixing\nstructure of bursts responsible for the higher-order temporal correlations. We\nalso show how to use the decomposition method for the systematic investigation\nof such higher-order correlations captured by the burst trees in the framework\nof randomized reference models. Finally, we devise a simple kernel-based model\nfor generating event sequences showing appropriate higher-order temporal\ncorrelations. Our method is a tool to make the otherwise overwhelming analysis\nof higher-order correlations in bursty time series tractable by turning it into\nthe analysis of a tree structure.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.13556v1"
    },
    {
        "title": "A simple decomposition of European temperature variability capturing the\n  variance from days to a decade",
        "authors": [
            "Philipp G Meyer",
            "Holger Kantz"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We analyze European temperature variability from station data with the method\nof detrended fluctuation analysis. This method is known to give a scaling\nexponent indicating long range correlations in time for temperature anomalies.\nHowever, by a more careful look at the fluctuation function we are able to\nexplain the emergent scaling behaviour by short time relaxation, the yearly\ncycle and one additional process. It turns out that for many stations this\ninterannual variability is an oscillatory mode with a period length of\napproximately 7-8 years, which is consistent with results of other methods. We\ndiscuss the spatial patterns in all parameters and validate the finding of the\n7-8 year period by comparing stations with and without this mode.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02212v1"
    },
    {
        "title": "GPS Fit Method for Paths of Non Drunken Sailors and its Connection to\n  Entropy",
        "authors": [
            "Fetze Pijlman"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Estimating the altimeters a cyclist has climbed from noisy GPS data is a\nchallenging problem. In this article a method is proposed that assumes that a\nperson locally takes the shortest path. This results in an algorithm that does\nnot need smoothing parameters. Moreover, it turns out that this assumption\nallows one to find a similarity between entropy and likelihood which results to\nthe introduction of an entropic force.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.06739v1"
    },
    {
        "title": "A Comparison of Adaptive and Template Matching Techniques for\n  Radio-Isotope Identification",
        "authors": [
            "Emma J. Hague",
            "Mark Kamuda",
            "William P. Ford",
            "Eric T. Moore",
            "Johanna Turk"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We compare and contrast the effectiveness of a set of adaptive and\nnon-adaptive algorithms for isotope identification based on gamma-ray spectra.\nOne dimensional energy spectra are simulated for a variety of dwell-times and\nsource to detector distances in order to reflect conditions typically\nencountered in radiological emergency response and environmental monitoring\napplications. We find that adaptive methods are more accurate and\ncomputationally efficient than non-adaptive in cases of operational interest.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.09816v1"
    },
    {
        "title": "Transforming Gaussian correlations. Applications to generating\n  long-range power-law correlated time series with arbitrary distribution",
        "authors": [
            "Pedro Carpena",
            "Pedro A. Bernaola-Galván",
            "Manuel Gómez-Extremera",
            "Ana V. Coronado"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  The observable outputs of many complex dynamical systems consist in time\nseries exhibiting autocorrelation functions of great diversity of behaviors,\nincluding long-range power-law autocorrelation functions, as a signature of\ninteractions operating at many temporal or spatial scales. Often, algorithms\nable to generate correlated noises reproducing the properties of real time\nseries produce \\textsl{Gaussian} outputs, while real, experimentally observed\ntime series are often non-Gaussian, and may follow distributions with a\ndiversity of behaviors concerning the support, the symmetry or the tail\nproperties. Here, we study how the correlation of two Gaussian variables\nchanges when they are transformed to follow a different destination\ndistribution. Specifically, we consider bounded and unbounded distributions,\nsymmetric and non-symmetric distributions, and distributions with different\ntail properties, from decays faster than exponential to heavy tail cases\nincluding power-laws, and we find how these properties affect the correlation\nof the final variables. We extend these results to Gaussian time series which\nare transformed to have a different marginal distribution, and show how the\nautocorrelation function of the final non-Gaussian time series depends on the\nGaussian correlations and on the final marginal distribution. As an application\nof our results, we propose how to generalize standard algorithms producing\nGaussian power-law correlated time series in order to create synthetic time\nseries with arbitrary distribution and controlled power-law correlations.\nFinally, we show a practical example of this algorithm by generating time\nseries mimicking the marginal distribution and the power-law tail of the\nautocorrelation function of a real time series: the absolute returns of stock\nprices.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.01725v1"
    },
    {
        "title": "Gaussian processes for data fulfilling linear differential equations",
        "authors": [
            "Christopher G. Albert"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  A method to reconstruct fields, source strengths and physical parameters\nbased on Gaussian process regression is presented for the case where data are\nknown to fulfill a given linear differential equation with localized sources.\nThe approach is applicable to a wide range of data from physical measurements\nand numerical simulations. It is based on the well-known invariance of the\nGaussian under linear operators, in particular differentiation. Instead of\nusing a generic covariance function to represent data from an unknown field,\nthe space of possible covariance functions is restricted to allow only Gaussian\nrandom fields that fulfil the homogeneous differential equation. The resulting\ntailored kernel functions lead to more reliable regression compared to using a\ngeneric kernel and makes some hyperparameters directly interpretable. For\ndifferential equations representing laws of physics such a choice limits\nrealizations of random fields to physically possible solutions. Source terms\nare added by superposition and their strength estimated in a probabilistic\nfashion, together with possibly unknown hyperparameters with physical meaning\nin the differential operator.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.03447v1"
    },
    {
        "title": "Border effect corrections for diagonal line based recurrence\n  quantification analysis measures",
        "authors": [
            "Hauke Kraemer",
            "Norbert Marwan"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Recurrence Quantification Analysis (RQA) defines a number of quantifiers,\nwhich base upon diagonal line structures in the recurrence plot (RP). Due to\nthe finite size of an RP, these lines can be cut by the borders of the RP and,\nthus, bias the length distribution of diagonal lines and, consequently, the\nline based RQA measures. In this letter we investigate the impact of the\nmentioned border effects and of the thickening of diagonal lines in an RP\n(caused by tangential motion) on the estimation of the diagonal line length\ndistribution, quantified by its entropy. Although a relation to the Lyapunov\nspectrum is theoretically expected, the mentioned entropy yields contradictory\nresults in many studies. Here we summarize correction schemes for both, the\nborder effects and the tangential motion and systematically compare them to\nmethods from the literature. We show that these corrections lead to the\nexpected behavior of the diagonal line length entropy, in particular meaning\nzero values in case of a regular motion and positive values for chaotic motion.\nMoreover, we test these methods under noisy conditions, in order to supply\npractical tools for applied statistical research.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.09211v1"
    },
    {
        "title": "Superstatistical approach to air pollution statistics",
        "authors": [
            "Griffin Williams",
            "Benjamin Schäfer",
            "Christian Beck"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Air pollution by Nitrogen Oxides (NOx) is a major concern in large cities as\nit has severe adverse health effects. However, the statistical properties of\nair pollutants are not fully understood. Here, we use methods borrowed from\nnonequilibrium statistical mechanics to construct suitable superstatistical\nmodels for air pollution statistics. In particular, we analyze time series of\nNitritic Oxide ($NO$) and Nitrogen Dioxide ($NO_2$) concentrations recorded at\nseveral locations throughout Greater London. We find that the probability\ndistributions of concentrations have heavy tails and that the dynamics is\nwell-described by $\\chi^2$ superstatistics for $NO$ and inverse $\\chi^2$\nsuperstatistics for $NO_2$. Our results can be used to give precise risk\nestimates of high-pollution situations and pave the way to mitigation\nstrategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.10433v2"
    },
    {
        "title": "On the Estimation of Mutual Information",
        "authors": [
            "Nicholas Carrara",
            "Jesse Ernst"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  In this paper we focus on the estimation of mutual information from finite\nsamples $(\\mathcal{X}\\times\\mathcal{Y})$. The main concern with estimations of\nmutual information is their robustness under the class of transformations for\nwhich it remains invariant: i.e. type I (coordinate transformations), III\n(marginalizations) and special cases of type IV (embeddings, products).\nEstimators which fail to meet these standards are not \\textit{robust} in their\ngeneral applicability. Since most machine learning tasks employ transformations\nwhich belong to the classes referenced in part I, the mutual information can\ntell us which transformations are most optimal\\cite{Carrara_Ernst}. There are\nseveral classes of estimation methods in the literature, such as non-parametric\nestimators like the one developed by Kraskov et. al\\cite{KSG}, and its improved\nversions\\cite{LNC}. These estimators are extremely useful, since they rely only\non the geometry of the underlying sample, and circumvent estimating the\nprobability distribution itself. We explore the robustness of this family of\nestimators in the context of our design criteria.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.00365v1"
    },
    {
        "title": "Characterizing stochastic time series with ordinal networks",
        "authors": [
            "Arthur A. B. Pessa",
            "Haroldo V. Ribeiro"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Approaches for mapping time series to networks have become essential tools\nfor dealing with the increasing challenges of characterizing data from complex\nsystems. Among the different algorithms, the recently proposed ordinal networks\nstand out due to its simplicity and computational efficiency. However,\napplications of ordinal networks have been mainly focused on time series\narising from nonlinear dynamical systems, while basic properties of ordinal\nnetworks related to simple stochastic processes remain poorly understood. Here,\nwe investigate several properties of ordinal networks emerging from random time\nseries, noisy periodic signals, fractional Brownian motion, and earthquake\nmagnitude series. For ordinal networks of random series, we present an approach\nfor building the exact form of the adjacency matrix, which in turn is useful\nfor detecting non-random behavior in time series and the existence of missing\ntransitions among ordinal patterns. We find that the average value of a local\nentropy, estimated from transition probabilities among neighboring nodes of\nordinal networks, is more robust against noise addition than the standard\npermutation entropy. We show that ordinal networks can be used for estimating\nthe Hurst exponent of time series with accuracy comparable with\nstate-of-the-art methods. Finally, we argue that ordinal networks can detect\nsudden changes in Earth seismic activity caused by large earthquakes.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.01406v1"
    },
    {
        "title": "Correction of IQ mismatch for a particle tracking radar",
        "authors": [
            "Felix Rech",
            "Kai Huang"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  For a better understanding of granular flow problems such as silo blockage,\navalanche triggering, mixing and segregation, it is essential to have a\n`microscopic' view of individual particles. In order to cope with the\ndifficulty arising from the opacity of granular materials, such as sands,\npowders and grains, a small scale bi-static radar system operating at $10$\\,GHz\n(X-band) was recently introduced to trace a sub-centimeter particle in three\ndimensions. Similar to a moving target indicator radar, the relative movement\nof the tracer with respect to each of the three receiving antennae is obtained\nvia comparing the phase shift of the electromagnetic wave traveling through the\ntarget area with an IQ-Mixer. From the azimuth and tilt angles of the receiving\nantennae obtained in the calibration, the target trajectory in a\nthree-dimensional Cartesian system is reconstructed. Using a free-falling\nsphere as a test case, we discuss the accuracy of this radar system and\npossible ways to enhance it by IQ mismatch corrections.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00750v1"
    },
    {
        "title": "SCOUT: Signal Correction and Uncertainty Quantification Toolbox in\n  MATLAB",
        "authors": [
            "Richard Semaan",
            "Vikas Yadav"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  This manuscript describes the software package SCOUT, which analyzes,\ncharacterizes, and corrects one-dimensional signals. Specifically, it allows to\ncheck and correct for stationarity, detect spurious samples, check for\nnormality, check for periodicity, filter, perform spectral analysis, determine\nthe integral time scale, and perform uncertainty analysis on individual and on\npropagated signals through a data reduction equation. The novelty of SCOUT lies\nin combining these various methods into one compact and easy-to-use toolbox,\nwhich enables students and professionals alike to analyze, characterize, and\ncorrect for signals without expert knowledge. The program is oriented towards\ntime traces, but an easy adaptation to spatial distributions can be performed\nby the user. SCOUT is available in two variants: a graphical user interface\n(GUI) and a script-based version. A key motivation of having two variants is to\noffer maximum flexibility to adaptively and visually adjust the analysis\nsettings using the GUI version and to enable large batch processing\ncapabilities and own code-integration using the script-based version. The\npackage includes both variants as well as three example scripts with their\ncorresponding signals.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.02387v1"
    },
    {
        "title": "Bayesian Optimization for machine learning algorithms in the context of\n  Higgs searches at the CMS experiment",
        "authors": [
            "Oriel Kiss"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Machine Learning algorithms, such as Boosted Decisions Trees and Deep Neural\nNetwork, are widely used in High-Energy-Physics. The aim of this study is to\napply Bayesian Optimization to tune the hyperparameters used in a machine\nlearning algorithm. This algorithm performs an energy regression process on\nphotons and electrons detected in the electromagnetic calorimeter at the\nCompact Muon Solenoid experiment operating at the Large Hadron Collider at\nCERN. The goal of this algorithm is to estimate the energy of photons and\nelectrons created during the collisions in the Compact Muon Solenoid, from the\nmeasured energy.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.02501v1"
    },
    {
        "title": "Estimation of roughness measurement bias originating from background\n  subtraction",
        "authors": [
            "David Nečas",
            "Petr Klapetek",
            "Miroslav Valtr"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  When measuring the roughness of rough surfaces, the limited sizes of scanned\nareas lead to its systematic underestimation. Levelling by polynomials and\nother filtering used in real-world processing of atomic force microscopy data\nincreases this bias considerably. Here a framework is developed providing\nexplicit expressions for the bias of squared mean square roughness in the case\nof levelling by fitting a model background function using linear least squares.\nThe framework is then applied to polynomial levelling, for both one-dimensional\nand two-dimensional data processing, and basic models of surface\nautocorrelation function, Gaussian and exponential. Several other common\nscenarios are covered as well, including median levelling, intermediate\nGaussian--exponential autocorrelation model and frequency space filtering.\nApplication of the results to other quantities, such as Rq, Sq, Ra and~Sa is\ndiscussed. The results are summarized in overview plots covering a range of\nautocorrelation functions and polynomial degrees, which allow graphical\nestimation of the bias.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.00433v1"
    },
    {
        "title": "Measures of spike train synchrony and directionality",
        "authors": [
            "Eero Satuvuori",
            "Irene Malvestio",
            "Thomas Kreuz"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Measures of spike train synchrony have become important tools in both\nexperimental and theoretical neuroscience. Three time-resolved measures called\nthe ISI-distance, the SPIKE-distance, and SPIKE-synchronization have already\nbeen successfully applied in many different contexts. These measures are time\nscale independent, since they consider all time scales as equally important.\nHowever, in real data one is typically less interested in the smallest time\nscales and a more adaptive approach is needed. Therefore, in the first part of\nthis Chapter we describe recently introduced generalizations of the three\nmeasures, that gradually disregard differences in smaller time-scales. Besides\nsimilarity, another very relevant property of spike trains is the temporal\norder of spikes. In the second part of this chapter we address this property\nand describe a very recently proposed algorithm, which quantifies the\ndirectionality within a set of spike train. This multivariate approach sorts\nmultiple spike trains from leader to follower and quantifies the consistency of\nthe propagation patterns. Finally, all measures described in this chapter are\nfreely available for download.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.04406v1"
    },
    {
        "title": "Fast Algorithm for computing a matrix transform used to detect trends in\n  noisy data",
        "authors": [
            "D. J. Kestner",
            "G. R. Ierley",
            "A. B. Kostinski"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  A recently discovered universal rank-based matrix method to extract trends\nfrom noisy time series is described in [1] but the formula for the output\nmatrix elements, implemented there as an open-access supplement MATLAB computer\ncode, is ${\\cal O}(N^4)$, with $N$ the matrix dimension. This can become\nprohibitively large for time series with hundreds of sample points or more.\nBased on recurrence relations, here we derive a much faster ${\\cal O}(N^2)$\nalgorithm and provide code implementations in MATLAB and in open-source JULIA.\nIn some cases one has the output matrix and needs to solve an inverse problem\nto obtain the input matrix. A fast algorithm and code for this companion\nproblem, also based on the above recurrence relations, are given. Finally, in\nthe narrower, but common, domains of (i) trend detection and (ii) parameter\nestimation of a linear trend, users require, not the individual matrix\nelements, but simply their accumulated mean value. For this latter case we\nprovide a yet faster ${\\cal O}(N)$ heuristic approximation that relies on a\nseries of rank one matrices. These algorithms are illustrated on a time series\nof high energy cosmic rays with $N > 4 \\times 10^4$. [1] Universal Rank-Order\nTransform to Extract Signals from Noisy Data, Glenn Ierley and Alex Kostinski,\nPhys. Rev. X 9 031039 (2019).\n",
        "pdf_link": "http://arxiv.org/pdf/2001.09607v1"
    },
    {
        "title": "Reducing network size and improving prediction stability of reservoir\n  computing",
        "authors": [
            "Alexander Haluszczynski",
            "Jonas Aumeier",
            "Joschka Herteux",
            "Christoph Räth"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Reservoir computing is a very promising approach for the prediction of\ncomplex nonlinear dynamical systems. Besides capturing the exact short-term\ntrajectories of nonlinear systems, it has also proved to reproduce its\ncharacteristic long-term properties very accurately. However, predictions do\nnot always work equivalently well. It has been shown that both short- and\nlong-term predictions vary significantly among different random realizations of\nthe reservoir. In order to gain an understanding on when reservoir computing\nworks best, we investigate some differential properties of the respective\nrealization of the reservoir in a systematic way. We find that removing nodes\nthat correspond to the largest weights in the output regression matrix reduces\noutliers and improves overall prediction quality. Moreover, this allows to\neffectively reduce the network size and, therefore, increase computational\nefficiency. In addition, we use a nonlinear scaling factor in the hyperbolic\ntangent of the activation function. This adjusts the response of the activation\nfunction to the range of values of the input variables of the nodes. As a\nconsequence, this reduces the number of outliers significantly and increases\nboth the short- and long-term prediction quality for the nonlinear systems\ninvestigated in this study. Our results demonstrate that a large optimization\npotential lies in the systematical refinement of the differential reservoir\nproperties for a given dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.03178v2"
    },
    {
        "title": "Cross-Spectrum Measurement Statistics",
        "authors": [
            "Antoine Baudiquez",
            "Éric Lantz",
            "Enrico Rubiola",
            "François Vernotte"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The cross-spectrum method consists in measuring a signal $c(t)$\nsimultaneously with two independent instruments. Each of these instruments\ncontributes to the global noise by its intrinsec (white) noise, whereas the\nsignal $c(t)$ that we want to characterize could be a (red) noise. We first\ndefine the real part of the cross-spectrum as a relevant estimator. Then, we\ncharacterize the probability density function (PDF) of this estimator knowing\nthe noise level (direct problem) as a Variance-Gamma (V$\\Gamma$) distribution.\nNext, we solve the \"inverse problem\" thanks to Bayes' theorem to obtain an\nupper limit of the noise level knowing the estimate. Checked by massive Monte\nCarlo simulations, V$\\Gamma$ proves to be perfectly reliable to any number of\ndegrees of freedom (dof). Finally we compare this method with an other method\nusing the Karhunen-Lo\\`{e}ve transfrom (KLT). We find an upper limit of the\nsignal level slightly different as the one of V$\\Gamma$ since KLT better takes\ninto account the available informations.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.07118v1"
    },
    {
        "title": "A generalized permutation entropy for random processes",
        "authors": [
            "José M. Amigó",
            "Roberto Dale",
            "Piergiulio Tempesta"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Permutation entropy measures the complexity of deterministic time series via\na data symbolic quantization consisting of rank vectors called ordinal patterns\nor just permutations. The reasons for the increasing popularity of this entropy\nin time series analysis include that (i) it converges to the Kolmogorov-Sinai\nentropy of the underlying dynamics in the limit of ever longer permutations,\nand (ii) its computation dispenses with generating and ad hoc partitions.\nHowever, permutation entropy diverges when the number of allowed permutations\ngrows super-exponentially with their length, as is usually the case when time\nseries are output by random processes. In this Letter we propose a generalized\npermutation entropy that is finite for random processes, including\ndiscrete-time dynamical systems with observational or dynamical noise.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13728v1"
    },
    {
        "title": "Estimation of the Randomness of Continuous and Discrete Signals Using\n  the Disentropy of the Autocorrelation",
        "authors": [
            "R. V. Ramos"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The amount of randomness in a signal generated by physical or non-physical\nprocess can reveal important information about that process. For example, the\npresence of randomness in ECG signals may indicate a cardiac disease. On the\nhand, the lack of randomness in a speech signal may indicate the speaker is a\nmachine. Hence, to quantify the amount of randomness in a signal is an\nimportant task in many different areas. In this direction, the present work\nproposes to use the disentropy of the autocorrelation function as a measure of\nrandomness. Examples using noisy and chaotic signals are shown.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01602v1"
    },
    {
        "title": "Conservation Laws and Spin System Modeling through Principal Component\n  Analysis",
        "authors": [
            "David Yevick"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  This paper examines several applications of principal component analysis\n(PCA) to physical systems. The first of these demonstrates that the principal\ncomponents in a basis of appropriate system variables can be employed to\nidentify physically conserved quantities. That is, if the general form of a\nphysical symmetry law is known, the PCA can identify an algebraic expression\nfor the symmetry from the observed system trajectories. Secondly, the\neigenvalue spectrum of the principal component spectrum for homogeneous\nperiodic spin systems is found to reflect the geometric shape of the boundary.\nFinally, the PCA is employed to generate synthetic spin realizations with\nprobability distributions in energy-magnetization space that closely resemble\nthat of the input realizations although statistical quantities are inaccurately\nreproduced.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01613v1"
    },
    {
        "title": "Modelling attenuation and velocity of ultrasonics in reconstituted milk\n  powder",
        "authors": [
            "Bernard Lacaze"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  In the context of food quality control, ultrasonics provide proven methods\nwhich are able to replace manual controls. The latter are subject to the lack\nof objectivity of human judgement. Automatic control increases reliability and\nreduces costs. This paper revisits data coming from ultrasonics through\nreconstituted milk powder. Two characteristics are studied using five\nproductions of a well known manufacturer. Measured attenuation and dispersion\nof ultrasonics are explained through stable probability laws and random\npropagation times. We have proved elsewhere that this model is available in\nmany propagation problems,in acoustics, ultrasonics and in the electromagnetic\nworld.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03279v1"
    },
    {
        "title": "A deep learning approach to multi-track location and orientation in\n  gaseous drift chambers",
        "authors": [
            "Pengcheng Ai",
            "Dong Wang",
            "Xiangming Sun",
            "Guangming Huang",
            "Zili Li"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Accurate measuring the location and orientation of individual particles in a\nbeam monitoring system is of particular interest to researchers in multiple\ndisciplines. Among feasible methods, gaseous drift chambers with hybrid pixel\nsensors have the great potential to realize long-term stable measurement with\nconsiderable precision. In this paper, we introduce deep learning to analyze\npatterns in the beam projection image to facilitate three-dimensional\nreconstruction of particle tracks. We propose an end-to-end neural network\nbased on segmentation and fitting for feature extraction and regression. Two\nsegmentation branches, named binary segmentation and semantic segmentation,\nperform initial track determination and pixel-track association. Then pixels\nare assigned to multiple tracks, and a weighted least squares fitting is\nimplemented with full back-propagation. Besides, we introduce a center-angle\nmeasure to judge the precision of location and orientation by combining two\nseparate factors. The initial position resolution achieves 8.8 $\\mu m$ for the\nsingle track and 11.4 $\\mu m$ (15.2 $\\mu m$) for the 1-3 tracks (1-5 tracks),\nand the angle resolution achieves 0.15$^{\\circ}$ and 0.21$^{\\circ}$\n(0.29$^{\\circ}$) respectively. These results show a significant improvement in\naccuracy and multi-track compatibility compared to traditional methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09930v2"
    },
    {
        "title": "A Fast Algorithm for Calculation of Thêo1",
        "authors": [
            "Ben Lewis"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Th\\^eo1 is a frequency stability statistic which is similar to the Allan\nvariance but can provide stability estimates at longer averaging factors and\nwith higher confidence. However, the calculation of Th\\^eo1 is significantly\nslower than the Allan variance, particularly for large data sets, due to a\nworse computational complexity. A faster algorithm for calculating the\n`all-$\\tau$' version of Th\\^eo1 is developed by identifying certain repeated\nsums and removing them with a recurrence relation. The new algorithm has a\nreduced computational complexity, equal to that of the Allan variance.\nComputation time is reduced by orders of magnitude for many datasets. The new,\nfaster algorithm does introduce an error due to accumulated floating point\nerrors in very large datasets. The error can be compensated for by increasing\nthe numerical precision used at critical steps. The new algorithm can also be\nused to increase the speed of Th\\^eoBr and Th\\^eoH which are more sophisticated\nstatistics derived from Th\\^eo1.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.11531v2"
    },
    {
        "title": "Error estimation in the method of quasi-optimal weights",
        "authors": [
            "A. D. Morozov",
            "A. V. Lokhov",
            "F. V. Tkachov"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We examine the problem of construction of confidence intervals within the\nbasic single-parameter, single-iteration variation of the method of\nquasi-optimal weights. Two kinds of distortions of such intervals due to\ninsufficiently large samples are examined, both allowing an analytical\ninvestigation. First, a criterion is developed for validity of the assumption\nof asymptotic normality together with a recipe for the corresponding\ncorrections. Second, a method is derived to take into account the systematic\nshift of the confidence interval due to the non-linearity of the theoretical\nmean of the weight as a function of the parameter to be estimated. A numerical\nexample illustrates the two corrections.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.12554v1"
    },
    {
        "title": "Unidimensional and Multidimensional Methods for Recurrence\n  Quantification Analysis with crqa",
        "authors": [
            "Moreno I. Coco",
            "Dan Mønster",
            "Giuseppe Leonardi",
            "Rick Dale",
            "Sebastian Wallot"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Recurrence quantification analysis is a widely used method for characterizing\npatterns in time series. This article presents a comprehensive survey for\nconducting a wide range of recurrence-based analyses to quantify the dynamical\nstructure of single and multivariate time series, and to capture coupling\nproperties underlying leader-follower relationships. The basics of recurrence\nquantification analysis (RQA) and all its variants are formally introduced\nstep-by-step from the simplest auto-recurrence to the most advanced\nmultivariate case. Importantly, we show how such RQA methods can be deployed\nunder a single computational framework in R using a substantially renewed\nversion our crqa 2.0 package. This package includes implementations of several\nrecent advances in recurrence-based analysis, among them applications to\nmultivariate data, and improved entropy calculations for categorical data. We\nshow concrete applications of our package to example data, together with a\ndetailed description of its functions and some guidelines on their usage.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.01954v1"
    },
    {
        "title": "Towards an improved Eigensystem Realization Algorithm for low-error\n  guarantees",
        "authors": [
            "Mohammad N. Murshed",
            "Moajjem Hossain Chowdhury",
            "Md. Nazmul Islam Shuzan",
            "M. Monir Uddin"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Eigensystem Realization Algorithm (ERA) is a tool that can produce a reduced\norder model (ROM) from just input-output data of a given system. ERA creates\nthe ROM while keeping the number of internal states to a minimum level. This\nwas first implemented by Juang and Pappa (1984) to analyze the vibration of\naerospace structures from impulse response. We reviewed ERA and tested it on\nsingle input single output (SISO) system as well as on multiple input single\noutput (MISO) system. ERA prediction agreed with the actual data. Unlike other\nmodel reduction techniques (Balanced truncation, balanced proper orthogonal\ndecomposition), ERA works just as fine without the need of the adjoint system,\nthat makes ERA a promising, completely data-driven, thrifty model reduction\nmethod. In this work, we propose a modified Eigensystem Realization Algorithm\nthat relies upon an optimally chosen time resolution for the output used and\nalso checks for good performance through frequency analysis. Four examples are\ndiscussed: the first two confirm the model generating ability and the last two\nillustrate its capability to produce a low-dimensional model (for a large scale\nsystem) that is much more accurate than the one produced by the traditional\nERA.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.02139v1"
    },
    {
        "title": "Parameter-free and fast nonlinear piecewise filtering. Application to\n  experimental physics",
        "authors": [
            "Barbara Pascal",
            "Nelly Pustelnik",
            "Patrice Abry",
            "Jean-Christophe Géminard",
            "Valérie Vidal"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Numerous fields of nonlinear physics, very different in nature, produce\nsignals and images, that share the common feature of being essentially\nconstituted of piecewise homogeneous phases. Analyzing signals and images from\ncorresponding experiments to construct relevant physical interpretations thus\noften requires detecting such phases and estimating accurately their\ncharacteristics (borders, feature differences, ...). However, situations of\nphysical relevance often comes with low to very low signal to noise ratio\nprecluding the standard use of classical linear filtering for analysis and\ndenoising and thus calling for the design of advanced nonlinear signal/image\nfiltering techniques. Additionally, when dealing with experimental physics\nsignals/images, a second limitation is the large amount of data that need to be\nanalyzed to yield accurate and relevant conclusions requiring the design of\nfast algorithms. The present work proposes a unified signal/image nonlinear\nfiltering procedure, with fast algorithms and a data-driven automated\nhyperparameter tuning, based on proximal algorithms and Stein unbiased\nestimator principles. The interest and potential of these tools are illustrated\nat work on low-confinement solid friction signals and porous media multiphase\nflows.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.03297v2"
    },
    {
        "title": "Multinomial, Poisson and Gaussian statistics in count data analysis",
        "authors": [
            "Jakob Lassa",
            "Magnus Egede Bøggild",
            "Per Hedegård",
            "Kim Lefmann"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  It is generally known that counting statistics is not correctly described by\na Gaussian approximation. Nevertheless, in neutron scattering, it is common\npractice to apply this approximation to the counting statistics; also at low\ncounting numbers. We show that the application of this approximation leads to\nskewed results not only for low-count features, such as background level\nestimation, but also for its estimation at double-digit count numbers. In\neffect, this approximation is shown to be imprecise on all levels of count.\nInstead, a Multinomial approach is introduced as well as a more standard\nPoisson method, which we compare with the Gaussian case. These two methods\noriginate from a proper analysis of a multi-detector setup and a standard\ntriple axis instrument.We devise a simple mathematical procedure to produce\nunbiased fits using the Multinomial distribution and demonstrate this method on\nsynthetic and actual inelastic scattering data. We find that the Multinomial\nmethod provide almost unbiased results, and in some cases outperforms the\nPoisson statistics. Although significantly biased, the Gaussian approach is in\ngeneral more robust in cases where the fitted model is not a true\nrepresentation of reality. For this reason, a proper data analysis toolbox for\nlow-count neutron scattering should therefore contain more than one model for\ncounting statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.03812v1"
    },
    {
        "title": "Reliable Detection of Causal Asymmetries in Dynamical Systems",
        "authors": [
            "Erik Laminski",
            "Klaus R. Pawelzik"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Knowledge about existence, strength, and dominant direction of causal\ninfluences is of paramount importance for understanding complex systems. With\nlimited amounts of realistic data, however, current methods for investigating\ncausal links among different observables from dynamical systems suffer from\nambiguous results. Missing is a statistically well defined approach that avoids\nfalse positive detections while being sensitive for weak interactions. Ideally,\nit should be able to infer directed causal influences also when\nsynchronizations occur. The proposed method exploits local inflations of\nmanifolds to obtain estimates of upper bounds on the information loss among\nstate reconstructions from two observables. It comes with a test for the\nabsence of causal influences. Simulated data demonstrate that it is robust to\nintrinsic noise, copes with synchronizations, and tolerates also measurement\nnoise.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.09486v2"
    },
    {
        "title": "Parameter free determination of optimum time delay",
        "authors": [
            "Thiago Lima Prado",
            "Vandertone Santos Machado",
            "Gilberto Corso",
            "Gustavo Zampier dos Santos Lima",
            "Sergio Roberto Lopes"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We show that the same maximum entropy principle applied to recurrence\nmicrostates configures a new way to properly compute the time delay necessary\nto correctly sample a data set. The new method retrieves results obtained using\ntraditional methods with the advantage of being independent of any free\nparameter. Since all parameters are automatically set, the method is suitable\nfor use in artificial (computational) intelligence algorithms, recovering\ncorrect information embedded in time series, and rationalizing the process of\ndata acquisition since only relevant data must be collected.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.02951v1"
    },
    {
        "title": "The spectrum decorrelation assumption for the cross-spectrum method",
        "authors": [
            "François Vernotte",
            "Antoine Baudiquez",
            "Enrico Rubiola"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  This paper presents a very simple method ensuring the independence of\nconsecutive spectra of the phase or frequency noise of an oscillator. This\ncondition is essential for using cross-spectrum averages.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.04473v1"
    },
    {
        "title": "Full Automation for Rapid Modulator Characterization and Accurate\n  Analysis Using SciPy",
        "authors": [
            "T. L. Yap",
            "A. Sasidhara",
            "N. X. Ang",
            "X. Guo",
            "W. Wang",
            "K. S. Ang",
            "S. L. Tan"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Modulator testing involved complex biasing conditions, hardware connections\nand data analysis. Also, any optical signal distortion due to the grating\ncoupler effect could potentially induce additional difficulty in setting the\ncorrect bias condition for an accurate measurement of the modulator\nperformance. In this paper, we proposed to use SciPy, an open-source scientific\ncomputing library, for automation in the silicon modulator test with bias\nsetting and data analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.05274v1"
    },
    {
        "title": "A Comparative Analysis on LaueUtil and PRECOGNITION Software Packages as\n  Tools in Treating the Small Molecule Time-Resolved Laue Diffraction\n  Measurements at High Flux X-ray facilities",
        "authors": [
            "J. J. Velazquez-Garcia",
            "J. Wong",
            "K. Basuroy",
            "D. Storozhuk",
            "S. Saouane",
            "R. Henning",
            "S. Techert"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Investigating metal organic systems with time-resolved photocrystallography\nposes a unique challenge while interpreting the time dependent photodifference\nmaps. In these difference Fourier maps, the signals correspond to the movement\nof heavy metal atoms always overpower the signals from much lighter atoms\nattached to them. For a systematic assessment of the quality of the\nphotodifference maps obtained from metal organic systems, in this work,\nLaueUtil and PRECOGNITION software were used to treat time-resolved Laue\ncrystallography data of a [2x2] matrix-like Fe(II) complex. The rigid spot\nidentification method in LaueUtil allows to identify and index > 250,000\nreflections per 10 datasets. Though this leads to low completeness (< 30%), the\nsoftware only treats the information from highly reliable diffraction spots. As\na result, clean photodifference maps and small values in the thermal scale\nfactor have been obtained. In the PRECOGNITION case, the package indexed more\nthan 160,000 reflections per dataset. The resulting completeness is higher\n(>86%) and useful photodifference maps can be obtained by careful data\ntreatment. However, the dependence on the refinement of the lambda curve as\nwell as the degradation of the sample may be reflected on the large thermal\nscale factors which also contribute as noise in the photodifference maps.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.05781v1"
    },
    {
        "title": "Nonparametric, data-based kernel interpolation for particle-tracking\n  simulations and kernel density estimation",
        "authors": [
            "David A Benson",
            "Diogo Bolster",
            "Stephen Pankavich",
            "Michael J Schmidt"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Traditional interpolation techniques for particle tracking include binning\nand convolutional formulas that use pre-determined (i.e., closed-form,\nparameteric) kernels. In many instances, the particles are introduced as point\nsources in time and space, so the cloud of particles (either in space or time)\nis a discrete representation of the Green's function of an underlying PDE. As\nsuch, each particle is a sample from the Green's function; therefore, each\nparticle should be distributed according to the Green's function. In short, the\nkernel of a convolutional interpolation of the particle sample \"cloud\" should\nbe a replica of the cloud itself. This idea gives rise to an iterative method\nby which the form of the kernel may be discerned in the process of\ninterpolating the Green's function. When the Green's function is a density,\nthis method is broadly applicable to interpolating a kernel density estimate\nbased on random data drawn from a single distribution. We formulate and\nconstruct the algorithm and demonstrate its ability to perform kernel density\nestimation of skewed and/or heavy-tailed data including breakthrough curves.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.06737v1"
    },
    {
        "title": "On the Feynman-alpha Method for Reflected Fissile Assemblies",
        "authors": [
            "Michael Y. Hua",
            "Jesson D. Hutchinson",
            "George E. McKenzie",
            "Shaun D. Clarke",
            "Sara A. Pozzi"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The Feynman-alpha method is a neutron noise technique that is used to\nestimate the prompt neutron period of fissile assemblies. The method and\nquantity are of widespread interest including in applications such as nuclear\ncriticality safety, safeguards and nonproliferation, and stockpile stewardship;\nthe prompt neutron period may also be used to infer the $k_\\text{eff}$\nmultiplication factor. The Feynman-alpha method is predicated on\ntime-correlated neutron detections that deviate from a Poisson random variable\ndue to multiplication. Traditionally, such measurements are diagnosed with\none-region point kinetics, but two-region models are required when the fissile\nassembly is reflected. This paper presents a derivation of the two-region point\nkinetics Feynman-alpha equations based on a double integration of the\nRossi-alpha equations, develops novel propagation of measurement uncertainty,\nand validates the theory. Validation is achieved with organic scintillator\nmeasurements of weapons-grade plutonium reflected by various amounts of copper\nto achieve $k_\\text{eff}$ values of 0.83-0.94 and prompt periods of 5-75 ns.\nThe results demonstrate that Feynman-alpha measurements should use the\ntwo-region model instead of the one-region model. The simplified one-region\nmodel deviates from the validated two-region models by as much as 10\\% in the\nestimate of the prompt neutron period, and the two-region model reduces to the\none-region model for small amounts of reflector. The Feynman-alpha estimates of\nthe prompt neutron period are compared to those of the Rossi-alpha approach.\nThe comparative results demonstrate that the Feynman-alpha method is more\nprecise than the Rossi-alpha method and more accurate for $k_\\text{eff}<0.92$,\nwhereas the Rossi-alpha method is generally more accurate for higher\nmultiplications.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.11768v4"
    },
    {
        "title": "An algorithm for the automatic deglitching of x-ray absorption\n  spectroscopy data",
        "authors": [
            "Samuel M. Wallace",
            "Marco A. Alsina",
            "Jean-François Gaillard"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Analysis of x-ray absorption spectroscopy (XAS) data often involves the\nremoval of artifacts or glitches from the acquired signal, a process commonly\nknown as deglitching. Glitches result either from specific orientations of\nmonochromator crystals or from scattering by crystallites in the sample itself.\nSince the precise energy or wavelength location and the intensity of glitches\nin a spectrum cannot always be predicted, deglitching is often performed on a\nper spectrum basis by the analyst. Some routines have been proposed, but they\nare prone to arbitrary selection of spectral artifacts and are often inadequate\nfor processing large data sets. Here we present a statistically robust\nalgorithm, implemented as a Python program, for the automatic detection and\nremoval of glitches that can be applied to a large number of spectra. It uses a\nSavitzky-Golay filter to smooth spectra and the generalized extreme Studentized\ndeviate test to identify outliers. We achieve robust, repeatable, and selective\nremoval of glitches using this algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.14467v1"
    },
    {
        "title": "Cosmic Background Removal with Deep Neural Networks in SBND",
        "authors": [
            " SBND Collaboration",
            "R. Acciarri",
            "C. Adams",
            "C. Andreopoulos",
            "J. Asaadi",
            "M. Babicz",
            "C. Backhouse",
            "W. Badgett",
            "L. Bagby",
            "D. Barker",
            "V. Basque",
            "M. C. Q. Bazetto",
            "M. Betancourt",
            "A. Bhanderi",
            "A. Bhat",
            "C. Bonifazi",
            "D. Brailsford",
            "A. G. Brandt",
            "T. Brooks",
            "M. F. Carneiro",
            "Y. Chen",
            "H. Chen",
            "G. Chisnall",
            "J. I. Crespo-Anadón",
            "E. Cristaldo",
            "C. Cuesta",
            "I. L. de Icaza Astiz",
            "A. De Roeck",
            "G. de Sá Pereira",
            "M. Del Tutto",
            "V. Di Benedetto",
            "A. Ereditato",
            "J. J. Evans",
            "A. C. Ezeribe",
            "R. S. Fitzpatrick",
            "B. T. Fleming",
            "W. Foreman",
            "D. Franco",
            "I. Furic",
            "A. P. Furmanski",
            "S. Gao",
            "D. Garcia-Gamez",
            "H. Frandini",
            "G. Ge",
            "I. Gil-Botella",
            "S. Gollapinni",
            "O. Goodwin",
            "P. Green",
            "W. C. Griffith",
            "R. Guenette",
            "P. Guzowski",
            "T. Ham",
            "J. Henzerling",
            "A. Holin",
            "B. Howard",
            "R. S. Jones",
            "D. Kalra",
            "G. Karagiorgi",
            "L. Kashur",
            "W. Ketchum",
            "M. J. Kim",
            "V. A. Kudryavtsev",
            "J. Larkin",
            "H. Lay",
            "I. Lepetic",
            "B. R. Littlejohn",
            "W. C. Louis",
            "A. A. Machado",
            "M. Malek",
            "D. Mardsen",
            "C. Mariani",
            "F. Marinho",
            "A. Mastbaum",
            "K. Mavrokoridis",
            "N. McConkey",
            "V. Meddage",
            "D. P. Méndez",
            "T. Mettler",
            "K. Mistry",
            "A. Mogan",
            "J. Molina",
            "M. Mooney",
            "L. Mora",
            "C. A. Moura",
            "J. Mousseau",
            "A. Navrer-Agasson",
            "F. J. Nicolas-Arnaldos",
            "J. A. Nowak",
            "O. Palamara",
            "V. Pandey",
            "J. Pater",
            "L. Paulucci",
            "V. L. Pimentel",
            "F. Psihas",
            "G. Putnam",
            "X. Qian",
            "E. Raguzin",
            "H. Ray",
            "M. Reggiani-Guzzo",
            "D. Rivera",
            "M. Roda",
            "M. Ross-Lonergan",
            "G. Scanavini",
            "A. Scarff",
            "D. W. Schmitz",
            "A. Schukraft",
            "E. Segreto",
            "M. Soares Nunes",
            "M. Soderberg",
            "S. Söldner-Rembold",
            "J. Spitz",
            "N. J. C. Spooner",
            "M. Stancari",
            "G. V. Stenico",
            "A. Szelc",
            "W. Tang",
            "J. Tena Vidal",
            "D. Torretta",
            "M. Toups",
            "C. Touramanis",
            "M. Tripathi",
            "S. Tufanli",
            "E. Tyley",
            "G. A. Valdiviesso",
            "E. Worcester",
            "M. Worcester",
            "G. Yarbrough",
            "J. Yu",
            "B. Zamorano",
            "J. Zennamo",
            "A. Zglam"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  In liquid argon time projection chambers exposed to neutrino beams and\nrunning on or near surface levels, cosmic muons and other cosmic particles are\nincident on the detectors while a single neutrino-induced event is being\nrecorded. In practice, this means that data from surface liquid argon time\nprojection chambers will be dominated by cosmic particles, both as a source of\nevent triggers and as the majority of the particle count in true\nneutrino-triggered events. In this work, we demonstrate a novel application of\ndeep learning techniques to remove these background particles by applying\nsemantic segmentation on full detector images from the SBND detector, the near\ndetector in the Fermilab Short-Baseline Neutrino Program. We use this technique\nto identify, at single image-pixel level, whether recorded activity originated\nfrom cosmic particles or neutrino interactions.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.01301v3"
    },
    {
        "title": "The electrostatic graph algorithm: a physics-defined method for\n  converting a time-series into a weighted complex network",
        "authors": [
            "Dimitrios Tsiotas",
            "Lykourgos Magafas",
            "Panos Argyrakis"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  This paper proposes a new method for converting a time-series into a weighted\ngraph (complex network), which builds on the electrostatic conceptualization\noriginating from physics. The proposed method conceptualizes a time-series as a\nseries of stationary, electrically charged particles, on which Coulomb-like\nforces can be computed. This allows generating electrostatic-like graphs\nassociated to time-series that, additionally to the existing transformations,\ncan be also weighted and sometimes disconnected. Within this context, the paper\nexamines the structural relevance between five different types of time-series\nand their associated graphs generated by the proposed algorithm and the\nvisibility graph, which is currently the most established algorithm in the\nliterature. The analysis compares the source time-series with the network-based\nnode-series generated by network measures that are arranged into the\nnode-ordering of the source time-series, in terms of linearity, chaotic\nbehaviour, stationarity, periodicity, and cyclical structure. It is shown that\nthe proposed electrostatic graph algorithm produces graphs that are more\nrelevant to the structure of the source time-series by introducing a\ntransformation that converts the time-series to graphs. This is more natural\nrather than algebraic, in comparison with existing physics-defined methods. The\noverall approach also suggests a methodological framework for evaluating the\nstructural relevance between the source time-series and their associated graphs\nproduced by any possible transformation.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.01487v1"
    },
    {
        "title": "Experimental noise in small-angle scattering can be assessed using the\n  Bayesian indirect Fourier transformation",
        "authors": [
            "Andreas Haahr Larsen",
            "Martin Cramer Pedersen"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Small-angle X-ray and neutron scattering are widely used to investigate soft\nmatter and biophysical systems. The experimental errors are essential when\nassessing how well a hypothesized model fits the data. Likewise, they are\nimportant when weights are assigned to multiple data sets used to refine the\nsame model. Therefore, it is problematic when experimental errors are over- or\nunder-estimated. A method is presented, using Bayesian indirect Fourier\ntransformation for small-angle scattering data, to assess whether or not a\ngiven small-angle scattering data set has over- or under-estimated experimental\nerrors. The method is effective on both simulated and experimental data, and\ncan be used to assess and rescale the errors accordingly. Even if the estimated\nexperimental errors are appropriate, it is ambiguous whether or not a model\nfits sufficiently well, as the \"true\" reduced $\\chi^2$ of the data is not\nnecessarily unity. This is particularly relevant for approaches where\noverfitting is an inherent challenge, such as reweighting of a simulated\nmolecular dynamics trajectory against small-angle scattering data or ab initio\nmodelling. Using the outlined method, it is shown that one can determine what\nreduced $\\chi^2$ to aim for when fitting a model against small-angle scattering\ndata. The method is easily accessible via the web interface BayesApp.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.04247v2"
    },
    {
        "title": "Bayesian model selection: Application to adjustment of fundamental\n  physical constants",
        "authors": [
            "Olha Bodnar",
            "Viktor Eriksson"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The location-scale model is usually present in physics and chemistry in\nconnection to the Birge ratio method for the adjustment of fundamental physical\nconstants such as the Planck constant or the Newtonian constant of gravitation,\nwhile the random effects model is the commonly used approach for meta-analysis\nin medicine. These two competitive models are used to increase the quoted\nuncertainties of the measurement results to make them consistent. The intrinsic\nBayes factor (IBF) is derived for the comparison of the random effects model to\nthe location-scale model, and we answer the question which model performs\nbetter for the determination of the Newtonian constant of gravitation. The\nresults of the empirical illustration support the application of the Birge\nratio method which is currently used in the adjustment of the CODATA 2018 value\nfor the Newtonian constant of gravitation together with its uncertainty. The\nresults of the simulation study illustrate that the suggested procedure for\nmodel selection is decisive even when data consist of a few measurement\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.01977v1"
    },
    {
        "title": "Network diffusion capacity unveiled by dynamical paths",
        "authors": [
            "T. A. Schieber",
            "L. C. Carpi",
            "P. M. Pardalos",
            "C. Masoller",
            "A. Díaz-Guilera",
            "M. G. Ravetti"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Improving the understanding of diffusive processes in networks with complex\ntopologies is one of the main challenges of today's complexity science. Each\nnetwork possesses an intrinsic diffusive potential that depends on its\nstructural connectivity. However, the diffusion of a process depends not only\non this topological potential but also on the dynamical process itself.\nQuantifying this potential will allow the design of more efficient systems in\nwhich it is necessary either to weaken or to enhance diffusion. Here we\nintroduce a measure, the {\\em diffusion capacity}, that quantifies, through the\nconcept of dynamical paths, the potential of an element of the system, and\nalso, of the system itself, to propagate information. Among other examples, we\nstudy a heat diffusion model and SIR model to demonstrate the value of the\nproposed measure. We found, in the last case, that diffusion capacity can be\nused as a predictor of the evolution of the spreading process. In general, we\nshow that the diffusion capacity provides an efficient tool to evaluate the\nperformance of systems, and also, to identify and quantify structural\nmodifications that could improve diffusion mechanisms.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.10736v1"
    },
    {
        "title": "Avoiding biases in binned fits",
        "authors": [
            "V. V. Gligorov",
            "S. Hageboeck",
            "T. Nanut",
            "A. Sciandra",
            "D. Y. Tou"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Binned maximum likelihood fits are an attractive option when analysing large\ndatasets, but require care when computing likelihoods of continuous PDFs in\nbins. For many years the widely used statistical modelling package RooFit\nevaluated probabilities at the bin centre, leading to significant biases for\nstrongly curved probability density functions. We demonstrate the biases with\nreal-world examples, and introduce a PDF class to RooFit that removes these\nbiases. The physics and computation performance of this new class are\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.13879v2"
    },
    {
        "title": "Universal uncertainty estimation for nuclear detector signals with\n  neural networks and ensemble learning",
        "authors": [
            "Pengcheng Ai",
            "Zhi Deng",
            "Yi Wang",
            "Chendi Shen"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Characterizing uncertainty is a common issue in nuclear measurement and has\nimportant implications for reliable physical discovery. Traditional methods are\neither insufficient to cope with the heterogeneous nature of uncertainty or\ninadequate to perform well with unknown mathematical models. In this paper, we\npropose using multi-layer convolutional neural networks for empirical\nuncertainty estimation and feature extraction of nuclear pulse signals. This\nmethod is based on deep learning, a recent development of machine learning\ntechniques, which learns the desired mapping function from training data and\ngeneralizes to unseen test data. Furthermore, ensemble learning is utilized to\nestimate the uncertainty originating from trainable parameters of the network\nand to improve the robustness of the whole model. To evaluate the performance\nof the proposed method, simulation studies, in comparison with curve fitting,\ninvestigate extensive conditions and show its universal applicability. Finally,\na case study is made using data from a NICA-MPD electromagnetic calorimeter\nmodule exposed to a test beam at DESY, Germany. The uncertainty estimation\nmethod successfully detected out-of-distribution samples and also achieved good\naccuracy in time and energy measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.04975v4"
    },
    {
        "title": "Stochastic approach for assessing the predictability of chaotic time\n  series using reservoir computing",
        "authors": [
            "Igor A Khovanov"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The applicability of machine learning for predicting chaotic dynamics relies\nheavily upon the data used in the training stage. Chaotic time series obtained\nby numerically solving ordinary differential equations embed a complicated\nnoise of the applied numerical scheme. Such a dependence of the solution on the\nnumeric scheme leads to an inadequate representation of the real chaotic\nsystem. A stochastic approach for generating training times series and\ncharacterising their predictability is suggested to address this problem. The\napproach is applied for analysing two chaotic systems with known properties,\nLorenz system and Anishchenko-Astakhov generator. Additionally, the approach is\nextended to critically assess a reservoir computing model used for chaotic time\nseries prediction. Limitations of reservoir computing for surrogate modelling\nof chaotic systems are highlighted.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.05483v1"
    },
    {
        "title": "Corrupted bifractal features in finite uncorrelated power-law\n  distributed data",
        "authors": [
            "Felipe Olivares",
            "Massimiliano Zanin"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Multifractal Detrended Fluctuation Analysis stands out as one of the most\nreliable methods for unveiling multifractal properties, specially when\nreal-world time series are under analysis. However, little is known about how\nseveral aspects, like artefacts during the data acquisition process, affect its\nresults. In this work we have numerically investigated the performance of\nMultifractal Detrended Fluctuation Analysis applied to synthetic finite\nuncorrelated data following a power-law distribution in the presence of\nadditive noise, and periodic and randomly-placed outliers. We have found that,\non one hand, spurious multifractality is observed as a result of data\nfiniteness, while additive noise leads to an underestimation of the exponents\n$h_q$ for $q<0$ even for low noise levels. On the other hand, additive periodic\nand randomly-located outliers result in a corrupted inverse multifractality\naround $q=0$. Moreover, the presence of randomly-placed outliers corrupts the\nentire multifractal spectrum, in a way proportional to their density. As an\napplication, the multifractal properties of the time intervals between\nsuccessive aircraft landings at three major European airports are investigated.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.07394v1"
    },
    {
        "title": "Vibrational quality classification of metallic turbine blades under\n  measurement uncertainty",
        "authors": [
            "Liangliang Cheng",
            "Vahid Yaghoubi",
            "Wim V. Paepegem",
            "Mathias Kersemans"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Non-destructive testing on metallic turbine blades is a challenging task due\nto their complex geometry. Vibrational test-ing such as Process Compensated\nResonance Testing (PCRT) has shown an efficient approach, which first measures\nthe vibrational response of the turbine blades, and then employs a classifier\nto determine if the quality of the turbine blades are good or bad. Our previous\nwork mainly concentrated on the development of Mahalanobis distance-based\nclassifiers which are fed by the measured vibrational features (such as\nresonant frequencies). In practice, however, measurement errors could lead to a\nbiased trained classifier, potentially resulting in the wrong quality\nclassifications of the turbine blade. In this study, we investigate the\nclassification problem of turbine blades under measurement uncertainty. For\nthis, the concept of Interval Mahalanobis Space is employed, leading to the\nIntegrated Interval Mahalanobis Classification system (IIMCS) classifier which\nhas high robustness against measurement uncertainty. The IIMCS employs Binary\nParticle Swarm Optimization (BPSO) to filter out those resonant frequencies\nwhich contrib-ute most to the information in the system. A Monte Carlo\nSimulation scheme is employed to analyze the sensitivity of the resonant\nfrequencies to the measurement uncertainty. This yields an indicator of\nreliability, indicating the confidence level of the final classification\nresults. The developed IIMCS methodology is applied to an experimental case\nstudy of equiaxed nickel alloy first-stage turbine blades, showing good and\nrobust classification performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.09099v1"
    },
    {
        "title": "Wind Farm Icing Loss Forecast Pertinent to Winter Extremes",
        "authors": [
            "Linyue Gao",
            "Teja Dasari",
            "Jiarong Hong"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The 2021 Texas power crisis has highlighted the vulnerability of the power\nsystem under wind extremes, particularly with the increasing penetration of\nenergy resources that depend on weather conditions (e.g., wind energy). The\ncurrent wind power forecast models do not effectively consider the impact of\nsuch extreme weather events. In the present study, we provide a fast and robust\nstatistical model developed using ten years of utility-scale turbine data at\nthe Eolos Wind Energy Research Station to forecast the icing losses under such\nweather conditions. This model covers different cold climate impacts, including\nprecipitation icing, frost contamination, and low-temperature effect. This\nmodel has been assessed using three large-scale (larger than 100 MW) wind farm\ndata involving turbines with different capacities and from different\nmanufacturers across multiple geographic regions. Notably, the model has been\nused to predict the wind power losses in entire Texas (larger than 91% of total\nwind installation in Texas) during 2021 Texas power crisis. The proposed model\ncan be easily integrated into the existing wind farm and power grid operations,\nallowing the power system operators can develop more appropriate and pinpointed\nplans to balance the severe and sudden energy deficits and increase the system\nintegrity under winter extremes.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.11953v1"
    },
    {
        "title": "Bias and synergy in the self-consistent approach of data analysis of ion\n  beam techniques",
        "authors": [
            "Tiago F. Silva",
            "Cleber L. Rodrigues",
            "Manfredo H. Tabacniks",
            "Udo von Toussaint",
            "Matej Mayer"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Using multiple ion beam analysis measurements, or techniques, combined with\nself-consistent data processing, generally allows extracting more (or more\naccurate) information from the measurements than processing separately data\nfrom single measurements. Solving ambiguities, improving the final depth\nresolution, defining constraints and extending applicability are the main\nstrengths of the data-fusion approach. It basically consists in formulating a\nmulti-objective minimization problem that can be tackled by the adoption of the\nweighted-sum method. A simulation study is reported in order to evaluate the\nsystematic error inserted in the analysis by the choice of a specific objective\nfunction, or even by the weights or normalization adopted in the weighted-sum\nmethod. We demonstrate that the bias of the analyzed objective functions\nasymptotically converges to the true value for better statistics. We also\ndemonstrate that the joint analysis inherits the accuracy of the most accurate\nmeasurement, establishing a rank of information content, where some\ncombinations of measurements are more valuable than others, i.e. when processed\ntogether they provide more information by means of a better constraint for the\nmulti-objective optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.13298v4"
    },
    {
        "title": "End-to-end analysis using image classification",
        "authors": [
            "Adam Aurisano",
            "Leigh H. Whitehead"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  End-to-end analyses of data from high-energy physics experiments using\nmachine and deep learning techniques have emerged in recent years. These\nanalyses use deep learning algorithms to go directly from low-level detector\ninformation directly to high-level quantities that classify the interactions.\nThe most popular class of algorithms for these analyses are convolutional\nneural networks that operate on experimental data formatted as images.\nEnd-to-end analyses skip stages of the traditional workflow that includes the\nreconstruction of particles produced in the interactions, and as such are not\nlimited by efficiency losses and sources of inaccuracy throughout the event\nreconstruction process. In many cases, deep learning end-to-end analyses have\nbeen shown to have significantly increased performance compared to previous\nstate-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03285v1"
    },
    {
        "title": "A Data-Driven Statistical-Stochastic Surrogate Modeling Strategy for\n  Complex Nonlinear Non-stationary Dynamics",
        "authors": [
            "Di Qi",
            "John Harlim"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We propose a statistical-stochastic surrogate modeling approach to predict\nthe response of the mean and variance statistics under various initial\nconditions and external forcing perturbations. The proposed modeling framework\nextends the purely statistical modeling approach that is practically limited to\nthe homogeneous statistical regime for high-dimensional state variables. The\nnew closure system allows one to overcome several practical issues that emerge\nin the non-homogeneous statistical regimes. First, the proposed ensemble\nmodeling that couples the mean statistics and stochastic fluctuations naturally\nproduces positive-definite covariance matrix estimation, which is a challenging\nissue that hampers the purely statistical modeling approaches. Second, the\nproposed closure model, which embeds a non-Markovian neural-network model for\nthe unresolved fluxes such that the variance of the dynamics is consistent,\novercomes the inherent instability of the stochastic fluctuation dynamics.\nEffectively, the proposed framework extends the classical stochastic parametric\nmodeling paradigm for the unresolved dynamics to a semi-parametric\nparameterization with a residual Long-Short-Term-Memory neural network\narchitecture. Third, based on empirical information metric, we provide an\nefficient and effective training procedure by fitting a loss function that\nmeasures the differences between response statistics. Supporting numerical\nexamples are provided with the Lorenz-96 model, a system of ODEs that admits\nthe characteristic of chaotic dynamics with both homogeneous and inhomogeneous\nstatistical regimes. In the latter case, we will see the effectiveness of the\nstatistical prediction even though the resolved Fourier modes corresponding to\nthe leading mean energy and variance spectra do not coincide.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.10612v2"
    },
    {
        "title": "Demystifying the measurement of periodic structures",
        "authors": [
            "David Nečas",
            "Andrew Yacoot",
            "Petr Klapetek"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Periodic structures are often found in various areas of nanoscience and\nnanotechnology with many of them being used for metrological purposes either to\ncalibrate instruments, or forming the basis of measuring devices such as\nencoders. Evaluating the period of one or two-dimensional periodic structures\nfrom topography measurements, e.g. performed using scanning probe microscopy\n(SPM) methods, can be achieved using different methodologies with many grating\nevaluation methods having been proposed in the past and applied to a handful of\nexamples. The optimum methodology for determining the grating pitch is not\nimmediately obvious. This paper reports the results of extensive large-scale\nsimulations and analysis to evaluate the performance of both direct and Fourier\nspace data processing methods. Many thousands of simulations have been\nperformed on a variety of different gratings under different measurement\nconditions and including the simulation of defects encountered in real life\nsituations. The paper concludes with a summary of the merits and disadvantages\nof the methods together with practical recommendations for the measurements of\nperiodic structures and for developing algorithms for processing them.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.12151v1"
    },
    {
        "title": "Pseudo-nonstationarity in the scaling exponents of finite interval time\n  series",
        "authors": [
            "K. H. Kiyani",
            "S. C. Chapman",
            "N. W. Watkins"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The accurate estimation of scaling exponents is central in the observational\nstudy of scale-invariant phenomena. Natural systems unavoidably provide\nobservations over restricted intervals; consequently a stationary stochastic\nprocess (time series) can yield anomalous time variation in the scaling\nexponents, suggestive of non-stationarity. The variance in the estimates of\nscaling exponents computed from an interval of N observations is known for\nfinite variance processes to vary as ~1/N as N goes to infinity for certain\nstatistical estimators; however, the convergence to this behaviour will depend\non the details of the process, and may be slow. We study the variation in the\nscaling of second order moments of the time series increments with N, for a\nvariety of synthetic and `real world' time series; and find that in particular\nfor heavy tailed processes, for realizable N, one is far from this 1/N limiting\nbehaviour. We propose a semi-empirical estimate for the minimum N needed to\nmake a meaningful estimate of the scaling exponents for model stochastic\nprocesses and compare these with some `real world' time series.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.2036v2"
    },
    {
        "title": "Identifying phase synchronization clusters in spatially extended\n  dynamical systems",
        "authors": [
            "Stephan Bialonski",
            "Klaus Lehnertz"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We investigate two recently proposed multivariate time series analysis\ntechniques that aim at detecting phase synchronization clusters in spatially\nextended, nonstationary systems with regard to field applications. The starting\npoint of both techniques is a matrix whose entries are the mean phase coherence\nvalues measured between pairs of time series. The first method is a mean field\napproach which allows to define the strength of participation of a subsystem in\na single synchronization cluster. The second method is based on an eigenvalue\ndecomposition from which a participation index is derived that characterizes\nthe degree of involvement of a subsystem within multiple synchronization\nclusters. Simulating multiple clusters within a lattice of coupled Lorenz\noscillators we explore the limitations and pitfalls of both methods and\ndemonstrate (a) that the mean field approach is relatively robust even in\nconfigurations where the single cluster assumption is not entirely fulfilled,\nand (b) that the eigenvalue decomposition approach correctly identifies the\nsimulated clusters even for low coupling strengths. Using the eigenvalue\ndecomposition approach we studied spatiotemporal synchronization clusters in\nlong-lasting multichannel EEG recordings from epilepsy patients and obtained\nresults that fully confirm findings from well established neurophysiological\nexamination techniques. Multivariate time series analysis methods such as\nsynchronization cluster analysis that account for nonlinearities in the data\nare expected to provide complementary information which allows to gain deeper\ninsights into the collective dynamics of spatially extended complex systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.2597v1"
    },
    {
        "title": "A New Estimator of Intrinsic Dimension Based on the Multipoint Morisita\n  Index",
        "authors": [
            "Jean Golay",
            "Mikhail Kanevski"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The size of datasets has been increasing rapidly both in terms of number of\nvariables and number of events. As a result, the empty space phenomenon and the\ncurse of dimensionality complicate the extraction of useful information. But,\nin general, data lie on non-linear manifolds of much lower dimension than that\nof the spaces in which they are embedded. In many pattern recognition tasks,\nlearning these manifolds is a key issue and it requires the knowledge of their\ntrue intrinsic dimension. This paper introduces a new estimator of intrinsic\ndimension based on the multipoint Morisita index. It is applied to both\nsynthetic and real datasets of varying complexities and comparisons with other\nexisting estimators are carried out. The proposed estimator turns out to be\nfairly robust to sample size and noise, unaffected by edge effects, able to\nhandle large datasets and computationally efficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.0369v7"
    },
    {
        "title": "Estimation of Particle Size Distribution and Aspect Ratio of\n  Non-Spherical Particles From Chord Length Distribution",
        "authors": [
            "Okpeafoh S. Agimelen",
            "Peter Hamilton",
            "Ian Haley",
            "Alison Nordon",
            "Massimiliano Vasile",
            "Jan Sefcik",
            "Anthony J. Mulholland"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Information about size and shape of particles produced in various\nmanufacturing processes is very important for process and product development\nbecause design of downstream processes as well as final product properties\nstrongly depend on these geometrical particle attributes. However, recovery of\nparticle size and shape information in situ during crystallisation processes\nhas been a major challenge. The focused beam reflectance measurement (FBRM)\nprovides the chord length distribution (CLD) of a population of particles in a\nsuspension flowing close to the sensor window. Recovery of size and shape\ninformation from the CLD requires a model relating particle size and shape to\nits CLD as well as solving the corresponding inverse problem.\n  This paper presents a comprehensive algorithm which produces estimates of\nparticle size distribution and particle aspect ratio from measured CLD data.\nWhile the algorithm searches for a global best solution to the inverse problem\nwithout requiring further a priori information on the range of particle sizes\npresent in the population or aspect ratio of particles, suitable regularisation\ntechniques based on relevant additional information can be implemented as\nrequired to obtain physically reasonable size distributions. We used the\nalgorithm to analyse CLD data for samples of needle-like crystalline particles\nof various lengths using two previously published CLD models for ellipsoids and\nfor thin cylinders to estimate particle size distribution and shape. We found\nthat the thin cylinder model yielded significantly better agreement with\nexperimental data, while estimated particle size distributions and aspect\nratios were in good agreement with those obtained from imaging.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.4399v2"
    },
    {
        "title": "Benford Analysis: A useful paradigm for spectroscopic analysis",
        "authors": [
            "Gaurav Bhole",
            "Abhishek Shukla",
            "T. S. Mahesh"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Benford's law is a statistical inference to predict the frequency of\nsignificant digits in naturally occurring numerical databases. In such\ndatabases this law predicts a higher occurrence of the digit 1 in the most\nsignificant place and decreasing occurrences to other larger digits. Although\ncounter-intuitive at first sight, Benford's law has seen applications in a wide\nvariety of fields like physics, earth-science, biology, finance etc. In this\nwork, we have explored the use of Benford's law for various spectroscopic\napplications. Although, we use NMR signals as our databases, the methods\ndescribed here may also be extended to other spectroscopic techniques. In\nparticular, with the help of Benford analysis, we demonstrate the detection of\nweak NMR signals and spectral corrections. We also explore a potential\napplication of Benford analysis in the image-processing of MRI data.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.5735v2"
    },
    {
        "title": "Local Filtering Fundamentally Against Wide Spectrum",
        "authors": [
            "Ge Wang",
            "Hengyong Yu",
            "Scott S. Verbridge",
            "Lizhi Sun"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Chen et al. (1) applied three-dimensional (3D) Fourier filtering together\nwith equal-slope tomographic reconstruction for an observation of nearly all\nthe atoms in a multiply twinned platinum nanoparticle. However, their\nmethodology suffers from fundamental methodological flaws, as initially brought\nup by a recent Communications Arising (2) and now analyzed in-depth in this\nreport written on June 20, 2014. The authors of (1) read this report and wrote\na reply containing 5 points. While we have solid reasons to disagree with their\npoints, we will not include our responses here, and will address their first\ntwo points using Nature's online commenting facility.\n  References 1. Chen, C.C., et al., Three-dimensional imaging of dislocations\nin a nanoparticle at atomic resolution. Nature 496(7443):74-79, 2013 2. Rez, P.\nand M.M.J. Treacy, Three-dimensional imaging of dislocations. Nature\n503(E1):74-79, 2013\n",
        "pdf_link": "http://arxiv.org/pdf/1408.6420v1"
    },
    {
        "title": "Detrending Moving Average Algorithm: Frequency Response and Scaling\n  Performances",
        "authors": [
            "Anna Carbone",
            "Ken Kiyono"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The Detrending Moving Average (DMA) algorithm has been widely used in its\nseveral variants for characterizing long-range correlations of random signals\nand sets (one-dimensional sequences or high-dimensional arrays) either over\ntime or space. In this paper, mainly based on analytical arguments, the scaling\nperformances of the centered DMA, including higher-order ones, are investigated\nby means of a continuous time approximation and a frequency response approach.\nOur results are also confirmed by numerical tests. The study is carried out for\nhigher-order DMA operating with moving average polynomials of different degree.\nIn particular, detrending power degree, frequency response, asymptotic scaling,\nupper limit of the detectable scaling exponent and finite scale range behavior\nwill be discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.01260v1"
    },
    {
        "title": "Expansion-maximization-compression algorithm with spherical harmonics\n  for single particle imaging with X-ray lasers",
        "authors": [
            "Julien Flamant",
            "Nicolas Le Bihan",
            "Andrew V. Martin",
            "Jonathan H. Manton"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In 3D single particle imaging with X-ray free-electron lasers, particle\norientation is not recorded during measurement but is instead recovered as a\nnecessary step in the reconstruction of a 3D image from the diffraction data.\nHere we use harmonic analysis on the sphere to cleanly separate the angu- lar\nand radial degrees of freedom of this problem, providing new opportunities to\nefficiently use data and computational resources. We develop the\nExpansion-Maximization-Compression algorithm into a shell-by-shell approach and\nimplement an angular bandwidth limit that can be gradually raised during the\nreconstruction. We study the minimum number of patterns and minimum rotation\nsampling required for a desired angular and radial resolution. These extensions\nprovide new av- enues to improve computational efficiency and speed of\nconvergence, which are critically important considering the very large datasets\nexpected from experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.01301v2"
    },
    {
        "title": "Identifying Excessively Rounded or Truncated Data",
        "authors": [
            "Kevin H. Knuth",
            "J. Patrick Castle",
            "Kevin R. Wheeler"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  All data are digitized, and hence are essentially integers rather than true\nreal numbers. Ordinarily this causes no difficulties since the truncation or\nrounding usually occurs below the noise level. However, in some instances, when\nthe instruments or data delivery and storage systems are designed with less\nthan optimal regard for the data or the subsequent data analysis, the effects\nof digitization may be comparable to important features contained within the\ndata. In these cases, information has been irrevocably lost in the truncation\nprocess. While there exist techniques for dealing with truncated data, we\npropose a straightforward method that will allow us to detect this problem\nbefore the data analysis stage. It is based on an optimal histogram binning\nalgorithm that can identify when the statistical structure of the digitization\nis on the order of the statistical structure of the data set itself.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.04292v1"
    },
    {
        "title": "Comment on \"Benchmarking Compressed Sensing, Super-Resolution, and\n  Filter Diagonalization\"",
        "authors": [
            "Vladimir A. Mandelshtam"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In a recent paper [Int. J. Quant. Chem. (2016) DOI: 10.1002/qua.25144,\narXiv:1502.06579] Markovich, Blau, Sanders, and Aspuru-Guzik presented a\nnumerical evaluation and comparison of three methods, Compressed Sensing (CS),\nSuper-Resolution (SR), and Filter Diagonalization (FDM), on their ability of\n\"recovering information\" from time signals, concluding that CS and RS\noutperform FDM. We argue that this comparison is invalid for the following\nreasons. FDM is a well established method designed for solving the harmonic\ninversion problem or, similarly, for the problem of spectral estimation, and as\nsuch should be applied only to problems of this kind. The authors incorrectly\nassume that the problem of data fitting is equivalent to the spectral\nestimation problem, regardless of what parametric form is used, and,\nconsequently, in all five numerical examples FDM is applied to the wrong\nproblem. Moreover, the authors' implementation of FDM turned out to be\nincorrect, leading to extremely bad results, caused by numerical instabilities.\nAs we demonstrate here, if implemented correctly, FDM could still be used for\nfitting the data, at least for the time signals composed of damped sinusoids,\nresulting in superior performance. In addition, we show that the published\narticle is full of inaccuracies, mistakes and incorrect statements.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00391v1"
    },
    {
        "title": "Study of Void Probability Scaling of Singly Charged Particles Produced\n  in Ultrarelativistic Nuclear Collision in Fractal Scenario",
        "authors": [
            "Susmita Bhaduri",
            "Dipak Ghosh"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In this paper, we study the fractality of void probability distribution\nmeasured in $^{32}$S-Ag/Br interaction at an incident energy of $200$ GeV per\nnucleon. A radically different and rigorous method called \\textit{Visibility\nGraph} analysis is used. This method is shown to reveal a strong scaling\ncharacter of void probability distribution in all pseurorapidity regions. The\nscaling exponent, called the Power of the Scale-freeness in Visibility\nGraph(PSVG), a quantitative parameter related to Hurst exponent, is strongly\nfound to be dependent on the rapidity window size.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00590v2"
    },
    {
        "title": "Shape and spin distributions of large object populations from random\n  projection areas",
        "authors": [
            "Hari Nortunen",
            "Mikko Kaasalainen"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We model the shape and spin characteristics of an object population when\nthere are not enough data to model its single members. The data are random\nprojection areas of the members. We construct a mapping $f(x)\\rightarrow C(y)$,\n$x\\in\\mathbb{R}^2$, $y\\in\\mathbb{R}$, where $f(x)$ is the distribution function\nof the shape elongation and spin vector obliquity, and $C(y)$ is the cumulative\ndistribution function of an observable $y$ describing the variation of the\nobserved projection areas of one member, and show that the mapping is\ninvertible. Using the projected area of an ellipsoid as our model, we obtain\nanalytical basis functions for a function series of $C(y)$ and prove uniqueness\nand stability properties of the inverse problem. Even though the model error is\nconsiderably larger than the measurement noise for realistic cases of arbitrary\nshapes (such as asteroids), the main characteristics of $f(x)$ (such as the\nlocations of peaks) are robustly recovered from the data.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00692v1"
    },
    {
        "title": "On the graphical extraction of multipole mixing ratios of nuclear\n  transitions",
        "authors": [
            "K. Rezynkina",
            "A. Lopez-Martens",
            "K. Hauschild"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We propose a novel graphical method for determining the mixing ratios\n{\\delta} and their associated uncertainties for mixed nuclear transitions. It\nincorporates the uncertainties both on both the measured and the theoretical\nconversion coefficients. The accuracy of the method has been studied by\nderiving the corresponding probability density function. The domains of\napplicability of the method are carefully defined.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00694v2"
    },
    {
        "title": "Multifractal methodology",
        "authors": [
            "Hadrien Salat",
            "Roberto Murcio",
            "Elsa Arcaute"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Various methods have been developed independently to study the\nmultifractality of measures in many different contexts. Although they all\nconvey the same intuitive idea of giving a \"dimension\" to sets where a quantity\nscales similarly within a space, they are not necessarily equivalent on a more\nrigorous level. This review article aims at unifying the multifractal\nmethodology by presenting the multifractal theoretical framework and principal\npractical methods, namely the moment method, the histogram method, multifractal\ndetrended fluctuation analysis (MDFA) and modulus maxima wavelet transform\n(MMWT), with a comparative and interpretative eye.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.02957v1"
    },
    {
        "title": "A simple predictor based on delay-induced negative group delay",
        "authors": [
            "Henning U. Voss"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  A very simple linear signal predictor that uses past predicted values rather\nthan past signal values for prediction is presented. Man-made or natural\nsystems utilizing this predictor would not require a memory of input signal\nvalues but only of already predicted, internalized states. This delay-induced\nnegative group delay (DINGD) predictor affords real-time prediction of signals\nwithout the need for a specific signal model. Its properties are derived\nanalytically and are numerically tested on various types of broadband input\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.07791v3"
    },
    {
        "title": "Change detection in complex dynamical systems using intrinsic phase and\n  amplitude synchronization",
        "authors": [
            "Ashif Sikandar Iquebal",
            "Satish Bukkapatnam",
            "Arun Srinivasa"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We present an approach for the detection of sharp change points (short-lived\nand persistent) in nonlinear and nonstationary dynamic systems under high\nlevels of noise by tracking the local phase and amplitude synchronization among\nthe components of a univariate time series signal. The signal components are\nderived via Intrinsic Time scale Decomposition (ITD)--a nonlinear,\nnon-parametric analysis method. We show that the signatures of sharp change\npoints are retained across multiple ITD components with a significantly higher\nprobability as compared to random signal fluctuations. Theoretical results are\npresented to show that combining the change point information retained across a\nspecific set of ITD components offers the possibility of detecting sharp\ntransitions with high specificity and sensitivity. Subsequently, we introduce a\nconcept of mutual agreement to identify the set of ITD components that are most\nlikely to capture the information about dynamical changes of interest and\ndefine an InSync statistic to capture this local information. Extensive\nnumerical, as well as real-world case studies involving benchmark\nneurophysiological processes and industrial machine sensor data, suggest that\nthe present method can detect sharp change points, on an average 62% earlier\n(in terms of average run length) as compared to other contemporary methods\ntested.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.00610v2"
    },
    {
        "title": "Sparse Identification for Nonlinear Optical Communication Systems: SINO\n  Method",
        "authors": [
            "Mariia Sorokina",
            "Stylianos Sygletos",
            "Sergei Turitsyn"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We introduce low complexity machine learning based approach for mitigating\nnonlinear impairments in optical fiber communications systems. The immense\nintricacy of the problem calls for the development of \"smart\" methodology,\nsimplifying the analysis without losing the key features that are important for\nrecovery of transmitted data. The proposed sparse identification method for\noptical systems (SINO) allows to determine the minimal (optimal) number of\ndegrees of freedom required for adaptive mitigation of detrimental nonlinear\neffects. We demonstrate successful application of the SINO method both for\nstandard fiber communication links and for few-mode\nspatial-division-multiplexing systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.01650v1"
    },
    {
        "title": "Adaptive Filtering to Enhance Noise Immunity of Impedance and Admittance\n  Spectroscopy: Comparison with Fourier Transformation",
        "authors": [
            "Daniil D. Stupin",
            "Sergei V. Koniakhin",
            "Nikolay A. Verlov",
            "Michael V. Dubina"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The time-domain technique for impedance spectroscopy consists of computing\nthe excitation voltage and current response Fourier images by fast or discrete\nFourier transformation and calculating their relation. Here we propose an\nalternative method for excitation voltage and current response processing for\nderiving a system impedance spectrum based on a fast and flexible adaptive\nfiltering method. We show the equivalence between the problem of adaptive\nfilter learning and deriving the system impedance spectrum. To be specific, we\nexpress the impedance via the adaptive filter weight coefficients. The\nnoise-canceling property of adaptive filtering is also justified. Using the RLC\ncircuit as a model system, we experimentally show that adaptive filtering\nyields correct admittance spectra and elements ratings in the high-noise\nconditions when the Fourier-transform technique fails. Providing the additional\nsensitivity of impedance spectroscopy, adaptive filtering can be applied to\notherwise impossible-to-interpret time-domain impedance data. The advantages of\nadaptive filtering are justified with practical living-cell impedance\nmeasurements.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06503v2"
    },
    {
        "title": "How Complex Is a Fractal? Head/tail Breaks and Fractional Hierarchy",
        "authors": [
            "Bin Jiang",
            "Ding Ma"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  A fractal bears a complex structure that is reflected in a scaling hierarchy,\nindicating that there are far more small things than large ones. This scaling\nhierarchy can be effectively derived using head/tail breaks - a clustering and\nvisualization tool for data with a heavy-tailed distribution - and quantified\nby an ht-index, indicating the number of clusters or hierarchical levels, a\nhead/tail breaks-induced integer. However, this integral ht-index has been\nfound to be less precise for many fractals at their different phrases of\ndevelopment. This paper refines the ht-index as a fraction to measure the\nscaling hierarchy of a fractal more precisely within a coherent whole, and\nfurther assigns a fractional ht-index - the fht-index - to an individual data\nvalue of a data series that represents the fractal. We developed two case\nstudies to demonstrate the advantages of the fht-index, in comparison with the\nht-index. We found that the fractional ht-index or fractional hierarchy in\ngeneral can help characterize a fractal set or pattern in a much more precise\nmanner. The index may help create intermediate map scales between two\nconsecutive map scales.\n  Keywords: Ht-index, fractal, scaling, complexity, fht-index\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00814v2"
    },
    {
        "title": "From physical linear systems to discrete-time series. A guide for\n  analysis of the sampled experimental data",
        "authors": [
            "Jakub Ślęzak",
            "Aleksander Weron"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Modelling physical data with linear discrete time series, namely Fractionally\nIntegrated Autoregressive Moving Average (ARFIMA), is a technique which\nachieved attention in recent years. However, these models are used mainly as a\nstatistical tool only, with weak emphasis on physical background of the model.\nThe main reason for this lack of attention is that ARFIMA model describes\ndiscrete-time measurements, whereas physical models are formulated using\ncontinuous-time parameter. In order to remove this discrepancy we show that\ntime series of this type can be regarded as sampled trajectories of the\ncoordinates governed by system of linear stochastic differential equations with\nconstant coefficients. The observed correspondence provides formulas linking\nARFIMA parameters and the coefficients of the underlying physical stochastic\nsystem, thus providing a bridge between continuous-time linear dynamical\nsystems and ARFIMA models.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.06018v1"
    },
    {
        "title": "qGaussian: Tools to Explore Applications of Tsallis Statistics",
        "authors": [
            "Wagner S. de Lima",
            "Emerson L. de Santa Helena"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  q-Gaussian distribution appear in many science areas where we can find\nsystems that could be described within a nonextensive framework. Usually, a way\nto assert that these systems belongs to nonextensive framework is by means of\nnumerical data analysis. To this end, we implement random number generator for\nq-Gaussian distribution, while we present how to computing its probability\ndensity function, cumulative density function and quantile function besides a\ntail weight measurement using robust statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.06172v1"
    },
    {
        "title": "The Onsager--Machlup functional for data assimilation",
        "authors": [
            "Nozomi Sugiura"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  When taking the model error into account in data assimilation, one needs to\nevaluate the prior distribution represented by the Onsager--Machlup functional.\nThrough numerical experiments, this study clarifies how the prior distribution\nshould be incorporated into cost functions for discrete-time estimation\nproblems. Consistent with previous theoretical studies, the divergence of the\ndrift term is essential in weak-constraint 4D-Var (w4D-Var), but it is not nec\nessary in Markov chain Monte Carlo with the Euler scheme. Although the former\nproperty may cause difficulties when implementing w4D-Var in large systems,\nthis paper proposes a new technique for estimating the divergence term and its\nderivative.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.06663v5"
    },
    {
        "title": "Recovery of the starting times of delayed signals",
        "authors": [
            "Luca Perotti",
            "Daniel Vrinceanu",
            "Daniel Bessis"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We present a new method to locate the starting points in time of an arbitrary\nnumber of (damped) delayed signals. For a finite data sequence, the method\npermits to first locate the starting point of the component with the longest\ndelay, and then --by iteration-- all the preceding ones. Numerical examples are\ngiven and noise sensitivity is tested for weak noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.07001v1"
    },
    {
        "title": "Monitoring crystal breakage in wet milling processes using inline\n  imaging and chord length distribution measurements",
        "authors": [
            "Okpeafoh S. Agimelen",
            "Vaclav Svoboda",
            "Bilal Ahmed",
            "Javier Cardona",
            "Jerzy Dziewierz",
            "Cameron J. Brown",
            "Thomas McGlone",
            "Alison Cleary",
            "Christos Tachtatzis",
            "Craig Michie",
            "Alastair J. Florence",
            "Ivan Andonovic",
            "Anthony J. Mulholland",
            "Jan Sefcik"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The success of the various secondary operations involved in the production of\nparticulate products depends on the production of particles with a desired size\nand shape from a previous primary operation such as crystallisation. This is\nbecause these properties of size and shape affect the behaviour of the\nparticles in the secondary processes. The size and the shape of the particles\nare very sensitive to the conditions of the crystallisation processes, and so\ncontrol of these processes is essential. This control requires the development\nof software tools that can effectively and efficiently process the sensor data\ncaptured in situ. However, these tools have various strengths and limitations\ndepending on the process conditions and the nature of the particles.\n  In this work, we employ wet milling of crystalline particles as a case study\nof a process which produces effects typical to crystallisation processes. We\nstudy some of the strengths and limitations of our previously introduced tools\nfor estimating the particle size distribution (PSD) and the aspect ratio from\nchord length distribution (CLD) and imaging data. We find situations where the\nCLD tool works better than the imaging tool and vice versa. However, in general\nboth tools complement each other, and can therefore be employed in a suitable\nmulti-objective optimisation approach to estimate PSD and aspect ratio.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.09186v1"
    },
    {
        "title": "Sigmoid-Based Refined Composite Multiscale Fuzzy Entropy and\n  t-Distributed Stochastic Neighbor Embedding Based Fault Diagnosis of Rolling\n  Bearing",
        "authors": [
            "Zhanwei Jiang",
            "Jinde Zheng",
            "Haiyang Pan",
            "Ziwei Pan"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Multiscale fuzzy entropy (MFE) has been a prevalent tool to quantify the\ncomplexity of time series. However, it is extremely sensitive to the\npredetermined parameters and length of time series and it may yield an\ninaccurate estimation of entropy or cause undefined entropy when the length of\ntime series is too short. In this paper the Sigmoid-based refined composite\nmultiscale fuzzy entropy (SRCMFE) is introduced to improve the robustness of\ncomplexity measurement of MFE for short time series analysis. Also SRCMFE is\nused to quantify the dynamical properties of mechanical vibration signals and\nbased on that a new rolling bearing fault diagnosis approach is proposed by\ncombining SRCMFE with t-distributed stochastic neighbor embedding (t-SNE) for\nfeature dimension and variable predictive models based class discrimination\n(VPMCD) for mode classification. In the proposed method, SRCMFE firstly is\nemployed to extract the complexity characteristic from vibration signals of\nrolling bearing and t-SNE for feature dimension reduction is utilized to obtain\na low dimensional manifold characteristic. Then VPMCD is employed to construct\na multi-fault classifier to fulfill an automatic fault diagnosis. Finally, the\nproposed approach is applied to experimental data of rolling bearing and the\nresults indicate that the proposed method can effectively distinguish different\nfault categories of rolling bearings.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.01383v1"
    },
    {
        "title": "Skewed distributions as limits of a formal evolutionary process",
        "authors": [
            "F. Sattin"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Time series of observables measured from complex systems do often exhibit\nnon-normal statistics, their statistical distributions (PDF's) are not gaussian\nand often skewed, with roughly exponential tails. Departure from gaussianity is\nrelated to the intermittent development of large-scale coherent structures. The\nexistence of these structures is rooted into the nonlinear dynamical equations\nobeyed by each system, therefore it is expected that some prior knowledge or\nguessing of these equations is needed if one wishes to infer the corresponding\nPDF; conversely, the empirical knowledge of the PDF does provide information\nabout the underlying dynamics. In this work we suggest that it is not always\nnecessary. We show how, under some assumptions, a formal evolution equation for\nthe PDF $p(x)$ can be written down, corresponding to the progressive\naccumulation of measurements of the generic observable $x$. The limiting\nsolution to this equation is computed analytically, and shown to interpolate\nbetween some of the most common distributions, Gamma, Beta and Gaussian PDF's.\nThe control parameter is just the ratio between the rms of the fluctuations and\nthe range of allowed values. Thus, no information about the dynamics is\nrequired.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.07097v1"
    },
    {
        "title": "Normal behaviour models for wind turbine vibrations: An alternative\n  approach",
        "authors": [
            "Pedro G. Lind",
            "Luis Vera-Tudela",
            "Matthias Wächter",
            "Martin Kühn",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The identification of abnormal behaviour in mechanical systems is key to\nanticipate and avoid their potential failure. Thus wind turbine health is\ncommonly assessed monitoring series of $10$-minute SCADA and high frequency\ndata from sensors. To monitor wind turbine vibrations, normal behaviour models\nare built to predict tower top accelerations and drive-train vibrations. Signal\ndeviations from model prediction are labelled as anomalies and are further\ninvestigated. More efficient models are expected to help enhancing the\nidentification of abnormal behaviour. In this paper we assess a stochastic\napproach to reconstruct the $1$ Hz tower top acceleration signal, which was\nmeasured in a wind turbine located at the wind farm Alpha Ventus in the German\nNorth Sea. We compare the resulting data reconstruction with that of a model\nbased on a neural network, which has been previously reported as a data-mining\nalgorithm suitable for reconstructing this signal. In order to focus the\ndiscussion on the similarities and differences of both approaches, we limit our\nevaluation to a single input-output system; in doing so, we avoid other\ndifferences, i.e. loading type, pre-processing or model complexity. Our results\npresent evidence that the stochastic approach outperforms the neural network in\nthe high frequency domain ($1$ Hz). Although neural network retrieve accurate\nstep-forward predictions, with low mean square errors, the stochastic approach\npredictions better preserve the statistics and the frequency components of the\noriginal signal, remaining high accuracy levels. The implementation of our\nstochastic approach is available as open source code and can easily be adapted\nfor other situations involving stochastic data reconstruction. Based on our\nfindings we argue that such an approach could be implemented in signal\nreconstruction for monitoring purposes or for abnormal behaviour detection.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.08676v1"
    },
    {
        "title": "Forecasting Extreme Events in the Complex Dynamics of a Semiconductor\n  Laser with Feedback",
        "authors": [
            "Meritxell Colet",
            "Andrés Aragoneses"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Complex systems performing spiking dynamics are widespread in Nature. They\ncover from earthquakes, to neurons, variable stars, social networks, or stock\nmarkets. Understanding and characterizing their dynamics is relevant in order\nto detect transitions, or to predict unwanted extreme events. Here we study the\noutput intensity of a semiconductor laser with feedback, in a regime where it\ndevelops a complex spiking behavior, under an ordinal patterns analysis. We\nunveil that the complex dynamics presents two competing behaviors that can be\ndistinguished with a thresholding method, and we use temporal correlations to\nforecast the extreme events, and transitions between dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.04538v1"
    },
    {
        "title": "Complexity Analysis of Chaos and Other Fluctuating Phenomena",
        "authors": [
            "Jamieson Brechtl",
            "Xie Xie",
            "Karen A. Dahmen",
            "Peter K. Liaw"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The refined composite multiscale-entropy algorithm was applied to the\ntime-dependent behavior of the Weierstrass functions, colored noise, and\nLogistic map to provide fresh insight into the dynamics of these fluctuating\nphenomena. For the Weierstrass function, the complexity of fluctuations was\nfound to increase with respect to the fractional dimension, D, of the graph.\nAdditionally, the sample-entropy curves increased in an exponential fashion\nwith increasing D. This increase in the complexity was found to correspond to a\nrising amount of irregularities in the oscillations. In terms of the colored\nnoise, the complexity of the fluctuations was found to be highest for the 1/f\nnoise (f is the frequency of the generated noise), which is in agreement with\nfindings in the literature. Moreover, the sample-entropy curves exhibited a\ndecreasing trend for noise when the spectral exponent, \\beta, was less than 1\nand obeyed an increasing trend when \\beta > 1. Importantly, a direct\nrelationship was observed between the power-law exponents for the curves and\nthe spectral exponents of the noise. For the logistic map, a correspondence was\nobserved between the complexity maps and its bifurcation diagrams.\nSpecifically, the map of the sample-entropy curves was negligible when the\nbifurcation parameter, R, varied between 3 - 3.5. Beyond these values, the\ncurves attained non-zero values that increased with increasing R, in general.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.07036v1"
    },
    {
        "title": "Point Divergence Gain and Multidimensional Data Sequences Analysis",
        "authors": [
            "Renata Rychtáriková",
            "Jan Korbel",
            "Petr Macháček",
            "Dalibor Štys"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We introduce novel information-entropic variables -- a Point Divergence Gain\n(${\\Omega}^{(l \\rightarrow m)}_\\alpha$), a Point Divergence Gain Entropy\n($I_\\alpha$), and a Point Divergence Gain Entropy Density ($P_\\alpha$) -- which\nare derived from the R\\'{e}nyi entropy and describe spatio-temporal changes\nbetween two consecutive discrete multidimensional distributions. The behavior\nof ${\\Omega}^{(l \\rightarrow m)}_\\alpha$ is simulated for typical distributions\nand, together with $I_\\alpha$ and $P_\\alpha$, applied in analysis and\ncharacterization of series of multidimensional datasets of computer-based and\nreal images.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.00183v2"
    },
    {
        "title": "On the Estimate Measurement Uncertainty of the Insertion Loss in a\n  Reverberation Chamber Including Frequency Stirring",
        "authors": [
            "Angelo Gifuni",
            "Luca Bastianelli",
            "Maurizio Migliaccio",
            "Franco Moglie",
            "Valter Mariani Primiani",
            "Gabriele Gradoni"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  In this paper, it is shown an enhancement of a previous model on the\nmeasurement standard uncertainty (MU) of the insertion loss (IL) in a\nreverberation chamber (RC) including frequency stirring (FS). Differently from\nthe previous model, the enhanced does not require specific conditions on the\nparameter to be measured. Such an enhancement is applicable for all usable\nmeasurement conditions in RCs. Moreover, a useful majorant is also shown; it is\nobtained under a weak condition on the coefficient of variation (CV) of the\nparameter to be measured. Results by measurements support the validity of the\nproposed enhancement and of the majorant.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.04468v2"
    },
    {
        "title": "Characterizing Subcritical Assemblies with Time of Flight Fixed by\n  Energy Estimation Distributions",
        "authors": [
            "Mateusz Monterial",
            "Peter Marleau",
            "Sara Pozzi"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We present the Time of Flight Fixed by Energy Estimation (TOFFEE) as a\nmeasure of the fission chain dynamics in subcritical assemblies. TOFFEE is the\ntime between correlated gamma rays and neutrons, subtracted by the estimated\ntravel time of the incident neutron from its proton recoil. The measured\nsubcritical assembly was the BeRP ball, a 4.482 kg sphere of alpha-phase\nweapons grade plutonium metal, which came in five configurations: bare, 0.5, 1,\nand 1.5 in iron, and 1 in nickel closed fitting shell reflectors. We extend the\nmeasurement with MCNPX-PoliMi simulations of shells ranging up to 6 inches in\nthickness, and two new reflector materials: aluminum and tungsten. We also\nsimulated the BeRP ball with different masses ranging from 1 to 8 kg. A\ntwo-region and single-region point kinetics models were used to model the\nbehavior of the positive side of the TOFFEE distribution from 0 to 100 ns. The\nsingle region model of the bare cases gave positive linear correlations between\nestimated and expected neutron decay constants and leakage multiplications. The\ntwo-region model provided a way to estimate neutron multiplication for the\nreflected cases, which correlated positively with expected multiplication, but\nthe nature of the correlation (sub or super linear) changed between material\ntypes. Finally, we found that the areal density of the reflector shells had a\nlinear correlation with the integral of the two-region model fit. Therefore, we\nexpect that with knowledge of reflector composition, one could determine the\nshell thickness, or vice versa. Furthermore, up to a certain amount and\nthickness of the reflector, the two-region model provides a way of\ndistinguishing bare and reflected plutonium assemblies.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.08126v1"
    },
    {
        "title": "State space reconstruction of spatially extended systems and of time\n  delayed systems from the time series of a scalar variable",
        "authors": [
            "Carlos Quintero-Quiroz",
            "M. C. Torrent",
            "Cristina Masoller"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The space-time representation of high-dimensional dynamical systems that have\na well defined characteristic time scale has proven to be very useful to deepen\nthe understanding of such systems and to uncover hidden features in their\noutput signals. Genuine analogies between one-dimensional (1D) spatially\nextended systems (1D SESs) and time delayed systems (TDSs) have been observed,\nincluding similar pattern formation and propagation of localized structures. An\nopen question is if such analogies are limited to the space-time\nrepresentation, or, if it is possible to reconstruct similar attractors, from\nthe time series of an observed variable. In this work we address this issue by\nconsidering a bistable 1D SES and two TDSs (a bistable system and a model of\ntwo lasers with time delayed coupling). In these three examples we find that we\ncan reconstruct the underlying attractor in a three-dimensional pseudo-space,\nwhere the evolution is governed by a polynomial potential. We also discuss the\nlimitations of the analogy between 1D SESs and TDSs.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.08340v2"
    },
    {
        "title": "Differential symbolic entropy in nonlinear dynamics complexity analysis",
        "authors": [
            "Wenpo Yao",
            "Jun Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Differential symbolic entropy, a measure for nonlinear dynamics complexity,\nis proposed in our contribution. With flexible controlling parameter, the\nchaotic deterministic measure takes advantage of local nonlinear dynamical\ninformation among three adjacent elements to extract nonlinear complexity. In\nnonlinear complexity detections of chaotic logistic series, DSEn (differential\nsymbolic entropy) has satisfied complexity extractions with the changes of\nchaotic features of logistic map. In nonlinear analysis of real-world\nphysiological heart signals, three kinds of heart rates are significantly\ndistinguished by DSEn in statistics, healthy young subjects > healthy elderly\npeople > CHF (congestive heart failure) patients, highlighting the\ncomplex-losing theory of aging and heart diseases in cardiac nonlinearity.\nMoreover, DSEn does not have high demand on data length and can extract\nnonlinear complexity at short data sets; therefore, it is an efficient\nparameter to characterize nonlinear dynamic complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.08416v2"
    },
    {
        "title": "Unfolding with Gaussian Processes",
        "authors": [
            "Adam Bozson",
            "Glen Cowan",
            "Francesco Spanò"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  A method to perform unfolding with Gaussian processes (GPs) is presented.\nUsing Bayesian regression, we define an estimator for the underlying truth\ndistribution as the mode of the posterior. We show that in the case where the\nbin contents are distributed approximately according to a Gaussian, this\nestimator is equivalent to the mean function of a GP conditioned on the maximum\nlikelihood estimator. Regularisation is introduced via the kernel function of\nthe GP, which has a natural interpretation as the covariance of the underlying\ndistribution. This novel approach allows for the regularisation to be informed\nby prior knowledge of the underlying distribution, and for it to be varied\nalong the spectrum. In addition, the full statistical covariance matrix for the\nestimator is obtained as part of the result. The method is applied to two\nexamples: a double-peaked bimodal distribution and a falling spectrum.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.01242v2"
    },
    {
        "title": "Extraction of Azimuthal Asymmetries using Optimal Observables",
        "authors": [
            "Jörg Pretz",
            "Fabian Müller"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Azimuthal asymmetries play an important role in scattering processes with\npolarized particles. This paper introduces a new procedure using event\nweighting to extract these asymmetries. It is shown that the resulting\nestimator has several advantages in terms of statistical accuracy, bias,\nassumptions on acceptance and luminosities compared to other estimators\ndiscussed in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.09452v2"
    },
    {
        "title": "Reconstruction of intermittent data time series as a superposition of\n  pulses",
        "authors": [
            "Sajidah Ahmed",
            "Odd Erik Garcia",
            "Audun Theodorsen"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Fluctuations in a vast range of physical systems can be described as a\nsuperposition of uncorrelated pulses with a fixed shape, a process commonly\nreferred to as a (generalized) shot noise or a filtered Poisson process. In\nthis contribution, we present a systematic study of a novel deconvolution\nmethod to estimate the arrival times and amplitudes of the pulses from\nrealizations of such processes. The method shows that time-series can be\nreconstructed for various pulse amplitude and waiting time distributions.\nDespite a constraint on positive-definite amplitudes, it is shown that negative\namplitudes may also be reconstructed by flipping the sign of the time series.\nThe method performs well under moderate amounts of additive noise, both white\nnoise and colored noise having the same correlation function as the process\nitself. The estimation of pulse shapes from the power spectrum is accurate\nexcept for excessively broad waiting time distributions. Although the method\nassumes constant pulse durations, it performs well under narrowly distributed\npulse durations. The most important constraint on the reconstruction is\ninformation-loss, which limits the method to intermittent processes. The ratio\nbetween the sampling time and the average waiting time between pulses must be\nabout 1/20 or smaller for a well sampled signal. Finally, given the system\nforcing, the average pulse function may be recovered. This recovery is only\nweakly constrained by the intermittency of the process.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.11033v5"
    },
    {
        "title": "Establishing a common data base of ice experiments and using machine\n  learning to understand and predict ice behavior",
        "authors": [
            "Leon Kellner",
            "Merten Stender",
            "Hauke Herrnring",
            "Rüdiger U. Franz von Bock und Polach",
            "Sören Ehlers",
            "Norbert Hoffmann",
            "Knut V. Høyland"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Machine learning and statistical tools are applied to identify how\nparameters, such as temperature, influence peak stress and ice behavior. To\nenable the analysis, a common and small scale experimental data base is\nestablished.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.03994v2"
    },
    {
        "title": "Probing high order dependencies with information theory",
        "authors": [
            "C Granero-Belinchón",
            "S. Roux",
            "P. Abry",
            "N. Garnier"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Information theoretic measures (entropies, entropy rates, mutual information)\nare nowadays commonly used in statistical signal processing for real-world data\nanalysis. The present work proposes the use of Auto Mutual Information (Mutual\nInformation between subsets of the same signal) and entropy rate as powerful\ntools to assess refined dependencies of any order in signal temporal dynamics.\nNotably, it is shown how two-point Auto Mutual Information and entropy rate\nunveil information conveyed by higher order statistic and thus capture details\nof temporal dynamics that are overlooked by the (two-point) correlation\nfunction. Statistical performance of relevant estimators for Auto Mutual\nInformation and entropy rate are studied numerically, by means of Monte Carlo\nsimulations, as functions of sample size, dependence structures and hyper\nparameters that enter their definition. Further, it is shown how Auto Mutual\nInformation permits to discriminate between several different non Gaussian\nprocesses, having exactly the same marginal distribution and covariance\nfunction. Assessing higher order statistics via multipoint Auto Mutual\nInformation is also shown to unveil the global dependence structure fo these\nprocesses, indicating that one of the non Gaussian actually has temporal\ndynamics that ressembles that of a Gaussian process with same covariance while\nthe other does not.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.05325v1"
    },
    {
        "title": "A Sheaf Theoretical Approach to Uncertainty Quantification of\n  Heterogeneous Geolocation Information",
        "authors": [
            "Cliff Joslyn",
            "Lauren Charles",
            "Chris DePerno",
            "Nicholas Gould",
            "Kathleen Nowak",
            "Brenda Praggastis",
            "Emilie Purvine",
            "Michael Robinson",
            "Jennifer Strules",
            "Paul Whitney"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Integration of heterogeneous sensors is a challenging problem across a range\nof applications. Prominent among these are multi-target tracking, where one\nmust combine observations from different sensor types in a meaningful way to\ntrack multiple targets. Because sensors have differing error models, we seek a\ntheoretically-justified quantification of the agreement among ensembles of\nsensors, both overall for a sensor collection, and also at a fine-grained level\nspecifying pairwise and multi-way interactions among sensors. We demonstrate\nthat the theory of mathematical sheaves provides a unified answer to this need,\nsupporting both quantitative and qualitative data. The theory provides\nalgorithms to globalize data across the network of deployed sensors, and to\ndiagnose issues when the data do not globalize cleanly. We demonstrate the\nutility of sheaf-based tracking models based on experimental data of a wild\npopulation of black bears in Asheville, North Carolina. A measurement model\ninvolving four sensors deployed among the bears and the team of scientists\ncharged with tracking their location is deployed. This provides a sheaf-based\nintegration model which is small enough to fully interpret, but of sufficient\ncomplexity to demonstrate the sheaf's ability to recover a holistic picture of\nthe locations and behaviors of both individual bears and the bear-human\ntracking system. A statistical approach was developed for comparison, a dynamic\nlinear model which was estimated using a Kalman filter. This approach also\nrecovered bear and human locations and sensor accuracies. When the observations\nare normalized into a common coordinate system, the structure of the dynamic\nlinear observation model recapitulates the structure of the sheaf model,\ndemonstrating the canonicity of the sheaf-based approach. But when the\nobservations are not so normalized, the sheaf model still remains valid.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.05487v1"
    },
    {
        "title": "Unsupervised Classification of Single-Molecule Data with Autoencoders\n  and Transfer Learning",
        "authors": [
            "Anton Vladyka",
            "Tim Albrecht"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Datasets from single-molecule experiments often reflect a large variety of\nmolecular behaviour. The exploration of such datasets can be challenging,\nespecially if knowledge about the data is limited and a priori assumptions\nabout expected data characteristics are to be avoided. Indeed, searching for\npre-defined signal characteristics is sometimes useful, but it can also lead to\ninformation loss and the introduction of expectation bias. Here, we demonstrate\nhow Transfer Learning-enhanced dimensionality reduction can be employed to\nidentify and quantify hidden features in single-molecule charge transport data,\nin an unsupervised manner. Taking advantage of open-access neural networks\ntrained on millions of seemingly unrelated image data, our results also show\nhow Deep Learning methodologies can readily be employed, even if the amount of\nproblem-specific, 'own' data is limited.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.01239v1"
    },
    {
        "title": "Software Challenges For HL-LHC Data Analysis",
        "authors": [
            " ROOT Team",
            "Kim Albertsson Brann",
            "Guilherme Amadio",
            "Sitong An",
            "Bertrand Bellenot",
            "Jakob Blomer",
            "Philippe Canal",
            "Olivier Couet",
            "Massimiliano Galli",
            "Enrico Guiraud",
            "Stephan Hageboeck",
            "Sergey Linev",
            "Pere Mato Vila",
            "Lorenzo Moneta",
            "Axel Naumann",
            "Alja Mrak Tadel",
            "Vincenzo Eduardo Padulano",
            "Fons Rademakers",
            "Oksana Shadura",
            "Matevz Tadel",
            "Enric Tejedor Saavedra",
            "Xavier Valls Pla",
            "Vassil Vassilev",
            "Stefan Wunsch"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The high energy physics community is discussing where investment is needed to\nprepare software for the HL-LHC and its unprecedented challenges. The ROOT\nproject is one of the central software players in high energy physics since\ndecades. From its experience and expectations, the ROOT team has distilled a\ncomprehensive set of areas that should see research and development in the\ncontext of data analysis software, for making best use of HL-LHC's physics\npotential. This work shows what these areas could be, why the ROOT team\nbelieves investing in them is needed, which gains are expected, and where\nrelated work is ongoing. It can serve as an indication for future research\nproposals and cooperations.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.07675v2"
    },
    {
        "title": "Digging Into MUD With Python: mudpy, bdata, and bfit",
        "authors": [
            "Derek Fujimoto"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Used to store the results of $\\mu$SR measurements at TRIUMF, the Muon Data\n(MUD) file format serves as a useful and flexible scheme that is both\nlightweight and self-describing. The application programming interface (API)\nfor these files is written in C and FORTRAN, languages not known for their ease\nof use. In contrast, Python is a language which emphasizes rapid prototyping\nand readability. This work describes three Python 3 packages to interface with\nMUD files and analyze their contents: mudpy, bdata, and bfit. The first enables\neasy access to the contents of any MUD file. The latter two are implemented\nspecifically for the implanted-ion $\\beta$-detected NMR ($\\beta$-NMR)\nexperiment at TRIUMF. These tools provide both an API and graphical user\ninterface (GUI) to help users extract and fit $\\beta$-NMR data.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.10395v1"
    },
    {
        "title": "Low-dimensional offshore wave input for extreme event quantification",
        "authors": [
            "Kenan Šehić",
            "Henrik Bredmose",
            "John D. Sørensen",
            "Mirza Karamehmedović"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  In offshore engineering design, nonlinear wave models are often used to\npropagate stochastic waves from an input boundary to the location of an\noffshore structure. Each wave realization is typically characterized by a\nhigh-dimensional input time series, and a reliable determination of the extreme\nevents is associated with substantial computational effort. As the sea depth\ndecreases, extreme events become more difficult to evaluate. We here construct\na low-dimensional characterization of the candidate input time series to\ncircumvent the search for extreme wave events in a high-dimensional input\nprobability space. Each wave input is represented by a unique low-dimensional\nset of parameters for which standard surrogate approximations, such as Gaussian\nprocesses, can estimate the short-term exceedance probability efficiently and\naccurately. We demonstrate the advantages of the new approach with a simple\nshallow-water wave model based on the Korteweg-de Vries equation for which we\ncan provide an accurate reference solution based on the simple Monte Carlo\nmethod. We furthermore apply the method to a fully nonlinear wave model for\nwave propagation over a sloping seabed. The results demonstrate that the\nGaussian process can learn accurately the tail of the heavy-tailed distribution\nof the maximum wave crest elevation based on only $1.7\\%$ of the required Monte\nCarlo evaluations.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.10861v1"
    },
    {
        "title": "Calculating permutation entropy without permutations",
        "authors": [
            "Alexander Vidybida"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  A method for analyzing sequential data sets, similar to the permutation\nentropy one, is discussed. The characteristic features of this method are as\nfollows: it preserves information about equal values, if any, in the embedding\nvectors; it is exempt of combinatorics; it delivers the same entropy value as\ndoes the permutation method, provided the embedding vectors do not have equal\ncomponents. In the latter case this method can be used instead of the\npermutation one. If embedding vectors have equal components this method could\nbe more precise in discriminating between similar data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11160v2"
    },
    {
        "title": "Model selection in the average of inconsistent data: an analysis of the\n  measured Planck-constant values",
        "authors": [
            "Giovanni Mana",
            "Enrico Massa",
            "Maria Predescu"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  When the data do not conform to the hypothesis of a known sampling-variance,\nthe fitting of a constant to a set of measured values is a long debated\nproblem. Given the data, fitting would require to find what measurand value is\nthe most trustworthy. Bayesian inference is here reviewed, to assign\nprobabilities to the possible measurand values. Different hypothesis about the\ndata variance are tested by Bayesian model comparison. Eventually, model\nselection is exemplified in deriving an estimate of the Planck constant.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.09428v1"
    },
    {
        "title": "Easy computation of the Bayes Factor to fully quantify Occam's razor",
        "authors": [
            "David J. Dunstan",
            "Joel Crowne",
            "Alan J. Drew"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The Bayes factor is the gold-standard figure of merit for comparing fits of\nmodels to data, for hypothesis selection and parameter estimation. However it\nis little used because it is computationally very intensive. Here it is shown\nhow Bayes factors can be calculated accurately and easily, so that any\nleast-squares or maximum-likelihood fits may be routinely followed by the\ncalculation of Bayes factors to guide the best choice of model and hence the\nbest estimations of parameters. Approximations to the Bayes factor, such as the\nBayesian Information Criterion (BIC), are increasingly used. Occam's razor\nexpresses a primary intuition, that parameters should not be multiplied\nunnecessarily, and that is quantified by the BIC. The Bayes factor quantifies\ntwo further intuitions. Models with physically-meaningful parameters are\npreferable to models with physically-meaningless parameters. Models that could\nfail to fit the data, yet which do fit, are preferable to models which span the\ndata space and are therefore guaranteed to fit the data. The outcomes of using\nBayes factors are often very different from traditional statistics tests and\nfrom the BIC. Three examples are given. In two of these examples, the easy\ncalculation of the Bayes factor is exact. The third example illustrates the\nrare conditions under which it has some error and shows how to diagnose and\ncorrect the error.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.09702v1"
    },
    {
        "title": "Optimal STEM Convergence Angle Selection using a Convolutional Neural\n  Network and the Strehl Ratio",
        "authors": [
            "Noah Schnitzer",
            "Suk Hyun Sung",
            "Robert Hovden"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Selection of the correct convergence angle is essential for achieving the\nhighest resolution imaging in scanning transmission electron microscopy (STEM).\nUse of poor heuristics, such as Rayleigh's quarter-phase rule, to assess probe\nquality and uncertainties in measurement of the aberration function result in\nincorrect selection of convergence angles and lower resolution. Here, we show\nthat the Strehl ratio provides an accurate and efficient to calculate criteria\nfor evaluating probe size for STEM. A convolutional neural network trained on\nthe Strehl ratio is shown to outperform experienced microscopists at selecting\na convergence angle from a single electron Ronchigram using simulated datasets.\nGenerating tens of thousands of simulated Ronchigram examples, the network is\ntrained to select convergence angles yielding probes on average 85% nearer to\noptimal size at millisecond speeds (0.02% human assessment time). Qualitative\nassessment on experimental Ronchigrams with intentionally introduced\naberrations suggests that trends in the optimal convergence angle size are well\nmodeled but high accuracy requires extensive training datasets. This near\nimmediate assessment of Ronchigrams using the Strehl ratio and machine learning\nhighlights a viable path toward rapid, automated alignment of\naberration-corrected electron microscopes.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.12281v1"
    },
    {
        "title": "Ordinal spectrum: a frequency domain characterization of complex time\n  series",
        "authors": [
            "Mario Chavez",
            "Johann H. Martinez"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Although classical spectral analysis is a natural approach to characterise\nlinear systems, it cannot describe a chaotic dynamics. Here, we propose the\nordinal spectrum, a method based on a spectral transformation of symbolic\nsequences, to characterise the complexity of a time series. In contrasts with\nother nonlinear mapping functions (e.g. the state-space reconstruction) the\nproposed representation is a natural approach to distinguish, in a frequency\ndomain, a chaotic behavior. We test the method in different synthetic and\nreal-world data. Our results suggest that the proposed approach may provide new\ninsights into the non-linear oscillations observed in different real data.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02547v1"
    },
    {
        "title": "A data-driven convergence criterion for iterative unfolding of smeared\n  spectra",
        "authors": [
            "M. Licciardi",
            "B. Quilain"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  A data-driven convergence criterion for the D'Agostini (Richardson-Lucy)\niterative unfolding is presented. It relies on the unregularized spectrum\n(infinite number of iterations), and allows a safe estimation of the bias and\nundercoverage induced by truncating the algorithm. In addition, situations\nwhere the response matrix is not perfectly known are also discussed, and show\nthat in most cases the unregularized spectrum is not an unbiased estimator of\nthe true distribution. Whenever a bias is introduced, either by truncation of\nby poor knowledge of the response, a way to retrieve appropriate coverage\nproperties is proposed.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.01096v1"
    },
    {
        "title": "Interlaboratory consensus building challenge",
        "authors": [
            "Giovanni Mana"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The manuscript is about an interlaboratory comparison which involved eleven\nmetrology institutes. It comprises four tasks: i) deriving a consensus value\nfrom these results; ii) evaluating the associated standard uncertainty; iii)\nproducing a coverage interval that, with 95\\% confidence, is believed to\ninclude the true value of which the consensus value is an estimate; iv)\nsuggesting how the measurement result from NIST may be compared with the\nconsensus value.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.09517v1"
    },
    {
        "title": "Detection of Dynamical Regime Transitions with Lacunarity as a\n  Multiscale Recurrence Quantification Measure",
        "authors": [
            "Tobias Braun",
            "Vishnu R. Unni",
            "R. I. Sujith",
            "Juergen Kurths",
            "Norbert Marwan"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We propose lacunarity as a novel recurrence quantification measure and\nillustrate its efficacy to detect dynamical regime transitions which are\nexhibited by many complex real-world systems. We carry out a recurrence plot\nbased analysis for different paradigmatic systems and nonlinear empirical data\nin order to demonstrate the ability of our method to detect dynamical\ntransitions ranging across different temporal scales. It succeeds to\ndistinguish states of varying dynamical complexity in the presence of noise and\nnon-stationarity, even when the time series is of short length. In contrast to\ntraditional recurrence quantifiers, no specification of minimal line lengths is\nrequired and rather geometric features beyond linear structures in the\nrecurrence plot can be accounted for. This makes lacunarity more broadly\napplicable as a recurrence quantification measure. Lacunarity is usually\ninterpreted as a measure of heterogeneity or translational invariance of an\narbitrary spatial pattern. In application to recurrence plots, it quantifies\nthe degree of heterogenity in the temporal recurrence patterns at all relevant\ntime scales. We demonstrate the potential of the proposed method when applied\nto empirical data, namely time series of acoustic pressure fluctuations from a\nturbulent combustor. Recurrence lacunarity captures both the rich variability\nin dynamical complexity of acoustic pressure fluctuations and shifting time\nscales encoded in the recurrence plots. Furthermore, it contributes to a better\ndistinction between stable operation and near blowout states of combustors.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.10136v1"
    },
    {
        "title": "AdaPT: Adaptable Particle Tracking for Spherical Microparticles in Lab\n  on Chip Systems",
        "authors": [
            "Kristina Dingel",
            "Rico Huhnstock",
            "André Knie",
            "Arno Ehresmann",
            "Bernhard Sick"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Due to its rising importance in science and technology in recent years,\nparticle tracking in videos presents itself as a tool for successfully\nacquiring new knowledge in the field of life sciences and physics. Accordingly,\ndifferent particle tracking methods for various scenarios have been developed.\nIn this article, we present a particle tracking application implemented in\nPython for, in particular, spherical magnetic particles, including\nsuperparamagnetic beads and Janus particles. In the following, we distinguish\nbetween two sub-steps in particle tracking, namely the localization of\nparticles in single images and the linking of the extracted particle positions\nof the subsequent frames into trajectories. We provide an intensity-based\nlocalization technique to detect particles and two linking algorithms, which\napply either frame-by-frame linking or linear assignment problem solving.\nBeyond that, we offer helpful tools to preprocess images automatically as well\nas estimate parameters required for the localization algorithm by utilizing\nmachine learning. As an extra, we have implemented a technique to estimate the\ncurrent spatial orientation of Janus particles within the x-y-plane. Our\nframework is readily extendable and easy-to-use as we offer a graphical user\ninterface and a command-line tool. Various output options, such as data frames\nand videos, ensure further analysis that can be automated.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.12003v1"
    },
    {
        "title": "Generalized asymptotic formulae for estimating statistical significance\n  in high energy physics analyses",
        "authors": [
            "M. J. Basso"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Within the framework of likelihood-based statistical tests for high energy\nphysics measurements, we derive generalized expressions for estimating the\nstatistical significance of discovery using the asymptotic approximations of\nWilks and Wald for a variety of measurement models. These models include\narbitrary numbers of signal regions, control regions, and Gaussian constraints.\nWe extend our expressions to use the representative or \"Asimov\" dataset\nproposed by Cowan et al. such that they are made data-free. While many of the\ngeneralized expressions are complicated and often involve solving systems of\ncoupled, multivariate equations, we show these expressions reduce to\nclosed-form results under simplifying assumptions. We also validate the\npredicted significance using toy-based data in select cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.04275v2"
    },
    {
        "title": "Robust test statistics for data sets with missing correlation\n  information",
        "authors": [
            "Lukas Koch"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Not all experiments publish their results with a description of the\ncorrelations between the data points. This makes it difficult to do hypothesis\ntests or model fits with that data, since just assuming no correlation can lead\nto an over- or underestimation of the resulting uncertainties. This work\npresents robust test statistics that can be used with data sets with missing\ncorrelation information. They are exact in the case of no correlation and\neither guaranteed to be conservative -- i.e. the uncertainty is never\nunderestimated -- in the presence of correlations, or they are also exact in\nthe degenerate case of perfect correlation between the data points.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.06172v2"
    },
    {
        "title": "ordpy: A Python package for data analysis with permutation entropy and\n  ordinal network methods",
        "authors": [
            "Arthur A. B. Pessa",
            "Haroldo V. Ribeiro"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Since Bandt and Pompe's seminal work, permutation entropy has been used in\nseveral applications and is now an essential tool for time series analysis.\nBeyond becoming a popular and successful technique, permutation entropy\ninspired a framework for mapping time series into symbolic sequences that\ntriggered the development of many other tools, including an approach for\ncreating networks from time series known as ordinal networks. Despite the\nincreasing popularity, the computational development of these methods is\nfragmented, and there were still no efforts focusing on creating a unified\nsoftware package. Here we present ordpy, a simple and open-source Python module\nthat implements permutation entropy and several of the principal methods\nrelated to Bandt and Pompe's framework to analyze time series and\ntwo-dimensional data. In particular, ordpy implements permutation entropy,\nTsallis and R\\'enyi permutation entropies, complexity-entropy plane,\ncomplexity-entropy curves, missing ordinal patterns, ordinal networks, and\nmissing ordinal transitions for one-dimensional (time series) and\ntwo-dimensional (images) data as well as their multiscale generalizations. We\nreview some theoretical aspects of these tools and illustrate the use of ordpy\nby replicating several literature results.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.06786v4"
    },
    {
        "title": "Deep learning polarization distributions in ferroelectrics from STEM\n  data: with and without atom finding",
        "authors": [
            "Ayana Ghosh",
            "Christopher T. Nelson",
            "Mark Oxley",
            "Xiaohang Zhang",
            "Maxim Ziatdinov",
            "Ichiro Takeuchi",
            "Sergei V. Kalinin"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Over the last decade, scanning transmission electron microscopy (STEM) has\nemerged as a powerful tool for probing atomic structures of complex materials\nwith picometer precision, opening the pathway toward exploring ferroelectric,\nferroelastic, and chemical phenomena on the atomic-scale. Analyses to date\nextracting a polarization signal from lattice coupled distortions in STEM\nimaging rely on discovery of atomic positions from intensity maxima/minima and\nsubsequent calculation of polarization and other order parameter fields from\nthe atomic displacements. Here, we explore the feasibility of polarization\nmapping directly from the analysis of STEM images using deep convolutional\nneural networks (DCNNs). In this approach, the DCNN is trained on the labeled\npart of the image (i.e., for human labelling), and the trained network is\nsubsequently applied to other images. We explore the effects of the choice of\nthe descriptors (centered on atomic columns and grid-based), the effects of\nobservational bias, and whether the network trained on one composition can be\napplied to a different one. This analysis demonstrates the tremendous potential\nof the DCNN for the analysis of high-resolution STEM imaging and spectral data\nand highlights the associated limitations.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12678v1"
    },
    {
        "title": "Uniqueness of the Random Illumination Microscopy Variance Equation",
        "authors": [
            "Simon Labouesse",
            "Jérôme Idier",
            "Anne Sentenac",
            "Thomas Mangeat"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Recently, it has been shown theoretically that fluorescence microscopy using\nrandom illuminations (RIM) yields a doubled lateral resolution and an improved\noptical sectioning. Moreover, an algorithm called algoRIM, based on variance\nmatching, has been successfully validated on numerous biological applications.\nHere, we propose a proof of uniqueness of the RIM variance equation, which\ncorresponds to a first theoretical validation of algoRIM.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.00493v1"
    },
    {
        "title": "Non-parametric estimation of a Langevin model driven by correlated noise",
        "authors": [
            "Clemens Willers",
            "Oliver Kamps"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Langevin models are frequently used to model various stochastic processes in\ndifferent fields of natural and social sciences. They are adapted to measured\ndata by estimation techniques such as maximum likelihood estimation, Markov\nchain Monte Carlo methods, or the non-parametric direct estimation method\nintroduced by Friedrich et al. The latter has the distinction of being very\neffective in the context of large data sets. Due to their $\\delta$-correlated\nnoise, standard Langevin models are limited to Markovian dynamics. A\nnon-Markovian Langevin model can be formulated by introducing a hidden\ncomponent that realizes correlated noise. For the estimation of such a\npartially observed diffusion a different version of the direct estimation\nmethod was introduced by Lehle et al. However, this procedure includes the\nlimitation that the correlation length of the noise component is small compared\nto that of the measured component. In this work we propose another version of\nthe direct estimation method that does not include this restriction. Via this\nmethod it is possible to deal with large data sets of a wider range of examples\nin an effective way. We discuss the abilities of the proposed procedure using\nseveral synthetic examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.02990v1"
    },
    {
        "title": "Different Environmental Conditions in Genetic Algorithm",
        "authors": [
            "Daekyung Lee",
            "Beom Jun Kim"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We propose an extended genetic algorithm (GA) with different local\nenvironmental conditions. Genetic entities, or configurations, are put on nodes\nin a ring structure, and location-dependent environmental conditions are\napplied for each entity. Our GA is motivated by the geographic aspect of\nnatural evolution: Geographic isolation reduces the diversity in a local group,\nbut at the same time, can enhance intergroup diversity. Mating of genetic\nentities across different environments can make it possible to search for broad\narea of the fitness landscape. We validate our extended GA for finding the\nground state of three-dimensional spin-glass system and find that the use of\ndifferent environmental conditions makes it possible to find the lower-energy\nspin configurations at relatively shorter computation time. Our extension of GA\nbelongs to a meta-optimization method and thus can be applied for a broad\nresearch area in which finding of the optimal state in a shorter computation\ntime is the key problem.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.12313v1"
    },
    {
        "title": "Information theory based Electron Paramagnetic Resonance dating",
        "authors": [
            "C. Tannous",
            "J. Gieraltowski"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Chronometric dating is becoming increasingly important in areas such as the\nOrigin and evolution of Life on Earth and other planets, Origin and evolution\nof the Earth and the Solar System... Electron Spin Resonance (ESR) dating is\nbased on exploiting effects of contamination by chemicals or ionizing\nradiation, on ancient matter through its absorption spectrum and lineshape.\nInterpreting absorption spectra as probability density functions (pdf), we use\nthe notion of Information Theory (IT) distance allowing us to position the\nmeasured lineshape with respect to standard limiting pdf's (Lorentzian and\nGaussian). This paves the way to perform dating when several interaction\npatterns between unpaired spins are present in geologic, planetary, meteorite\nor asteroid matter namely classical-dipolar (for ancient times) and\nquantum-exchange-coupled (for recent times). In addition, accurate bounds to\nage are provided by IT from the evaluation of distances with respect to the\nLorentz and Gauss distributions. Dating arbitrary periods of\ntimes~\\cite{Anderson} and exploiting IT to introduce rigorous and accurate date\nvalues might have interesting far reaching implications not only in Geophysics,\nGeochronology~\\cite{Bahain}, Planetary Science but also in Mineralogy,\nArchaeology, Biology, Anthropology~\\cite{Aitken},\nPaleoanthropology~\\cite{Taylor,Richter}...\n",
        "pdf_link": "http://arxiv.org/pdf/2105.01971v3"
    },
    {
        "title": "Appropriate basis selection based on Bayesian inference for analyzing\n  measured data reflecting photoelectron wave interference",
        "authors": [
            "Yasuhiko Igarashi",
            "Fabio Iesari",
            "Hiroyuki Setoyama",
            "Toshihiro Okajima",
            "Hiroyuki Kumazoe",
            "Ichiro Akai",
            "Masato Okada"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  In this study, we applied Bayesian inference for extended X-ray absorption\nfine structure (EXAFS) to select an appropriate basis from among Fourier,\nwavelet and advanced Fourier bases, and we extracted a radial distribution\nfunction (RDF) and physical parameters from only EXAFS signals using physical\nprior knowledge, which is to be realized in general in condensed systems. To\nevaluate our method, the well-known EXAFS spectrum of copper was used for the\nEXAFS data analysis. We found that the advanced Fourier basis is selected as an\nappropriate basis for the regression of the EXAFS signal in a quantitative way\nand that the estimation of the Debye-Waller factor can be robustly realized\nonly by using the advanced Fourier basis. Bayesian inference based on minimal\nrestrictions allows us to not only eliminate some unphysical results but also\nselect an appropriate basis. Generally, FEFF analysis is used for estimating\nphysical parameters such as Deby-Waller and extracting RDF. Bayesian inference\nenables us to simultaneously select an appropriate basis and optimized physical\nparameters without FEFF analysis, which results in extracting RDF from only\nEXAFS signals. These advantages lead to the general usage of Bayesian inference\nfor EXAFS data analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.02341v1"
    },
    {
        "title": "Generalization of Higuchi's fractal dimension for multifractal analysis\n  of time series with limited length",
        "authors": [
            "Carlos Carrizales-Velazquez",
            "Reik V. Donner",
            "Lev Guzmán-Vargas"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We introduce a generalization of Higuchi's estimator of the fractal dimension\nas a new way to characterize the multifractal spectrum of univariate time\nseries. The resulting multifractal Higuchi dimension analysis (MF-HDA) method\nconsiders the order-$q$ moments of the partition function provided by the\nlength of the time series graph at different levels of subsampling. The results\nobtained for different types of stochastic processes as well as real-world\nexamples of word length series from fictional texts demonstrate that MF-HDA\nprovides a reliable estimate of the multifractal spectrum already for moderate\ntime series lengths. Practical advantages as well as disadvantages of the new\napproach as compared to other state-of-the-art methods of multifractal analysis\nare discussed, highlighting the particular potentials of MF-HDA to distinguish\nmono- from multi-fractal dynamics based on relatively short time series.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.11055v1"
    },
    {
        "title": "Multifractal test for nonlinear changes in time series",
        "authors": [
            "Damian G. Kelty-Stephen",
            "Elizabeth Lane",
            "Madhur Mangalam"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The creativity and emergence of biological and psychological behavior are\nnonlinear. However, that does not necessarily mean only that the measurements\nof the behaviors are curvilinear. Furthermore, the linear model might fail to\nreduce these measurements to a sum of independent random factors, implying\nnonlinear changes over time. The present work reviews some of the concepts\nimplicated in linear changes over time and details the mathematical steps\ninvolved. It introduces multifractality as a mathematical framework helpful in\ndetermining whether and to what degree the measured time series exhibits\nnonlinear changes over time. The mathematical steps include multifractal\nanalysis and surrogate data production for resolving when multifractality\nentails nonlinear changes over time. Ultimately, when measurements fail to fit\nthe structures of the traditional linear model, multifractal modeling gives us\nthe means to make those nonlinear excursions explicit and perhaps permit the\ndevelopment of theory that draws on both linear and nonlinear processes.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.13113v1"
    },
    {
        "title": "Neural network--featured timing systems for radiation detectors:\n  performance evaluation based on bound analysis",
        "authors": [
            "Pengcheng Ai",
            "Zhi Deng",
            "Yi Wang",
            "Linmao Li"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Waveform sampling systems are used pervasively in the design of front end\nelectronics for radiation detection. The introduction of new feature extraction\nalgorithms (eg. neural networks) to waveform sampling has the great potential\nto substantially improve the performance and enrich the capability. To analyze\nthe limits of such algorithms and thus illuminate the direction of resolution\noptimization, in this paper we systematically simulate the detection procedure\nof contemporary radiation detectors with an emphasis on pulse timing. Neural\nnetworks and variants of constant fraction discrimination are studied in a wide\nrange of analog channel frequency and noise level. Furthermore, we propose an\nestimation of multivariate Cram\\'er Rao lower bound within the model using\nintrinsic-extrinsic parametrization and prior information. Two case studies\n(single photon detection and shashlik-type calorimeter) verify the reliability\nof the proposed method and show it works as a useful guideline when assessing\nthe abilities of various feature extraction algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.14687v4"
    },
    {
        "title": "Unveiling Node Mass through Self-Consistent Gravity Model",
        "authors": [
            "Daekyung Lee",
            "Wonguk Cho",
            "Heetae Kim",
            "Gunn Kim",
            "Hyeong-Chai Jeong",
            "Beom Jun Kim"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The gravity model, inspired by Newton's law of universal gravitation, has\nlong served as a primary tool for interpreting trade flows between countries,\nusing a country's economic `mass' as a key determinant. Despite its wide\napplication, the definition of `mass' within this model remains ambiguous. It\nis often approximated using indicators like GDP, which may not accurately\nreflect a country's true trade potential. Here, we introduce a data-driven,\nself-consistent numerical approach that redefines `mass' from a static proxy to\na dynamic attribute inferred directly from flow data. We infer mass\ndistribution and interaction nature through our method, mirroring Newton's\napproach to understanding gravity. Our methodology accurately identifies\npredefined embeddings and reconstructs system attributes when applied to\nsynthetic flow data, demonstrating its strong predictive power and\nadaptability. Further application to real-world trade networks yields critical\ninsights, revealing the spatial spectrum of trade flows and the economic mass\nof countries, two key features unexplored in depth by existing models. Our\nmethodology not only enables accurate reconstruction of the original flow but\nalso allows for a deep understanding of the unique capabilities of each node\nwithin the network. This study marks a significant shift in the understanding\nand application of the gravity model, providing a more comprehensive tool for\nanalyzing complex systems and uncovering new insights into various fields,\nincluding global trade, traffic engineering, epidemic disease prevention, and\ninfrastructure design.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.10025v3"
    },
    {
        "title": "Combining Coupled Skorokhod SDEs and Lattice Gas Frameworks for\n  Multi-fidelity Modelling of Complex Behavioral Systems",
        "authors": [
            "Thi Kim Thoa Thieu",
            "Roderick Melnik"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  To model reliably behavioral systems with complex bio-social interactions,\naccounting for uncertainty quantification, is critical for many application\nareas. However, in terms of the mathematical formulation of the corresponding\nproblems, one of the major challenges is coming from the fact that\ncorresponding stochastic processes should, in most cases, be considered in\nbounded domains, possibly with obstacles. This has been known for a long time\nand yet, very little has been done for the quantification of uncertainties in\nmodelling complex behavioral systems described by such stochastic processes. In\nthis paper, we address this challenge by considering a coupled system of\nSkorokhod-type stochastic differential equations (SDEs) describing interactions\nbetween active and passive participants of a mixed-population group. In\ndeveloping a multi-fidelity modelling methodology for such behavioral systems,\nwe combine low- and high-fidelity results obtained from (a) the solution of the\nunderlying coupled system of SDEs and (b) simulations with a\nstatistical-mechanics-based lattice gas model, where we employ a kinetic Monte\nCarlo procedure. Furthermore, we provide representative numerical examples of\nhealthcare systems, subject to an epidemic, where the main focus in our\nconsiderations is given to an interacting particle system of asymptomatic and\nsusceptible populations.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.11269v1"
    },
    {
        "title": "Live processing of momentum-resolved STEM data for first moment imaging\n  and ptychography",
        "authors": [
            "Achim Strauch",
            "Dieter Weber",
            "Alexander Clausen",
            "Anastasiia Lesnichaia",
            "Arya Bangun",
            "Benjamin März",
            "Feng Jiao Lyu",
            "Qing Chen",
            "Andreas Rosenauer",
            "Rafal Dunin-Borkowski",
            "Knut Müller-Caspary"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  A reformulated implementation of single-sideband ptychography enables\nanalysis and display of live detector data streams in 4D scanning transmission\nelectron microscopy (STEM) using the LiberTEM open-source platform. This is\ncombined with live first moment and further virtual STEM detector analysis.\nProcessing of both real experimental and simulated data shows the\ncharacteristics of this method when data is processed progressively, as opposed\nto the usual offline processing of a complete dataset. In particular, the\nsingle side band method is compared to other techniques such as the enhanced\nptychographic engine in order to ascertain its capability for structural\nimaging at increased specimen thickness. Qualitatively interpretable live\nresults are obtained also if the sample is moved, or magnification is changed\nduring the analysis. This allows live optimization of instrument as well as\nspecimen parameters during the analysis. The methodology is especially expected\nto improve contrast- and dose-efficient in-situ imaging of weakly scattering\nspecimens, where fast live feedback during the experiment is required.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.13457v2"
    },
    {
        "title": "Discriminating chaotic and stochastic time series using permutation\n  entropy and artificial neural networks",
        "authors": [
            "B. R. R. Boaretto",
            "R. C. Budzinski",
            "K. L. Rossi",
            "T. L. Prado",
            "S. R. Lopes",
            "C. Masoller"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Extracting relevant properties of empirical signals generated by nonlinear,\nstochastic, and high-dimensional systems is a challenge of complex systems\nresearch. Open questions are how to differentiate chaotic signals from\nstochastic ones, and how to quantify nonlinear and/or high-order temporal\ncorrelations. Here we propose a new technique to reliably address both\nproblems. Our approach follows two steps: first, we train an artificial neural\nnetwork (ANN) with flicker (colored) noise to predict the value of the\nparameter, $\\alpha$, that determines the strength of the correlation of the\nnoise. To predict $\\alpha$ the ANN input features are a set of probabilities\nthat are extracted from the time series by using symbolic ordinal analysis.\nThen, we input to the trained ANN the probabilities extracted from the time\nseries of interest, and analyze the ANN output. We find that the $\\alpha$ value\nreturned by the ANN is informative of the temporal correlations present in the\ntime series. To distinguish between stochastic and chaotic signals, we exploit\nthe fact that the difference between the permutation entropy (PE) of a given\ntime series and the PE of flicker noise with the same $\\alpha$ parameter is\nsmall when the time series is stochastic, but it is large when the time series\nis chaotic. We validate our technique by analysing synthetic and empirical time\nseries whose nature is well established. We also demonstrate the robustness of\nour approach with respect to the length of the time series and to the level of\nnoise. We expect that our algorithm, which is freely available, will be very\nuseful to the community.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.02860v1"
    },
    {
        "title": "Evaluation of efficiency index of friction energy dissipation devices\n  using endurance time method",
        "authors": [
            "A. Shirkhani",
            "B. Farahmand Azar",
            "M. Charkhtab Basim"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Various methods have been presented to improve the performance of buildings\nagainst earthquakes. Friction damper device is one of the energy dissipation\ndevices that appropriately absorbs and dissipates the input energy and\ndecreases displacements. In this paper, the possibility of using endurance time\nmethod to determine the efficiency index and optimum slip load for these\ndampers was investigated by comparing the results of endurance time and\nnonlinear time history analyses. The efficiency indexes acquired from the\naverage of results for nonlinear time history and endurance time analyses were\nclose to each other. In this research, by assuming identical optimum slip load\nfor the dampers in all stories, the normalized damper strength was increased in\na number of equal steps ranging from zero to one to determine the efficiency\nindex of dampers in each step. Then, the optimum slip load of dampers in the\nsteel frames was calculated according to the minimum efficiency index of\ndampers. As a result, employing the endurance time method instead of a high\nnumber of nonlinear time history analyses is also possible, and using the\nendurance time method diminishes 57% of computational endeavors. Lastly, a\nrelation for acquiring the optimum slip load of the friction damper devices in\nsteel structures was proposed in terms of the weight of the structures. After\nadding optimal FDDs to the structures and investigating the effectiveness of\nthe dampers, it was concluded that by using endurance time excitation function\nwith better energy consistency, the endurance time results could be improved.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.04530v1"
    },
    {
        "title": "Efficient Bayesian estimation of the generalized Langevin equation from\n  data",
        "authors": [
            "Clemens Willers",
            "Oliver Kamps"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Modeling non-Markovian time series is a recent topic of research in many\nfields such as climate modeling, biophysics, molecular dynamics, or finance.\nThe generalized Langevin equation (GLE), given naturally by the Mori-Zwanzig\nprojection formalism, is a frequently used model including memory effects. In\napplications, a specific form of the GLE is most often obtained on a\ndata-driven basis. Here, Bayesian estimation has the advantage of providing\nboth suitable model parameters and their credibility in a straightforward way.\nIt can be implemented in the approximating case of white noise, which, far from\nthermodynamic equilibrium, is consistent with the fluctuation-dissipation\ntheorem. However, the exploration of the posterior, which is done via Markov\nchain Monte Carlo sampling, is numerically expensive, which makes the analysis\nof large data sets unfeasible. In this work, we discuss an efficient\nimplementation of Bayesian estimation of the GLE based on a piecewise constant\napproximation of the drift and diffusion functions of the model. In this case,\nthe characteristics of the data are represented by only a few coefficients, so\nthat the numerical cost of the procedure is significantly reduced and\nindependent of the length of the data set. Further, we propose a modification\nof the memory term of the GLE, leading to an equivalent model with an emphasis\non the impact of trends, which ensures that an estimate of the standard\nLangevin equation provides an effective initial guess for the GLE. We\nillustrate the capabilities of both the method and the model by an example from\nturbulence.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.04560v3"
    },
    {
        "title": "Effect of pseudo datasets for the classification-based engineering\n  design",
        "authors": [
            "Xianping Du",
            "Kai Zhang",
            "Onur Bilgen",
            "Laurent Burlion",
            "Hongyi Xu"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Machine learning classification techniques have been used widely to recognize\nthe feasible design domain and discover hidden patterns in engineering design.\nAn accurate classification model needs a large dataset; however, generating a\nlarge dataset is costly for complex simulation-based problems. After training\nby a small dataset, surrogate models can generate a large pseudo dataset\nefficiently. Errors, however, may be introduced by surrogate modeling. This\npaper investigates the mutual effect of a large pseudo dataset and surrogate\nmodeling uncertainty. Four widely used methods, i.e., Naive Bayes classifier,\nsupport vector machine, random forest regression, and artificial neural network\nfor classification, are studied on four benchmark problems. Kriging is used as\nthe basic surrogate model method. The results show that a large pseudo dataset\nimproves the classification accuracy, which depends on both design problems and\nclassification algorithms. Except for the Naive Bayes, the other three methods\nare recommended for using pseudo data to improve classification performance.\nAlso, a wind turbine design problem is used to illustrate the effect of the\npseudo dataset on feasible subspace recognition. The large pseudo dataset\nimproves the recognized subspace bound greatly, which can be reproduced by\nclassification models well except for the Naive Bayes classifier. Under the\nuncertainty of surrogate modeling, the random forest presents high robustness\nto recognize the feasible design domain bound, while the artificial neural\nnetwork demonstrates a high sensitivity to uncertainty with the recognized\nbound deteriorated.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.05562v1"
    },
    {
        "title": "Posterior sampling for inverse imaging problems on the sphere in\n  seismology and cosmology",
        "authors": [
            "Augustin Marignier",
            "Jason D. McEwen",
            "Ana M. G. Ferreira",
            "Thomas D. Kitching"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Inverse problems defined on the sphere arise in many fields, including\nseismology and cosmology where problems are defined on the globe and the cosmic\nsphere. These are generally high-dimensional and computationally very complex\nand, as a result, sampling the posterior of spherical inverse problems is a\nchallenging task. In this work, we describe a framework that leverages a\nproximal Markov chain Monte Carlo (MCMC) algorithm to efficiently sample the\nhigh-dimensional space of spherical inverse problems with a sparsity-promoting\nwavelet prior. We detail the modifications needed for the algorithm to be\napplied to spherical problems, and give special consideration to the crucial\nforward modelling step which contains spherical harmonic transforms that are\ncomputationally expensive. By sampling the posterior, our framework allows for\nfull and flexible uncertainty quantification, something which is not possible\nwith other methods based on, for example, convex optimisation. We demonstrate\nour framework in practice on full-sky cosmological mass-mapping and on a common\nproblem in global seismic tomography. We find that our approach is potentially\nuseful at moderate resolutions, such as those of interest in seismology. Our\nframework is generally limited by resolution requirements, such as those\nrequired for astrophysical applications, due to the poor scaling of the\ncomplexity of spherical harmonic transforms with resolution. A new Python\npackage, pxmcmc, containing the proximal MCMC sampler, measurement operators,\nwavelet transforms and sparse priors is made publicly available.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.06500v3"
    },
    {
        "title": "Statistical Inference of 1D Persistent Nonlinear Time Series and\n  Application to Predictions",
        "authors": [
            "Johannes A. Kassel",
            "Holger Kantz"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We introduce a method for reconstructing macroscopic models of\none-dimensional stochastic processes with long-range correlations from sparsely\nsampled time series by combining fractional calculus and discrete-time Langevin\nequations. The method is illustrated for the ARFIMA(1,d,0) process and a\nnonlinear auto-regressive toy model with multiplicative noise. We reconstruct a\nmodel for daily mean temperature data recorded at Potsdam (Germany) and use it\nto predict the first frost date by computing the mean first passage time of the\nreconstructed process and the zero degree Celsius temperature line,\nillustrating the potential of long-memory models for predictions in the\nsubseasonal-to-seasonal range.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.14619v2"
    },
    {
        "title": "Proposal of representative portfolios for federal roadway bridges in\n  Northeastern Brazil",
        "authors": [
            "G. H. F. Cavalcante",
            "E. M. V. Pereira",
            "I. D. Rodrigues",
            "L. C. M. Vieira Junior",
            "J. E. Padgett",
            "G. H. Siqueira"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  This paper presents a statistical analysis of federal highway bridges\ncommonly found in Northeastern Brazil to develop a portfolio, or statistically\nrepresentative characterization of bridges across the region. A detailed study\nof bridges under the supervision of the National Department of Infrastructure\nand Transportation is conducted and four representative bridge classes are\ndefined: two of them consist of single-span bridges and the others are\nmulti-span continuous bridges with non-integral or no abutments and different\nbridge decks. Discrete and continuous distributions describe random variables\nto consider their variability in the analyses. However, some parameters are\ndefined as function of the random variables, since a strong correlation is\nobserved. Future bridge assessment studies should use the geometries of this\nbridge portfolio to evaluate the regional impacts due to natural hazards, or\nfurnish models for updating, among other applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.00934v1"
    },
    {
        "title": "Towards more reasonable identifications of the symmetries in noisy\n  digital images from periodic and aperiodic crystals",
        "authors": [
            "Peter Moeck"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  A geometric form of information theory allows for reasonable, i.e.\nprobabilistic, evidence-ranking based, and generalized noise-level dependent,\nclassifications of the crystallographic and quasicrystallographic symmetries in\nnoisy digital images. Such classifications are based solely on the image pixel\nintensity values, justifiable assumptions about the aggregate distribution of\ngeneralized noise in the images, asymptotic extrapolations to zero-noise\nimages, and rational symmetry model selections with maximized predictive\naccuracy in the presence of both symmetry-inclusion relations and\npseudo-symmetries. Preferring a well developed geometric form of information\ntheory over a theoretically possible geometric-Bayesian approach for these\nclassifications is the only subjective choice made. Using digital data planes\nand assuming approximately Gaussian distributed generalized noise, reasonable\ncrystallographic and quasicrystallographic symmetry classifications can be made\nfor noisy images from both scanning probe and transmission electron\nmicroscopes. A binary type classification of structurally very similar\nmate-rials into either a quasicrystal or one of its rational/crystalline\napproximants based on the approximate point symmetries in their noisy digital\nimages is proposed here for the first time.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.01237v2"
    },
    {
        "title": "Assessing time series irreversibility through micro-scale trends",
        "authors": [
            "Massimiliano Zanin"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Time irreversibility, defined as the lack of invariance of the statistical\nproperties of a system or time series under the operation of time reversal, has\nreceived an increasing attention during the last decades, thanks to the\ninformation it provides about the mechanisms underlying the observed dynamics.\nFollowing the need of analysing real-world time series, many irreversibility\nmetrics and tests have been proposed, each one associated with different\nrequirements in terms of e.g. minimum time series length or computational cost.\nWe here build upon previously proposed tests based on the concept of\npermutation patterns, but deviating from them through the inclusion of\ninformation about the amplitude of the signal and how this evolves over time.\nWe show, by means of synthetic time series, that the results yielded by this\nmethod are complementary to the ones obtained by using permutation patterns\nalone, thus suggesting that \"one irreversibility metric does not fit all\" We\nfurther apply the proposed metric to the analysis of two real-world data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.06272v1"
    },
    {
        "title": "A comparison of sports-related head accelerations with and without\n  direct head impacts",
        "authors": [
            "Samuel J. Raymond",
            "Yuzhe Liu",
            "Nicholas J. Cecchi",
            "Eli Rice",
            "Ashlyn A. Callan",
            "Landon P. Watson",
            "Sohrab Sami",
            "Zhou Zhou",
            "Xiaogai Li",
            "Svein Kleiven",
            "Michael Zeineh",
            "David B. Camarillo"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Concussion and repeated exposure to mild traumatic brain injury are risks for\nathletes in many sports. While direct head impacts are analyzed to improve the\ndetection and awareness of head acceleration events so that an athlete's brain\nhealth can be appropriately monitored and treated. However, head accelerations\ncan also be induced by impacts with little or no head involvement. In this work\nwe evaluated if impacts that do not involve direct head contact, such as being\npushed in the torso, can be sufficient in collegiate American football to\ninduce head accelerations comparable to direct head impacts. Datasets of\nimpacts with and without direct head contact were collected and compared. These\ndatasets were gathered using a state-of-the-art impact detection algorithm\nembedded in an instrumented mouthguard to record head kinematics. Video\nanalysis was used to differentiate between impact types. In total, 15 impacts\nof each type were used in comparison, with clear video screenshots available to\ndistinguish each impact type. Analysis of the kinematics showed that the\nimpacts without direct head contact achieved similar levels of linear and\nangular accelerations during impact compared to those from direct head impacts.\nFinite element analyses using the median and peak kinematic signals were used\nto calculate maximum principal strain of the brain. Statistical analysis\nrevealed that no significant difference was found between the two datasets\nbased on a Bonferroni-adjusted p-value threshold of 0.017 , with the exception\nof peak linear acceleration. Impacts without direct head contact showed higher\nmean values of peak linear acceleration values of 17.6 g compared to the\ndirect-head impact mean value of 6.1g. These results indicated that impacts\nother than direct head impacts could still produce meaningful kinematic loads\nin the head and as such should be included in athlete health monitoring.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.08763v2"
    },
    {
        "title": "Bridging microscopy with molecular dynamics and quantum simulations: An\n  AtomAI based pipeline",
        "authors": [
            "Ayana Ghosh",
            "Maxim Ziatdinov",
            "Ondrej Dyck",
            "Bobby Sumpter",
            "Sergei V. Kalinin"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Recent advances in (scanning) transmission electron microscopy have enabled\nroutine generation of large volumes of high-veracity structural data on 2D and\n3D materials, naturally offering the challenge of using these as starting\ninputs for atomistic simulations. In this fashion, theory will address\nexperimentally emerging structures, as opposed to the full range of\ntheoretically possible atomic configurations. However, this challenge is highly\nnon-trivial due to the extreme disparity between intrinsic time scales\naccessible to modern simulations and microscopy, as well as latencies of\nmicroscopy and simulations per se. Addressing this issue requires as a first\nstep bridging the instrumental data flow and physics-based simulation\nenvironment, to enable the selection of regions of interest and exploring them\nusing physical simulations. Here we report the development of the machine\nlearning workflow that directly bridges the instrument data stream into\nPython-based molecular dynamics and density functional theory environments\nusing pre-trained neural networks to convert imaging data to physical\ndescriptors. The pathways to ensure the structural stability and compensate for\nthe observational biases universally present in the data are identified in the\nworkflow. This approach is used for a graphene system to reconstruct optimized\ngeometry and simulate temperature-dependent dynamics including adsorption of Cr\nas an ad-atom and graphene healing effects. However, it is universal and can be\nused for other material systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.04541v3"
    },
    {
        "title": "Solving an elastic inverse problem using Convolutional Neural Networks",
        "authors": [
            "Nachiket H. Gokhale"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We explore the application of a Convolutional Neural Network (CNN) to image\nthe shear modulus field of an almost incompressible, isotropic, linear elastic\nmedium in plane strain using displacement or strain field data. This problem is\nimportant in medicine because the shear modulus of suspicious and potentially\ncancerous growths in soft tissue is elevated by about an order of magnitude as\ncompared to the background of normal tissue. Imaging the shear modulus field\ntherefore can lead to high-contrast medical images. Our imaging problem is:\nGiven a displacement or strain field (or its components), predict the\ncorresponding shear modulus field. Our CNN is trained using 6000 training\nexamples consisting of a displacement or strain field and a corresponding shear\nmodulus field. We observe encouraging results which warrant further research\nand show the promise of this methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.07859v1"
    },
    {
        "title": "Inferring potential landscapes from noisy trajectories",
        "authors": [
            "J. Shepard Bryan IV",
            "Prithviraj Basak",
            "John Bechhoefer",
            "Steve Presse"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  While particle trajectories encode information on their governing potentials,\npotentials can be challenging to robustly extract from trajectories.\nMeasurement errors may corrupt a particle's position, and sparse sampling of\nthe potential limits data in higher-energy regions such as barriers. We develop\na Bayesian method to infer potentials of arbitrary shape alongside measurement\nnoise. As an alternative to Gaussian process priors over potentials, we\nintroduce structured kernel interpolation to the Natural Sciences which allows\nus to extend our analysis to large data sets. Our method is validated on 1D and\n2D experimental trajectories for particles in a feedback trap.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02347v1"
    },
    {
        "title": "The Statistics of the Cross-Spectrum and the Spectrum Average:\n  Generalization to Multiple Instruments",
        "authors": [
            "Antoine Baudiquez",
            "Éric Lantz",
            "Enrico Rubiola",
            "François Vernotte"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  This article addresses the measurement of the power spectrum of red noise\nprocesses at the lowest frequencies, where the minimum acquisition time is so\nlong that it is impossible to average on a sequence of data record. Therefore,\naveraging is possible only on simultaneous observation of multiple instruments.\nThis is the case of radio astronomy, which we take as the paradigm, but\nexamples may be found in other fields such as climatology and geodesy. We\ncompare the Bayesian confidence interval of the red-noise parameter using two\nestimators, the spectrum average and the cross-spectrum. While the spectrum\naverage is widely used, the cross-spectrum using multiple instruments is rather\nuncommon. With two instruments, the cross-spectrum estimator leads to the\nVariance-Gamma distribution. A generalization to $q$ devices is provided, with\nthe example of the observation of millisecond pulsars with 5 radio telescopes.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.13969v1"
    },
    {
        "title": "The detection matrix as a model-agnostic tool to estimate the number of\n  degrees of freedom in mechanical systems and engineering structures",
        "authors": [
            "Paolo Celli",
            "Maurizio Porfiri"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Estimating the number of degrees of freedom of a mechanical system or an\nengineering structure from the time-series of a small set of sensors is a basic\nproblem in diagnostics, which, however, is often overlooked when monitoring\ntheir health and integrity. In this work, we demonstrate the applicability of\nthe network-theoretic concept of detection matrix as a tool to solve this\nproblem. From this estimation, we illustrate the possibility to identify\ndamage. The detection matrix, recently introduced by Haehne et al. in the\ncontext of network theory, is assembled from the transient response of a few\nnodes as a result of non-zero initial conditions: its rank offers an estimate\nof the number of nodes in the network itself. The use of the detection matrix\nis completely model-agnostic, whereby it does not require any knowledge of the\nsystem dynamics. Here, we show that, with a few modifications, this same\nprinciple applies to discrete systems, such as spring-mass lattices and\ntrusses. Moreover, we discuss how damage in one or more members causes the\nappearance of distinct jumps in the singular values of this matrix, thereby\nopening the door to structural health monitoring applications, without the need\nfor a complete model reconstruction.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.02831v1"
    },
    {
        "title": "Permutation Jensen-Shannon distance: A versatile and fast symbolic tool\n  for complex time series analysis",
        "authors": [
            "Luciano Zunino",
            "Felipe Olivares",
            "Haroldo V. Ribeiro",
            "Osvaldo A. Rosso"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The main motivation of this paper is to introduce the permutation\nJensen-Shannon distance, a symbolic tool able to quantify the degree of\nsimilarity between two arbitrary time series. This quantifier results from the\nfusion of two concepts, the Jensen-Shannon divergence and the encoding scheme\nbased on the sequential ordering of the elements in the data series. The\nversatility and robustness of this ordinal symbolic distance for characterizing\nand discriminating different dynamics are illustrated through several numerical\nand experimental applications. Results obtained allow us to be optimistic about\nits usefulness in the field of complex time series analysis. Moreover, thanks\nto its simplicity, low computational cost, wide applicability and less\nsusceptibility to outliers and artifacts, this ordinal measure can efficiently\nhandle large amounts of data and help to tackle the current big data\nchallenges.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12107v1"
    },
    {
        "title": "Classification of events from $α$-induced reactions in the MUSIC\n  detector via statistical and ML methods",
        "authors": [
            "Krishnan Raghavan",
            "Melina L. Avila",
            "Prasanna Balaprakash",
            "Heshani Jayatissa",
            "Daniel Santiago-Gonzalez"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The Multi-Sampling Ionization Chamber (MUSIC) detector is typically used to\nmeasure nuclear reaction cross sections relevant for nuclear astrophysics,\nfusion studies, and other applications. From the MUSIC data produced in one\nexperiment scientists carefully extract an order of $10^3$ events of interest\nfrom about $10^{9}$ total events, where each event can be represented by an\n18-dimensional vector. However, the standard data classification process is\nbased on expert-driven, manually intensive data analysis techniques that\nrequire several months to identify patterns and classify the relevant events\nfrom the collected data. To address this issue, we present a method for the\nclassification of events originating from specific $\\alpha$-induced reactions\nby combining statistical and machine learning methods that require\nsignificantly less input from the domain scientist, relative to the standard\ntechnique.\n  We applied the new method to two experimental data sets and compared our\nresults with those obtained using traditional methods. With few exceptions, the\nnumber of events classified by our method agrees within $\\pm20\\%$ with the\nresults obtained using traditional methods. With the present method, which is\nthe first of its kind for the MUSIC data, we have established the foundation\nfor the automated extraction of physical events of interest from experiments\nusing the MUSIC detector.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.03137v1"
    },
    {
        "title": "Physics is the New Data",
        "authors": [
            "Sergei V. Kalinin",
            "Maxim Ziatdinov",
            "Bobby G. Sumpter",
            "Andrew D. White"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The rapid development of machine learning (ML) methods has fundamentally\naffected numerous applications ranging from computer vision, biology, and\nmedicine to accounting and text analytics. Until now, it was the availability\nof large and often labeled data sets that enabled significant breakthroughs.\nHowever, the adoption of these methods in classical physical disciplines has\nbeen relatively slow, a tendency that can be traced to the intrinsic\ndifferences between correlative approaches of purely data-based ML and the\ncausal hypothesis-driven nature of physical sciences. Furthermore, anomalous\nbehaviors of classical ML necessitate addressing issues such as explainability\nand fairness of ML. We also note the sequence in which deep learning became\nmainstream in different scientific disciplines - starting from medicine and\nbiology and then towards theoretical chemistry, and only after that, physics -\nis rooted in the progressively more complex level of descriptors, constraints,\nand causal structures available for incorporation in ML architectures. Here we\nput forth that over the next decade, physics will become a new data, and this\nwill continue the transition from dot-coms and scientific computing concepts of\nthe 90ies to big data of 2000-2010 to deep learning of 2010-2020 to\nphysics-enabled scientific ML.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.05095v1"
    },
    {
        "title": "Compression-Complexity with Ordinal Patterns for Robust Causal Inference\n  in Irregularly-Sampled Time Series",
        "authors": [
            "Aditi Kathpalia",
            "Pouya Manshour",
            "Milan Paluš"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Distinguishing cause from effect is a scientific challenge resisting\nsolutions from mathematics, statistics, information theory and computer\nscience. Compression-Complexity Causality (CCC) is a recently proposed\ninterventional measure of causality, inspired by Wiener-Granger's idea. It\nestimates causality based on change in dynamical compression-complexity (or\ncompressibility) of the effect variable, given the cause variable. CCC works\nwith minimal assumptions on given data and is robust to irregular-sampling,\nmissing-data and finite-length effects. However, it only works for\none-dimensional time series. We propose an ordinal pattern symbolization scheme\nto encode multidimensional patterns into one-dimensional symbolic sequences,\nand thus introduce the Permutation CCC (PCCC), which retains all advantages of\nthe original CCC and can be applied to data from multidimensional systems with\npotentially hidden variables. PCCC is tested on numerical simulations and\napplied to paleoclimate data characterized by irregular and uncertain sampling\nand limited numbers of samples.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.11731v1"
    },
    {
        "title": "Assigning degrees of stochasticity to blazar light curves in the radio\n  band using complex networks",
        "authors": [
            "Belén Acosta-Tripailao",
            "Walter Max-Moerbeck",
            "Denisse Pastén",
            "Pablo S. Moya"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  {We focus on characterizing the high-energy emission mechanisms of blazars by\nanalyzing the variability in the radio band of the light curves of more than a\nthousand sources. We are interested in assigning complexity parameters to these\nsources, modeling the time series of the light curves with the method of the\nHorizontal Visibility Graph (HVG), which allows us to obtain properties from\ndegree distributions, such as a characteristic exponent to describe its\nstochasticity and the Kullback-Leibler Divergence (KLD), presenting a new\nperspective to the methods commonly used to study Active Galactic Nuclei (AGN).\nWe contrast these parameters with the excess variance, an astronomical\nmeasurement of variability in light curves, at the same time we use the\nspectral classification of the sources. While it is not possible to find\nsignificant correlations with the excess variance, the degree distributions\nextracted from the network are detecting differences related to the spectral\nclassification of blazars. These differences suggest a chaotic behavior in the\ntime series for the BL Lac sources and a correlated stochastic behavior in the\ntime series for the FSRQ sources. Our results show that complex networks may be\na valuable alternative tool to study AGNs according to the variability of their\nenergy output.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04568v1"
    },
    {
        "title": "Discriminating abilities of threshold-free evaluation metrics in link\n  prediction",
        "authors": [
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Link prediction is a paradigmatic and challenging problem in network science,\nwhich attempts to uncover missing links or predict future links, based on known\ntopology. A fundamental but still unsolved issue is how to choose proper\nmetrics to fairly evaluate prediction algorithms. The area under the receiver\noperating characteristic curve (AUC) and the balanced precision (BP) are the\ntwo most popular metrics in early studies, while their effectiveness is\nrecently under debate. At the same time, the area under the precision-recall\ncurve (AUPR) becomes increasingly popular, especially in biological studies.\nBased on a toy model with tunable noise and predictability, we propose a method\nto measure the discriminating abilities of any given metric. We apply this\nmethod to the above three threshold-free metrics, showing that AUC and AUPR are\nremarkably more discriminating than BP, and AUC is slightly more discriminating\nthan AUPR. The result suggests that it is better to simultaneously use AUC and\nAUPR in evaluating link prediction algorithms, at the same time, it warns us\nthat the evaluation based only on BP may be unauthentic. This article provides\na starting point towards a comprehensive picture about effectiveness of\nevaluation metrics for link prediction and other classification problems.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04615v3"
    },
    {
        "title": "How accurate can combined measurements be -- experiment, simulation, and\n  theory",
        "authors": [
            "B. Mirbach",
            "M. Boguslawski"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  In this paper we investigate the question of how much combined measurements\ncan increase the accuracy of additive quantities. Therefore, we consider a set\nof measurements from a selection of all possible combinations of the $n$\nlabeled masses and then estimate the individual weights of the $n$ masses by a\nlinear regression approach. We present experimental results which motivate\ncomprehensive simulation campaigns. These simulations provide valid statistical\nstatements and reliable forecasts of the experimental results. A profound\nanalytical treatment in turn supports these simulation outcomes with excellent\nconsistency. One important achievement therein is a general analytical\nexpression for the estimate's error, not only limited to the two particular\nweighing schemes presented. It turns out that combined measurements allow to\nestimate the weight of mass elements with an accuracy that under-runs by orders\nof magnitude the resolution of the scale used. As the error depends on the\namount of measurements, one gains higher accuracy with increasing effort. In a\nbroader sense, our work wants to promote the method and give inspirations to\napplications in various metrological fields beyond high-precision mass\ndetermination. Moreover, the novel simulations and analytic formulas enable the\ndesign of optimal experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.07879v1"
    },
    {
        "title": "Confidence curves for UQ validation: probabilistic reference vs. oracle",
        "authors": [
            "Pascal Pernot"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Confidence curves are used in uncertainty validation to assess how large\nuncertainties ($u_{E}$) are associated with large errors ($E$). An oracle curve\nis commonly used as reference to estimate the quality of the tested datasets.\nThe oracle is a perfect, deterministic, error predictor, such as $|E|=\\pm\nu_{E}$, which corresponds to a very unlikely error distribution in a\nprobabilistic framework and is unable unable to inform us on the calibration of\n$u_{E}$. I propose here to replace the oracle by a probabilistic reference\ncurve, deriving from the more realistic scenario where errors should be random\ndraws from a distribution with standard deviation $u_{E}$. The probabilistic\ncurve and its confidence interval enable a direct test of the quality of a\nconfidence curve. Paired with the probabilistic reference, a confidence curve\ncan be used to check the calibration and tightness of prediction uncertainties.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.15272v2"
    },
    {
        "title": "Complexity emerges in measures of the marking dynamics in football games",
        "authors": [
            "A. Chacoma",
            "M. N. Kuperman",
            "O. V. Billoni"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  In this article, we study the dynamics of marking in football matches. To do\nthis, we surveyed and analyzed a database containing the trajectories of\nplayers from both teams on the field of play during three professional games.\nWe describe the dynamics through the construction of temporal bipartite\nnetworks of proximity. Based on the introduced concept of proximity, the nodes\nare the players, and the links are defined between opponents that are close\nenough to each other at a given moment. By studying the evolution of the\nheterogeneity parameter of the networks during the game, we characterized a\nscaling law for the average shape of the fluctuations, unveiling the emergence\nof complexity in the system. Moreover, we proposed a simple model to simulate\nthe players' motion in the field from where we obtained the evolution of a\nsynthetic proximity network. We show that the model captures with a remarkable\nagreement the complexity of the empirical case, hence it proves to be helpful\nto elucidate the underlying mechanisms responsible for the observed phenomena.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.09992v2"
    },
    {
        "title": "Efficient Bayesian estimation of a non-Markovian Langevin model driven\n  by correlated noise",
        "authors": [
            "Clemens Willers",
            "Oliver Kamps"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Data-driven modeling of non-Markovian dynamics is a recent topic of research\nwith applications in many fields such as climate research, molecular dynamics,\nbiophysics, or wind power modeling. In the frequently used standard Langevin\nequation, memory effects can be implemented through an additional hidden\ncomponent which functions as correlated noise, thus resulting in a\nnon-Markovian model. It can be seen as part of the model class of partially\nobserved diffusions which are usually adapted to observed data via Bayesian\nestimation, whereby the difficulty of the unknown noise values is solved\nthrough a Gibbs sampler. However, when regarding large data sets with a length\nof $10^6$ or $10^7$ data points, sampling the distribution of the same amount\nof latent variables is unfeasible. For the model discussed in this work, we\nsolve this issue through a direct derivation of the posterior distribution of\nthe Euler-Maruyama approximation of the model via analytical marginalization of\nthe latent variables. Yet, in the case of a nonlinear noise process, the\ninverse problem of model estimation proves to be ill-posed and still\nnumerically expensive. We handle these complications by restricting the noise\nto an Ornstein-Uhlenbeck process, which considerably reduces the ambiguity of\nthe estimation. Further, in this case, the estimation can be performed very\nefficiently if the drift and diffusion functions of the observed component are\napproximated in a piecewise constant manner. We illustrate the resulting\nprocedure of efficient Bayesian estimation of the considered non-Markovian\nLangevin model by an example from turbulence.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.10637v1"
    },
    {
        "title": "Unbiased two-windows approach for Welch's method",
        "authors": [
            "Eduardo Martini"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Periodogram methods are widely used for the estimation of power- and\ncross-spectra, of which Welch's method is the most popular. Previous studies\nhave analyzed the variance of the power spectra estimates and developed\nanalytical probability functions, showing that the approach is unbiased when\napplied to white-noise signals or in the limit of infinite window lengths.\nHowever, no explicit expression for the estimation bias is available for more\ncomplex signals when finite windows are used. In this study, we show that, for\nfinite window lengths, Welch's method is biased for all signals other than the\nwhite-noise signal. A novel two-window approach that is unbiased when applied\nto signals with bounded correlation lengths is proposed. Numerical experiments\nare used to illustrate the advantages of the novel approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.12159v1"
    },
    {
        "title": "Comparison of recent estimators of uncertainty on the mean for small\n  measurement samples with normal and non-normal error distributions",
        "authors": [
            "Pascal Pernot",
            "Jean-Paul Berthet"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We review the alternative proposals introduced recently in the literature to\nupdate the standard formula to estimate the uncertainty on the mean of repeated\nmeasurements, and we compare their performances on synthetic examples with\nnormal and non-normal error distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.04180v2"
    },
    {
        "title": "Research on the spectral reconstruction of a low-dimensional filter\n  array micro-spectrometer based on a truncated singular value\n  decomposition-convex optimization algorithm",
        "authors": [
            "Jiakun Zhang",
            "Liu Zhang",
            "Ying Song",
            "Yan Zheng"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Currently, the engineering of miniature spectrometers mainly faces three\nproblems: the mismatch between the number of filters at the front end of the\ndetector and the spectral reconstruction accuracy; the lack of a stable\nspectral reconstruction algorithm; and the lack of a spectral reconstruction\nevaluation method suitable for engineering. Therefore, based on 20 sets of\nfilters, this paper classifies and optimizes the filter array by the K-means\nalgorithm and particle swarm algorithm, and obtains the optimal filter\ncombination under different matrix dimensions. Then, the truncated singular\nvalue decomposition-convex optimization algorithm is used for high-precision\nspectral reconstruction, and the detailed spectral reconstruction process of\ntwo typical target spectra is described. In terms of spectral evaluation, due\nto the strong randomness of the target detected during the working process of\nthe spectrometer, the standard value of the target spectrum cannot be obtained.\nTherefore, for the first time, we adopt the method of joint cross-validation of\nmultiple sets of data for spectral evaluation. The results show that when the\nrandom error of positive or negative 2 code values is applied multiple times\nfor reconstruction, the spectral angle cosine value between the reconstructed\ncurves becomes more than 0.995, which proves that the spectral reconstruction\nunder this algorithm has high stability. At the same time, the spectral angle\ncosine value of the spectral reconstruction curve and the standard curve can\nreach above 0.99, meaning that it realizes a high-precision spectral\nreconstruction effect. A high-precision spectral reconstruction algorithm based\non truncated singular value-convex optimization, which is suitable for\nengineering applications, is established in this paper, providing important\nscientific research value for the engineering application of\nmicro-spectrometers.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.07531v1"
    },
    {
        "title": "TomocuPy: efficient GPU-based tomographic reconstruction with\n  asynchronous data processing",
        "authors": [
            "Viktor Nikitin"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Fast 3D data analysis and steering of a tomographic experiment by changing\nenvironmental conditions or acquisition parameters require fast, close to\nreal-time, 3D reconstruction of large data volumes. Here we present a\nperformance-optimized TomocuPy package as a GPU alternative to the\ncommonly-used CPU-based TomoPy package for tomographic reconstruction. TomocuPy\nutilizes modern hardware capabilities to organize a 3D asynchronous\nreconstruction involving parallel read-write operations with storage drives,\nCPU-GPU data transfers, and GPU computations. In the asynchronous\nreconstruction, all the operations are timely overlapped to almost fully hide\nall data management time. Since most cameras work with less than 16-bit digital\noutput, we furthermore optimize the memory usage and processing speed by using\n16-bit floating-point arithmetic. As a result, 3D reconstruction with TomocuPy\nbecame 20-30 times faster than its multithreaded CPU equivalent. Full\nreconstruction (including read-write operations and methods initialization) of\na 2048x2048x2048 tomographic volume takes less than 7~s on a single Nvidia\nTesla A100 and PCIe 4.0 NVMe SSD, and scales almost linearly increasing the\ndata size. To simplify operation at synchrotron beamlines, TomocuPy provides an\neasy-to-use command-line interface. Efficacy of the package was demonstrated\nduring a tomographic experiment on gas-hydrate formation in porous samples,\nwhere a steering option was implemented as a lens-changing mechanism for\nzooming to regions of interest.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.08450v2"
    },
    {
        "title": "Network inference combining mutual information rate and statistical\n  tests",
        "authors": [
            "Chris G. Antonopoulos"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  In this paper, we present a method that combines information-theoretical and\nstatistical approaches to infer connectivity in complex networks using\ntime-series data. The method is based on estimations of the Mutual Information\nRate for pairs of time-series and on statistical significance tests for\nconnectivity acceptance using the false discovery rate method for multiple\nhypothesis testing. We provide the mathematical background on Mutual\nInformation Rate, discuss the statistical significance tests and the false\ndiscovery rate. Further on, we present results for correlated normal-variates\ndata, coupled circle and coupled logistic maps, coupled Lorenz systems and\ncoupled stochastic Kuramoto phase oscillators. Following up, we study the\neffect of noise on the presented methodology in networks of coupled stochastic\nKuramoto phase oscillators and of coupling heterogeneity degree on networks of\ncoupled circle maps. We show that the method can infer the correct number and\npairs of connected nodes, by means of receiver operating characteristic curves.\nIn the more realistic case of stochastic data, we demonstrate its ability to\ninfer the structure of the initial connectivity matrices. The method is also\nshown to recover the initial connectivity matrices for dynamics on the nodes of\nErd\\H{o}s-R\\'enyi and small-world networks with varying coupling heterogeneity\nin their connections. The highlight of the proposed methodology is its ability\nto infer the underlying network connectivity based solely on the recorded\ndatasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.14063v1"
    },
    {
        "title": "Monitoring Public Behavior During a Pandemic Using Surveys:\n  Proof-of-Concept Via Epidemic Modelling",
        "authors": [
            "Andreas Koher",
            "Frederik Jørgensen",
            "Michael Bang Petersen",
            "Sune Lehmann"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Implementing a lockdown for disease mitigation is a balancing act:\nNon-pharmaceutical interventions can reduce disease transmission significantly,\nbut interventions also have considerable societal costs. Therefore,\ndecision-makers need near real-time information to calibrate the level of\nrestrictions. We fielded daily surveys in Denmark during the second wave of the\nCOVID-19 pandemic to monitor public response to the announced lockdown. A key\nquestion asked respondents to state their number of close contacts within the\npast 24 hours. Here, we establish a link between survey data, mobility data,\nand hospitalizations via epidemic modelling. Using Bayesian analysis, we then\nevaluate the usefulness of survey responses as a tool to monitor the effects of\nlockdown and then compare the predictive performance to that of mobility data.\nWe find that, unlike mobility, self-reported contacts decreased significantly\nin all regions before the nation-wide implementation of non-pharmaceutical\ninterventions and improved predicting future hospitalizations compared to\nmobility data. A detailed analysis of contact types indicates that contact with\nfriends and strangers outperforms contact with colleagues and family members\n(outside the household) on the same prediction task. Representative surveys\nthus qualify as a reliable, non-privacy invasive monitoring tool to track the\nimplementation of non-pharmaceutical interventions and study potential\ntransmission paths.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.01472v2"
    },
    {
        "title": "Curvelet-based Model for the Generation of Anisotropic Fractional\n  Brownian Fields",
        "authors": [
            "Marcos Vinícius Cândido Henriques"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We propose a curvelet-based model for the generation of Anisotropic\nFractional Brownian Fields, that are suited to model systems with\norientation-dependent self-similar properties. The synthesis procedure consists\nof generating coefficients in the curvelet space with zero-mean Gaussian\ndistribution. This approach allows the representation of natural systems having\nstochastic behavior in some degree and also obeying to a given angular\ndistribution of correlations. Examples of such systems are found in\nheterogeneous geological structures, in anisotropic materials and in complex\ndisordered media.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.01939v1"
    },
    {
        "title": "Testing the data framework for an AI algorithm in preparation for high\n  data rate X-ray facilities",
        "authors": [
            "Hongwei Chen",
            "Sathya R. Chitturi",
            "Rajan Plumley",
            "Lingjia Shen",
            "Nathan C. Drucker",
            "Nicolas Burdet",
            "Cheng Peng",
            "Sougata Mardanya",
            "Daniel Ratner",
            "Aashwin Mishra",
            "Chun Hong Yoon",
            "Sanghoon Song",
            "Matthieu Chollet",
            "Gilberto Fabbris",
            "Mike Dunne",
            "Silke Nelson",
            "Mingda Li",
            "Aaron Lindenberg",
            "Chunjing Jia",
            "Youssef Nashed",
            "Arun Bansil",
            "Sugata Chowdhury",
            "Adrian E. Feiguin",
            "Joshua J. Turner",
            "Jana B. Thayer"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The advent of next-generation X-ray free electron lasers will be capable of\ndelivering X-rays at a repetition rate approaching 1 MHz continuously. This\nwill require the development of data systems to handle experiments at these\ntype of facilities, especially for high throughput applications, such as\nfemtosecond X-ray crystallography and X-ray photon fluctuation spectroscopy.\nHere, we demonstrate a framework which captures single shot X-ray data at the\nLCLS and implements a machine-learning algorithm to automatically extract the\ncontrast parameter from the collected data. We measure the time required to\nreturn the results and assess the feasibility of using this framework at high\ndata volume. We use this experiment to determine the feasibility of solutions\nfor `live' data analysis at the MHz repetition rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.10137v1"
    },
    {
        "title": "Locating the eigenshield of a network via perturbation theory",
        "authors": [
            "Ming-Yang Zhou",
            "Manuel Sebastian Mariani",
            "Hao Liao",
            "Rui Mao",
            "Yi-Cheng Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The functions of complex networks are usually determined by a small set of\nvital nodes. Finding the best set of vital nodes (eigenshield nodes) is\ncritical to the network's robustness against rumor spreading and cascading\nfailures, which makes it one of the fundamental problems in network science.\nThe problem is challenging as it requires to maximize the influence of nodes in\nthe set while simultaneously minimizing the redundancies between the set's\nnodes. However, the redundancy mechanism is rarely investigated by previous\nstudies. Here we introduce the matrix perturbation framework to find a small\n``eigenshield\" set of nodes that, when removed, lead to the largest drop in the\nnetwork's spectral radius. We show that finding the ``eigenshield\" nodes can be\ntranslated into the optimization of an objective function that simultaneously\naccounts for the individual influence of each node and redundancy between\ndifferent nodes.\n  We analytically quantify the influence redundancy that explains why an\nimportant node might play an insignificant role in the ``eigenshield\" node set.\nExtensive experiments under diverse influence maximization problems, ranging\nfrom network dismantling to spreading maximization, demonstrate that the\neigenshield detection tends to significantly outperforms state-of-the-art\nmethods across most problems. Our findings shed light on the mechanisms that\nmay lie at the core of the function of vital nodes in complex network.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.15920v1"
    },
    {
        "title": "Study of nonlinear optical diffraction patterns using machine learning\n  models based on ResNet 152 architecture",
        "authors": [
            "Behnam Pishnamazi",
            "Ehsan Koushki"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  As the advancements in the field of artificial intelligence and nonlinear\noptics continues new methods can be used to better describe and determine\nnonlinear optical phenomena. In this research we aimed to analyze the\ndiffraction patterns of an organic material and determine the nonlinear\nrefraction index of the material in question by utilizing ResNet 152\nconvolutional neural network architecture in the regions of laser power that\nthe diffraction rings are not clearly distinguishable. This approach can open\nnew sights for optical material characterization in situations where the\nconventional methods do not apply.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.05122v2"
    },
    {
        "title": "Data-Driven Prognosis of Failure Detection and Prediction of Lithium-ion\n  Batteries",
        "authors": [
            "Hamed Sadegh Kouhestani",
            "Lin Liu",
            "Ruimin Wang",
            "Abhijit Chandra"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Battery prognostics and health management predictive models are essential\ncomponents of safety and reliability protocols in battery management system\nframeworks. Overall, developing a robust and efficient battery model that\naligns with the current literature is a useful step in ensuring the safety of\nbattery function. For this purpose, a multi-physics, multi-scale deterministic\ndata-driven prognosis (DDP) is proposed that only relies on in situ\nmeasurements of data and estimates the failure based on the curvature\ninformation extracted from the system. Unlike traditional applications that\nrequire explicit expression of conservation principle, the proposed method\ndevices a local conservation functional in the neighborhood of each data point\nwhich is represented as the minimization of curvature in the system. By\neliminating the need for offline training, the method can predict the onset of\ninstability for a variety of systems over a prediction horizon. The prediction\nhorizon to prognosticate the instability, alternatively, is considered as the\nremaining useful life (RUL) metric. The framework is then employed to analyze\nthe health status of Li-ion batteries. Based on the results, it has\ndemonstrated that the DDP technique can accurately predict the onset of failure\nof Li-ion batteries.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.01236v1"
    },
    {
        "title": "Boundary Conditions for the Parametric Kalman Filter forecast submited",
        "authors": [
            "M. Sabathier",
            "O. Pannekoucke",
            "V. Maget",
            "N. Dahmen"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  This paper is a contribution to the exploration of the parametric Kalman\nfilter (PKF), which is an approximation of the Kalman filter, where the error\ncovariances are approximated by a covariance model. Here we focus on the\ncovariance model parameterized from the variance and the anisotropy of the\nlocal correlations, and whose parameters dynamics provides a proxy for the full\nerror-covariance dynamics. For this covariance model, we aim to provide the\nboundary condition to specify in the prediction of PKF for bounded domains,\nfocusing on Dirichlet and Neumann conditions when they are prescribed for the\nphysical dynamics. An ensemble validation is proposed for the transport\nequation and for the heterogeneous diffusion equation over a bounded 1D domain.\nThis ensemble validation requires to specify the auto-correlation time-scale\nneeded to populate boundary perturbation that leads to prescribed uncertainty\ncharacteristics. The numerical simulations show that the PKF is able to\nreproduce the uncertainty diagnosed from the ensemble of forecast appropriately\nperturbed on the boundaries, which show the ability of the PKF to handle\nboundaries in the prediction of the uncertainties. It results that Dirichlet\ncondition on the physical dynamics implies Dirichlet condition on the variance\nand on the anisotropy. Codes are available at\nhttps://github.com/opannekoucke/pkf-boundary\n",
        "pdf_link": "http://arxiv.org/pdf/2212.10921v2"
    },
    {
        "title": "Comparative statistical study of two local clustering coefficient\n  formulations as tropical cyclone markers for climate networks",
        "authors": [
            "Mikhail Krivonosov",
            "Olga Vershinina",
            "Anna Pirova",
            "Shraddha Gupta",
            "Oleg Kanakov",
            "Jürgen Kurths"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We introduce a new formulation of local clustering coefficient for weighted\ncorrelation networks. This new formulation is based upon a definition\nintroduced previously in the neuroscience context and aimed at compensating for\nspurious correlations caused by indirect interactions. We modify this\ndefinition further by replacing Pearson's pairwise correlation coefficients and\nthree-way partial correlation coefficients by the respective Kendall's rank\ncorrelations. This reduces statistical sample size requirements to compute the\ncorrelations, which translates into the possibility of using shorter time\nwindows and hence into a shorter response time of the real-time climate network\nanalysis. We construct evolving climate networks of mean sea level pressure\nfluctuations and analyze anomalies of local clustering coefficient in these\nnetworks. We develop a broadly applicable statistical methodology to study\nassociation between spatially inhomogeneous georeferenced multivariate time\nseries and binary-valued spatiotemporal data (or other data reducible to this\nrepresentation) and use it to compare the newly proposed formulation of local\nclustering coefficient (for weighted correlation networks) to the conventional\none (for unweighted graphs) in terms of the association of these measures in\nclimate networks to tropical cyclones. Thus we substantiate the previously made\nobservation that tropical cyclones are associated with anomalously high values\nof local clustering coefficient, and confirm that the new formulation shows a\nstronger association.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.13934v1"
    },
    {
        "title": "Time series analysis using persistent homology of distance matrix",
        "authors": [
            "Takashi Ichinomiya"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The analysis of nonlinear dynamics is an important issue in numerous fields\nof science. In this study, we propose a new method to analyze the time series\ndata using persistent homology (PH). The key idea is the application of PH to\nthe distance matrix. Using this method, we can obtain the topological features\nembedded in the trajectories. We apply this method to the logistic map,\nR\\\"ossler system, and electrocardiogram data. The results reveal that our\nmethod can effectively identify nonlocal characteristics of the attractor and\ncan classify data based on the amount of noise.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.03369v1"
    },
    {
        "title": "pySODM: Simulating and Optimizing Dynamical Models in Python 3",
        "authors": [
            "Tijs W. Alleman",
            "Christian Stevens",
            "Jan M. Baetens"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  In this work, we present our generic framework to construct, simulate, and\ncalibrate dynamical systems in Python 3. Its goal is to reduce the time it\ntakes to implement a dynamical system with $n$-dimensional states represented\nby coupled ordinary differential equations (ODEs), simulate the system\ndeterministically or stochastically, and, calibrate the system using\nn-dimensional data. We demonstrate our code's capabilities by building three\nmodels in the context of two case studies. First, we forecast the yields of the\nenzymatic esterification reaction of D-glucose and lauric acid, performed in a\ncontinuous-flow, packed-bed reactor. The model yields a satisfactory\ndescription of the reaction yields under different flow rates and can be\napplied to design a viable process. Second, we build a stochastic,\nage-stratified model to make forecasts on the evolution of influenza in Belgium\nduring the 2017-2018 season. Using only limited data, our simple model was able\nto make a fairly accurate assessment of the future course of the epidemic. By\npresenting real-world case studies from two scientific disciplines, we\ndemonstrate our code's applicability across domains.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.10664v3"
    },
    {
        "title": "Complexity and chaotic behavior of the U.S. rivers and estimation of\n  their prediction horizon",
        "authors": [
            "Dragutin T. Mihailovic",
            "Slavica Malinovic-Milićevic",
            "Jeongwoo Hanc",
            "Vijay P. Singh"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  A streamflow time series encompasses a large amount of hidden information and\nreliable prediction of its behavior in the future remains a challenge. It seems\nthat the use of information measures can significantly contribute to\ndetermining the time horizon of rivers and improving predictability. Using the\nKolmogorov complexity (KC) and its derivatives (KC spectrum and its highest\nvalue), and Lyapunov exponent (LE), it has previously been shown that the\ndegree of streamflow predictability depends on human activities, environmental\nfactors, and natural characteristics. This paper applied the KC and LE measures\nto investigate the randomness and chaotic behavior of monthly streamflow of\n1879 rivers from the United States for a period from 1950 to 2015 and evaluated\ntheir time horizons via the Lyapunov and Kolmogorov time (LT and KT,\nrespectively).\n",
        "pdf_link": "http://arxiv.org/pdf/2301.11983v1"
    },
    {
        "title": "Emergence of extreme events in a quasi-periodic oscillator",
        "authors": [
            "Premraj Durairaj",
            "Sathiyadevi Kanagaraj",
            "Suresh Kumarasamy",
            "Karthikeyan Rajagopal"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Extreme events are unusual and rare large-amplitude fluctuations that occur\ncan unexpectedly in nonlinear dynamical systems. Events above the extreme event\nthreshold of the probability distribution of a nonlinear process characterize\nextreme events. Different mechanisms for the generation of extreme events and\ntheir prediction measures have been reported in the literature. Based on the\nproperties of extreme events, such as rare in frequency of occurrence and\nextreme in amplitude, various studies have shown that extreme events are both\nlinear and nonlinear in nature. Interestingly, in this work, we report on a\nspecial class of extreme events which are nonchaotic and nonperiodic. These\nnonchaotic extreme events appear in between the quasi-periodic and chaotic\ndynamics of the system. We report the existence of such extreme events with\nvarious statistical measures and characterization techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.13386v1"
    },
    {
        "title": "Pooling Probability Distributions and the Partial Information\n  Decomposition",
        "authors": [
            "Steven J. van Enk"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Notwithstanding various attempts to construct a Partial Information\nDecomposition (PID) for multiple variables by defining synergistic, redundant,\nand unique information, there is no consensus on how one ought to precisely\ndefine either of these quantities. One aim here is to illustrate how that\nambiguity -- or, more positively, freedom of choice -- may arise. Using the\nbasic idea that information equals the average reduction in uncertainty when\ngoing from an initial to a final probability distribution, synergistic\ninformation will likewise be defined as a difference between two entropies. One\nterm is uncontroversial and characterizes ``the whole'' information that source\nvariables carry jointly about a target variable $\\varT$. The other term then is\nmeant to characterize the information carried by the ``sum of its parts.'' Here\nwe interpret that concept as needing a suitable probability distribution\naggregated (``pooled'') from multiple marginal distributions (the parts).\nAmbiguity arises in the definition of the optimum way to pool two (or more)\nprobability distributions. Independent of the exact definition of optimum\npooling, the concept of pooling leads to a lattice that differs from the\noften-used redundancy-based lattice. One can associate not just a number (an\naverage entropy) with each node of the lattice, but (pooled) probability\ndistributions. As an example,one simple and reasonable approach to pooling is\npresented, which naturally gives rise to the overlap between different\nprobability distributions as being a crucial quantity that characterizes both\nsynergistic and unique information.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.02251v2"
    },
    {
        "title": "Markov-modulated model for landing flow dynamics: An ordinal analysis\n  validation",
        "authors": [
            "Felipe Olivares",
            "Luciano Zunino",
            "Massimiliano Zanin"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Air transportation is a complex system characterised by a plethora of\ninteractions at multiple temporal and spatial scales; as a consequence, even\nsimple dynamics like sequencing aircraft for landing can lead to the appearance\nof emergent behaviours, which are both difficult to control and detrimental to\noperational efficiency. We propose a model, based on a modulated Markov jitter,\nto represent ordinal pattern properties of real landing operations in European\nairports. The parameters of the model are tuned by minimising the distance\nbetween the probability distributions of ordinal patterns generated by the real\nand synthetic sequences, as estimated by the Permutation Jensen-Shannon\nDistance. We show that the correlation between consecutive hours in the landing\nflow changes between airports, and that it can be interpreted as a metric of\nefficiency. We further compare the dynamics pre and post COVID-19, showing how\nthis has changed beyond what can be attributed to a simple reduction of\ntraffic. We finally draw some operational conclusions, and discuss the\napplicability of these findings in a real operational environment.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.02702v1"
    },
    {
        "title": "Proving new physics by measuring cosmic ray fluxes",
        "authors": [
            "A. Chilingarian",
            "G. Hovsepyan"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The paper aims to demonstrate how the measurements of different species of\ncosmic ray flux can lead to a meaningful physical inference. We want to show\nwhen and how it is possible to path the way from measurement to physical\ninference and how we can prove that measurements are not artifacts or equipment\nfailures but manifestations of a new physical phenomenon.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.02824v1"
    },
    {
        "title": "Interpolated inverse discrete wavelet transforms in additive and\n  non-additive spectral background correction",
        "authors": [
            "Teemu Härkönen",
            "Erik Vartiainen"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  We demonstrate the applicability of using interpolated inverse discrete\nwavelet transforms as a general tool for modeling additive or multiplicative\nbackground or error signals in spectra. Additionally, we propose an\nunsupervised way of estimating the optimal wavelet basis along with the model\nparameters. We apply the method to experimental Raman spectra of phthalocyanine\nblue, aniline black, naphthol red, pigment yellow 150, and pigment red 264\npigments to remove their additive background and to CARS spectra of adenosine\nphosphate, fructose, glucose, and sucrose to remove their multiplicative\nbackground signals.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.08721v1"
    },
    {
        "title": "Understanding the complex dynamics of climate change in south-west\n  Australia using Machine Learning",
        "authors": [
            "Alka Yadav",
            "Sourish Das",
            "K Shuvo Bakar",
            "Anirban Chakraborti"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The Standardized Precipitation Index (SPI) is used to indicate the\nmeteorological drought situation - a negative (or positive) value of SPI would\nimply a dry (or wet) condition in a region over a period. The climate system is\nan excellent example of a complex system since there is an interplay and\ninter-relation of several climate variables. It is not always easy to identify\nthe factors that may influence the SPI, or their inter-relations (including\nfeedback loops). Here, we aim to study the complex dynamics that SPI has with\nthe SST, NINO 3.4 and Indian Ocean Dipole (IOD), using a machine learning\napproach. Our findings are: (i) IOD was negatively correlated to SPI till 2008;\n(ii) until 2004, SST was negatively correlated with SPI; (iii) from 2005 to\n2014, the SST had swung between negative and positive correlations; (iv) since\n2014, we observed that the regression coefficient ($\\delta$) corresponding to\nSST has always been positive; (v) the SST has an upward trend, and the positive\nupward trend of $\\delta$ implied that SPI has been positively correlated with\nSST in recent years; and finally, (vi) the current value of SPI has a\nsignificant positive correlation with a past SPI value with a periodicity of\nabout 7.5 years. Examining the complex dynamics, we used a statistical machine\nlearning approach to construct an inferential network of these climate\nvariables, which revealed that SST and NINO 3.4 directly couples with SPI,\nwhereas IOD indirectly couples with SPI through SST and NINO 3.4. The system\nalso indicated that Nino 3.4 has a significant negative effect on SPI.\nInterestingly, there seems to be a structural change in the complex dynamics of\nthe four climate variables, some time in 2008. Though a simple 12-month moving\naverage of SPI has a negative trend towards drought, the complex dynamics of\nSPI with other climate variables indicate a wet season for western Australia.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.11465v1"
    },
    {
        "title": "Analytic error function and numeric inverse obtained by geometric means",
        "authors": [
            "Dmitri Martila",
            "Stefan Groote"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Using geometric considerations, we provide a clear derivation of the integral\nrepresentation for the error function, known as the Craig formula. We calculate\nthe corresponding power series expansion and prove the convergence. The same\ngeometric means finally help to systematically derive handy formulas that\napproximate the inverse error function. Our approach can be used for\napplications in e.g.\\ high-speed Monte Carlo simulations where this function is\nused extensively.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.12639v3"
    },
    {
        "title": "Extreme events prediction from nonlocal partial information in a\n  spatiotemporally chaotic microcavity laser",
        "authors": [
            "V. A Pammi",
            "M. G. Clerc",
            "S. Coulibaly",
            "S. Barbay"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The forecasting of high-dimensional, spatiotemporal nonlinear systems has\nmade tremendous progress with the advent of model-free machine learning\ntechniques. However, in real systems it is not always possible to have all the\ninformation needed; only partial information is available for learning and\nforecasting. This can be due to insufficient temporal or spatial samplings, to\ninaccessible variables or to noisy training data. Here, we show that it is\nnevertheless possible to forecast extreme events occurrence in incomplete\nexperimental recordings from a spatiotemporally chaotic microcavity laser using\nreservoir computing. Selecting regions of maximum transfer entropy, we show\nthat it is possible to get higher forecasting accuracy using nonlocal data vs\nlocal data thus allowing greater warning times, at least twice the time horizon\npredicted from the nonlinear local Lyapunov exponent.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.03097v1"
    },
    {
        "title": "Enhancing Graph Topology and Clustering Quality: A Modularity-Guided\n  Approach",
        "authors": [
            "Yongyu Wang",
            "Shiqi Hao",
            "Xiaoyang Wang",
            "Xiaotian Zhuang"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Current modularity-based community detection algorithms attempt to find\ncluster memberships that maximize modularity within a fixed graph topology.\nDiverging from this conventional approach, our work introduces a novel strategy\nthat employs modularity to guide the enhancement of both graph topology and\nclustering quality through a maximization process. Specifically, we present a\nmodularity-guided approach for learning sparse graphs with high modularity by\niteratively pruning edges between distant clusters, informed by algorithmically\ngenerated clustering results. To validate the theoretical underpinnings of\nmodularity, we designed experiments that establish a quantitative relationship\nbetween modularity and clustering quality. Extensive experiments conducted on\nvarious real-world datasets demonstrate that our method significantly\noutperforms state-of-the-art graph construction methods in terms of clustering\naccuracy. Moreover, when compared to these leading methods, our approach\nachieves up to a hundredfold increase in graph construction efficiency on\nlarge-scale datasets, illustrating its potential for broad application in\ncomplex network analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.16103v2"
    },
    {
        "title": "Using various machine learning algorithms for quantitative analysis in\n  Laser induced breakdown spectroscopy",
        "authors": [
            "Mohsen Rezaei",
            "Fatemeh Rezaei",
            "Parvin Karimi"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Laser induced breakdown spectroscopy technique is employed for quantitative\nanalysis of aluminum samples by different classical machine learning\napproaches. A Q-switch Nd:YAG laser at fundamental harmonic of 1064 nm is\nutilized for creation of LIBS plasma for prediction of constituent\nconcentrations of the aluminum standard alloys. In current research,\nconcentration prediction is performed by linear approaches of support vector\nregression, multiple linear regression, principal component analysis integrated\nwith MLR and SVR, and as well as nonlinear algorithms of artificial neural\nnetwork, kernelized support vector regression , and the integration of\ntraditional principal component analysis with KSVR, and ANN. Furthermore,\ndimension reduction is applied on various methodologies by PCA algorithm for\nimproving the quantitative analysis. The results presented that the combination\nof PCA with KSVR algorithm model had the best efficiency in predictions of the\nmost of elements among other classical machine learning algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.07786v1"
    },
    {
        "title": "Apparent universality of $1/f$ spectra as an artifact of finite-size\n  effects",
        "authors": [
            "M. A. Korzeniowska",
            "A. Theodorsen",
            "M. Rypdal",
            "O. E. Garcia"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Power spectral density scaling with frequency $f$ as $1/f^\\beta$ and $\\beta\n\\approx 1$ is widely found in natural and socio-economic systems. Consequently,\nit has been suggested that such self-similar spectra reflect the universal\ndynamics of complex phenomena. Here, we show that for a superposition of\nuncorrelated pulses with a power-law distribution of duration times the\nestimated scaling exponents $\\bar{\\beta}$ depend on the system size. We derive\na parametrized, closed-form expression for the power spectral density, and\ndemonstrate that for $\\beta \\in [0,2]$ the estimated scaling exponents have a\nbias towards $\\bar{\\beta}=1$. For $\\beta=0$ and $\\beta=2$ the explicit\nlogarithmic corrections to frequency scaling are derived. The bias is\nparticularly strong when the scale invariance spans less than four decades in\nfrequency. Since this is the case for the majority of empirical data, the\nboundedness of systems well described by the superposition of uncorrelated\npulses may contribute to overemphasizing the universality of $1/f$.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.08371v3"
    },
    {
        "title": "Multifidelity uncertainty quantification with models based on dissimilar\n  parameters",
        "authors": [
            "Xiaoshu Zeng",
            "Gianluca Geraci",
            "Michael S. Eldred",
            "John D. Jakeman",
            "Alex A. Gorodetsky",
            "Roger Ghanem"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Multifidelity uncertainty quantification (MF UQ) sampling approaches have\nbeen shown to significantly reduce the variance of statistical estimators while\npreserving the bias of the highest-fidelity model, provided that the\nlow-fidelity models are well correlated. However, maintaining a high level of\ncorrelation can be challenging, especially when models depend on different\ninput uncertain parameters, which drastically reduces the correlation. Existing\nMF UQ approaches do not adequately address this issue. In this work, we propose\na new sampling strategy that exploits a shared space to improve the correlation\namong models with dissimilar parametrization. We achieve this by transforming\nthe original coordinates onto an auxiliary manifold using the adaptive basis\n(AB) method~\\cite{Tipireddy2014}. The AB method has two main benefits: (1) it\nprovides an effective tool to identify the low-dimensional manifold on which\neach model can be represented, and (2) it enables easy transformation of\npolynomial chaos representations from high- to low-dimensional spaces. This\nlatter feature is used to identify a shared manifold among models without\nrequiring additional evaluations. We present two algorithmic flavors of the new\nestimator to cover different analysis scenarios, including those with legacy\nand non-legacy high-fidelity data. We provide numerical results for analytical\nexamples, a direct field acoustic test, and a finite element model of a nuclear\nfuel assembly. For all examples, we compare the proposed strategy against both\nsingle-fidelity and MF estimators based on the original model parametrization.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.08644v1"
    },
    {
        "title": "Introducing the Single-Atom Real-Space Global Minimization Method for\n  Solving Small Structures in Single Crystal X-ray Crystallography",
        "authors": [
            "Xiaodong Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  A new method for solving small X-ray structures with up to couple of hundreds\nof atoms in the unit cell has been developed. The method works by locating\natoms one-by-one via global minimization of a newly defined single-atom R1\nfactor in real-space. In total forty test cases (of twenty samples), every\nresulting model has at least 52% atoms located correctly, and in more than half\nof the cases this percentage is 90% and higher. For most\nheavier-atoms-containing (heavier than Si) small structures it is sufficient to\nuse a single-solution strategy (using the as-collected reflection intensities).\nBut for most lighter-atoms-only (C, N, O and F) small structures it is\nnecessary to try multi-solution strategy: try different B values for modifying\nthe reflection intensities by multiplying them with exp(B s2) before feeding to\nthe method, where s = sin{\\theta}/{\\lambda}, and pick the best solution. This\nmethod has been demonstrated to be very effective for locating heavier atoms,\nso, it has potential applications in macromolecular crystallography by\nassisting determination of heavy-atom substructures.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09706v3"
    },
    {
        "title": "Higher-order asymptotic corrections and their application to the Gamma\n  Variance Model",
        "authors": [
            "Enzo Canonero",
            "Alessandra Rosalba Brazzale",
            "Glen Cowan"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  We present improved methods for calculating confidence intervals and\n$p$-values in situations where standard asymptotic approaches fail due to small\nsample sizes. We apply these techniques to a specific class of statistical\nmodel that can incorporate uncertainties in parameters that themselves\nrepresent uncertainties (informally, \"errors on errors\") called the Gamma\nVariance Model. This model contains fixed parameters, generically called\n$\\varepsilon$, that represent the relative uncertainties in estimates of\nstandard deviations of Gaussian distributed measurements. If the $\\varepsilon$\nparameters are small, one can construct confidence intervals and $p$-values\nusing standard asymptotic methods. This is formally similar to the familiar\nsituation of a large data sample, in which estimators for all adjustable\nparameters have Gaussian distributions. Here we address the important case\nwhere the $\\varepsilon$ parameters are not small and as a consequence the\nasymptotic distributions do not represent a good approximation. We investigate\nimproved test statistics based on the technology of higher-order asymptotics\n($p^*$ approximation and Bartlett correction).\n",
        "pdf_link": "http://arxiv.org/pdf/2304.10574v3"
    },
    {
        "title": "Analysing the Grain size and asymmetry of the particle distribution\n  using auto-correlation technique",
        "authors": [
            "Vanitha Patnala",
            "Salla Gangi Reddy",
            "Shashi Prabhakar",
            "R. P. Singh",
            "Venkateswarlu Annapureddy"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Extracting the grain size from the microscopic images is a rigorous task\ninvolving much human expertise and manual effort. While calculating the grain\nsize, we will be utilizing a finite number of particles which may lead to an\nuncertainty in the measurement. To avoid this difficulty, we utilize a simple\nmathematical tool, the auto-correlation function, to determine the grain size.\nThe random particle distribution and the finite width Gaussian histogram of\nparticle size has motivated us to utilize the auto-correlation function, which\nhas been extensively studied for finding the size of random optical patterns.\nThe finite width of the correlation function provides the grain size, and the\ndifference in correlation length along two mutually independent directions\nprovides information about the asymmetry present in the particle distribution,\ni.e., the deviation from a spherical shape. The results may find applications\nin material, pharmaceutical, chemical, and biological studies where extracting\nthe grain size is essential.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03734v1"
    },
    {
        "title": "Extension of the Langevin power curve analysis by separation per\n  operational state",
        "authors": [
            "Christian Wiedemann",
            "Henrik M. Bette",
            "Matthias Wächter",
            "Jan A. Freund",
            "Thomas Guhr",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  In the last few years, the dynamical characterization of the power output of\na wind turbine by means of a Langevin equation has been well established. For\nthis approach, temporally highly resolved measurements of wind speed and power\noutput are used to obtain the drift and diffusion coefficients of the energy\nconversion process. These coefficients fully determine a Langevin stochastic\ndifferential equation with Gaussian white noise. The drift term specifies the\ndeterministic behavior of the system whereas the diffusion term describes the\nstochastic behavior of the system. A precise estimation of these coefficients\nis essential to understand the dynamics of the power conversion process of the\nwind turbine. We show that the dynamics of the power output of a wind turbine\nhave a hidden dependency on turbine's different operational states. Here, we\nuse an approach based on clustering Pearson correlation matrices for different\nobservables on a moving time window to identify different operational states.\nWe have identified five operational states in total, for example the state of\nrated power. Those different operational states distinguish non-stationary\nbehavior in the mutual dependencies and represent different turbine control\nsettings. As a next step, we condition our Langevin analysis on these different\nstates to reveal distinctly different behaviors of the power conversion process\nfor each operational state. Moreover, in our new representation hysteresis\neffects which have typically appeared in the Langevin dynamics of wind turbines\nseem to be resolved. We assign these typically observed hysteresis effects\nclearly to the change of the wind energy system between our estimated different\noperational states. In this contribution, we discuss further consequences for\nthe meaning of hysteric switching and detection of malbehaviors in wind\nturbines.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.15512v4"
    },
    {
        "title": "Forecast modeling a time series of water reservoir levels using\n  exponential smoothing method",
        "authors": [
            "Lydiane F. Souza"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Exponential smoothing is a time series forecasting method that presents the\nforecast based on trend and seasonality components. In this work, we study the\nbehavior of two time series that describe the level of the water reservoirs of\nthe Descoberto and Santa Maria dams. We trained the fifteen models present in\nthe Pregels taxonomy, the criterion for choosing the model consists of the\nmodel with the lowest Akaike information criterion. The results indicate that\nthe exponential smoothing model with damped additive trend and additive\nseasonality best describes both time series.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.05199v1"
    },
    {
        "title": "Statistical analysis of event classification in experimental data",
        "authors": [
            "Rudolf Frühwirth",
            "Winfried Mitaroff"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The paper addresses general aspects of experimental data analysis, dealing\nwith the separation of ``signal vs. background''. It consists of two parts.\n  Part I is a tutorial on statistical event classification, Bayesian inference,\nand test optimization. Aspects of the base data sample if being created by\nPoisson processes are discussed, and a method for estimating the unknown\nnumbers of signal and background events is presented. Data quality of the\nselected events sample is assessed by the expected purity and background\ncontamination.\n  Part II contains a rigorous statistical analysis of the methods discussed in\nPart I. Both Bayesian and frequentist estimators of the unknown\nsignal/background content are investigated. The estimates and their stochastic\nuncertainties are calculated for various conjugate priors in the Bayesian case,\nand for three choices of the virtual parent population in the frequentist case.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.16566v1"
    },
    {
        "title": "A statistical learning framework for mapping indirect measurements of\n  ergodic systems to emergent properties",
        "authors": [
            "Nicholas Hindley",
            "Stephen J. DeVience",
            "Ella Zhang",
            "Leo L. Cheng",
            "Matthew S. Rosen"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The discovery of novel experimental techniques often lags behind contemporary\ntheoretical understanding. In particular, it can be difficult to establish\nappropriate measurement protocols without analytic descriptions of the\nunderlying system-of-interest. Here we propose a statistical learning framework\nthat avoids the need for such descriptions for ergodic systems. We validate\nthis framework by using Monte Carlo simulation and deep neural networks to\nlearn a mapping between low-field nuclear magnetic resonance spectra and proton\nexchange rates in ethanol-water mixtures. We found that trained networks\nexhibited normalized-root-mean-square errors of less than 1% for exchange rates\nunder 150 s-1 but performed poorly for rates above this range. This\ndifferential performance occurred because low-field measurements are\nindistinguishable from one another at fast exchange. Nonetheless, where a\ndiscoverable relationship between indirect measurements and emergent dynamics\nexists, we demonstrate the possibility of approximating it without the need for\nprecise analytic descriptions, allowing experimental science to flourish in the\nmidst of ongoing theoretical work\n",
        "pdf_link": "http://arxiv.org/pdf/2307.10494v1"
    },
    {
        "title": "Statistical characterization of residual noise in the low-rank\n  approximation filter framework, general theory and application to\n  hyperpolarized tracer spectroscopy",
        "authors": [
            "R. Francischello",
            "M. F. Santarelli",
            "A. Flori",
            "L. Menichetti",
            "M. Geppi"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The use of low-rank approximation filters in the field of NMR is increasing\ndue to their flexibility and effectiveness. Despite their ability to reduce the\nMean Square Error between the processed signal and the true signal is well\nknown, the statistical distribution of the residual noise is still undescribed.\nIn this article, we show that low-rank approximation filters are equivalent to\nlinear filters, and we calculate the mean and the covariance matrix of the\nprocessed data. We also show how to use this knowledge to build a maximum\nlikelihood estimator, and we test the estimator's performance with a Montecarlo\nsimulation of a 13C pyruvate metabolic tracer. While the article focuses on NMR\nspectroscopy experiment with hyperpolarized tracer, we also show that the\nresults can be applied to tensorial data (e.g. using HOSVD) or 1D data (e.g.\nCadzow filter).\n",
        "pdf_link": "http://arxiv.org/pdf/2307.12352v2"
    },
    {
        "title": "Information temperature as a parameter of random sequence complexity",
        "authors": [
            "O. V. Usatenko",
            "G. M. Pritula"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  In this study, we continue our exploration of the concept of information\ntemperature as a characteristic of random sequences. We describe methods for\nintroducing the information temperature in the context of binary high-order\nMarkov chain with step-wise memory and investigate the application of the\ntemperature as a parameter of the sequence complexity. We aim to define\ncomplexity based on the derivative of entropy with respect to information\ntemperature, drawing an analogy to thermodynamic heat capacity. The maximum\ncomplexity of a random sequence is achieved when its information \"heat\ncapacity\" approaches its highest possible value, which is directly influenced\nby the sequence memory depth. We also discuss the potential of utilizing\ninformation temperature as an indicator of the intellectual level exhibited by\nany text-generating agent.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.12841v1"
    },
    {
        "title": "Self-consistent autocorrelation for finite-area bias correction in\n  roughness measurement",
        "authors": [
            "David Nečas"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Scan line levelling, a ubiquitous and often necessary step in AFM data\nprocessing, can cause a severe bias on measured roughness parameters such as\nmean square roughness or correlation length. Although bias estimates have been\nformulated, they aimed mainly at assessing the severity of the problem for\nindividual measurements. Practical bias correction methods are still missing.\nThis work exploits the observation that the bias of autocorrelation function\n(ACF) can be expressed in terms of the function itself, permitting a\nself-consistent formulation. From this two correction approaches are developed,\nboth with the aim to obtain convenient formulae which can be easily applied in\npractice. The first modifies standard analytical models of ACF to incorporate,\nin expectation, the bias and thus actually match the data the models are used\nto fit. The second inverts the relation between true and estimated ACF to\nrealise a model-free correction. Both are tested using simulated and\nexperimental data and found effective.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.11304v3"
    },
    {
        "title": "Probabilistic Mixture Model-Based Spectral Unmixing",
        "authors": [
            "Oliver Hoidn",
            "Aashwin Mishra",
            "Apurva Mehta"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Identifying pure components in mixtures is a common yet challenging problem.\nThe associated unmixing process requires the pure components, also known as\nendmembers, to be sufficiently spectrally distinct. Even with this requirement\nmet, extracting the endmembers from a single mixture is impossible; an ensemble\nof mixtures with sufficient diversity is needed. Several spectral unmixing\napproaches have been proposed, many of which are connected to hyperspectral\nimaging. However, most of them assume highly diverse collections of mixtures\nand extremely low-loss spectroscopic measurements. Additionally, non-Bayesian\nframeworks do not incorporate the uncertainty inherent in unmixing. We propose\na probabilistic inference approach that explicitly incorporates noise and\nuncertainty, enabling us to unmix endmembers in collections of mixtures with\nlimited diversity. We use a Bayesian mixture model to jointly extract endmember\nspectra and mixing parameters while explicitly modeling observation noise and\nthe resulting inference uncertainties. We obtain approximate distributions over\nendmember coordinates for each set of observed spectra while remaining robust\nto inference biases from the lack of pure observations and presence of\nnon-isotropic Gaussian noise. Access to reliable uncertainties on the unmixing\nsolutions would enable robust solutions as well as informed decision making.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.13117v2"
    },
    {
        "title": "Neutron Scattering Cross-Section Correction Incorporating Neutron\n  Wavelength Effects",
        "authors": [
            "Karrie E. An",
            "Guan-Rong Huang",
            "Changwoo Do",
            "Wei-Ren Chen"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  This study outlines a numerical methodology aimed at rectifying the neutron\nscattering cross-sections of fundamental elements across a range of low neutron\nenergies typically employed in general neutron scattering experiments. By using\nthe experimental power law relationship governing the cross-section's\ndependence on neutron wavelength, we establish a mathematical connection\nbetween these two variables. Leveraging this relationship, the scheme of\ncentral moment expansion is adopted to correct the cross-sections that are\napplicable to general neutron wavelength distributions commonly encountered in\nexperimental scenarios. Importantly, our proposed method eliminates the\nrequirement for knowledge about the functional form of the distribution.\nConsequently, this approach offers the capability to reconstruct neutron\nscattering data without introducing distortions stemming from the\nenergy-dependent cross-sections of different types of elements within materials\nduring experimental measurements. Ultimately, this advancement facilitates a\nmore precise interpretation and analysis of material structures based on their\nscattering signatures.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.15343v1"
    },
    {
        "title": "Pattern detection in bipartite networks: a review of terminology,\n  applications and methods",
        "authors": [
            "Zachary Neal",
            "Annabel Cadieux",
            "Diego Garlaschelli",
            "Nicholas J. Gotelli",
            "Fabio Saracco",
            "Tiziano Squartini",
            "Shade T. Shutters",
            "Werner Ulrich",
            "Guanyang Wang",
            "Giovanni Strona"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Two dimensional matrices with binary (0/1) entries are a common data\nstructure in many research fields. Examples include ecology, economics,\nmathematics, physics, psychometrics and others. Because the columns and rows of\nthese matrices represent distinct entities, they can equivalently be expressed\nas a pair of bipartite networks that are linked by projection. A variety of\ndiversity statistics and network metrics can then be used to quantify patterns\nin these matrices and networks. But what should these patterns be compared to?\nIn all of these disciplines, researchers have recognized the necessity of\ncomparing an empirical matrix to a benchmark set of \"null\" matrices created by\nrandomizing certain elements of the original data. This common need has\nnevertheless promoted the independent development of methodologies by\nresearchers who come from different backgrounds and use different terminology.\nHere, we provide a multidisciplinary review of randomization techniques for\nmatrices representing binary, bipartite networks. We aim to translate the\nconcepts from different technical domains into a common language that is\naccessible to a broad scientific audience. Specifically, after briefly\nreviewing examples of binary matrix structures across different fields, we\nintroduce the major approaches and common strategies for randomizing these\nmatrices. We then explore the details of and performance of specific\ntechniques, and discuss their limitations and computational challenges. In\nparticular, we focus on the conceptual importance and implementation of\nstructural constraints on the randomization, such as preserving row or columns\nsums of the original matrix in each of the randomized matrices. Our review\nserves both as a guide for empiricists in different disciplines, as well as a\nreference point for researchers working on theoretical and methodological\ndevelopments in matrix randomization methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.01284v1"
    },
    {
        "title": "Uncertainty of short-term Wind Power Forecasts -- A methodology for\n  on-line Assessment",
        "authors": [
            "Georges Kariniotakis",
            "Pierre Pinson"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The paper introduces a new methodology for assessing on-line the prediction\nrisk of short-term wind power forecasts. The first part of this methodology\nconsists in computing confidence intervals with a confidence level defined by\nthe end-user. The resampling approach is used for this purpose since it permits\nto avoid a restrictive hypothesis on the distribution of the errors. It has\nbeen however appropriately adapted for the wind power prediction problem taking\ninto account the dependency of the errors on the level of predicted power\nthrough appropriately defined fuzzy sets. The second part of the proposed\nmethodology introduces two indices, named as MRI and PRI, that quantify the\nmeteorological risk by measuring the spread of multi-scenario Numerical Weather\nPredictions and wind power predictions respectively. The multi-scenario\nforecasts considered here are based on the 'poor man's' ensembles approach. The\ntwo indices are used either to fine-tune the confidence intervals or to give\nsignals to the operator on the prediction risk, i.e. the probabilities for the\noccurrence of high prediction errors depending on the weather stability. A\nrelation between these indices and the level of prediction error is shown.\nEvaluation results over a three-year period on the case of a wind farm in\nDenmark and over a one-year period on the case of several farms in Ireland are\ngiven. The proposed methodology has an operational nature and can be applied to\nall kinds of wind power forecasting models\n",
        "pdf_link": "http://arxiv.org/pdf/2310.02618v1"
    },
    {
        "title": "Simulation and Analysis of Two Toy Models",
        "authors": [
            "Yifan Zhang",
            "Qing Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The matching problem and the distribution law of Galton boards with\ninteractions are studied in this paper. The general matching problem appeals at\nmany scenarios, such as the reaction rate of molecules and the hailing rate of\nride-hailing drivers. The Galton board is often used in the classroom as a\ndemonstration experiment for the probability distribution of independent\nevents. The two problems are mathematically modeled and numerically simulated.\nThe expected value of matching rate is derived as an analytical solution of the\npartial differential equation and confirmed by simulation experiments. The\ninteractions were introduced to Galton boards via two parameters in the toy\nmodel, which lead to Gaussian distributions of independent events cannot fit\nthe experimental data well. Instead, 'quantum' Fermi-Dirac distributions\nunexpectedly conforms to simulation experiments. The exclusivity between\nparticles leads to negative Chemical potential in the distribution function,\nand the temperature parameter increases with the interaction intensity $\\alpha$\nand flow rate $N_{sm}$. The relations between parameters can be expressed as a\nconjecture formula within large parameters range.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.02646v1"
    },
    {
        "title": "Improved treatment of the $T_2$ molecular final-states uncertainties for\n  the KATRIN neutrino-mass measurement",
        "authors": [
            "S. Schneidewind",
            "J. Schürmann",
            "A. Lokhov",
            "C. Weinheimer",
            "A. Saenz"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The KArlsruhe TRItium Neutrino experiment (KATRIN) aims to determine the\neffective mass of the electron antineutrino via a high-precision measurement of\nthe tritium beta-decay spectrum in its end-point region. The target\nneutrino-mass sensitivity of 0.2 eV / c^2 at 90% C.L. can only be achieved in\nthe case of high statistics and a good control of the systematic uncertainties.\nOne key systematic effect originates from the calculation of the molecular\nfinal states of T_2 beta decay. In the first neutrino-mass analyses of KATRIN\nthe contribution of the uncertainty of the molecular final-states distribution\n(FSD) was estimated via a conservative phenomenological approach to be 0.02\neV^2 / c^4. In this paper a new procedure is presented for estimating the\nFSD-related uncertainties by considering the details of the final-states\ncalculation, i.e. the uncertainties of constants, parameters, and functions\nused in the calculation as well as its convergence itself as a function of the\nbasis-set size used in expanding the molecular wave functions. The calculated\nuncertainties are directly propagated into the experimental observable, the\nsquared neutrino mass m_nu^2. With the new procedure the FSD-related\nuncertainty is constrained to 0.0013 eV^2 / c^4, for the experimental\nconditions of the first KATRIN measurement campaign.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.12634v1"
    },
    {
        "title": "Minimum Connected Dominating Set and Backbone of a Random Graph",
        "authors": [
            "Yusupjan Habibulla",
            "Hai-Jun Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  We study the minimum dominating set problem as a representative combinatorial\noptimization challenge with a global topological constraint. The requirement\nthat the backbone induced by the vertices of a dominating set should be a\nconnected subgraph makes the problem rather nontrivial to investigate by\nstatistical physics methods. Here we convert this global connectivity\nconstraint into a set of local vertex constraints and build a spin glass model\nwith only five coarse-grained vertex states. We derive a set of coarse-grained\nbelief-propagation equations and obtain theoretical predictions on the relative\nsizes of minimum dominating sets for regular random and Erd\\\"os-R\\'enyi random\ngraph ensembles. We also implement an efficient message-passing algorithm to\nconstruct close-to-minimum connected dominating sets and backbone subgraphs for\nsingle random graph instances. Our theoretical strategy may also be inspiring\nfor some other global topological constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.15980v1"
    },
    {
        "title": "A quick algorithm to compute an approximated power spectral density from\n  an arbitrary Allan deviation",
        "authors": [
            "Fabrizio De Marchi",
            "Michael K. Plumaris",
            "Eric A. Burt",
            "Luciano Iess"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Complex architectures for wireless communications, digital electronics and\nspace-based navigation interlink several oscillator-based devices such as\nclocks, transponders and synthesizers. Estimators characterizing their\nstability are critical for addressing the impact of random fluctuations (noise)\non the overall system performance. Manufacturers typically specify this as an\nAllan/Hadamard Variance (AVAR/HVAR) profile in the _integration_ time domain,\nyet, stochastic processes governing the noise take place in the _Fourier_\nfrequency domain in the shape of a Power Spectral Density (PSD) function. Both\nare second-moment measures of the time series, however, it is only possible to\ntranslate unambiguously from the PSD to the AVAR/HVAR, not vice versa, except\nin the case of a single noise type, which is severely limiting in real-life\napplications. This note elaborates an analytical method to generate an\napproximated PSD expressed as a set of power-laws defined in specific intervals\nin the frequency domain, starting from an AVAR/HVAR expressed a set of\npower-laws in the time domain. The proposed algorithm is straightforward to\nimplement, applicable to all noise types (and combinations thereof) and can be\nself-validated by reconstructing the corresponding AVAR/HVAR by direct\ncalculus. We also report on its limitations of and analytical expressions of\nthe continuous version of this algorithm. Coupling with well-established\nalgorithms relying on the PSD for power-law noise generation, the ensuing\nmethod encompasses the capability for generating multi-colored noise in\nend-to-end simulations, as demonstrated hereby for NASA's Deep Space Atomic\nClock.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.00598v1"
    },
    {
        "title": "A sliding window-based algorithm for faster transformation of time\n  series into complex networks",
        "authors": [
            "R. Carmona-Cabezas",
            "J. Gomez-Gomez",
            "E. Gutierrez de Rave",
            "F. J. Jimenez-Hornero"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  A new alternative method to approximate the Visibility Graph (VG) of a time\nseries has been introduced here. It exploits the fact that most of the nodes in\nthe resulting network are not connected to those that are far away from them.\nThis means that the adjacency matrix is almost empty, and its nonzero values\nare close to the main diagonal. This new method is called Sliding Visibility\nGraph (SVG). Numerical tests have been performed for several time series,\nshowing a time efficiency that scales linearly with the size of the series\n[O(N)], in contrast to the original VG that does so quadratically [O(N2)]. This\nfact is noticeably convenient when dealing with very large time series. The\nresults obtained from the SVG of the studied time series have been compared to\nthe exact values of the original VG. As expected, the SVG outcomes converge\nvery rapidly to the desired ones, especially for random and stochastic series.\nAlso, this method can be extended to the analysis of time series that evolve in\nreal time, since it does not require the entire dataset to perform the analysis\nbut a shorter segment of it. The length segment can remain constant, making\npossible a simple analysis as the series evolves in time.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.10688v1"
    },
    {
        "title": "The Jaccard Similarity Mean",
        "authors": [
            "Gonzalo Travieso",
            "Luciando da F. Costa"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The arithmetic mean plays a central role in science and technology, being\ndirectly related to the concepts of statistical expectance and centrality. Yet,\nit is highly susceptible to the presence of outliers or biased interference in\nthe original dataset to which it is applied. Described recently, the concept of\nsimilarity means has been preliminary found to have marked robustness to those\nsame effects, especially when adopting the Jaccard similarity index. The\npresent work is aimed at investigating further the properties of similarity\nmeans, especially regarding their range, translating and scaling properties,\nsensitivity and robustness to outliers. Several interesting contributions are\nreported, including an effective algorithm for obtaining the similarity mean,\nthe analytic and experimental identification of a number of properties, as well\nas the confirmation of the potential stability of the similarity mean to the\npresence of outliers. The present work also describes an application\ncase-example in which the Jaccard similarity is successfully employed to study\ncycles of sunspots, with interesting results.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.12959v2"
    },
    {
        "title": "Rotation-Invariant Rapid TRISO-Fueled Pebble Identification Based on\n  Feature Matching and Point Cloud Registration",
        "authors": [
            "Ming Fang",
            "Angela Di Fulvio"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Pebble bed reactor (PBR) relying on TRISO-fueled pebbles is one of the most\npromising Gen-IV reactor designs because of intrinsic safety and thermal\nefficiency. Fuel pebbles flow through PBR's core and the identification of\nindividual pebbles exiting the core will be beneficial to improve safeguards\nand fuel management. We propose a pebble identification method that is fast,\naccurate, robust, and applicable to PBRs containing hundreds of thousands of\npebbles. The identification relies on the internal distribution of TRISO fuel\nparticles, which is a unique feature of each pebble. We experimentally\ndemonstrated that X-ray CT can extract the particle distribution with high\naccuracy. We then applied the algorithm to identify a single pebble in a data\nset of 100,000 pebbles achieving 100% identification accuracy in 90,000 tests\nwith the presence of arbitrary rotations and measurement noises. The average\ntime to identify one pebble is below 50 s, compatible with PBR operation.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.02006v1"
    },
    {
        "title": "Application of deep learning to the estimation of normalization\n  coefficients in diffusion-based covariance models",
        "authors": [
            "Folke K Skrunes",
            "Mayeul Destouches",
            "Anthony Weaver",
            "Guillaume Coulaud",
            "Olivier Goux",
            "Corentin Lapeyre"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Variational data assimilation in ocean models depends on the ability to model\ngeneral correlation operators in the presence of coastlines. Grid-point filters\nbased on diffusion operators are widely used for this purpose, but come with a\ncomputational bottleneck - the costly estimation of normalization factors for\nevery model grid point. In this paper, we show that a simple convolutional\nneural network can effectively learn these normalization factors with better\naccuracy than the current operational methods. Our network is tested with a\ntwo-dimensional diffusion operator from the NEMOVAR ocean data assimilation\nsystem, applied to a global ocean grid with approximately one degree horizontal\nresolution. The network is trained on exact normalization factors estimated by\na brute-force method. Knowing that convolutional networks can only model\ntranslation-equivariant functions, we ensure that the normalization estimation\nproblem is indeed translation-equivariant. Specifically, we show how the number\nof inputs of this problem can be reduced while preserving translation\nequivariance. Adding the distance to the coastline as an input channel is found\nto improve the performance of the network around coastlines. Extensions to\nthree-dimensional diffusion and to higher horizontal resolutions are discussed.\nRemoving the computational bottleneck associated with normalization opens the\nway to using adaptive correlation models for operational ocean data\nassimilation. The code for this work is publicly available at\nhttps://github.com/FolkeKS/DL-normalization/tree/core-features\n",
        "pdf_link": "http://arxiv.org/pdf/2312.05068v1"
    },
    {
        "title": "Signal significance incorporating systematic uncertainty for continuous\n  test",
        "authors": [
            "Yi Ding",
            "Weiming Song",
            "Kai Zhu"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  To properly estimate signal significance while accounting for both\nstatistical and systematic uncertainties, we conducted a study to analyze the\nimpact of typical systematic uncertainties, such as background shape, signal\nshape, and the number of backgrounds, on significance calculation using the\ncontinuous test method. Our investigation reveals unexpected and complex\nfeatures, leading us to recommend a conservative approach: one should estimate\nsignal significance by conducting trials with as many as possible combinations\nof various uncertainties associated with the fitting procedure, and then select\nthe \"worst\" outcome as the final result.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.10643v1"
    },
    {
        "title": "Drawing a better understanding of flood quantiles from a bag",
        "authors": [
            "Christine Poulard",
            "Renard Benjamin",
            "Gonzalez-Sosa Enrique",
            "Caillouet Laurie"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The \"100-year flood\" is commonly used, for instance in newspapers, but flood\nhazard assessment is more complex than it seems. We first describe an animation\nentitled \"bag of floods\" to make flood quantiles more concrete, using marbles\nwhose colour corresponds to a class of return period. Discussing the analogies\nand differences between drawing a marble from the bag and the next annual flood\nmake it easier to explain that flood hazard assessment (i) must not be focussed\non the \"100-yr flood\", (ii) is often expressed as a probability over one given\nyear, but for planning it should be estimated over a much longer duration (like\nsuccessive draws from the bag) and (iii) variability is significant and\nmatters. Scripts allowing to simulate long series of draws confirm that\nempirical probabilities get close to theoretical probabilities, but also\nillustrate less intuitive results : on average one quarter of 100 successive\ndraws, contains two floods or more with a discharge exceeding the \"100-yr\ndischarge\". To go further, Sample2Gumbel is a teaching tool drawing annual\nmaxima discharges. It compares on a graph (i) a \"real distribution\", coded in\nthe script and used to draw a sample, (ii) the sample expressed with respect to\n\"plotting position\", expressed as a return period but which is in fact a crude\nestimation to allow plotting, (iii) the distribution fitted on the sample. This\ndemo tool illustrates the variability of different tries, with samples of the\nsame length, and shows how uncertainty evolves with the sample size. To improve\nit, more distributions could be included, and damage estimation could be added.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.11068v1"
    },
    {
        "title": "Model adaptive phase space reconstruction",
        "authors": [
            "Jayesh M. Dhadphale",
            "K. Hauke Kraemer",
            "Maximilian Gelbrecht",
            "Jürgen Kurths",
            "Norbert Marwan",
            "R. I. Sujith"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Phase space reconstruction (PSR) methods allow for the analysis of\nlow-dimensional data with methods from dynamical system theory, but their\napplication to prediction models, like those from machine learning (ML), is\nlimited. Therefore, we present here a model adaptive phase space reconstruction\n(MAPSR) method that unifies the process of PSR with the modeling of the\ndynamical system. MAPSR is a differentiable PSR method that enables the use of\nML methods and is based on the idea of time delay embedding. For achieving\ndifferentiable, continuous, real-valued delays, which can be optimized using\ngradient descent, the discrete time signal is converted to a continuous time\nsignal. The delay vector, which stores all potential embedding delays and the\ntrainable parameters of the model, are simultaneously updated to achieve an\noptimal time delay embedding for the observed system. MAPSR does not rely on\nany threshold or statistical criterion for determining the dimension and the\nset of delay values for the embedding process. The quality of the\nreconstruction is evaluated by the prediction loss. We apply the proposed\napproach to uni- and multivariate time series stemming from regular and chaotic\ndynamical systems and a turbulent combustor to test the generalizability of the\nmethod and compare our results with established PSR methods. We find that for\nthe Lorenz system, the model trained with the MAPSR method is able to predict\nchaotic time series for nearly 7 to 8 Lyapunov time scales, which is found to\nbe much better compared to other PSR methods (AMI-FNN and PECUZAL methods). For\nthe univariate time series from the turbulent combustor, the long-term\nprediction error of the model trained using the MAPSR method stays in between\nthat of AMI-FNN and PECUZAL methods for the regime of chaos, and for the regime\nof intermittency, the MAPSR method outperforms the AMI-FNN and PECUZAL methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.14189v1"
    },
    {
        "title": "Building a Life Cycle Assessment Model using Bayesian Networks",
        "authors": [
            "Cedric Fraces Gasmi",
            "Wennan Long"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  This paper introduces the Oilfield Pollutant Graphical Model (OPGM), an\ninnovative approach designed to improve the benchmarking and uncertainty\nanalysis of greenhouse gas (GHG) emissions in oilfields. Building on the robust\nfoundation provided by the Oil Production Greenhouse Gas Emission Estimator\n(OPGEE) framework, OPGM retains all essential functionalities of the latest\nOPGEE iteration (v3.0c), while offering substantial improvements in user\nexperience and computational performance. Key advances of OPGM include a\nstreamlined user interface for more intuitive interaction, which facilitates\ntransparent visualization of intermediate results and thus contributes to a\nmore interpretable and accessible analysis process. A notable feature of the\nOPGM is its ability to naturally perform sensitivity analyzes. This is achieved\nby allowing users to seamlessly transition nodes from deterministic to\nprobabilistic, thereby integrating uncertainty directly into the core structure\nof the model. OPGM achieves remarkable computational efficiency, executing\nanalyzes at a speed 1e+5 times faster than the Excel-based OPGEE, thus\nfacilitating rapid large-scale emissions assessments. This leap in processing\nspeed represents a significant step forward in emissions modeling, enabling\nmore agile and accurate environmental impact assessments. The integration of\nOPGM into existing Life Cycle Assessment (LCA) practices holds the promise of\nsignificantly improving the precision and speed of environmental impact\nanalyses, offering a vital tool for policymakers and industry stakeholders in\ntheir efforts to better understand and manage the environmental impacts of\noilfield operations.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.10170v1"
    },
    {
        "title": "\"Layer-by-layer\" Unsupervised Clustering of Statistically Relevant\n  Fluctuations in Noisy Time-series Data of Complex Dynamical Systems",
        "authors": [
            "Matteo Becchi",
            "Federico Fantolino",
            "Giovanni M. Pavan"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Complex systems are typically characterized by intricate internal dynamics\nthat are often hard to elucidate. Ideally, this requires methods that allow to\ndetect and classify in unsupervised way the microscopic dynamical events\noccurring in the system. However, decoupling statistically relevant\nfluctuations from the internal noise remains most often non-trivial. Here we\ndescribe \"Onion Clustering\": a simple, iterative unsupervised clustering method\nthat efficiently detects and classifies statistically relevant fluctuations in\nnoisy time-series data. We demonstrate its efficiency by analyzing simulation\nand experimental trajectories of various systems with complex internal\ndynamics, ranging from the atomic- to the microscopic-scale, in- and\nout-of-equilibrium. The method is based on an iterative detect-classify-archive\napproach. In similar way as peeling the external (evident) layer of an onion\nreveals the internal hidden ones, the method performs a first detection and\nclassification of the most populated dynamical environment in the system and of\nits characteristic noise. The signal of such dynamical cluster is then removed\nfrom the time-series data and the remaining part, cleared-out from its noise,\nis analyzed again. At every iteration, the detection of hidden dynamical\nsub-domains is facilitated by an increasing (and adaptive) relevance-to-noise\nratio. The process iterates until no new dynamical domains can be uncovered,\nrevealing, as an output, the number of clusters that can be effectively\ndistinguished/classified in statistically robust way as a function of the\ntime-resolution of the analysis. Onion Clustering is general and benefits from\nclear-cut physical interpretability. We expect that it will help analyzing a\nvariety of complex dynamical systems and time-series data.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.07786v5"
    },
    {
        "title": "End-to-End Analysis Automation over Distributed Resources with Luigi\n  Analysis Workflows",
        "authors": [
            "Marcel Rieger"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  In particle physics, workflow management systems are primarily used as\ntailored solutions in dedicated areas such as Monte Carlo production. However,\nphysicists performing data analyses are usually required to steer their\nindividual, complex workflows manually, frequently involving job submission in\nseveral stages and interaction with distributed storage systems by hand. This\nprocess is not only time-consuming and error-prone, but also leads to\nundocumented relations between particular workloads, rendering the steering of\nan analysis a serious challenge. This article presents the Luigi Analysis\nWorkflow (Law) Python package which is based on the open-source pipelining tool\nLuigi, originally developed by Spotify. It establishes a generic design pattern\nfor analyses of arbitrary scale and complexity, and shifts the focus from\nexecuting to defining the analysis logic. Law provides the building blocks to\nseamlessly integrate with interchangeable remote resources without, however,\nlimiting itself to a specific choice of infrastructure. In particular, it\nintroduces the concept of complete separation between analysis algorithms on\nthe one hand, and run locations, storage locations, and software environments\non the other hand. To cope with the sophisticated demands of end-to-end HEP\nanalyses, Law supports job execution on WLCG infrastructure (ARC, gLite,\nCMS-CRAB) as well as on local computing clusters (HTCondor, Slurm, LSF), remote\nfile access via various protocols using the Grid File Access Library (GFAL2),\nand an environment sandboxing mechanism with support for sub-shells and virtual\nenvironments, as well as Docker and Singularity containers. Moreover, the novel\napproach ultimately aims for analysis preservation out-of-the-box. Law is\ndeveloped open-source and independent of any experiment or the language of\nexecuted code, and its user-base increased steadily over the past years.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.17949v1"
    },
    {
        "title": "Characterizing unstructured data with the nearest neighbor permutation\n  entropy",
        "authors": [
            "Leonardo G. J. M. Voltarelli",
            "Arthur A. B. Pessa",
            "Luciano Zunino",
            "Rafael S. Zola",
            "Ervin K. Lenzi",
            "Matjaz Perc",
            "Haroldo V. Ribeiro"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Permutation entropy and its associated frameworks are remarkable examples of\nphysics-inspired techniques adept at processing complex and extensive datasets.\nDespite substantial progress in developing and applying these tools, their use\nhas been predominantly limited to structured datasets such as time series or\nimages. Here, we introduce the k-nearest neighbor permutation entropy, an\ninnovative extension of the permutation entropy tailored for unstructured data,\nirrespective of their spatial or temporal configuration and dimensionality. Our\napproach builds upon nearest neighbor graphs to establish neighborhood\nrelations and uses random walks to extract ordinal patterns and their\ndistribution, thereby defining the k-nearest neighbor permutation entropy. This\ntool not only adeptly identifies variations in patterns of unstructured data,\nbut also does so with a precision that significantly surpasses conventional\nmeasures such as spatial autocorrelation. Additionally, it provides a natural\napproach for incorporating amplitude information and time gaps when analyzing\ntime series or images, thus significantly enhancing its noise resilience and\npredictive capabilities compared to the usual permutation entropy. Our research\nsubstantially expands the applicability of ordinal methods to more general data\ntypes, opening promising research avenues for extending the permutation entropy\ntoolkit for unstructured data.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.13122v1"
    },
    {
        "title": "Distinguishing subsampled power laws from other heavy-tailed\n  distributions",
        "authors": [
            "Silja Sormunen",
            "Lasse Leskelä",
            "Jari Saramäki"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Distinguishing power-law distributions from other heavy-tailed distributions\nis challenging, and this task is often further complicated by subsampling\neffects. In this work, we evaluate the performance of two commonly used methods\nfor detecting power-law distributions - the maximum likelihood method of\nClauset et al. and the extreme value method of Voitalov et al. - in\ndistinguishing subsampled power laws from two other heavy-tailed distributions,\nthe lognormal and the stretched exponential distributions. We focus on a random\nsubsampling method commonly applied in network science and biological sciences.\nIn this subsampling scheme, we are ultimately interested in the frequency\ndistribution of elements with a certain number of constituent parts, and each\npart is selected to the subsample with an equal probability. We investigate how\nwell the results obtained from subsamples generalize to the original\ndistribution. Our results show that the power-law exponent of the original\ndistribution can be estimated fairly accurately from subsamples, but\nclassifying the distribution correctly is more challenging. The maximum\nlikelihood method falsely rejects the power-law hypothesis for a large fraction\nof subsamples from power-law distributions. While the extreme value method\ncorrectly recognizes subsampled power-law distributions with all tested\nsubsampling depths, its capacity to distinguish power laws from the\nheavy-tailed alternatives is limited. However, these false positives tend to\nresult not from the subsampling itself but from the estimators' inability to\nclassify the original sample correctly. In fact, we show that the extreme value\nmethod can sometimes be expected to perform better on subsamples than on the\noriginal samples from the lognormal and the stretched exponential\ndistributions, while the contrary is true for the main tests included in the\nmaximum likelihood method.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.09614v1"
    },
    {
        "title": "Temporal scaling theory for bursty time series with clusters of\n  arbitrarily many events",
        "authors": [
            "Hang-Hyun Jo",
            "Tibebe Birhanu",
            "Naoki Masuda"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Long-term temporal correlations in time series in a form of an event sequence\nhave been characterized using an autocorrelation function (ACF) that often\nshows a power-law decaying behavior. Such scaling behavior has been mainly\naccounted for by the heavy-tailed distribution of interevent times (IETs),\ni.e., the time interval between two consecutive events. Yet little is known\nabout how correlations between consecutive IETs systematically affect the\ndecaying behavior of the ACF. Empirical distributions of the burst size, which\nis the number of events in a cluster of events occurring in a short time\nwindow, often show heavy tails, implying that arbitrarily many consecutive IETs\nmay be correlated with each other. In the present study, we propose a model for\ngenerating a time series with arbitrary functional forms of IET and burst size\ndistributions. Then, we analytically derive the ACF for the model time series.\nIn particular, by assuming that the IET and burst size are power-law\ndistributed, we derive scaling relations between power-law exponents of the ACF\ndecay, IET distribution, and burst size distribution. These analytical results\nare confirmed by numerical simulations. Our approach helps to rigorously and\nanalytically understand the effects of correlations between arbitrarily many\nconsecutive IETs on the decaying behavior of the ACF.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.17093v1"
    },
    {
        "title": "Physics-based linear regression for high-dimensional forward uncertainty\n  quantification",
        "authors": [
            "Ziqi Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  We introduce linear regression using physics-based basis functions optimized\nthrough the geometry of an inner product space. This method addresses the\nchallenge of surrogate modeling with high-dimensional input, as the\nphysics-based basis functions encode problem-specific information. We\ndemonstrate the method using a proof-of-concept nonlinear random vibration\nexample.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.08006v2"
    },
    {
        "title": "Tracking Dynamical Transitions using Link Density of Recurrence Networks",
        "authors": [
            "Rinku Jacob",
            "R. Misra",
            "K P Harikrishnan",
            "G Ambika"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  We present Link Density (LD) computed from the Recurrence Network (RN) of a\ntime series data as an effective measure that can detect dynamical transitions\nin a system. We illustrate its use using time series from the standard Rossler\nsystem in the period doubling transitions and the transition to chaos.\nMoreover, we find that the standard deviation of LD can be more effective in\nhighlighting the transition points. We also consider the variations in data\nwhen the parameter of the system is varying due to internal or intrinsic\nperturbations but at a time scale much slower than that of the dynamics. In\nthis case also, the measure LD and its standard deviation correctly detect\ntransition points in the underlying dynamics of the system. The computation of\nLD requires minimal computing resources and time, and works well with short\ndata sets. Hence, we propose this measure as a tool to track transitions in\ndynamics from data, facilitating quicker and more effective analysis of large\nnumber of data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.19357v1"
    },
    {
        "title": "Uncertainties in ROC (Receiver Operating Characteristic) Curves Derived\n  from Counting Data",
        "authors": [
            "M. P. Fewell"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The ROC (receiver operating characteristic) curve is a widely used device for\nassessing decision-making systems. It seems surprising, in view of its history\ndating back to World War Two, that the assignment of uncertainties to a ROC\ncurve is apparently not settled. This note returns to the question, focusing on\nthe application of ROC curves to the analysis of data from counting experiments\nand taking a practical operational approach to the concept of uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.11396v2"
    },
    {
        "title": "A Bayesian Framework to Investigate Radiation Reaction in Strong Fields",
        "authors": [
            "E. E. Los",
            "C. Arran",
            "E. Gerstmayr",
            "M. J. V. Streeter",
            "Z. Najmudin",
            "C. P. Ridgers",
            "G. Sarri",
            "S. P. D Mangles"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Recent experiments aiming to measure phenomena predicted by strong field\nquantum electrodynamics have done so by colliding relativistic electron beams\nand high-power lasers. In such experiments, measurements of the collision\nparameters are not always feasible, however, precise knowledge of these\nparameters is required for accurate tests of strong-field quantum\nelectrodynamics. Here, we present a novel Bayesian inference procedure which\ninfers collision parameters that could not be measured on-shot. This procedure\nis applicable to all-optical non-linear Compton scattering experiments\ninvestigating radiation reaction. The framework allows multiple diagnostics to\nbe combined self-consistently and facilitates the inclusion of prior or known\ninformation pertaining to the collision parameters. Using this Bayesian\nanalysis, the relative validity of the classical, quantum-continuous and\nquantum-stochastic models of radiation reaction were compared for a series of\ntest cases, which demonstrate the accuracy and model selection capability of\nthe framework and and highlight its robustness in the event that the\nexperimental values of fixed parameters differ from their values in the models.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19420v1"
    },
    {
        "title": "Extreme-value Statistics: Rudiments and applications",
        "authors": [
            "Evangelos Matsinos"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  This study provides a summary of the theory which enables the analysis of\nextreme values, i.e., of measurements acquired from the observation of\nextraordinary/rare physical phenomena. The formalism is developed in a\ntransparent way, tailored to the application to real-world problems. Three\nexamples of the application of the theory are detailed: first, an old problem,\nrelating to the return period of floods caused by the Rh{\\^o}ne river, is\nrevisited; second, the frequency of occurrence and the severity of meteorite\nimpacts on the Earth are examined; the data, which are analysed in the third\nexample, relate to the longevity of supercentenarians.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.00725v1"
    },
    {
        "title": "A meshless method to compute the proper orthogonal decomposition and its\n  variants from scattered data",
        "authors": [
            "Iacopo Tirelli",
            "Miguel Alfonso Mendez",
            "Andrea Ianiro",
            "Stefano Discetti"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Complex phenomena can be better understood when broken down into a limited\nnumber of simpler \"components\". Linear statistical methods such as the\nprincipal component analysis and its variants are widely used across various\nfields of applied science to identify and rank these components based on the\nvariance they represent in the data. These methods can be seen as\nfactorisations of the matrix collecting all the data, assuming it consists of\ntime series sampled from fixed points in space. However, when data sampling\nlocations vary over time, as with mobile monitoring stations in meteorology and\noceanography or with particle tracking velocimetry in experimental fluid\ndynamics, advanced interpolation techniques are required to project the data\nonto a fixed grid before the factorisation. This interpolation is often\nexpensive and inaccurate. This work proposes a method to decompose scattered\ndata without interpolating. The approach employs physics-constrained radial\nbasis function regression to compute inner products in space and time. The\nmethod provides an analytical and mesh-independent decomposition in space and\ntime, demonstrating higher accuracy. Our approach allows distilling the most\nrelevant \"components\" even for measurements whose natural output is a\ndistribution of data scattered in space and time, maintaining high accuracy and\nmesh independence.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.03173v2"
    },
    {
        "title": "Model selection for extremal dependence structures using deep learning:\n  Application to environmental data",
        "authors": [
            "Manaf Ahmed",
            "Véronique Maume-Deschamps",
            "Pierre Ribereau"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  This paper introduces a new methodology for extreme spatial dependence\nstructure selection. It is based on deep learning techniques, specifically\nConvolutional Neural Networks -CNNs. Two schemes are considered: in the first\nscheme, the matching probability is evaluated through a single CNN while in the\nsecond scheme, a hierarchical procedure is proposed: a first CNN is used to\nselect a max-stable model, then another network allows to select the most\nadapted covariance function, according to the selected max-stable model. This\nmodel selection approach demonstrates performs very well on simulations. In\ncontrast, the Composite Likelihood Information Criterion CLIC faces issues in\nselecting the correct model. Both schemes are applied to a dataset of 2m air\ntemperature over Iraq land, CNNs are trained on dependence structures\nsummarized by the Concurrence probability.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.13276v1"
    },
    {
        "title": "Partial information decomposition for mixed discrete and continuous\n  random variables",
        "authors": [
            "Chiara Barà",
            "Yuri Antonacci",
            "Marta Iovino",
            "Ivan Lazic",
            "Luca Faes"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The framework of Partial Information Decomposition (PID) unveils complex\nnonlinear interactions in network systems by dissecting the mutual information\n(MI) between a target variable and several source variables. While PID measures\nhave been formulated mostly for discrete variables, with only recent extensions\nto continuous systems, the case of mixed variables where the target is discrete\nand the sources are continuous is not yet covered properly. Here, we introduce\na PID scheme whereby the MI between a specific state of the discrete target and\n(subsets of) the continuous sources is expressed as a Kullback-Leibler\ndivergence and is estimated through a data-efficient nearest-neighbor strategy.\nThe effectiveness of this PID is demonstrated in simulated systems of mixed\nvariables and showcased in a physiological application. Our approach is\nrelevant to many scientific problems, including sensory coding in neuroscience\nand feature selection in machine learning.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.13506v1"
    },
    {
        "title": "Predictability Analysis and Prediction of Discrete Weather and Financial\n  Time-Series Data with a Hamiltonian-Based Filter-Projection Approach",
        "authors": [
            "Henrik Kiefer",
            "Denis Furtel",
            "Cihan Ayaz",
            "Anton Klimek",
            "Jan O. Daldrop",
            "Roland R. Netz"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The generalized Langevin equation (GLE), derived by projection from a general\nmany-body Hamiltonian, exactly describes the dynamics of an arbitrary\ncoarse-grained variable in a complex environment. However, analysis and\nprediction of real-world data with the GLE is hampered by slow transient or\nseasonal data components and time-discretization effects. Machine-learning (ML)\ntechniques work but are computer-resource demanding and difficult to interpret.\nWe show that by convolution filtering, time-series data decompose into fast,\ntransient and seasonal components that each obey Hamiltonian dynamics and,\nthus, can be separately analyzed by projection techniques. We introduce methods\nto extract all GLE parameters from highly discretized time-series data and to\nforecast future data including the environmental stochasticity. For\ndaily-resolved weather data, our analysis reveals non-Markovian memory that\ndecays over a few days. Our prediction accuracy is comparable to commercial\n(weather.com) and ML long short-term memory (LSTM) methods at a reduced\ncomputational cost by a factor of $10^2-10^3$ compared to LSTM. For financial\ndata, memory is very short-ranged and the dynamics effectively is Markovian, in\nagreement with the efficient-market hypothesis; consequently, models simpler\nthan the GLE are sufficient. Our GLE framework is an efficient and\ninterpretable method for the analysis and prediction of complex time-series\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.15026v2"
    },
    {
        "title": "Sparse flow reconstruction methods to reduce the costs of analyzing\n  large unsteady datasets",
        "authors": [
            "Spencer L. Stahl",
            "Stuart I. Benton"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The cost of writing, transferring, and storing large data from unsteady\nsimulations limits access to the entire solution, often leaving much of the\nflow under-sampled or unanalyzed. For example, modeling transient behavior of\nrare dynamic events requires 3D snapshots at high sampling rates over long\nperiods, generating significant amounts of data and creating challenges for\npractical CFD workflows, especially with limited memory resources and costly\nGPU writing penalties. In this work, multiple sparse flow reconstruction (SFR)\nmethods are developed to approximate a full unsteady solution using far fewer\nsparse measurements, thus reducing writing costs, data storage, and enabling\nhigher sampling rates. SFR is motivated by a large-eddy simulation of rare\ninlet distortion events, demonstrating that down-sampling full snapshots and\nsupplementing them with high-frequency sparse measurements can drastically cut\nwriting time for GPU solvers and nearly eliminate this cost for CPU solvers.\nThe simplest single-equation \"snapshot\" SFR method can be compressed further\nusing Proper Orthogonal Decomposition (POD-SFR) or a more efficient double\nPOD-SFR variant. A streaming SFR modification improves reconstruction\nefficiency when local memory is limited. A sensitivity study evaluates\ntrade-offs between sparse sampling rates and reconstruction accuracy, offering\nbest practices. To offset error of using random sparse measurements, SFR\nexactly preserves dynamics in key regions by prescribing sparse measurement\nlocations, used here to capture distortion events. Distortion events are\nevaluated using the conditional space-time proper orthogonal decomposition\n(CST-POD) to pursue physical insights that characterize the upstream causality\nat full resolution. A validation study of CST-POD modes confirms SFR\neffectiveness at retaining the event dynamics with substantial computational\nand memory savings.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12627v1"
    },
    {
        "title": "An Analytical Approach to the Jaccard Similarity Index",
        "authors": [
            "Gonzalo Travieso",
            "Alexandre Benatti",
            "Luciano da F. Costa"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The Jaccard similarity index has often been employed in science and\ntechnology as a means to quantify the similarity between two sets. When\nmodified to operate on real-valued values, the Jaccard similarity index can be\napplied to compare vectors, an operation which plays a central role in\nvisualization, classification, and modeling. The present work aims at\ndeveloping an analytical approach for estimating the probability density of the\nJaccard similarity values as implied by set of data elements characterized by\nspecific statistical densities, with emphasis on the uniform and normal cases.\nSeveral theoretical and practical situations can benefit directly from such an\napproach, as it allows several of the properties of the similarity comparisons\namong a given dataset to be better understood and anticipated. Situations in\nwhich the described approach can be applied include the estimation and\nvisualization of data interrelationships in terms of similarity networks, as\nwell as diverse problems in data analysis, pattern recognition and scientific\nmodeling. In addition to presenting the analytical developments and results,\nexamples are also provided in order to illustrate the potential of the\napproach. The work also includes extension of the reported developments to\nmodifications of the Jaccard index intended for regularization and control of\nthe sharpness of the implemented comparisons.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.16436v1"
    },
    {
        "title": "Optimal Spatiotemporal Resolutions",
        "authors": [
            "Domiziano Doria",
            "Simone Martino",
            "Matteo Becchi",
            "Giovanni M. Pavan"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  In general, the comprehension of any type of complex system depends on the\nresolution used to look at the phenomena occurring within it. But identifying a\npriori, for example, the best time frequencies/scales to study a certain system\nover-time, or the spatial distances at which looking for correlations,\nsymmetries, and fluctuations, is most often non-trivial. Here we describe an\nunsupervised approach that, starting solely from the data of a system, allows\nlearning the characteristic length-scales of the key events/processes\ndominating it and the optimal spatiotemporal resolutions to characterize them.\nWe test this approach on time-series data obtained from simulation or\nexperimental trajectories of various example many-body complex systems ranging\nfrom the atomic- to the macroscopic-scale and having diverse internal dynamic\ncomplexities. Our method automatically performs the analysis of the system's\ndata, analyzing correlations at all relevant inter-particle distances and at\nall possible inter-frame intervals in which their time-series can be\nsubdivided: namely, at all space-and-time resolutions. The optimal\nspatiotemporal resolution for studying a certain system thus steps-out as that\nmaximizing information extraction-and-classification from the system's data,\nwhich we prove being related to the characteristic spatiotemporal length-scales\nof the local/collective physical events dominating it. This approach is broadly\napplicable and can be used to optimize the study of different types of data\n(static distributions, time-series, or signals). The concept of 'optimal\nresolution' has general character and provides a robust basis to characterize\nany type of system based on its data, as well as to guide data analysis in\ngeneral.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.13741v2"
    },
    {
        "title": "Using Partial Structure R1 to Do Molecular Replacement Calculations",
        "authors": [
            "Xiaodong Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The concept of partial structure R1 (pR1) is a generalization of the concept\nof single atom R1 (sR1) (Zhang & Donahue, 2024). The hypothesis is that the\ndeepest hole of a pR1 map determines the orientation and location of a missing\nfragment. In current implementation, the calculation is divided into two steps.\nThe first step is to detect possible orientations of all missing fragments by\nthe holes of a pR1 map of a free-standing fragment in a 3-dimensional\norientation space. The second step is to determine the orientation and location\nof a missing fragment. To this end, if done strictly, all the candidate\norientations are tried. With each candidate orientation, the best choice of\nlocation of the missing fragment is determined by the deepest hole of a pR1 map\nin a 3-dimensional location space. This best choice is combined with the trial\norientation to form one candidate orientation-location. After trying all\ncandidate orientations, a list of candidate orientation-locations are formed,\nfrom which, the one with the lowest R1 determines the orientation and location\nof a missing fragment. Then a newer pR1 is defined by including the atoms of\nthis newly determined fragment into the known atoms. This newer pR1 is used to\ndetermine the next missing fragment in the same way. To shorten the calculation\ntime, the possible locations of all missing fragments can be predicted by the\nholes of a pR1 map of a completely disoriented model of a fragment. All these\nideas of using pR1 to do molecular replacement calculations have been\ndemonstrated by four example data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.14034v1"
    },
    {
        "title": "Transient and Periodic Steady-State Characteristics of the Local Heat\n  Transfer Measurement by Thermal Perturbation with Gaussian Power Density\n  Distribution & A Supplementary Perspective with Comments",
        "authors": [
            "Zhongyuan Shi",
            "Tao Dong",
            "Zhaochu Yang"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The local heat transfer coefficient measurement with temperature oscillation\ninduced by periodic thermal perturbation - usually via a Gaussian laser beam,\nwas investigated for the impact of the spikiness (i.e., the standard deviation)\nelaborated in comparison with the analytical model for dimensional analysis.\nThe statistically more robust technique that relies on the linearity of the\nspatial phase distribution of the test point array was favored when the target\nBiot number approaches unity in terms of its order of magnitude. The preferred\nupper limit for thermographic scanning was discussed as the simplification of\nlater data processing is concerned. Nonetheless, the time elapsed for an\nacceptable periodic steady state, which in principle leans to the higher end of\nthe target Biot number spectrum in a log scale, indicates the benefit from the\ntime series of pointwise temperature measurement - as in the conventional\nsingle-blow testing, where the effect of spikiness, as well as that of the\nlocation of individual test point, holds. Note that the vicinity as the target\nBiot number approaches unity was again observed with higher preference. A\nsupplementary perspective was provided to the concerns, including noise\ntolerance, sampling rate, test duration, the spikiness of the imposed heat\nflux, and the accuracy-related parameters, in the measurement of local Biot\nnumber with thermal perturbation. The optimization was implemented with a\nGaussian process surrogate model for data processing, within the specified\nparametric range of interest. The two commonly employed temporal modes of the\nimposed heat flux were compared with counterintuitive features discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.00040v1"
    },
    {
        "title": "Sparse identification of evolution equations via Bayesian model\n  selection",
        "authors": [
            "Tim W. Kroll",
            "Oliver Kamps"
        ],
        "category": "physics.data-an",
        "published_year": "2025",
        "summary": "  The quantitative formulation of evolution equations is the backbone for\nprediction, control, and understanding of dynamical systems across diverse\nscientific fields. Besides deriving differential equations for dynamical\nsystems based on basic scientific reasoning or prior knowledge in recent times\na growing interest emerged to infer these equations purely from data. In this\narticle, we introduce a novel method for the sparse identification of nonlinear\ndynamical systems from observational data, based on the observation how the key\nchallenges of the quality of time derivatives and sampling rates influence this\nproblem. Our approach combines system identification based on thresholded least\nsquares minimization with additional error measures that account for both the\ndeviation between the model and the time derivative of the data, and the\nintegrated performance of the model in forecasting dynamics. Specifically, we\nintegrate a least squares error as well as the Wasserstein metric for estimated\nmodels and combine them within a Bayesian optimization framework to efficiently\ndetermine optimal hyperparameters for thresholding and weighting of the\ndifferent error norms. Additionally, we employ distinct regularization\nparameters for each differential equation in the system, enhancing the method's\nprecision and flexibility. We demonstrate the capabilities of our approach\nthrough applications to dynamical fMRI data and the prototypical example of a\nwake flow behind a cylinder. In the wake flow problem, our method identifies a\nsparse, accurate model that correctly captures transient dynamics, oscillation\nperiods, and phase information, outperforming existing methods. In the fMRI\nexample, we show how our approach extracts insights from a trained recurrent\nneural network, offering a novel avenue for explainable AI by inferring\ndifferential equations that capture potentially causal relationships.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.01476v1"
    },
    {
        "title": "High-Performance Data Format for Scientific Data Storage and Analysis",
        "authors": [
            "Gagik Gavalian"
        ],
        "category": "physics.data-an",
        "published_year": "2025",
        "summary": "  In this article, we present the High-Performance Output (HiPO) data format\ndeveloped at Jefferson Laboratory for storing and analyzing data from Nuclear\nPhysics experiments. The format was designed to efficiently store large amounts\nof experimental data, utilizing modern fast compression algorithms. The purpose\nof this development was to provide organized data in the output, facilitating\naccess to relevant information within the large data files. The HiPO data\nformat has features that are suited for storing raw detector data,\nreconstruction data, and the final physics analysis data efficiently,\neliminating the need to do data conversions through the lifecycle of\nexperimental data. The HiPO data format is implemented in C++ and JAVA, and\nprovides bindings to FORTRAN, Python, and Julia, providing users with the\nchoice of data analysis frameworks to use. In this paper, we will present the\ngeneral design and functionalities of the HiPO library and compare the\nperformance of the library with more established data formats used in data\nanalysis in High Energy and Nuclear Physics (such as ROOT and Parquete).\n",
        "pdf_link": "http://arxiv.org/pdf/2501.07666v2"
    },
    {
        "title": "Disentangling sources of multifractality in time series",
        "authors": [
            "Robert Kluszczyński",
            "Stanisław Drożdż",
            "Jarosław Kwapień",
            "Tomasz Stanisz",
            "Marcin Wątorek"
        ],
        "category": "physics.data-an",
        "published_year": "2025",
        "summary": "  This contribution addresses the question commonly asked in scientific\nliterature about the sources of multifractality in time series. Two primary\nsources are typically considered. These are temporal correlations and heavy\ntails in the distribution of fluctuations. Most often, they are treated as two\nindependent components, while true multifractality cannot occur without\ntemporal correlations. The distributions of fluctuations affect the span of the\nmultifractal spectrum only when correlations are present. These issues are\nillustrated here using series generated by several model mathematical cascades,\nwhich by design build correlations into these series. The thickness of the\ntails of fluctuations in such series is then governed by an appropriate\nprocedure of adjusting them to $q$-Gaussian distributions, and $q$ is treated\nas a variable parameter that, while preserving correlations, allows to tune\nthese distributions to the desired functional form. Multifractal detrended\nfluctuation analysis (MFDFA), as the most commonly used practical method for\nquantifying multifractality, is then used to identify the influence of the\nthickness of the fluctuation tails in the presence of temporal correlations on\nthe width of multifractal spectra. The obtained results point to the Gaussian\ndistribution, so $q=1$, as the appropriate reference distribution to evaluate\nthe contribution of fatter tails to the width of multifractal spectra. An\nappropriate procedure is presented to make such estimates.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.08898v1"
    },
    {
        "title": "Joint Bayesian Treatment of Poisson and Gaussian Experiments in a\n  Chi-squared Statistic",
        "authors": [
            "Dennis Silverman"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  Bayesian Poisson probability distributions for the average n can be\nanalytically converted into equivalent chi-squared distributions. These can\nthen be combined with other Gaussian or Bayesian Poisson distributions to make\na total chi-squared distribution. This allows the usual treatment of\nchi-squared contours but now with both Poisson and Gaussian statistics\nexperiments. This is illustrated with the case of neutrino oscillations.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9808004v2"
    },
    {
        "title": "Are We Cruising a Hypothesis Space?",
        "authors": [
            "Carlos C. Rodriguez"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  This paper is about Information Geometry, a relatively new subject within\nmathematical statistics that attempts to study the problem of inference by\nusing tools from modern differential geometry. This paper provides an overview\nof some of the achievements and possible future applications of this subject to\nphysics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9808009v1"
    },
    {
        "title": "Towards the Observation of Signal over Background in Future Experiments",
        "authors": [
            "S. I. Bityukov",
            "N. V. Krasnikov"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  We propose a method to estimate the probability of new physics discovery in\nfuture high energy physics experiments. Physics simulation gives both the\naverage numbers $<N_b>$ of background and $<N_s>$ of signal events. We find\nthat the proper definition of the significance for $<N_b>, <N_s> \\gg 1$ is\n$S_{12} = \\sqrt{<N_s>+<N_b>} - \\sqrt{<N_b>}$ in comparison with often used\nsignificances $S_1 = <N_s>/\\sqrt{<N_b>}$ and $S_2 = <N_s>/\\sqrt{<N_s> +\n<N_b>}$. We also propose a method for taking into account the systematical\nerrors related to nonexact knowledge of background and signal cross sections.\nAn account of such systematics is very essential in the search for\nsupersymmetry at LHC.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9808016v1"
    },
    {
        "title": "Information content in Gaussian noise: optimal compression rates",
        "authors": [
            "August Romeo",
            "Enrique Gaztanaga",
            "Jose Barriga",
            "Emilio Elizalde"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  We approach the theoretical problem of compressing a signal dominated by\nGaussian noise. We present expressions for the compression ratio which can be\nreached, under the light of Shannon's noiseless coding theorem, for a linearly\nquantized stochastic Gaussian signal (noise). The compression ratio decreases\nlogarithmically with the amplitude of the frequency spectrum $P(f)$ of the\nnoise. Entropy values and compression rates are shown to depend on the shape of\nthis power spectrum, given different normalizations. The cases of white noise\n(w.n.), $f^{n_p}$ power-law noise ---including $1/f$ noise---, (w.n.$+1/f$)\nnoise, and piecewise (w.n.+$1/f |$ w.n.$+1/f^2$) noise are discussed, while\nquantitative behaviours and useful approximations are provided.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9809004v2"
    },
    {
        "title": "On Observability of Signal over Background",
        "authors": [
            "S. I. Bityukov",
            "N. V. Krasnikov"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  Several statistics used by physicists to declare the signal observability\nover the background are compared. It is shown that the frequentist method of\ntesting a precise hypothesis allows one to estimate the power value of criteria\nwith specified level of significance for the considered statistics by Monte\nCarlo calculations. The application of this approach for the analysis of\ndiscovery potential of experiments is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9809037v2"
    },
    {
        "title": "Jeffreys priors versus experienced physicist priors - arguments against\n  objective Bayesian theory",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  I review the problem of the choice of the priors from the point of view of a\nphysicist interested in measuring a physical quantity, and I try to show that\nthe reference priors often recommended for the purpose (Jeffreys priors) do not\nfit to the problem. Although it may seem surprising, it is easier for an\n``experienced physicist'' to accept subjective priors, or even purely\nsubjective elicitation of probabilities, without explicit use of the Bayes'\ntheorem. The problem of the use of reference priors is set in the more general\ncontext of ``Bayesian dogmatism'', which could really harm Bayesianism.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9811045v1"
    },
    {
        "title": "How to quantify deterministic and random influences on the statistics of\n  the foreign exchange market",
        "authors": [
            "Rudolf Friedrich",
            "Joachim Peinke",
            "Christoph Renner"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  It is shown that prize changes of the US dollar - German Mark exchange rates\nupon different delay times can be regarded as a stochastic Marcovian process.\nFurthermore we show that from the empirical data the Kramers-Moyal coefficients\ncan be estimated.\n  Finally, we present an explicite Fokker-Planck equation which models very\nprecisely the empirical probabilitiy distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9901034v1"
    },
    {
        "title": "Experimental evidence of a phase transition to fully developed\n  turbulence in a wake flow",
        "authors": [
            "St. Lück",
            "J. Peinke",
            "R. Friedrich"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  The transition to fully developed turbulence of a wake behind a circular\ncylinder is investigated with respect to its statistics. In particular, we\nevaluated the probability density functions of velocity increments on different\nlength scales $r$. Evidence is presented that the $r$-dependence of the\nvelocity increments can be taken as Markov processes in the far field, as well\nas, in the near field of the cylinder wake. With the estimation of the\ndeterministic part of these Markov processes, as a function of the distance\nfrom the cylinder, we are able to set the transition to fully developed\nturbulence in analogy with a phase transition. We propose that the appearing\norder parameter corresponds to the presence of large scale coherent structures\nclose to the cylinder.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9902010v1"
    },
    {
        "title": "Overcoming priors anxiety",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  The choice of priors may become an insoluble problem if priors and Bayes'\nrule are not seen and accepted in the framework of subjectivism. Therefore, the\nmeaning and the role of subjectivity in science is considered and defended from\nthe pragmatic point of view of an ``experienced scientist''. The case for the\nuse of subjective priors is then supported and some recommendations for routine\nand frontier measurement applications are given. The issue of reference priors\nis also considered from the practical point of view and in the general context\nof ``Bayesian dogmatism''.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9906048v1"
    },
    {
        "title": "Approaching the parameter estimation quality of maximum likelihood via\n  generalized moments",
        "authors": [
            "Fyodor V. Tkachov"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  A simple criterion is presented for a practical construction of generalized\nmoments that allow one to approach the theoretical Rao-Cramer limit for\nparameter estimation while avoiding the complexity of the maximum likelihood\nmethod in the cases of complicated probability distributions and/or very large\nevent samples.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0001019v1"
    },
    {
        "title": "Forecast and event control: On what is and what cannot be possible",
        "authors": [
            "Karl Svozil"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  Consequences of the basic and most evident consistency requirement-that\nmeasured events cannot happen and not happen at the same time-are shortly\nreviewed. Particular emphasis is given to event forecast and event control. As\na consequence, particular, very general bounds on the forecast and control of\nevents within the known laws of physics are derived. These bounds are of a\nglobal, statistical nature and need not affect singular events or groups of\nevents.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0001025v2"
    },
    {
        "title": "Wavelet Analysis of Solar Activity",
        "authors": [
            "Stefano Sello"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  Using wavelet analysis approach, the temporal variations of solar activity on\ntime scales ranging from days to decades, are examined from the daily time\nseries of sunspot numbers. A hierarchy of changing complex periods are careful\ndetected and related cycles compared with results from recent similar analyses.\nA general determination of the main Schwabe cycle length variations is also\nsuggested on the basis of the wavelet amplitude distribution extracted from the\nlocal wavelet power map.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0001042v1"
    },
    {
        "title": "A Comment on the Roe-Woodroofe Construction of Poisson Confidence\n  Intervals",
        "authors": [
            "Mark Mandelkern",
            "Jonas Schultz"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  We consider the Roe-Woodroofe construction of confidence intervals for the\ncase of a Poisson distributed variate where the mean is the sum of a known\nbackground and an unknown non-negative signal. We point out that the intervals\ndo not have coverage in the usual sense but can be made to have such with a\nmodification that does not affect the believability and other desirable\nfeatures of this attractive construction. A similar modification can be used to\nprovide coverage to the construction recently proposed by Cousins for the\nGaussian-with-boundary problem.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0004052v1"
    },
    {
        "title": "A path-integral approach to the collisionless Boltzmann gas",
        "authors": [
            "C. Y. Chen"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  On contrary to the customary thought, the well-known ``lemma'' that the\ndistribution function of a collisionless Boltzmann gas keeps invariant along a\nmolecule's path represents not the strength but the weakness of the standard\ntheory. One of its consequences states that the velocity distribution at any\npoint is a condensed ``image'' of all, complex and even discontinuous,\nstructures of the entire spatial space. Admitting the inability to describe the\nentire space with a microscopic quantity, this paper introduces a new type of\ndistribution function, called the solid-angle-average distribution function.\nWith help of the new distribution function, the dynamical behavior of\ncollisionless Boltzmann gas is formulated in terms of a set of integrals\ndefined by molecular paths. In the new formalism, not only that the\ndifficulties associated with the standard theory are surmounted but also that\nsome of practical gases become calculable in terms of today's computer.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0006033v1"
    },
    {
        "title": "Estimating the K-function of a point process with an application to\n  cosmology",
        "authors": [
            "Michael L. Stein",
            "Jean M. Quashnock",
            "Ji Meng Loh"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  Motivated by the study of an important data set for understanding the\nlarge-scale structure of the universe, this work considers the estimation of\nthe reduced second moment function, or K-function, of a stationary point\nprocess observed over a large number of segments of possibly varying lengths.\nTheory and simulation are used to compare the behavior of isotropic and rigid\nmotion correction estimators and some modifications of these estimators. These\nresults generally support the use of modified versions of the rigid motion\ncorrection. When applied to a catalog of astronomical objects known as\nabsorbers, the proposed methods confirm results from earlier analyses of the\nabsorber catalog showing clear evidence of clustering up to 50 Mpc and marginal\nevidence for clustering of matter on spatial scales beyond 100 Mpc, which is\nbeyond the distance at which clustering of matter is now generally accepted to\nexist.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0006047v1"
    },
    {
        "title": "Confidence intervals for the parameter of Poisson distribution in\n  presence of background",
        "authors": [
            "S. I. Bityukov",
            "N. V. Krasnikov"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  A results of numerical procedure for construction of confidence intervals for\nparameter of Poisson distribution for signal in the presence of background\nwhich has Poisson distribution with known value of parameter are presented. It\nis shown that the described procedure has both Bayesian and frequentist\ninterpretations.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0009064v5"
    },
    {
        "title": "Capacity of multivariate channels with multiplicative noise: I.Random\n  matrix techniques and large-N expansions for full transfer matrices",
        "authors": [
            "Anirvan Mayukh Sengupta",
            "Partha Pratim Mitra"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  We study memoryless, discrete time, matrix channels with additive white\nGaussian noise and input power constraints of the form $Y_i = \\sum_j H_{ij} X_j\n+ Z_i$, where $Y_i$ ,$X_j$ and $Z_i$ are complex, $i=1..m$, $j=1..n$, and $H$\nis a complex $m\\times n$ matrix with some degree of randomness in its entries.\nThe additive Gaussian noise vector is assumed to have uncorrelated entries. Let\n$H$ be a full matrix (non-sparse) with pairwise correlations between matrix\nentries of the form $ E[H_{ik} H^*_{jl}] = {1\\over n}C_{ij} D_{kl} $, where\n$C$,$D$ are positive definite Hermitian matrices. Simplicities arise in the\nlimit of large matrix sizes (the so called large-N limit) which allow us to\nobtain several exact expressions relating to the channel capacity. We study the\nprobability distribution of the quantity $ f(H) = \\log \\det (1+P H^{\\dagger}S\nH) $. $S$ is non-negative definite and hermitian, with $Tr S=n$. Note that the\nexpectation $E[f(H)]$, maximised over $S$, gives the capacity of the above\nchannel with an input power constraint in the case $H$ is known at the receiver\nbut not at the transmitter. For arbitrary $C$,$D$ exact expressions are\nobtained for the expectation and variance of $f(H)$ in the large matrix size\nlimit. For $C=D=I$, where $I$ is the identity matrix, expressions are in\naddition obtained for the full moment generating function for arbitrary\n(finite) matrix size in the large signal to noise limit. Finally, we obtain the\nchannel capacity where the channel matrix is partly known and partly unknown\nand of the form $\\alpha I+ \\beta H$, $\\alpha,\\beta$ being known constants and\nentries of $H$ i.i.d. Gaussian with variance $1/n$. Channels of the form\ndescribed above are of interest for wireless transmission with multiple\nantennae and receivers.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0010081v1"
    },
    {
        "title": "Nonlinear limits to the information capacity of optical fiber\n  communications",
        "authors": [
            "Partha P. Mitra",
            "Jason B. Stark"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  The exponential growth in the rate at which information can be communicated\nthrough an optical fiber is a key element in the so called information\nrevolution. However, like all exponential growth laws, there are physical\nlimits to be considered. The nonlinear nature of the propagation of light in\noptical fiber has made these limits difficult to elucidate. Here we obtain\nbasic insights into the limits to the information capacity of an optical fiber\narising from these nonlinearities. The key simplification lies in relating the\nnonlinear channel to a linear channel with multiplicative noise, for which we\nare able to obtain analytical results. In fundamental distinction to the linear\nadditive noise case, the capacity does not grow indefinitely with increasing\nsignal power, but has a maximal value. The ideas presented here have broader\nimplications for other nonlinear information channels, such as those involved\nin sensory transduction in neurobiology. These have been often examined using\nadditive noise linear channel models, and as we show here, nonlinearities can\nchange the picture qualitatively.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0011016v1"
    },
    {
        "title": "Effect of nonstationarities on detrended fluctuation analysis",
        "authors": [
            "Zhi Chen",
            "Plamen Ch. Ivanov",
            "Kun Hu",
            "H. Eugene Stanley"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  Detrended fluctuation analysis (DFA) is a scaling analysis method used to\nquantify long-range power-law correlations in signals. Many physical and\nbiological signals are ``noisy'', heterogeneous and exhibit different types of\nnonstationarities, which can affect the correlation properties of these\nsignals. We systematically study the effects of three types of\nnonstationarities often encountered in real data. Specifically, we consider\nnonstationary sequences formed in three ways: (i) stitching together segments\nof data obtained from discontinuous experimental recordings, or removing some\nnoisy and unreliable parts from continuous recordings and stitching together\nthe remaining parts -- a ``cutting'' procedure commonly used in preparing data\nprior to signal analysis; (ii) adding to a signal with known correlations a\ntunable concentration of random outliers or spikes with different amplitude,\nand (iii) generating a signal comprised of segments with different properties\n-- e.g. different standard deviations or different correlation exponents. We\ncompare the difference between the scaling results obtained for stationary\ncorrelated signals and correlated signals with these three types of\nnonstationarities.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0111103v2"
    },
    {
        "title": "Products of Random Matrices",
        "authors": [
            "A. D. Jackson",
            "B. Lautrup",
            "P. Johansen",
            "M. Nielsen"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  We derive analytic expressions for infinite products of random 2x2 matrices.\nThe determinant of the target matrix is log-normally distributed, whereas the\nremainder is a surprisingly complicated function of a parameter characterizing\nthe norm of the matrix and a parameter characterizing its skewness. The\ndistribution may have importance as an uncommitted prior in statistical image\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0202037v1"
    },
    {
        "title": "Multifractal detrended fluctuation analysis of nonstationary time series",
        "authors": [
            "Jan W. Kantelhardt",
            "Stephan A. Zschiegner",
            "Eva Koscielny-Bunde",
            "Armin Bunde",
            "Shlomo Havlin",
            "H. Eugene Stanley"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  We develop a method for the multifractal characterization of nonstationary\ntime series, which is based on a generalization of the detrended fluctuation\nanalysis (DFA). We relate our multifractal DFA method to the standard partition\nfunction-based multifractal formalism, and prove that both approaches are\nequivalent for stationary signals with compact support. By analyzing several\nexamples we show that the new method can reliably determine the multifractal\nscaling behavior of time series. By comparing the multifractal DFA results for\noriginal series to those for shuffled series we can distinguish multifractality\ndue to long-range correlations from multifractality due to a broad probability\ndensity function. We also compare our results with the wavelet transform\nmodulus maxima (WTMM) method, and show that the results are equivalent.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0202070v1"
    },
    {
        "title": "Finding an Upper Limit in the Presence of Unknown Background",
        "authors": [
            "S. Yellin"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Experimenters report an upper limit if the signal they are trying to detect\nis non-existent or below their experiment's sensitivity. Such experiments may\nbe contaminated with a background too poorly understood to subtract. If the\nbackground is distributed differently in some parameter from the expected\nsignal, it is possible to take advantage of this difference to get a stronger\nlimit than would be possible if the difference in distribution were ignored. We\ndiscuss the ``Maximum Gap'' method, which finds the best gap between events for\nsetting an upper limit, and generalize to ``Optimum Interval'' methods, which\nuse intervals with especially few events. These methods, which apply to the\ncase of relatively small backgrounds, do not use binning, are relatively\ninsensitive to cuts on the range of the parameter, are parameter independent\n(i.e., do not change when a one-one change of variables is made), and provide\ntrue, though possibly conservative, classical one-sided confidence intervals.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0203002v2"
    },
    {
        "title": "Single or combined location measurements of the same parameter without\n  prior probability. (Altern. title, Parametric inference as useful illusion;\n  Part 1)",
        "authors": [
            "George Kahrimanis"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Motivation. This version is based solely on the calculus of probability,\nexcluding any statistical principle. \"Location measurement\" means the pdf of\nthe error is known. When the datum is obtained, intuition suggests something\nlike a pdf for the parameter; here we attempt a critical examination of its\nmeaning.\n  Summary. In default of prior probability the parameter is not defined as a\nrandom variable, hence there can be no genuine prior-free parametric inference.\nNevertheless prior-free predictive inference regarding any future datum is\ngenerated directly from the datum of a location measurement. Such inference\nturns out as if obtained from a certain pdf (\"fiducial\") indirectly associated\nwith the parameter. This false pdf can expedite predictive inference, but is\ninappropriate in the analysis of combined measurements (unless they all are\nlocation measurements of the same parameter). Also it has the same distribution\nas the ostensible Bayesian posterior from a uniform \"prior\". However, if any of\nthese spurious entities is admitted in the analysis, inconsistent results\nfollow. When we combine measurements, we find that the quantisation errors,\ninevitable in data recording, must be taken into consideration. These errors\ncannot be folded into predictive inference in an exact sense; that is, we\ncannot render a predictive distribution of a future datum except as an\napproximation.\n  Keywords: location measurement; combination of observations; parametric\ninference; predictive inference; prior-free inference; quantisation error;\ndigitisation; frequentist interpretation; the fiducial argument; fiducial\nprobability; pivotal inference; intuitive assessment; prior-free assessment\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0203090v2"
    },
    {
        "title": "Inductive Logic: From Data Analysis to Experimental Design",
        "authors": [
            "Kevin H. Knuth"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  In celebration of the work of Richard Threlkeld Cox, we explore inductive\nlogic and its role in science touching on both experimental design and analysis\nof experimental results. In this exploration we demonstrate that the duality\nbetween the logic of assertions and the logic of questions has important\nconsequences. We discuss the conjecture that the relevance or bearing, b, of a\nquestion on an issue can be expressed in terms of the probabilities, p, of the\nassertions that answer the question via the entropy.\n  In its application to the scientific method, the logic of questions,\ninductive inquiry, can be applied to design an experiment that most effectively\naddresses a scientific issue. This is performed by maximizing the relevance of\nthe experimental question to the scientific issue to be resolved. It is shown\nthat these results are related to the mutual information between the experiment\nand the scientific issue, and that experimental design is akin to designing a\ncommunication channel that most efficiently communicates information relevant\nto the scientific issue to the experimenter. Application of the logic of\nassertions, inductive inference (Bayesian inference) completes the experimental\nprocess by allowing the researcher to make inferences based on the information\nobtained from the experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0204068v1"
    },
    {
        "title": "Maximum-likelihood absorption tomography",
        "authors": [
            "J. Rehacek",
            "Z. Hradil",
            "M. Zawisky",
            "W. Treimer",
            "M. Strobl"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Maximum-likelihood methods are applied to the problem of absorption\ntomography. The reconstruction is done with the help of an iterative algorithm.\nWe show how the statistics of the illuminating beam can be incorporated into\nthe reconstruction. The proposed reconstruction method can be considered as a\nuseful alternative in the extreme cases where the standard ill-posed\ndirect-inversion methods fail.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0206057v1"
    },
    {
        "title": "Confidence Limits and their Robustness",
        "authors": [
            "Rajendran Raja"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Confidence limits are common place in physics analysis. Great care must be\ntaken in their calculation and use, especially in cases of limited statistics\nwhen often one-sided limits are quoted. In order to estimate the stability of\nthe confidence levels to addition of more data and/or change of cuts, we argue\nthat the variance of their sampling distributions be calculated in addition to\nthe limit itself. The square root of the variance of their sampling\ndistribution can be thought of as a statistical error on the limit. We thus\nintroduce the concept of statistical errors of confidence limits and argue that\nnot only should limits be calculated but also their errors in order to\nrepresent the results of the analysis to the fullest. We show that comparison\nof two different limits from two different experiments becomes easier when\ntheir errors are also quoted. Use of errors of confidence limits will lead to\nabatement of the debate on which method is best suited to calculate confidence\nlimits.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0207058v1"
    },
    {
        "title": "A Measure of the Goodness of Fit in Unbinned Likelihood Fits",
        "authors": [
            "Rajendran Raja"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Maximum likelihood fits to data can be done using binned data (histograms)\nand unbinned data. With binned data, one gets not only the fitted parameters\nbut also a measure of the goodness of fit. With unbinned data, currently, the\nfitted parameters are obtained but no measure of goodness of fit is available.\nThis remains, to date, an unsolved problem in statistics. Using Bayes theorem\nand likelihood ratios, we provide a method by which both the fitted quantities\nand a measure of the goodness of fit are obtained for unbinned likelihood fits,\nas well as errors in the fitted quantities. We provide an ansatz for\ndetermining Bayesian a priori probabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0207083v2"
    },
    {
        "title": "Marginalization using the metric of the likelihood",
        "authors": [
            "R. Preuss",
            "V. Dose"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Although the likelihood function is normalizeable with respect to the data\nthere is no guarantee that the same holds with respect to the model parameters.\nThis may lead to singularities in the expectation value integral of these\nparameters, especially if the prior information is not sufficient to take care\nof finite integral values. However, the problem may be solved by obeying the\ncorrect Riemannian metric imposed by the likelihood. This will be demonstrated\nfor the example of the electron temperature evaluation in hydrogen plasmas.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0207125v1"
    },
    {
        "title": "Improvement of experimental data via consistency conditions",
        "authors": [
            "G. Kontrym-Sznajd"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Interdependencies between experimental spectra, representing line or plane\nprojections of electronic densities, are derived from their consistency and\nsymmetry conditions. Some additional relations for plane projections are\nobtained by treating them as line projections of line projections.\n  The knowledge of these dependencies can be utilised both for an improvement\nof experimental data and for a verification of various techniques used for\ncorrecting e.g. two-dimensional (or one-dimensional) angular correlation of\nannihilation radiation spectra and Compton scattering profiles.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0208009v1"
    },
    {
        "title": "Bayesian analysis of magnetic island dynamics",
        "authors": [
            "R. Preuss",
            "M. Maraschek",
            "H. Zohm",
            "V. Dose"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  We examine a first order differential equation with respect to time coming up\nin the description of magnetic islands in magnetically confined plasmas. The\nfree parameters of this equation are obtained by employing Bayesian probability\ntheory. Additionally a typical Bayesian change point is solved in the process\nof obtaining the data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0208036v1"
    },
    {
        "title": "Coordinated, Interactive Data Visualization for Neutron Scattering Data",
        "authors": [
            "D. J. Mikkelson",
            "R. L. Mikkelson",
            "T. G. Worlton",
            "A. Chatterjee",
            "J. P. Hammonds",
            "P. F. Peterson",
            "A. J. Schultz"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  The overall design of the Integrated Spectral Analysis Workbench (ISAW),\nbeing developed at Argonne, provides for an extensible, highly interactive,\ncollaborating set of viewers for neutron scattering data. Large arbitrary\ncollections of spectra from multiple detectors can be viewed as an image, a\nscrolled list of individual graphs, or using a 3D representation of the\ninstrument showing the detector positions. Data from an area detector can be\ndisplayed using a contour or intensity map as well as an interactive table.\nSelected spectra can be displayed in tables or on a conventional graph. A\nunique characteristic of these viewers is their interactivity and coordination.\nThe position \"pointed at\" by the user in one viewer is sent to other viewers of\nthe same DataSet so they can track the position and display relevant\ninformation. Specialized viewers for single crystal neutron diffractometers are\nbeing developed. A \"proof-of-concept\" viewer that directly displays the 3D\nreciprocal lattice from a complete series of runs on a single crystal\ndiffractometer has been implemented.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0210082v1"
    },
    {
        "title": "Broad distribution effects in sums of lognormal random variables",
        "authors": [
            "M. Romeo",
            "V. Da Costa",
            "F. Bardou"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  The lognormal distribution describing, e.g., exponentials of Gaussian random\nvariables is one of the most common statistical distributions in physics. It\ncan exhibit features of broad distributions that imply qualitative departure\nfrom the usual statistical scaling associated to narrow distributions.\nApproximate formulae are derived for the typical sums of lognormal random\nvariables. The validity of these formulae is numerically checked and the\nphysical consequences, e.g., for the current flowing through small tunnel\njunctions, are pointed out.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0211065v2"
    },
    {
        "title": "A modified Least Squares Lattice filter to identify non stationary\n  process",
        "authors": [
            "Elena Cuoco"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  In this paper the author proposes to use the Least Squares Lattice filter\nwith forgetting factor to estimate time-varying parameters of the model for\nnoise processes. We simulated an Auto-Regressive (AR) noise process in which we\nlet the parameters of the AR vary in time. We investigate a new way of\nimplementation of Least Squares Lattice filter in following the non stationary\ntime series for stochastic process. Moreover we introduce a modified Least\nSquares Lattice filter to whiten the time-series and to remove the non\nstationarity. We apply this algorithm to the identification of real times\nseries data produced by recorded voice.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0211077v1"
    },
    {
        "title": "Statistical Inverse Problem",
        "authors": [
            "Yu. I. Bogdanov"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  A fundamental problem of statistical data analysis, distribution density\nestimation by experimental data, is considered. A new method with optimal\nasymptotic behavior, the root density estimator, is developed. The method\nproposed may be applied to its full extent to solve the statistical inverse\nproblem of quantum mechanics, namely, estimating the psi function on the basis\nof the results of mutually complementing experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0211109v1"
    },
    {
        "title": "Statistical Analysis of Composite Spectra",
        "authors": [
            "A. Y. Abul-Magd",
            "H. L. Harney",
            "M. H. Simbel",
            "H. A. Weidenmueller"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  We consider nearest neighbor spacing distributions of composite ensembles of\nlevels. These are obtained by combining independently unfolded sequences of\nlevels containing only few levels each. Two problems arise in the spectral\nanalysis of such data. One problem lies in fitting the nearest neighbor spacing\ndistribution to the histogram of level spacings obtained from the data. We show\nthat the method of Bayesian inference is superior to this procedure. The second\nproblem occurs when one unfolds such short sequences. We show that the\nunfolding procedure generically leads to an overestimate of the chaoticity\nparameter. This trend is absent in the presence of long-range level\ncorrelations. Thus, composite ensembles of levels from a system with long-range\nspectral stiffness yield reliable information about the chaotic behavior of the\nsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0212049v3"
    },
    {
        "title": "On signal-noise decomposition of timeseries using the continuous wavelet\n  transform: Application to sunspot index",
        "authors": [
            "J. Polygiannakis",
            "P. Preka-Papadema",
            "X. Moussas"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We show that the continuous wavelet transform can provide a unique\ndecomposition of a timeseries in to 'signal-like' and 'noise-like' components:\n  From the overall wavelet spectrum two mutually independent skeleton spectra\ncan be extracted, allowing the separate detection and monitoring in even\nnon-stationary timeseries of the evolution of (a) both stable but also\ntransient, evolving periodicities, such as the output of low dimensional\ndynamical systems and (b) scale-invariant structures, such as discontinuities,\nself-similar structures or noise. An indicative application to the\nmonthly-averaged sunspot index reveals, apart from the well-known 11-year\nperiodicity, 3 of its harmonics, the 2-year periodicity (quasi-biennial\noscillation, QBO) and several more (some of which detected previously in\nvarious solar, earth-solar connection and climate indices), here proposed being\njust harmonics of the QBO, in all supporting the double-cycle solar magnetic\ndynamo model (Benevolenskaya, 1998, 2000). The scale maximal spectrum reveals\nthe presence of 1/f fluctuations with timescales up to 1 year in the sunspot\nnumber, indicating that the solar magnetic configurations involved in the\ntransient solar activity phenomena with those characteristic timescales are in\na self-organized-critical state (SOC), as previously proposed for the solar\nflare occurence (Lu and Hamilton, 1991).\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0301030v1"
    },
    {
        "title": "How many clusters? An information theoretic perspective",
        "authors": [
            "Susanne Still",
            "William Bialek"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  Clustering provides a common means of identifying structure in complex data,\nand there is renewed interest in clustering as a tool for the analysis of large\ndata sets in many fields. A natural question is how many clusters are\nappropriate for the description of a given system. Traditional approaches to\nthis problem are based either on a framework in which clusters of a particular\nshape are assumed as a model of the system or on a two-step procedure in which\na clustering criterion determines the optimal assignments for a given number of\nclusters and a separate criterion measures the goodness of the classification\nto determine the number of clusters. In a statistical mechanics approach,\nclustering can be seen as a trade--off between energy-- and entropy--like\nterms, with lower temperature driving the proliferation of clusters to provide\na more detailed description of the data. For finite data sets, we expect that\nthere is a limit to the meaningful structure that can be resolved and therefore\na minimum temperature beyond which we will capture sampling noise. This\nsuggests that correcting the clustering criterion for the bias which arises due\nto sampling errors will allow us to find a clustering solution at a temperature\nwhich is optimal in the sense that we capture maximal meaningful structure --\nwithout having to define an external criterion for the goodness or stability of\nthe clustering. We show that, in a general information theoretic framework, the\nfinite size of a data set determines an optimal temperature, and we introduce a\nmethod for finding the maximal number of clusters which can be resolved from\nthe data in the hard clustering limit.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0303011v2"
    },
    {
        "title": "Statistical Yield Modeling for IC Manufacture: Hierarchical Fault\n  Distributions",
        "authors": [
            "Yu. I. Bogdanov",
            "N. A. Bogdanova",
            "V. L. Dshkhunyan"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  A hierarchical approach to the construction of compound distributions for\nprocess-induced faults in IC manufacture is proposed. Within this framework,\nthe negative binomial distribution and the compound binomial distribution are\ntreated as level-1 models. The hierarchical approach to fault distribution\noffers an integrated picture of how fault density varies from region to region\nwithin a wafer, from wafer to wafer within a batch, and so on. A theory of\ncompound-distribution hierarchies is developed by means of generating\nfunctions. With respect to applications, hierarchies of yield means and yield\nprobability-density functions are considered and an in-process measure of yield\nloss is introduced. It is shown that the hierarchical approach naturally\nembraces the Bayesian approach.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0303039v1"
    },
    {
        "title": "Data compression using correlations and stochastic processes in the\n  ALICE Time Projection chamber",
        "authors": [
            "M. Ivanov",
            "A. Nicolaucig",
            "A. Krechtchouk"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  In this paper lossless and a quasi lossless algorithms for the online\ncompression of the data generated by the Time Projection Chamber (TPC) detector\nof the ALICE experiment at CERN are described. The first algorithm is based on\na lossless source code modelling technique, i.e. the original TPC signal\ninformation can be reconstructed without errors at the decompression stage. The\nsource model exploits the temporal correlation that is present in the TPC data\nto reduce the entropy of the source. The second algorithm is based on a lossy\nsource code modelling technique. In order to evaluate the consequences of the\nerror introduced by the lossy compression, the results of the trajectory\ntracking algorithms that process data offline are analyzed, in particular, with\nrespect to the noise introduced by the compression. The offline analysis has\ntwo steps: cluster finder and track finder. The results on how these algorithms\nare affected by the lossy compression are reported. In both compression\ntechnique entropy coding is applied to the set of events defined by the source\nmodel to reduce the bit rate to the corresponding source entropy. Using TPC\nsimulated data, the lossless and the lossy compression achieve a data reduction\nto 49.2% of the original data rate and respectively in the range of 35% down to\n30% depending on the desired precision.In this study we have focused on methods\nwhich are easy to implement in the frontend TPC electronics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306133v1"
    },
    {
        "title": "Characterization and diagnostics of combustion thermoacoustic\n  instabilities using nonlinear dynamics and topological methods",
        "authors": [
            "G. Gianni",
            "G. Mariotti",
            "E. Paganini",
            "S. Sello"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The onset of thermoacoustic instabilities in lean-premixed gas-turbine\ncombustors is a crucial problem leading to degradation in engine and emissions\nperformance and shortened component life. The main aim of this study is to\npropose a methodology based both on concepts of nonlinear dynamics and on\ngeometric-topological invariants, for the characterization of attractors\nrelated to measurements based on the flame spontaneous light emission, like OH*\nradical, in order to classify different phases of the combustion process and to\nbetter recognize the transition mechanisms leading to the thermoacoustic\ninstabilities. Preliminary results, clearly show the powerfulness of the\napproach to show the dynamical evolution of the flame and to evidence the onset\nof the thermoacoustic instabilities: in particular the topological invariant\nindex (genus and related quantities) appear s as the best candidate for an\nearly indicator of the dynamical transition, characterized by the onset of a\nmore persistent, low entropy torus (quasi-periodic motion), as the related\nthermoacoustic attractor.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306199v1"
    },
    {
        "title": "Pulse Shape Discrimination Techniques in Scintillating CsI(Tl) Crystals",
        "authors": [
            "S. C. Wu"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  There are recent interests with CsI(Tl) scintillating crystals for Dark\nMatter experiments. The key merit is the capability to differentiate nuclear\nrecoil (nr) signatures from the background $\\beta / \\gamma$-events due to\nambient radioactivity on the basis of their different pulse shapes. One of the\nmajor experimental challenges is to perform such pulse shape analysis in the\nstatistics-limited domain where the light output is close to the detection\nthreshold. Using data derived from measurements with low energy $\\gamma$'s and\nnuclear recoils due to neutron elastic scatterings, it was verified that the\npulse shapes between $\\beta / \\gamma$-events are different. Several methods of\npulse shape discrimination are studied, and their relative merits are compared.\nFull digitization of the pulse shapes is crucial to achieve good\ndiscrimination. Advanced software techniques with mean time, neural network and\nlikelihood ratios give rise to satisfactory performance, and are superior to\nthe conventional Double Charge method commonly applied at higher energies.\nPulse shape discrimination becomes effective starting at a light yield of about\n20 photo-electrons. This corresponds to a detection threshold of about 5 keV\nelectron-equivalence energy, or 40$-$50 keV recoil kinetic energy, in realistic\nexperiments.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0307002v2"
    },
    {
        "title": "Co-operation of two-state processes and 1/f noise",
        "authors": [
            "Giovanni Zanella"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  A general physical model is presented for 1/f noise. The main questions\nraised by this type of noise can be solved if at the origin of the phenomenon\nwe consider many similar like RTN two-state processes in co-operation among\nthem to generate a Gaussian average process with 1/f power spectrum. Even if\nthe originating RTN processes have the same relaxation time, their co-operation\nproduces in short times secondary RTN processes with a distribution of\nrelaxation times which generates again 1/f noise. An extension of the model to\na single two-state process viewed in \"series\" reveals once more the appearance\nof 1/f noise. Experimental results, found in the literature, agree with this\nmodel under various aspects.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0307011v1"
    },
    {
        "title": "Information Metric on Instanton Moduli Spaces in Nonlinear Sigma Models",
        "authors": [
            "Shigeaki Yahikozawa"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We study the information metric on instanton moduli spaces in two-dimensional\nnonlinear sigma models. In the CP^1 model, the information metric on the moduli\nspace of one instanton with the topological charge Q=k which is any positive\ninteger is a three-dimensional hyperbolic metric, which corresponds to\nEuclidean anti--de Sitter space-time metric in three dimensions, and the\noverall scale factor of the information metric is (4k^2)/3; this means that the\nsectional curvature is -3/(4k^2). We also calculate the information metric in\nthe CP^2 model.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0307131v2"
    },
    {
        "title": "Multilevel Clustering Fault Model for IC Manufacture",
        "authors": [
            "Yu. I. Bogdanov",
            "N. A. Bogdanova",
            "A. V. Rudnev"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  A hierarchical approach to the construction of compound distributions for\nprocess-induced faults in IC manufacture is proposed. Within this framework,\nthe negative binomial distribution is treated as level-1 models. The\nhierarchical approach to fault distribution offers an integrated picture of how\nfault density varies from region to region within a wafer, from wafer to wafer\nwithin a batch, and so on. A theory of compound-distribution hierarchies is\ndeveloped by means of generating functions. A study of correlations, which\nnaturally appears in microelectronics due to the batch character of IC\nmanufacture, is proposed. Taking these correlations into account is of\nsignificant importance for developing procedures for statistical quality\ncontrol in IC manufacture. With respect to applications, hierarchies of yield\nmeans and yield probability-density functions are considered.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0310012v1"
    },
    {
        "title": "Relative Entropy and Inductive Inference",
        "authors": [
            "Ariel Caticha"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  We discuss how the method of maximum entropy, MaxEnt, can be extended beyond\nits original scope, as a rule to assign a probability distribution, to a\nfull-fledged method for inductive inference. The main concept is the (relative)\nentropy S[p|q] which is designed as a tool to update from a prior probability\ndistribution q to a posterior probability distribution p when new information\nin the form of a constraint becomes available. The extended method goes beyond\nthe mere selection of a single posterior p, but also addresses the question of\nhow much less probable other distributions might be. Our approach clarifies how\nthe entropy S[p|q] is used while avoiding the question of its meaning.\nUltimately, entropy is a tool for induction which needs no interpretation.\nFinally, being a tool for generalization from special examples, we ask whether\nthe functional form of the entropy depends on the choice of the examples and we\nfind that it does. The conclusion is that there is no single general theory of\ninductive inference and that alternative expressions for the entropy are\npossible.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0311093v1"
    },
    {
        "title": "Deviations of the distributions of seismic energies from the\n  Gutenberg-Richter law",
        "authors": [
            "V. Pisarenko",
            "D. Sornette",
            "M. Rodkin"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  A new non-parametric statistic is introduced for the characterization of\ndeviations from power laws. It is tested on the distribution of seismic\nenergies given by the Gutenberg-Richter law. Based on the two first statistical\nlog-moments, it evaluates quantitatively the deviations of the distribution of\nscalar seismic moments from a power-like (Pareto) law. This statistic is close\nto zero for the Pareto law with arbitrary power index, and deviates from zero\nfor any non-Pareto distribution. A version of this statistic for discrete\ndistribution of quantified magnitudes is also given. A methodology based on\nthis statistics consisting in scanning the lower threshold for earthquake\nenergies provides an explicit visualization of deviations from the Pareto law,\nsurpassing in sensitivity the standard Hill estimator or other known\ntechniques. This new statistical technique has been applied to shallow\nearthquakes (h < 70 km) both in subduction zones and in mid-ocean ridge zones\n(using the Harvard catalog of seismic moments, 1977-2000), and to several\nregional catalogs of magnitudes (California, Japan, Italy, Greece). We discover\nevidence for log-periodicity and thus for a discrete hierarchy of scales for\nlow-angle dipping, low-strain subduction zones with a preferred scaling ratio\ng=7+-1 for seismic moments, compatible with a preferred scaling ratio of 2 for\nlinear rupture sizes, and consistent with previous reports. We propose a\npossible mechanism in terms of cascades of fault competitions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0312020v1"
    },
    {
        "title": "A statistical analysis of acoustic emission signals for tool condition\n  monitoring (TCM)",
        "authors": [
            "G. Pontuale",
            "F. A. Farrelly",
            "A. Petri",
            "L. Pitolli"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  The statistical properties of acoustic emission signals for tool condition\nmonitoring (TCM) applications in mechanical lathe machining are analyzed in\nthis paper. Time series data and root mean square (RMS) values at various tool\nwear levels are shown to exhibit features that can be put into relation with\nageing in both cases. In particular, the histograms of raw data show power-law\ndistributions above a cross-over value, in which newer cutting tools exhibit\nmore numerous larger events compared with more worn-out ones. For practical\npurposes, statistics based on RMS values are more feasible, and the analysis of\nthese also reveals discriminating age-related features. The assumption that\nexperimental RMS histograms follow a Beta (b) distribution has also been\ntested. The residuals of the modeling b functions indicate that the search for\na more appropriate fitting function for the experimental distribution is\ndesirable.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0312148v2"
    },
    {
        "title": "Diffusion-based method for producing density equalizing maps",
        "authors": [
            "Michael T. Gastner",
            "M. E. J. Newman"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Map makers have long searched for a way to construct cartograms -- maps in\nwhich the sizes of geographic regions such as countries or provinces appear in\nproportion to their population or some other analogous property. Such maps are\ninvaluable for the representation of census results, election returns, disease\nincidence, and many other kinds of human data. Unfortunately, in order to scale\nregions and still have them fit together, one is normally forced to distort the\nregions' shapes, potentially resulting in maps that are difficult to read. Many\nmethods for making cartograms have been proposed, some of them extremely\ncomplex, but all suffer either from this lack of readability or from other\npathologies, like overlapping regions or strong dependence on the choice of\ncoordinate axes. Here we present a new technique based on ideas borrowed from\nelementary physics that suffers none of these drawbacks. Our method is\nconceptually simple and produces useful, elegant, and easily readable maps. We\nillustrate the method with applications to the results of the 2000 US\npresidential election, lung cancer cases in the State of New York, and the\ngeographical distribution of stories appearing in the news.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0401102v1"
    },
    {
        "title": "Pattern Recognition and Event Reconstruction in Particle Physics\n  Experiments",
        "authors": [
            "Rainer Mankel"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  This report reviews methods of pattern recognition and event reconstruction\nused in modern high energy physics experiments. After a brief introduction into\ngeneral concepts of particle detectors and statistical evaluation, different\napproaches in global and local methods of track pattern recognition are\nreviewed with their typical strengths and shortcomings. The emphasis is then\nmoved to methods which estimate the particle properties from the signals which\npattern recognition has associated. Finally, the global reconstruction of the\nevent is briefly addressed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0402039v1"
    },
    {
        "title": "Competition between Two Kinds of Correlations in Literary Texts",
        "authors": [
            "S. S. Melnyk",
            "O. V. Usatenko",
            "V. A. Yampol'skii",
            "V. A. Golick"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  A theory of additive Markov chains with long-range memory is used for\ndescription of correlation properties of coarse-grained literary texts. The\ncomplex structure of the correlations in texts is revealed. Antipersistent\ncorrelations at small distances, L < 300, and persistent ones at L > 300 define\nthis nontrivial structure. For some concrete examples of literary texts, the\nmemory functions are obtained and their power-law behavior at long distances is\ndisclosed. This property is shown to be a cause of self-similarity of texts\nwith respect to the decimation procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0402042v2"
    },
    {
        "title": "Using a neural network approach for muon reconstruction and triggering",
        "authors": [
            "E. Etzion",
            "H. Abramowicz",
            "Y. Benhammou",
            "D. Horn",
            "L. Levinson",
            "R. Livneh"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  The extremely high rate of events that will be produced in the future Large\nHadron Collider requires the triggering mechanism to take precise decisions in\na few nano-seconds. We present a study which used an artificial neural network\ntriggering algorithm and compared it to the performance of a dedicated\nelectronic muon triggering system. Relatively simple architecture was used to\nsolve a complicated inverse problem. A comparison with a realistic example of\nthe ATLAS first level trigger simulation was in favour of the neural network. A\nsimilar architecture trained after the simulation of the electronics first\ntrigger stage showed a further background rejection.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0402070v1"
    },
    {
        "title": "Trajectory structures and transport",
        "authors": [
            "Madalina Vlad",
            "Florin Spineanu"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  The special problem of transport in 2-dimensional divergence-free stochastic\nvelocity fields is studied by developing a statistical approach, the nested\nsubensemble method. The nonlinear process of trapping determined by such fields\ngenerates trajectory structures whose statistical characteristics are\ndetermined. These structures strongly influence the transport.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0403004v1"
    },
    {
        "title": "Experimental test of the probability density function of true value of\n  Poisson distribution parameter by single observation of number of events",
        "authors": [
            "S. I. Bityukov",
            "V. A. Medvedev",
            "V. V. Smirnova",
            "Yu. V. Zernii"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  The empirical probability density function for the conditional distribution\nof the true value of Poisson distribution parameter on one measurement is\nconstructed by computer experiment. The analysis of the obtained distributions\nconfirms that these distributions are gamma-distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0403069v1"
    },
    {
        "title": "What is a Question?",
        "authors": [
            "Kevin H. Knuth"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  A given question can be defined in terms of the set of statements or\nassertions that answer it. Application of logical inference to these sets of\nassertions allows one to derive the logic of inquiry among questions. There are\ninteresting symmetries between the logics of inference and inquiry; where\nprobability describes the degree to which a premise implies an assertion, there\nexists an analogous measure that describes the bearing or relevance that a\nquestion has on an outstanding issue. These have been extended to suggest that\nthe logic of inquiry results in functional relationships analogous to, although\nmore general than, those found in information theory.\n  Employing lattice theory, I examine in greater detail the structure of the\nspace of assertions and questions demonstrating that the symmetries between the\nlogical relations in each of the spaces derive directly from the lattice\nstructure. Furthermore, I show that while symmetries between the spaces exist,\nthe two lattices are not isomorphic. The lattice of assertions is described by\na Boolean lattice 2^N, whereas the lattice of assuredly real questions is shown\nto be a sublattice of the free distributive lattice 2^(2^N). Thus there does\nnot exist a one-to-one mapping of assertions to questions, there is no\nreflection symmetry between the two spaces, and questions in general do not\npossess complements. Last, with these lattice structures in mind, I discuss the\nrelationship between probability, relevance, and entropy.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0403089v1"
    },
    {
        "title": "Stochastic analysis of different rough surfaces",
        "authors": [
            "M. Waechter",
            "F. Riess",
            "Th. Schimmel",
            "U. Wendt",
            "J. Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  This paper shows in detail the application of a new stochastic approach for\nthe characterization of surface height profiles, which is based on the theory\nof Markov processes. With this analysis we achieve a characterization of the\nscale dependent complexity of surface roughness by means of a Fokker-Planck or\nLangevin equation, providing the complete stochastic information of multiscale\njoint probabilities. The method is applied to several surfaces with different\nproperties, for the purpose of showing the utility of this method in more\ndetails. In particular we show the evidence of Markov properties, and we\nestimate the parameters of the Fokker-Planck equation by pure, parameter-free\ndata analysis. The resulting Fokker-Planck equations are verified by numerical\nreconstruction of conditional probability density functions. The results are\ncompared with those from the analysis of multi-affine and extended multi-affine\nscaling properties which is often used for surface topographies. The different\nsurface structures analysed here show in details advantages and disadvantages\nof these methods.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0404015v2"
    },
    {
        "title": "Increment definitions for scale dependent analysis of stochastic data",
        "authors": [
            "Matthias Waechter",
            "Alexei Kouzmitchev",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  It is common for scale-dependent analysis of stochastic data to use the\nincrement $\\Delta(t,r) = \\xi(t+r) - \\xi(t)$ of a data set $\\xi(t)$ as a\nstochastic measure, where $r$ denotes the scale. For joint statistics of\n$\\Delta(t,r)$ and $\\Delta(t,r')$ the question how to nest the increments on\ndifferent scales $r,r'$ is investigated. Here we show that in some cases\nspurious correlations between scales can be introduced by the common\nleft-justified definition. The consequences for a Markov process are discussed.\nThese spurious correlations can be avoided by an appropriate nesting of\nincrements. We demonstrate this effect for different data sets and show how it\ncan be detected and quantified. The problem allows to propose a unique method\nto distinguish between experimental data generated by a noiselike or a\nLangevin-like random-walk process, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0404021v2"
    },
    {
        "title": "Goodness-of-fit tests in many dimensions",
        "authors": [
            "A. van Hameren"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  A method is presented to construct goodness-of-fit statistics in many\ndimensions for which the distribution of all possible test results in the limit\nof an infinite number of data becomes Gaussian if also the number of dimensions\nbecomes infinite. Furthermore, an explicit example is presented, for which this\ndistribution as good as only depends on the expectation value and the variance\nof the statistic for any dimension larger than one.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0405008v1"
    },
    {
        "title": "The Astumian's Paradox",
        "authors": [
            "Edward W. Piotrowski",
            "Jan Sladkowski"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We discuss some aspects of Astumian suggestions that combination of biased\ngames (Parrondo's paradox) can explain performance of molecular motors.\nUnfortunately the model is flawed by explicit asymmetry overlooked by the\nauthor. In addition, we show that taking into consideration stakes allows to\nremove the paradoxical behaviour of Parrondo games.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0408122v1"
    },
    {
        "title": "Measuring questions: relevance and its relation to entropy",
        "authors": [
            "Kevin H. Knuth"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  The Boolean lattice of logical statements induces the free distributive\nlattice of questions. Inclusion on this lattice is based on whether one\nquestion answers another. Generalizing the zeta function of the question\nlattice leads to a valuation called relevance or bearing, which is a measure of\nthe degree to which one question answers another. Richard Cox conjectured that\nthis degree can be expressed as a generalized entropy. With the assistance of\nyet another important result from Janos Aczel, I show that this is indeed the\ncase, and that the resulting inquiry calculus is a natural generalization of\ninformation theory. This approach provides a new perspective on the Principle\nof Maximum Entropy.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0409084v1"
    },
    {
        "title": "X-ray properties of the transient pulsar 3A 0535+262 in quiescence",
        "authors": [
            "U. Mukherjee",
            "B. Paul"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We present the timing and spectral properties of the transient Be/X-ray\nbinary pulsar 3A 0535+262 during quiescence using three observations with the\nnarrow field imaging instruments (NFI) of BeppoSAX. Assuming a distance of 2\nkpc for this system, the 2-10 keV X-ray luminosities measured from the three\nobservations are in the range of 1.5-4.0 $\\times$ 10$^{33}$ erg s$^{-1}$,\nindicating a very low rate of accretion. We report the detection of pulsations\nat a very low luminosity of 2 $\\times$ 10$^{33}$ erg s$^{-1}$ during one of the\nthree observations, though at this accretion rate the system is expected to be\nin the centrifugally inhibited regime. The X-ray spectra for the unpulsed\nobservations are best modeled as power law type while a combined model of power\nlaw and black-body is required to fit the pulsed spectrum.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0410255v1"
    },
    {
        "title": "A new set of Monte Carlo moves for lattice random-walk models of biased\n  diffusion",
        "authors": [
            "Michel G. Gauthier",
            "Gary W. Slater"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We recently demonstrated that standard fixed-time lattice random-walk models\ncannot be modified to properly represent biased diffusion processes in more\nthan two dimensions. The origin of this fundamental limitation appears to be\nthe fact that traditional Monte Carlo moves do not allow for simultaneous jumps\nalong each spatial direction. We thus propose a new algorithm to transform\nbiased diffusion problems into lattice random walks such that we recover the\nproper dynamics for any number of spatial dimensions and for arbitrary values\nof the external field. Using a hypercubic lattice, we redefine the basic Monte\nCarlo moves, including the transition probabilities and the corresponding time\ndurations, in order to allow for simultaneous jumps along all Cartesian axes.\nWe show that our new algorithm can be used both with computer simulations and\nwith exact numerical methods to obtain the mean velocity and the diffusion\ncoefficient of point-like particles in any dimensions and in the presence of\nobstacles.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0411139v1"
    },
    {
        "title": "Estimation of time delay by coherence analysis",
        "authors": [
            "R. B. Govindan",
            "J. Raethjen",
            "F. Kopper",
            "J. C. Claussen",
            "G. Deuschl"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Using coherence analysis (which is an extensively used method to study the\ncorrelations in frequency domain, between two simultaneously measured signals)\nwe estimate the time delay between two signals. This method is suitable for\ntime delay estimation of narrow band coherence signals for which the\nconventional methods cannot be reliably applied. We show by analysing coupled\nR\\\"ossler attractors with a known delay, that the method yields satisfactory\nresults. Then, we apply this method to human pathologic tremor. The delay\nbetween simultaneously measured traces of Electroencephalogram (EEG) and\nElectromyogram (EMG) data of subjects with essential hand tremor is calculated.\nWe find that there is a delay of 11-27 milli-seconds ($ms$) between the tremor\ncorrelated parts (cortex) of the brain (EEG) and the trembling hand (EMG) which\nis in agreement with the experimentally observed delay value of 15 $ms$ for the\ncortico-muscular conduction time. By surrogate analysis we calculate error-bars\nof the estimated delay.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0411141v1"
    },
    {
        "title": "Observations of three slow glitches in the spin rate of the pulsar\n  B1822-09",
        "authors": [
            "T. V. Shabanova"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Three slow glitches in the rotation rate of the pulsar B1822-09 were revealed\nover the 1995-2004 interval. The slow glitches observed are characterized by a\ngradual increase in the rotation frequency with a long timescale of several\nmonths, accompanied by a rapid decrease in the magnitude of the frequency first\nderivative by 1-2 per cent of the initial value and subsequent exponential\nincrease back to its initial value on the same timescale. The cumulative\nfractional increase in the pulsar rotation rate for the three glitches amounts\nto Delta_nu/nu ~ 7 10^{-8}.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0411151v1"
    },
    {
        "title": "Application of Multifractal Measures to Tehran Price Index",
        "authors": [
            "P. Norouzzadeh",
            "G. R. Jafari"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We report an empirical study of Tehran Price Index (TEPIX). To analyze our\ndata we use various methods like as, rescaled range analysis ($R/S$), modified\nrescaled range analysis (Lo's method), Detrended Fluctuation Analysis (DFA) and\ngeneralized Hurst exponents analysis. Based on numerical results, the scaling\nrange of TEPIX returns is specified, long memory effect or long range\ncorrelation property in this market is investigated, characteristic exponent\nfor probability distribution function of TEPIX returns is derived and finally\nthe stage of development in Tehran Stock Exchange is determined.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0412067v1"
    },
    {
        "title": "Inferring the success parameter p of a binomial model from small samples\n  affected by background",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  The problem of inferring the binomial parameter p from x successes obtained\nin n trials is reviewed and extended to take into account the presence of\nbackground, that can affect the data in two ways: a) fake successes are due to\na background modeled as a Poisson process of known intensity; b) fake trials\nare due to a background modeled as a Poisson process of known intensity, each\ntrial being characterized by a known success probability p_b.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0412069v1"
    },
    {
        "title": "Optimal Determination of the Equilibrium Displacement of a Damped\n  Harmonic Oscillator in the Presence of Thermal Noise",
        "authors": [
            "M. W. Moore",
            "J. H. Steffen",
            "P. E. Boynton"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Using a matched filter technique, we derive the minimum variance, unbiased\nestimator for the equilibrium displacement of a damped harmonic oscillator in\nthermal equilibrium when interactions with the thermal bath are the leading\nsource of noise. We compare the variance in this optimal estimator with the\nvariance in other, commonly used estimators in the presence of pure thermal\nnoise and pure white noise. We also compare the variance in these estimators\nfor a mixture of white and thermal noise. This result has implications for\nexperimental design and the collection and analysis of data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0412102v1"
    },
    {
        "title": "Six years of BeppoSAX observations of blazars: a spectral catalog",
        "authors": [
            "D. Donato",
            "R. M. Sambruna",
            "M. Gliozzi"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We present a spectral catalog for blazars based on the BeppoSAX archive. The\nsample includes 44 High-energy peaked BL Lacs (HBLs), 14 Low-energy peaked BL\nLacs (LBLs), and 28 Flat Spectrum Radio Quasars (FSRQs). A total of 168 LECS,\nMECS, and PDS spectra were analyzed, corresponding to observations taken in the\nperiod 1996--2002. The 0.1--50 keV continuum of LBLs and FSRQs is generally\nfitted by a single power law with Galactic column density. A minority of the\nobservations of LBLs (25%) and FSRQs (15%) is best fitted by more complex\nmodels like the broken power law or the continuously curved parabola. These\nlatter models provide also the best description for half of the HBL spectra.\nComplex models are more frequently required for sources with fluxes F_{2-10\nkeV} > 10^-11 cm-2 s-1, corresponding to spectra with higher signal-to-noise\nratio. As a result, considering sources with flux above this threshold, the\npercentage of spectra requiring those models increases for all the classes. We\nnote that there is a net separation of X-ray spectral properties between HBLs\non one side, and LBLs and FSRQs on the other, the distinction between LBLs and\nFSRQs is more blurry. This is most likely related to ambiguities in the optical\nclassification of the two classes.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0412114v2"
    },
    {
        "title": "Late comment on Astumian's paradox",
        "authors": [
            "L. Pal"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  In 2001 Astumian published a very simple game which can be described by a\nMarkov chain with absorbing initial and final states. In August 2004 Piotrowski\nand Sladowski asserted that Astumian's analysis was flawed. However, as was\nshown by Astumian, this statement was wrong. In this comment the properties of\nMarkov chains corresponding to games that are more general than that studied by\nAstumian, are investigated.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0412162v1"
    },
    {
        "title": "Variable Step Random Walks and Self-Similar Distributions",
        "authors": [
            "Gemunu H. Gunaratne",
            "Joseph L. McCauley",
            "Matthew Nicol",
            "Andrei Torok"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We study a scenario under which variable step random walks give anomalous\nstatistics. We begin by analyzing the Martingale Central Limit Theorem to find\na sufficient condition for the limit distribution to be non-Gaussian. We note\nthat the theorem implies that the scaling index $\\zeta$ is 1/2. For\ncorresponding continuous time processes, it is shown that the probability\ndensity function $W(x;t)$ satisfies the Fokker-Planck equation. Possible forms\nfor the diffusion coefficient are given, and related to $W(x,t)$. Finally, we\nshow how a time-series can be used to distinguish between these variable\ndiffusion processes and L\\'evy dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0412182v1"
    },
    {
        "title": "A Multifractal Detrended Fluctuation Description of Iranian Rial-US\n  Dollar Exchange Rate",
        "authors": [
            "P. Norouzzadeh"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  The miltifractal properties and scaling behaviour of the exchange rate\nvariations of the Iranian rial against the US dollar from a daily perspective\nis numerically investigated. For this purpose the multifractal detrended\nfluctuation analysis (MF-DFA) is used. Through multifractal analysis, the\nscaling exponents, generalized Hurst exponents, generalized fractal dimensions\nand singularity spectrum are derived. Moreover, contribution of two major\nsources of multifractality, that is, fat-tailed probability distributions and\nnonlinear temporal correlations are studied.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0502150v1"
    },
    {
        "title": "Review of Nonlinear Methods and Modelling",
        "authors": [
            "Frank G. Borg"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  The first part of this Review describes a few of the main methods that have\nbeen employed in non-linear time series analysis with special reference to\nbiological applications (biomechanics). The second part treats the physical\nbasis of posturogram data (human balance) and EMG (electromyography, a measure\nof muscle activity).\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0503026v1"
    },
    {
        "title": "Why does the Standard GARCH(1,1) model work well?",
        "authors": [
            "G. R. Jafari",
            "A. Bahraminasab",
            "P. Norouzzadeh"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  The AutoRegressive Conditional Heteroskedasticity (ARCH) and its generalized\nversion (GARCH) family of models have grown to encompass a wide range of\nspecifications, each of them is designed to enhance the ability of the model to\ncapture the characteristics of stochastic data, such as financial time series.\nThe existing literature provides little guidance on how to select optimal\nparameters, which are critical in efficiency of the model, among the infinite\nrange of available parameters. We introduce a new criterion to find suitable\nparameters in GARCH models by using Markov length, which is the minimum time\ninterval over which the data can be considered as constituting a Markov\nprocess. This criterion is applied to various time series and results support\nthe known idea that GARCH(1,1) model works well.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0503027v2"
    },
    {
        "title": "Nonlinear Statistical Modelling and Model Discovery for\n  Cardiorespiratory Data",
        "authors": [
            "D. G. Luchinsky",
            "V. N. Smelyanskiy",
            "M. M. Millonas",
            "A. Stefanovska",
            "P. V. E. McClintock"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We present a Bayesian dynamical inference method for characterizing\ncardiorespiratory (CR) dynamics in humans by inverse modelling from blood\npressure time-series data. This new method is applicable to a broad range of\nstochastic dynamical models, and can be implemented without severe\ncomputational demands. A simple nonlinear dynamical model is found that\ndescribes a measured blood pressure time-series in the primary frequency band\nof the CR dynamics. The accuracy of the method is investigated using surrogate\ndata with parameters close to the parameters inferred in the experiment. The\nconnection of the inferred model to a well-known beat-to-beat model of the\nbaroreflex is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0503053v1"
    },
    {
        "title": "Two-Scale Kirchhoff Theory: Comparison of Experimental Observations With\n  Theoretical Prediction",
        "authors": [
            "G. R. Jafari",
            "P. Kaghazchi",
            "R. S. Dariani",
            "A. Iraji zad",
            "S. M. Mahdavi",
            "M. Reza Rahimi Tabar",
            "N. Taghavinia"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We introduce a non-perturbative two scale Kirchhoff theory, in the context of\nlight scattering by a rough surface. This is a two scale theory which considers\nthe roughness both in the wavelength scale (small scale) and in the scales much\nlarger than the wavelength of the incident light (large scale). The theory can\nprecisely explain the small peaks which appear at certain scattering angles.\nThese peaks can not be explained by one scale theories. The theory was assessed\nby calculating the light scattering profiles using the Atomic Force Microscope\n(AFM) images, as well as surface profilometer scans of a rough surface, and\ncomparing the results with experiments. The theory is in good agreement with\nthe experimental results.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0504149v1"
    },
    {
        "title": "Superposition of similar single-sided RTN processes and 1/f noise",
        "authors": [
            "Giovanni Zanella"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  In this paper it is demonstrated that a 1/f power spectrum appears in the\nprocess originated by the superposition of many similar single-sided RTN\nprocesses with the same relaxation time. The non-relaxed regime, the Gaussian\nnature and the average periodicity of the resulting fluctuation, are\nresponsible for the generation of 1/f noise, thanks to the coincidences of\nthese RTN processes. The decomposition of the resulting fluctuation, in a set\nof further single-sided RTN processes with a distribution of relaxation times,\npermits us to demonstrate once more the generation of the 1/f noise.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0504164v2"
    },
    {
        "title": "Estimation of coupling between oscillators from short time series via\n  phase dynamics modeling: limitations and application to EEG data",
        "authors": [
            "D. A. Smirnov",
            "M. B. Bodrov",
            "J. L. Perez Velazquez",
            "R. A. Wennberg",
            "B. P. Bezruchko"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We demonstrate in numerical experiments that estimators of strength and\ndirectionality of coupling between oscillators based on modeling of their phase\ndynamics [D.A. Smirnov and B.P. Bezruchko, Phys. Rev. E 68, 046209 (2003)] are\nwidely applicable. Namely, although the expressions for the estimators and\ntheir confidence bands are derived for linear uncoupled oscillators under the\ninfluence of independent sources of Gaussian white noise, they turn out to\nallow reliable characterization of coupling from relatively short time series\nfor different properties of noise, significant phase nonlinearity of the\noscillators, and non-vanishing coupling between them. We apply the estimators\nto analyze a two-channel human intracranial epileptic electroencephalogram\n(EEG) recording with the purpose of epileptic focus localization.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0506117v1"
    },
    {
        "title": "Stealth Communication: Zero-Power Classical Communication, Zero-Quantum\n  Quantum Communication and Environmental-Noise Communication",
        "authors": [
            "Laszlo B. Kish"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  An alternative physical way of communication, communication by the inherent\nbackground noise, is proposed which does not need net energy transfer in the\ninformation channel. The communicator devices do dissipate energy; however,\nthey do not emit net energy into the channel, instead of that, they modulate\nthe parameters of inherent spontaneous fluctuations in the channel. The method\ncan use two different mechanisms, thermal noise (Johnson-Nyquist noise) for\nclassical communication, and vacuum fluctuations/zero point energy (quantum\nuncertainty noise) for quantum communication. The strongest advantage of the\nmethod that this is apparently the most hidden (stealth) way of communication,\nbecause it is using the inherent background noise for communication, therefore\nit is extremely difficult or impossible to discover its presence. With proper\nwave-based arrangements and specific conditions, the sender and the receiver\ncan easily detect eavesdropper activities, so that the eavesdropper is detected\nas soon as she extracts a single bit of information, thus the security of the\nmethod is comparable to the security of quantum communication/quantum key\ndistribution schemes. Finally, concerning practical applications, environmental\nnoise, out of the fundamental/inherent fluctuations, can also be used for this\nkind of communication provided that is sufficiently stationary.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0508135v4"
    },
    {
        "title": "Multifractal Detrended Fluctuation Analysis of Sunspot Time Series",
        "authors": [
            "M. Sadegh Movahed",
            "G. R. Jafari",
            "F. Ghasemi",
            "Sohrab Rahvar",
            "M. Reza Rahimi Tabar"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We use multifractal detrended fluctuation analysis (MF-DFA), to See query 1\nstudy sunspot number fluctuations. The result of the MF-DFA shows that there\nare three crossover timescales in the fluctuation function. We discuss how the\nexistence of the crossover timescales is related to a sinusoidal trend. Using\nFourier detrended fluctuation analysis, the sinusoidal trend is eliminated. The\nHurst exponent of the time series without the sinusoidal trend is $0.12\\pm\n0.01$. Also we find that these fluctuations have multifractal nature. Comparing\nthe MF-DFA results for the remaining data set to those for shuffled and\nsurrogate series, we conclude that its multifractal nature is almost entirely\ndue to long range correlations.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0508149v2"
    },
    {
        "title": "Reactions to extreme events: moving threshold model",
        "authors": [
            "Eduardo G. Altmann",
            "Sarah Hallerberg",
            "Holger Kantz"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  In spite of precautions to avoid the harmful effects of extreme events, we\nexperience recurrently phenomena that overcome the preventive barriers. These\nbarriers usually increase drastically right after the occurrence of such\nextreme events, but steadily decay in their absence. In this paper we consider\na simple model that mimics the evolution of the protection barriers to study\nthe efficiency of the system's reaction to extreme events and how it changes\nour perception of the sequence of extreme events itself. We obtain that the\nusual method of fighting extreme events introduces a periodicity in their\noccurrence and is generally less efficient than the use of a constant barrier.\nOn the other hand, it shows a good adaptation to the presence of slow\nnon-stationarities.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0508170v1"
    },
    {
        "title": "Characteristic market behaviors caused by intervention in a foreign\n  exchange market",
        "authors": [
            "Takayuki Mizuno",
            "Yukiko Umeno Saito",
            "Tsutomu Watanabe",
            "Hideki Takayasu"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  In foreign exchange markets monotonic rate changes can be observed in time\nscale of order of an hour on the days that governmental interventions took\nplace. We estimate the starting time of an intervention using this\ncharacteristic behavior of the exchange rates. We find that big amount of\ninterventions can shift the averaged rate about 1 yen per 1 dollar in an hour,\nand the rate change distribution becomes asymmetric for a few hours.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0508172v1"
    },
    {
        "title": "Cross-country hierarchical structure and currency crisis",
        "authors": [
            "Guillermo J. Ortega",
            "David Matesanz"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Using data from a sample of 28 representatives countries, we propose a\nclassification of currency crises consequences based on the ultrametric\nanalysis of the real exchange rate movements time series, without any further\nassumption. By using the matrix of synchronous linear correlation coefficients\nand the appropriate metric distance between pairs of countries, we were able to\nconstruct a hierarchical tree of countries. This economic taxonomy provides\nrelevant information regarding liaisons between countries and a meaningful\ninsight about the contagion phenomenon\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0508188v1"
    },
    {
        "title": "A characteristic time scale of tick quotes on foreign currency markets",
        "authors": [
            "Aki-Hiro Sato"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  This study investigates that a characteristic time scale on an exchange rate\nmarket (USD/JPY) is examined for the period of 1998 to 2000. Calculating power\nspectrum densities for the number of tick quotes per minute and averaging them\nover the year yield that the mean power spectrum density has a peak at high\nfrequencies. Consequently it means that there exist the characteristic scales\nwhich dealers act in the market. A simple agent model to explain this\nphenomenon is proposed. This phenomena may be a result of stochastic resonance\nwith exogenous periodic information and physiological fluctuations of the\nagents. This may be attributed to the traders' behavior on the market. The\npotential application is both quantitative characterization and classification\nof foreign currency markets.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0509142v1"
    },
    {
        "title": "On statistical properties of traded volume in financial markets",
        "authors": [
            "Jeferson de Souza",
            "Luis G. Moyano",
            "Silvio M. Duarte Queiros"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  In this article we study the dependence degree of the traded volume of the\nDow Jones 30 constituent equities by using a nonextensive generalised form of\nthe Kullback-Leibler information measure. Our results show a slow decay of the\ndependence degree as a function of the lag. This feature is compatible with the\nexistence of non-linearities in this type time series. In addition, we\nintroduce a dynamical mechanism whose associated stationary probability density\nfunction (PDF) presents a good agreement with the empirical results.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0510112v2"
    },
    {
        "title": "Long-Term Trends in Given Name Frequencies in England and Wales",
        "authors": [
            "Douglas A. Galbi"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  The frequency distribution of personal given names offers important evidence\nabout the information economy. This paper presents data on the popularity of\nthe most frequent personal given names (first names) in England and Wales over\nthe past millennium. The popularity of a name is its frequency relative to the\ntotal name instances sampled. The data show that the popularity distribution of\nnames, like the popularity of other symbols and artifacts associated with the\ninformation economy, can be helpfully viewed as a power law. Moreover, the data\non name popularity suggest that historically distinctive changes in the\ninformation economy occurred in conjunction with the Industrial Revolution.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0511021v1"
    },
    {
        "title": "Multifractal analysis of the long-range correlations in the cardiac\n  dynamics of Drosophila melanogaster",
        "authors": [
            "Nikolay K. Vitanov",
            "Elka D. Yankulova"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Time series of heartbeat activity of humans can exhibit long-range\ncorrelations. In this paper we show that such kind of correlations can exist\nfor the heartbeat activity of much simpler species like Drosophila\nmelanogaster. By means of the method of multifractal detrended fluctuation\nanalysis (MFDFA) we calculate fractal spectra $f(\\alpha)$ and $h(q)$ and\ninvestigate the correlation properties of heartbeat activity of Drosophila with\ngenetic hearth defects for three consequent generations of species. We observe\nthat opposite to the case of humans the time series of the heartbeat activity\nof healtly Drosophila do not have scaling properties. Time series from flies\nwith genetic defects can be long-range correllated and can have multifractal\nproperties. The fractal heartbeat dynamics of Drosophila is transferred from\ngeneration to generation.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0511079v1"
    },
    {
        "title": "Fits, and especially linear fits, with errors on both axes, extra\n  variance of the data points and other complications",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  The aim of this paper, triggered by some discussions in the astrophysics\ncommunity raised by astro-ph/0508529, is to introduce the issue of `fits' from\na probabilistic perspective (also known as Bayesian), with special attention to\nthe construction of model that describes the `network of dependences' (a\nBayesian network) that connects experimental observations to model parameters\nand upon which the probabilistic inference relies. The particular case of\nlinear fit with errors on both axes and extra variance of the data points\naround the straight line (i.e. not accounted by the experimental errors) is\nshown in detail. Some questions related to the use of linear fit formulas to\nlog-linearized exponential and power laws are also sketched, as well as the\nissue of systematic errors.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0511182v1"
    },
    {
        "title": "Reply to Comment on \"The origin of bursts and heavy tails in human\n  dynamics\"",
        "authors": [
            "A. -L. Barabasi",
            "K. -I. Goh",
            "A. Vazquez"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Understanding human dynamics is of major scientific and practical importance\nand can be increasingly addressed in a quantitative fashion thanks to\nelectronic records capturing various human activity patterns. The authors of\nRef. [1] revisit the datasets studied in Ref. [2], making four technical\nobservations. Some of the observations of Ref. [1] are based on the authors'\nunfamiliarity with the details of the data collection process and have little\nrelevance to the findings of Ref. [2] and others are resolved in quantitative\nfashion by other authors [3].\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0511186v1"
    },
    {
        "title": "Comparison of the Hirsch-index with standard bibliometric indicators and\n  with peer judgment for 147 chemistry research groups",
        "authors": [
            "Anthony F. J. van Raan"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  In this paper we present characteristics of the statistical correlation\nbetween the Hirsch (h-) index and several standard bibliometric indicators, as\nwell as with the results of peer review judgment. We use the results of a large\nevaluation study of 147 university chemistry research groups in the Netherlands\ncovering the work of about 700 senior researchers during the period 1991-2000.\nThus, we deal with research groups rather than individual scientists, as we\nconsider the research group as the most important work floor unit in research,\nparticularly in the natural sciences. Furthermore, we restrict the citation\nperiod to a three-year window instead of life time counts in order to focus on\nthe impact of recent work and thus on current research performance. Results\nshow that the h-index and our bibliometric crown indicator both relate in a\nquite comparable way with peer judgments. But for smaller groups in fields with\nless heavy citation traffic the crown indicator appears to be a more\nappropriate measure of research performance.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0511206v2"
    },
    {
        "title": "The Growth of Business Firms: Theoretical Framework and Empirical\n  Evidence",
        "authors": [
            "Dongfeng Fu",
            "Fabio Pammolli",
            "S. V. Buldyrev",
            "Massimo Riccaboni",
            "Kaushik Matia",
            "Kazuko Yamasaki",
            "H. E. Stanley"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We introduce a model of proportional growth to explain the distribution of\nbusiness firm growth rates. The model predicts that the distribution is\nexponential in the central part and depicts an asymptotic power-law behavior in\nthe tails with an exponent 3. Because of data limitations, previous studies in\nthis field have been focusing exclusively on the Laplace shape of the body of\nthe distribution. In this article, we test the model at different levels of\naggregation in the economy, from products to firms to countries, and we find\nthat the model's predictions agree with empirical growth distributions and\nsize-variance relationships.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0512005v1"
    },
    {
        "title": "Spectrometer Calibration by Expectation Maximization Method",
        "authors": [
            "Lulin Yuan"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Expectation Maximization (EM) algorithm is a parameter estimation method from\nincomplete observations. In this paper, an implementation of this method to the\ncalibration of HKS spectrometer at Jefferson Lab is described. We show that the\napplication of EM method is able to calibrate the spectrometer properly in the\npresence of high background noise, while the traditional nonlinear Least Square\nmethod fail. The preliminary results of HKS spectrometer calibration is\npresented.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0512132v1"
    },
    {
        "title": "Characteristic time scales of tick quotes on foreign currency markets:\n  an empirical study and agent-based model",
        "authors": [
            "Aki-Hiro Sato"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Power spectrum densities for the number of tick quotes per minute (market\nactivity) on three currency markets (USD/JPY, EUR/USD, and JPY/EUR) for periods\nfrom January 1999 to December 2000 are analyzed. We find some peaks on the\npower spectrum densities at a few minutes. We develop the double-threshold\nagent model and confirm that stochastic resonance occurs for the market\nactivity of this model. We propose a hypothesis that the periodicities found on\nthe power spectrum densities can be observed due to stochastic resonance.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0512163v2"
    },
    {
        "title": "Analysis of Rainfall records in India: Self Organized Criticality and\n  Scaling",
        "authors": [
            "A. Sarkar",
            "P. Barat"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  The time series data of the monthly rainfall records (for the time period\n1871-2002) in All India and different regions of India are analyzed. It is\nfound that the distributions of the rainfall intensity exhibit perfect power\nlaw behavior. The scaling analysis revealed two distinct scaling regions in the\nrainfall time series.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0512197v1"
    },
    {
        "title": "Escape of a Uniform Random Walk from an Interval",
        "authors": [
            "T. Antal",
            "S. Redner"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We study the first-passage properties of a random walk in the unit interval\nin which the length of a single step is uniformly distributed over the finite\nrange [-a,a]. For a of the order of one, the exit probabilities to each edge of\nthe interval and the exit time from the interval exhibit anomalous properties\nstemming from the change in the minimum number of steps to escape the interval\nas a function of the starting point. As a decreases, first-passage properties\napproach those of continuum diffusion, but non-diffusive effects remain because\nof residual discreteness effects\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0512221v1"
    },
    {
        "title": "On the multi-fractal structure of traded volume in financial markets",
        "authors": [
            "L. G. Moyano",
            "J. de Souza",
            "S. M. Duarte Queiros"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  In this pre-print we explore the multi-fractal properties of 1 minute traded\nvolume of the equities which compose the Dow Jones 30. We also evaluate the\nweights of linear and non-linear dependences in the multi-fractal structure of\nthe observable. Our results show that the multi-fractal nature of traded volume\ncomes essencially from the non-Gaussian form of the probability density\nfunctions and from non-linear dependences.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0512240v1"
    },
    {
        "title": "Multiple scattering and attenuation corrections in Deep Inelastic\n  Neutron Scattering experiments",
        "authors": [
            "J. Dawidowski",
            "J. J. Blostein",
            "J. R. Granada"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Multiple scattering and attenuation corrections in Deep Inelastic Neutron\nScattering experiments are analyzed. The theoretical basis is stated, and a\nMonte Carlo procedure to perform the calculation is presented. The results are\ncompared with experimental data. The importance of the accuracy in the\ndescription of the experimental parameters is tested, and the implications of\nthe present results on the data analysis procedures is examined.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0601109v1"
    },
    {
        "title": "Statistical Properties of the Returns of Stock Prices of International\n  Markets",
        "authors": [
            "GabJin Oh",
            "Cheol-Jun Um",
            "Seunghwan Kim"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We investigate statistical properties of daily international market indices\nof seven countries, and high-frequency $S&P500$ and KOSDAQ data, by using the\ndetrended fluctuation method and the surrogate test. We have found that the\nreturns of international stock market indices of seven countries follow a\nuniversal power-law distribution with an exponent of $\\zeta \\approx 3$, while\nthe Korean stock market follows an exponential distribution with an exponent of\n$\\beta \\approx 0.7$. The Hurst exponent analysis of the original return, and\nits magnitude and sign series, reveal that the long-term-memory property, which\nis absent in the returns and sign series, exists in the magnitude time series\nwith $0.7 \\leq H \\leq 0.8$. The surrogate test shows that the magnitude time\nseries reflects the non-linearity of the return series, which helps to reveal\nthat the KOSDAQ index, one of the emerging markets, shows higher volatility\nthan a mature market such as the {S&P} 500 index.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0601126v1"
    },
    {
        "title": "Level Crossing Analysis of the Stock Markets",
        "authors": [
            "G. R. Jafari",
            "M. S. Movahed",
            "S. M. Fazeli",
            "M. Reza Rahimi Tabar",
            "S. F. Masoudi"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We investigate the average frequency of positive slope $\\nu_{\\alpha}^{+}$,\ncrossing for the returns of market prices.\n  The method is based on stochastic processes which no scaling feature is\nexplicitly required. Using this method we define new quantity to quantify stage\nof development and activity of stocks exchange. We compare the Tehran and\nwestern stock markets and show that some stocks such as Tehran (TEPIX) and New\nZealand (NZX) stocks exchange are emerge, and also TEPIX is a non-active market\nand financially motivated to absorb capital.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0601205v2"
    },
    {
        "title": "Changed reference models in proportionality analysis",
        "authors": [
            "Enrique Ordaz Romay"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In the statistical analysis of objects, samples and populations with\nquantitative variables, in many occasions we are interested in knowing the\nproportions that exist between the different variables from a same object; if\nthese proportions have relations of normality among them, that is to say, if in\nthe sample are these same proportions, and finally if the deviations of\nconcrete physical or statistical normality have meaning. The present study\nsuggests the strategy of to use reference models for the analysis of the\nproportionality, using like example the studies of human proportionality from\nthe method Phantom of Ross and Wilson and its extension towards scalable\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0602064v1"
    },
    {
        "title": "Detecting rich-club ordering in complex networks",
        "authors": [
            "Vittoria Colizza",
            "Alessandro Flammini",
            "M. Angeles Serrano",
            "Alessandro Vespignani"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Uncovering the hidden regularities and organizational principles of networks\narising in physical systems ranging from the molecular level to the scale of\nlarge communication infrastructures is the key issue for the understanding of\ntheir fabric and dynamical properties [1-5]. The ``rich-club'' phenomenon\nrefers to the tendency of nodes with high centrality, the dominant elements of\nthe system, to form tightly interconnected communities and it is one of the\ncrucial properties accounting for the formation of dominant communities in both\ncomputer and social sciences [4-8]. Here we provide the analytical expression\nand the correct null models which allow for a quantitative discussion of the\nrich-club phenomenon. The presented analysis enables the measurement of the\nrich-club ordering and its relation with the function and dynamics of networks\nin examples drawn from the biological, social and technological domains.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0602134v1"
    },
    {
        "title": "Collective synchronization induced by epidemic dynamics on complex\n  networks with communities",
        "authors": [
            "Gang Yan",
            "Zhong-Qian Fu",
            "Jie Ren",
            "Wen-Xu Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Much recent empirical evidence shows that \\textit{community structure} is\nubiquitous in the real-world networks. In this Letter, we propose a growth\nmodel to create scale-free networks with the tunable strength (noted by $Q$) of\ncommunity structure and investigate the influence of community strength upon\nthe collective synchronization induced by SIRS epidemiological process. Global\nand local synchronizability of the system is studied by means of an order\nparameter and the relevant finite-size scaling analysis is provided. The\nnumerical results show that, a phase transition occurs at $Q_c\\simeq0.835$ from\nglobal synchronization to desynchronization and the local synchronization is\nweakened in a range of intermediately large $Q$. Moreover, we study the impact\nof mean degree $<k>$ upon synchronization on scale-free networks.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0602137v2"
    },
    {
        "title": "Transformation of equations in analysis of proportionality through\n  referent models",
        "authors": [
            "Enrique Ordaz Romay"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In proportionality of objects, samples or populations, usually we work with Z\nscore of proportionality calculated through referent models, instead directly\nwith the variables of the objects in itself. In these studies we have the\nnecessity to transform, the equations that use the variables of the object, in\nequations that directly use like variables Z score. In the present work a\nmethod is developed to transform the parametric equations, in equations in\nvariables Z using like example the studies of human proportionality from the\nPhantom stratagem of Ross and Wilson.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0602154v1"
    },
    {
        "title": "Functional dissipation microarrays for classification",
        "authors": [
            "D. Napoletani",
            "D. C. Struppa",
            "T. Sauer",
            "V. Morozov",
            "N. Vsevolodov",
            "C. Bailey"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In this article, we describe a new method of extracting information from\nsignals, called functional dissipation, that proves to be very effective for\nenhancing classification of high resolution, texture-rich data. Our algorithm\nbypasses to some extent the need to have very specialized feature extraction\ntechniques, and can potentially be used as an intermediate, feature enhancement\nstep in any classification scheme.\n  Functional dissipation is based on signal transforms, but uses the transforms\nrecursively to uncover new features. We generate a variety of masking functions\nand `extract' features with several generalized matching pursuit iterations. In\neach iteration, the recursive process modifies several coefficients of the\ntransformed signal with the largest absolute values according to the specific\nmasking function; in this way the greedy pursuit is turned into a slow,\ncontrolled, dissipation of the structure of the signal that, for some masking\nfunctions, enhances separation among classes.\n  Our case study in this paper is the classification of crystallization\npatterns of amino acids solutions affected by the addition of small quantities\nof proteins.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0603002v2"
    },
    {
        "title": "Wavelet analysis of event by event fluctuations",
        "authors": [
            "P. Manimaran",
            "Prasanta K. Panigrahi"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The temporal fluctuations of produced hadron density in heavy ion collisions,\nmodelled by 2D Ising model at temperatures $T_c$ and below, are studied through\na recently developed wavelet based fluctuation analysis method. At $T_c$,\nlong-range correlated multifractal behavior, matching with the recently\nobserved Hurst exponent $H\\simeq 1$, is found. Below $T_c$ uncorrelated\nmonofractal behavior is seen. The correlation behavior compares well with the\nresults obtained from continuous wavelet based average wavelet co-efficient\nmethod, as well as with Fourier power spectral analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0603150v1"
    },
    {
        "title": "Additive N-Step Markov Chains as Prototype Model of Symbolic Stochastic\n  Dynamical Systems with Long-Range Correlations",
        "authors": [
            "Z. A. Mayzelis",
            "S. S. Apostolov",
            "S. S. Mel'nyk",
            "O. V. Usatenko",
            "V. A. Yampol'skii"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A theory of symbolic dynamic systems with long-range correlations based on\nthe consideration of the binary N-step Markov chains developed earlier in Phys.\nRev. Lett. 90, 110601 (2003) is generalized to the biased case (non equal\nnumbers of zeros and unities in the chain). In the model, the conditional\nprobability that the i-th symbol in the chain equals zero (or unity) is a\nlinear function of the number of unities (zeros) among the preceding N symbols.\nThe correlation and distribution functions as well as the variance of number of\nsymbols in the words of arbitrary length L are obtained analytically and\nverified by numerical simulations. A self-similarity of the studied stochastic\nprocess is revealed and the similarity group transformation of the chain\nparameters is presented. The diffusion Fokker-Planck equation governing the\ndistribution function of the L-words is explored. If the persistent\ncorrelations are not extremely strong, the distribution function is shown to be\nthe Gaussian with the variance being nonlinearly dependent on L. An equation\nconnecting the memory and correlation function of the additive Markov chain is\npresented. This equation allows reconstructing a memory function using a\ncorrelation function of the system. Effectiveness and robustness of the\nproposed method is demonstrated by simple model examples. Memory functions of\nconcrete coarse-grained literary texts are found and their universal power-law\nbehavior at long distances is revealed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0603170v2"
    },
    {
        "title": "Memory functions and Correlations in Additive Binary Markov Chains",
        "authors": [
            "S. S. Melnyk",
            "O. V. Usatenko",
            "V. A. Yampol'skii",
            "S. S. Apostolov",
            "Z. A. Mayzelis"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A theory of additive Markov chains with long-range memory, proposed earlier\nin Phys. Rev. E 68, 06117 (2003), is developed and used to describe statistical\nproperties of long-range correlated systems. The convenient characteristics of\nsuch systems, a memory function, and its relation to the correlation properties\nof the systems are examined. Various methods for finding the memory function\nvia the correlation function are proposed. The inverse problem (calculation of\nthe correlation function by means of the prescribed memory function) is also\nsolved. This is demonstrated for the analytically solvable model of the system\nwith a step-wise memory function.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0603171v1"
    },
    {
        "title": "Equivalence of Markov's Symbolic Sequences to Two-Sided Chains",
        "authors": [
            "S. S. Apostolov",
            "Z. A. Mayzelis",
            "O. V. Usatenko",
            "V. A. Yampol'skii"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A new object of the probability theory, two-sided chain of events (symbols),\nis introduced. A theory of multi-steps Markov chains with long-range memory,\nproposed earlier in Phys. Rev. E 68, 06117 (2003), is developed and used to\nestablish the correspondence between these chains and two-sided ones. The\nMarkov chain is proved to be statistically equivalent to the definite two-sided\none and vice versa. The results obtained for the binary chains are generalized\nto the chains taking on the arbitrary number of states.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0603172v1"
    },
    {
        "title": "Finding Scientific Gems with Google",
        "authors": [
            "P. Chen",
            "H. Xie",
            "S. Maslov",
            "S. Redner"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We apply the Google PageRank algorithm to assess the relative importance of\nall publications in the Physical Review family of journals from 1893--2003.\nWhile the Google number and the number of citations for each publication are\npositively correlated, outliers from this linear relation identify some\nexceptional papers or \"gems\" that are universally familiar to physicists.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0604130v1"
    },
    {
        "title": "Detecting degree symmetries in networks",
        "authors": [
            "Petter Holme"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The surrounding of a vertex in a network can be more or less symmetric. We\nderive measures of a specific kind of symmetry of a vertex which we call degree\nsymmetry -- the property that many paths going out from a vertex have\noverlapping degree sequences. These measures are evaluated on artificial and\nreal networks. Specifically we consider vertices in the human metabolic\nnetwork. We also measure the average degree-symmetry coefficient for different\nclasses of real-world network. We find that most studied examples are weakly\npositively degree-symmetric. The exceptions are an airport network (having a\nnegative degree-symmetry coefficient) and one-mode projections of social\naffiliation networks that are rather strongly degree-symmetric.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605029v1"
    },
    {
        "title": "Mean number of visits to sites in Levy flights",
        "authors": [
            "M. Ferraro",
            "L. Zaninetti"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Formulas are derived to compute the mean number of times a site has been\nvisited during symmetric Levy flights. Unrestricted Levy flights are considered\nfirst, for lattices of any dimension: conditions for the existence of finite\nasymptotic maps of the visits over the lattice are analysed and a connection is\nmade with the transience of the flight. In particular it is shown that flights\non lattices of dimension greater than one are always transient. For an interval\nwith absorbing boundaries the mean number of visits reaches stationary values,\nwhich are computed by means of numerical and analytical methods; comparisons\nwith Monte Carlo simulations are also presented.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605119v1"
    },
    {
        "title": "Greatly enhancing the modeling accuracy for distributed parameter\n  systems by nonlinear time/space separation",
        "authors": [
            "Hai-Tao Zhang",
            "Chen-Kun Qi",
            "Tao Zhou",
            "Han-Xiong Li"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  An effective modeling method for nonlinear distributed parameter systems\n(DPSs) is critical for both physical system analysis and industrial\nengineering. In this Rapid Communication, we propose a novel DPS modeling\napproach, in which a high-order nonlinear Volterra series is used to separate\nthe time/space variables. With almost no additional computational complexity,\nthe modeling accuracy is improved more than 20 times in average comparing with\nthe traditional method.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605136v3"
    },
    {
        "title": "World University Rankings - A Principal Component Analysis",
        "authors": [
            "Joao E. Steiner"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In order to establish which parameters and corresponding weights are more\nappropriate for the assessment of academic excellence in the context of\nuniversity rankings, I have made a multivariate data analysis on a set of 13\nparameters for 178 institutions. I found that the three more relevant\ncomponents are academic excellence (48%), internationalism (14%) and\nfaculty/student ratio (8%). It is shown that these components are not\ncorrelated.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605252v1"
    },
    {
        "title": "A Test for the Presence of a Signal",
        "authors": [
            "Wolfgang A. Rolke",
            "Angel M. Lopez"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We describe a statistical hypothesis test for the presence of a signal based\non the likelihood ratio statistic. We derive the test for a case of interest\nand also show that for that case the test works very well, even far out in the\ntails of the distribution. We also study extensions of the test to cases where\nthere are multiple channels.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0606006v2"
    },
    {
        "title": "Data production models for the CDF experiment",
        "authors": [
            "J. Antos",
            "M. Babik",
            "D. Benjamin",
            "S. Cabrera",
            "A. W. Chan",
            "Y. C. Chen",
            "M. Coca",
            "B. Cooper",
            "K. Genser",
            "K. Hatakeyama",
            "S. Hou",
            "T. L. Hsieh",
            "B. Jayatilaka",
            "A. C. Kraan",
            "R. Lysak",
            "I. V. Mandrichenko",
            "A. Robson",
            "M. Siket",
            "B. Stelzer",
            "J. Syu",
            "P. K. Teng",
            "S. C. Timm",
            "T. Tomura",
            "E. Vataga",
            "S. A. Wolbers",
            "P. Yeh"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The data production for the CDF experiment is conducted on a large Linux PC\nfarm designed to meet the needs of data collection at a maximum rate of 40\nMByte/sec. We present two data production models that exploits advances in\ncomputing and communication technology. The first production farm is a\ncentralized system that has achieved a stable data processing rate of\napproximately 2 TByte per day. The recently upgraded farm is migrated to the\nSAM (Sequential Access to data via Metadata) data handling system. The software\nand hardware of the CDF production farms has been successful in providing large\ncomputing and data throughput capacity to the experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0606039v1"
    },
    {
        "title": "Naming Game on small-world networks: the role of clustering structure",
        "authors": [
            "Bo-Yu Lin",
            "Jie Ren",
            "Hui-Jie Yang",
            "Bing-Hong Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Naming Game is a recently proposed model for describing how a multi-agent\nsystem can converge towards a consensus state in a self-organized way. In this\npaper, we investigate this model on the so-called homogeneous small-world\nnetworks and focus on the influence of the triangular topology on the dynamics.\nOf all the topological quantities, the clustering coefficient is found to play\na significant role in the dynamics of the Naming Game. On the one hand, it\naffects the maximum memory of each agent; on the other hand, it inhibits the\ngrowing of clusters in which agents share a common word, i.e., a larger\nclustering coefficient will cause a slower convergence of the system. We also\nfind a quantitative relationship between clustering coefficient and the maximum\nmemory.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607001v2"
    },
    {
        "title": "Delayed information flow effect in economy systems. An ACP model study",
        "authors": [
            "Janusz Miskiewicz",
            "Marcel Ausloos"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Applying any strategy requires some knowledge about the past state of the\nsystem. Unfortunately in the case of economy collecting information is a\ndifficult, expensive and time consuming process. Therefore the information\nabout the system is known at the end of some well defined intervals, e. g.\ncompany reports, inflation data, GDP etc. They describe a (market) situation in\nthe past. The time delay is specific to the market branch. It can be very short\n(e.g. stock market offer is updated every minute or so and this information is\nimmediately available) or long, like months in the case of agricultural market,\nwhen the decisions are taken based on the results from the previous harvest.\n  The analysis of the information flow delay can be based on the ACP model of\nspatial evolution of economic systems. The entities can move on a square\nlattice and when meeting take one of the two following decisions: merge or\ncreate a new entity. The decision is based on the system state, which is known\nwith some time delay. The effect of system's feedback is investigated. We\nconsider the case of company distribution evolution in a heterogenous field.\nThe information flow time delay implies different final states, including\ncycles.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607107v3"
    },
    {
        "title": "Bayesian Analysis of the Conditional Correlation Between Stock Index\n  Returns with Multivariate SV Models",
        "authors": [
            "Anna Pajor"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In the paper we compare the modelling ability of discrete-time multivariate\nStochastic Volatility models to describe the conditional correlations between\nstock index returns. We consider four trivariate SV models, which differ in the\nstructure of the conditional covariance matrix. Specifications with zero,\nconstant and time-varying conditional correlations are taken into account. As\nan example we study trivariate volatility models for the daily log returns on\nthe WIG, SP500, and FTSE100 indexes. In order to formally compare the relative\nexplanatory power of SV specifications we use the Bayesian principles of\ncomparing statistic models. Our results are based on the Bayes factors and\nimplemented through Markov Chain Monte Carlo techniques. The results indicate\nthat the most adequate specifications are those that allow for time-varying\nconditional correlations and that have as many latent processes as there are\nconditional variances and covariances. The empirical results clearly show that\nthe data strongly reject the assumption of constant conditional correlations.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607176v1"
    },
    {
        "title": "Long-term memory in the Irish market (ISEQ): evidence from wavelet\n  analysis",
        "authors": [
            "Adel Sharkasi",
            "Heather J. Ruskin",
            "Martin Crane"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Researchers have used many different methods to detect the possibility of\nlong-term dependence (long memory) in stock market returns, but evidence is in\ngeneral mixed. In this paper, three different tests, (namely Rescaled Range\n(R/S), its modified form, and the semi-parametric method (GPH)), in addition to\na new approach using the discrete wavelet transform, (DWT), have been applied\nto the daily returns of five Irish Stock Exchange (ISEQ) indices. These methods\nhave also been applied to the volatility measures (namely absolute and squared\nreturns). The aim is to investigate the existence of long-term memory\nproperties. The indices are Overall, Financial, General, Small Cap and ITEQ and\nthe results of these approaches show that there is no evidence of long-range\ndependence in the returns themselves, while there is strong evidence for such\ndependence in the squared and absolute returns. Moreover, the discrete wavelet\ntransform (DWT) provides additional insight on the series breakdown. In\nparticular, in comparison to other methods, the benefit of the wavelet\ntransform is that it provides a way to study the sensitivity of the series to\nincreases in amplitude of fluctuations as well as changes in frequency.\nFinally, based on results for these methods, in particular, those for DWT of\nraw (or original), squared and absolute returns, it can be concluded that there\nis strong indication for persistence in the volatilities of the emerging stock\nmarket returns for the Irish data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607182v1"
    },
    {
        "title": "A method for extracting the scaling exponents of a self-affine,\n  non-Gaussian process from a finite length timeseries",
        "authors": [
            "K. Kiyani",
            "S. C. Chapman",
            "B. Hnat"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We address the generic problem of extracting the scaling exponents of a\nstationary, self-affine process realised by a timeseries of finite length,\nwhere information about the process is not known a priori. Estimating the\nscaling exponents relies upon estimating the moments, or more typically\nstructure functions, of the probability density of the differenced timeseries.\nIf the probability density is heavy tailed, outliers strongly influence the\nscaling behaviour of the moments. From an operational point of view, we wish to\nrecover the scaling exponents of the underlying process by excluding a minimal\npopulation of these outliers. We test these ideas on a synthetically generated\nsymmetric alpha-stable Levy process and show that the Levy exponent is\nrecovered in up to the 6th order moment after only ~0.1-0.5% of the data are\nexcluded. The scaling properties of the excluded outliers can then be tested to\nprovide additional information about the system.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607238v1"
    },
    {
        "title": "Long-range dependence in Interest Rates and Monetary Policy",
        "authors": [
            "Daniel O. Cajueiro",
            "Benjamin M. Tabak"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  This paper studies the dynamics of Brazilian interest rates for short-term\nmaturities. The paper employs developed techniques in the econophysics\nliterature and tests for long-range dependence in the term structure of these\ninterest rates for the last decade. Empirical results suggest that the degree\nof long-range dependence has changed over time due to changes in monetary\npolicy, specially in the short-end of the term structure of interest rates.\nTherefore, we show that it is possible to identify monetary arrangements using\nthese techniques from econophysics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607245v1"
    },
    {
        "title": "Econophysics of interest rates and the role of monetary policy",
        "authors": [
            "Daniel O. Cajueiro",
            "Benjamin M. Tabak"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  This paper presents empirical evidence using recently developed techniques in\neconophysics suggesting that the degree of long-range dependence in interest\nrates depends on the conduct of monetary policy. We study the term structure of\ninterest rates for the US and find evidence that global Hurst exponents change\ndramatically according to Chairman Tenure in the Federal Reserve Board and also\nwith changes in the conduct of monetary policy. In the period from 1960's until\nthe monetarist experiment in the beginning of the 1980's interest rates had a\nsignificant long-range dependence behavior. However, in the recent period, in\nthe second part of the Volcker tenure and in the Greenspan tenure, interest\nrates do not present long-range dependence behavior. These empirical findings\ncast some light on the origins of long-range dependence behavior in financial\nassets.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607246v1"
    },
    {
        "title": "On the maximum drawdown during speculative bubbles",
        "authors": [
            "Giulia Rotundo",
            "Mauro Navarra"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A taxonomy of large financial crashes proposed in the literature locates the\nburst of speculative bubbles due to endogenous causes in the framework of\nextreme stock market crashes, defined as falls of market prices that are\noutlier with respect to the bulk of drawdown price movement distribution. This\npaper goes on deeper in the analysis providing a further characterization of\nthe rising part of such selected bubbles through the examination of drawdown\nand maximum drawdown movement of indices prices. The analysis of drawdown\nduration is also performed and it is the core of the risk measure estimated\nhere.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607250v2"
    },
    {
        "title": "Frequency analysis of tick quotes on the foreign exchange market and\n  agent-based modeling: A spectral distance approach",
        "authors": [
            "Aki-Hiro Sato"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  High-frequency financial data of the foreign exchange market (EUR/CHF,\nEUR/GBP, EUR/JPY, EUR/NOK, EUR/SEK, EUR/USD, NZD/USD, USD/CAD, USD/CHF,\nUSD/JPY, USD/NOK, and USD/SEK) are analyzed by utilizing the Kullback-Leibler\ndivergence between two normalized spectrograms of the tick frequency and the\ngeneralized Jensen-Shannon divergence among them. The temporal structure\nvariations of the similarity between currency pairs is detected and\ncharacterized. A simple agent-based model in which $N$ market participants\nexchange $M$ currency pairs is proposed. The equation for the tick frequency is\napproximately derived theoretically. Based on the analysis of this model, the\nspectral distance of the tick frequency is associated with the similarity of\nthe behavior (perception and decision) of the market participants in exchanging\nthese currency pairs.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607273v2"
    },
    {
        "title": "Asset Price Dynamics in a Financial Market with Heterogeneous Trading\n  Strategies and Time Delays",
        "authors": [
            "Giuseppe Garofalo",
            "Alessandro Sansone"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In this paper we present a continuous time dynamical model of heterogeneous\nagents interacting in a financial market where transactions are cleared by a\nmarket maker. The market is composed of fundamentalist, trend following and\ncontrarian agents who process information from the market with different time\ndelays. Each class of investor is characterized by path dependent risk\naversion. We also allow for the possibility of evolutionary switching between\ntrend following and contrarian strategies. We find that the system shows\nperiodic, quasi-periodic and chaotic dynamics as well as synchronization\nbetween technical traders. Furthermore, the model is able to generate time\nseries of returns that exhibit statistical properties similar to those of the\nS&P500 index, which is characterized by excess kurtosis, volatility clustering\nand long memory\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607276v1"
    },
    {
        "title": "Minimum Entropy Density Method for the Time Series Analysis",
        "authors": [
            "Jeong Won Lee",
            "Joongwoo Brian Park",
            "Hang-Hyun Jo",
            "Jae-Suk Yang",
            "Hie-Tae Moon"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The entropy density is an intuitive and powerful concept to study the\ncomplicated nonlinear processes derived from physical systems. We develop the\nminimum entropy density method (MEDM) to detect the structure scale of a given\ntime series, which is defined as the scale in which the uncertainty is\nminimized, hence the pattern is revealed most. The MEDM is applied to the\nfinancial time series of Standard and Poor's 500 index from February 1983 to\nApril 2006. Then the temporal behavior of structure scale is obtained and\nanalyzed in relation to the information delivery time and efficient market\nhypothesis.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607282v2"
    },
    {
        "title": "Multifractal Properties of the Ukraine Stock Market",
        "authors": [
            "A. Ganchuk",
            "V. Derbentsev",
            "V. Soloviev"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Recently the statistical characterizations of financial markets based on\nphysics concepts and methods attract considerable attentions. We used two\npossible procedures of analyzing multifractal properties of a time series. The\nfirst one uses the continuous wavelet transform and extracts scaling exponents\nfrom the wavelet transform amplitudes over all scales. The second method is the\nmultifractal version of the detrended fluctuation analysis method (MF-DFA). The\nmultifractality of a time series we analysed by means of the difference of\nvalues singularity stregth as a suitable way to characterise multifractality.\nSingularity spectrum calculated from daily returns using a sliding 1000 day\ntime window in discrete steps of 1-10 days. We discovered that changes in the\nmultifractal spectrum display distinctive pattern around significant\n\"drawdowns\". Finally, we discuss applications to the construction of crushes\nprecursors at the financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608009v1"
    },
    {
        "title": "Detrending Moving Average variance: a derivation of the scaling law",
        "authors": [
            "Sergio Arianos",
            "Anna Carbone"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The Hurst exponent $H$ of long range correlated series can be estimated by\nmeans of the Detrending Moving Average (DMA) method. A computational tool\ndefined within the algorithm is the generalized variance $\n\\sigma_{DMA}^2={1}/{(N-n)}\\sum_i [y(i)-\\widetilde{y}_n(i)]^2\\:$, with\n$\\widetilde{y}_n(i)= {1}/{n}\\sum_{k}y(i-k)$ the moving average, $n$ the moving\naverage window and $N$ the dimension of the stochastic series $y(i)$. This\nability relies on the property of $\\sigma_{DMA}^2$ to scale as $n^{2H}$. Here,\nwe analytically show that $\\sigma_{DMA}^2$ is equivalent to $C_H n^{2H}$ for\n$n\\gg 1$ and provide an explicit expression for $C_H$.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608313v1"
    },
    {
        "title": "A Generalized Preferential Attachment Model for Business Firms Growth\n  Rates: I. Empirical Evidence",
        "authors": [
            "Fabio Pammolli",
            "Dongfeng Fu",
            "S. V. Buldyrev",
            "Massimo Riccaboni",
            "Kaushik Matia",
            "Kazuko Yamasaki",
            "H. E. Stanley"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We introduce a model of proportional growth to explain the distribution\n$P(g)$ of business firm growth rates. The model predicts that $P(g)$ is Laplace\nin the central part and depicts an asymptotic power-law behavior in the tails\nwith an exponent $\\zeta=3$. Because of data limitations, previous studies in\nthis field have been focusing exclusively on the Laplace shape of the body of\nthe distribution. We test the model at different levels of aggregation in the\neconomy, from products, to firms, to countries, and we find that the its\npredictions are in good agreement with empirical evidence on both growth\ndistributions and size-variance relationships.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0609011v1"
    },
    {
        "title": "Knowledge Network Approach to Noise Reduction",
        "authors": [
            "Arturo Berrones"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Previous preliminary results on the application of knowledge networks to\nnoise reduction in stationary harmonic and weakly chaotic signals are extended\nto more general cases. The formalism gives a novel algorithm from which\nstatistical tests for the identification of deterministic behavior in noisy\nstationary time series can be constructed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0609048v1"
    },
    {
        "title": "Fast Spatial Prediction from Inhomogeneously Sampled Data Based on\n  Generalized Random Fields with Gibbs Energy Functionals",
        "authors": [
            "D. T. Hristopulos",
            "S. N. Elogne"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  An explicit optimal linear spatial predictor is derived. The spatial\ncorrelations are imposed by means of Gibbs energy functionals with explicit\ncoupling coefficients instead of covariance matrices. The model inference\nprocess is based on physically identifiable constraints corresponding to\ndistinct terms of the energy functional. The proposed predictor is compared\nwith the geostatistical linear optimal filter (kriging) using simulated data.\nThe agreement between the two methods is excellent. The proposed framework\nallows a unified approach to the problems of parameter inference, spatial\nprediction and simulation of spatial random fields.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0609071v1"
    },
    {
        "title": "Direct sample estimates of multidimensional quadratic statistical\n  functions: application to the anisotropic KPZ equation",
        "authors": [
            "Ivailo S. Atanasov",
            "Oleg I. Yordanov"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We suggest a class of direct sample estimates for the two-point quadratic\nstatistical functions of multidimensional data, which includes: estimates of\nthe sample autocovariance function (AcF), sample mean square increment (also,\nstructure) function, and the estimate of the power spectrum. The central\nestimate for the class is the sample AcF, which is constructed as to represent\nthe finite Fourier transform of the periodogram estimate of the spectrum and is\npositive semidefinite. The estimate explicitly account for the anisotropy of\nthe fields in all spatial directions and is illustrated on two examples: the\nmorphology of the Grab nebula and the surface roughness generated as a solution\nof the anisotropic Kardar-Parisi-Zhang equation. We also provide an expression\nof the covariance of the sample AcF in the case of data assumed to be drawn\nfrom a two-dimensional Gaussian field with a known mean.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0609082v1"
    },
    {
        "title": "The Flow Dimension and Capacity for Structuring Urban Street Networks",
        "authors": [
            "Bin Jiang"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  This paper aims to measure the efficiency of urban street networks (a kind of\ncomplex networks) from the perspective of the multidimensional chain of\nconnectivity (or flow). More specifically, we define two quantities: flow\ndimension and flow capacity, to characterize structures of urban street\nnetworks. To our surprise for the topologies of urban street networks,\npreviously confirmed as a form of small world and scale-free networks, we find\nthat (1) the range of their flow dimension is rather wider than their random\nand regular counterparts, (2) their flow dimension shows a power-law\ndistribution, and (3) they have a higher flow capacity than their random and\nregular counterparts. The findings confirm that (1) both the wider range of\nflow dimension and the higher flow capacity can be a signature of small world\nnetworks, and (2) the flow capacity can be an alternative quantity for\nmeasuring the efficiency of networks or that of the individual nodes. The\nfindings are illustrated using three urban street networks (two in the Europe\nand one in the USA).\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0609095v4"
    },
    {
        "title": "On data analysis and variable selection: the minimum entropy analysis",
        "authors": [
            "Chih-Yuan Tseng",
            "Chien-Chih CHen"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In this work, we present a minimum entropy analysis scheme for variable\nselection and preliminary data analysis. The variable selection can be achieved\nby the increasing preference of variables. We show such a preference to has a\nunqiue form, which is given by the entropy of models associated with variables.\nEvaluating the entropy provides a complete ranking scheme of variables. This\nscheme not only indicates preferred variables but also may reveal the system's\nnature and properties. We illustrate the proposed scheme to analyze a set of\ngeological data for three carbonate rock units in Texas and Oklahoma, and\ncompare to the discriminant function analysis. The result suggests this scheme\nto provide a quick and robust analysis, and the use in data analysis is\npromising.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0609250v1"
    },
    {
        "title": "Variable selection based on entropic criterion and its application to\n  the debris-flow triggering",
        "authors": [
            "Chien-chih Chen",
            "Chih-Yuan Tseng",
            "Jia-Jyun Dong"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We propose a new data analyzing scheme, the method of minimum entropy\nanalysis (MEA), in this paper. New MEA provides a quantitative criterion to\nselect relevant variables for modeling the physical system interested. Such\nmethod can be easily extended to various geophysical/geological data analysis,\nwhere many relevant or irrelevant available measurements may obscure the\nunderstanding of the highly complicated physical system like the triggering of\ndebris-flows. After demonstrating and testing the MEA method, we apply this\nmethod to a dataset of debris-flow occurrences in Taiwan and successfully find\nout three relevant variables, i.e. the hydrological form factor, numbers and\nareas of landslides, to the triggering of observed debris-flow events due to\nthe 1996 Typhoon Herb.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0609252v1"
    },
    {
        "title": "High-Order Correlation Functions of Binary Multi-Step Markov Chains",
        "authors": [
            "S. S. Apostolov",
            "Z. A. Mayzelis",
            "O. V. Usatenko",
            "V. A. Yampol'skii"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Two approaches to studying the correlation functions of the binary Markov\nsequences are considered. The first of them is based on the study of\nprobability of occurring different ''words'' in the sequence. The other one\nuses recurrence relations for correlation functions. These methods are applied\nfor two important particular classes of the Markov chains. These classes\ninclude the Markov chains with permutative conditional probability functions\nand the additive Markov chains with the small memory functions. The exciting\nproperty of the self-similarity (discovered in Phys. Rev. Lett. 90, 110601\n(2003) for the additive Markov chain with the step-wise memory function) is\nproved to be the intrinsic property of any permutative Markov chain.\nApplicability of the correlation functions of the additive Markov chains with\nthe small memory functions to calculating the thermodynamic characteristics of\nthe classical Ising spin chain with long-range interaction is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0610081v1"
    },
    {
        "title": "On Computation of Combined IVS EOP Series",
        "authors": [
            "Zinovy Malkin"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Three topics related to computation of the combined IVS EOP series are\ndiscussed. The first one is the consistency of the VLBI EOP series with the\nIERS reference frames ITRF and ICRF. Not all IVS analysis centers use ITRF/ICRF\nas reference system for EOP solutions. Some of them realize global solution for\nsimultaneous determination of CRF, TRF and EOP with no-net-rotation constrains\nw.r.t. ITRF/ICRF. Analysis shows that such a method can hardly provide\nconsistency of computed EOP series with the IERS reference frames with\nsufficient accuracy. Direct use of ITRF and ICRF for computation of EOP series\nsubmitted to the IERS seems preferable. Second problem is the long-time\nstability of the IVS EOP series. Analysis of yearly biases w.r.t. C04 and NEOS\nis presented. A possible ways are proposed to save long time stability of the\ncombined IVS EOP series. At last, various strategies of computation of weighted\nmean value are considered. It's shown that usual methods used for this purpose\ndo not provide satisfactory result for the error of the mean. A new method is\nproposed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0610251v1"
    },
    {
        "title": "Infectious Default Model with Recovery and Continuous Limit",
        "authors": [
            "Ayaka Sakata",
            "Masato Hisakado",
            "Shintaro Mori"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We introduce an infectious default and recovery model for N obligors.\nObligors are assumed to be exchangeable and their states are described by N\nBernoulli random variables S_{i} (i=1,...,N). They are expressed by multiplying\nindependent Bernoulli variables X_{i},Y_{ij},Y'_{ij}, and default and recovery\ninfections are described by Y_{ij} and Y'_{ij}. We obtain the default\nprobability function P(k) for k defaults. Taking its continuous limit, we find\ntwo nontrivial probability distributions with the reflection symmetry of S_{i}\n\\leftrightarrow 1-S_{i}. Their profiles are singular and oscillating and we\nunderstand it theoretically. We also compare P(k) with an implied default\ndistribution function inferred from the quotes of iTraxx-CJ. In order to\nexplain the behavior of the implied distribution, the recovery effect may be\nnecessary.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0610275v2"
    },
    {
        "title": "Maximum Likelihood Estimation of Drift and Diffusion Functions",
        "authors": [
            "D. Kleinhans",
            "R. Friedrich"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The maximum likelihood approach is adapted to the problem of estimation of\ndrift and diffusion functions of stochastic processes from measured time\nseries. We reconcile a previously devised iterative procedure [Kleinhans et\nal., Physics Letters A (346), 2005] and put the application of the method on a\nfirm theoretical basis.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0611102v2"
    },
    {
        "title": "Optimal Proton Trapping in a Neutron Lifetime Experiment",
        "authors": [
            "Kevin J. Coakley"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In a neutron lifetime experiment conducted at the National Institute of\nStandards and Technology, protons produced by neutron decay events are confined\nin a Penning trap. In each run of the experiment, there is a trapping stage of\nduration $\\tau$. After the trapping stage, protons are purged from the trap. A\nproton detector provides incomplete information because it goes dead after\ndetecting the first of any purged protons. Further, there is a dead time\n$\\delta$ between the end of the trapping stage in one run and the beginning of\nthe next trapping stage in the next run. Based on the fraction of runs where a\nproton is detected, I estimate the trapping rate $\\lambda$ by the method of\nmaximum likelihood. I show that the expected value of the maximum likelihood\nestimate is infinite. To obtain a maximum likelihood estimate with a finite\nexpected value and a well-defined and finite variance, I restrict attention to\na subsample of all realizations of the data. This subsample excludes an\nexceedingly rare realization that yields an infinite-valued estimate of\n$\\lambda$. I present asymptotically valid formulas for the bias,\nroot-mean-square prediction error, and standard deviation of the maximum\nlikelihood estimate of $\\lambda$ for this subsample. Based on nominal values of\n$\\lambda$ and the dead time $\\delta$, I determine the optimal duration of the\ntrapping stage $\\tau$ by minimizing the root-mean-square prediction error of\nthe estimate.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0612036v2"
    },
    {
        "title": "Topological fractal networks introduced by mixed degree distribution",
        "authors": [
            "Liuhua Zou",
            "Wenjiang Pei",
            "Tao Li",
            "Zhenya He",
            "Yiuming Cheung"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Several fundamental properties of real complex networks, such as the\nsmall-world effect, the scale-free degree distribution, and recently discovered\ntopological fractal structure, have presented the possibility of a unique\ngrowth mechanism and allow for uncovering universal origins of collective\nbehaviors. However, highly clustered scale-free network, with power-law degree\ndistribution, or small-world network models, with exponential degree\ndistribution, are not self-similarity. We investigate networks growth mechanism\nof the branching-deactivated geographical attachment preference that learned\nfrom certain empirical evidence of social behaviors. It yields high clustering\nand spectrums of degree distribution ranging from algebraic to exponential,\naverage shortest path length ranging from linear to logarithmic. We observe\nthat the present networks fit well with small-world graphs and scale-free\nnetworks in both limit cases (exponential and algebraic degree distribution\nrespectively), obviously lacking self-similar property under a length-scale\ntransformation. Interestingly, we find perfect topological fractal structure\nemerges by a mixture of both algebraic and exponential degree distributions in\na wide range of parameter values. The results present a reliable connection\namong small-world graphs, scale-free networks and topological fractal networks,\nand promise a natural way to investigate universal origins of collective\nbehaviors.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0612056v2"
    },
    {
        "title": "Characterizing and modeling cyclic behavior in non-stationary time\n  series through multi-resolution analysis",
        "authors": [
            "Dilip P. Ahalpara",
            "Amit Verma",
            "Prasanta K. Panigrahi",
            "Jitendra C. Parikh"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A method based on wavelet transform and genetic programming is proposed for\ncharacterizing and modeling variations at multiple scales in non-stationary\ntime series. The cyclic variations, extracted by wavelets and smoothened by\ncubic splines, are well captured by genetic programming in the form of\ndynamical equations. For the purpose of illustration, we analyze two different\nnon-stationary financial time series, S&P CNX Nifty closing index of the\nNational Stock Exchange (India) and Dow Jones industrial average closing values\nthrough Haar, Daubechies-4 and continuous Morlet wavelets for studying the\ncharacter of fluctuations at different scales, before modeling the cyclic\nbehavior through GP. Cyclic variations emerge at intermediate time scales and\nthe corresponding dynamical equations reveal characteristic behavior at\ndifferent scales.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0612221v1"
    },
    {
        "title": "Turbulence Time Series Data Hole Filling using Karhunen-Loeve and ARIMA\n  methods",
        "authors": [
            "M. P. J. L. Chang",
            "H. Nazari",
            "C. O. Font",
            "G. C. Gilbreath",
            "E Oh"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Measurements of optical turbulence time series data using unattended\ninstruments over long time intervals inevitably lead to data drop-outs or\ndegraded signals. We present a comparison of methods using both Principal\nComponent Analysis, which is also known as the Karhunen--Loeve decomposition,\nand ARIMA that seek to correct for these event-induced and mechanically-induced\nsignal drop-outs and degradations. We report on the quality of the correction\nby examining the Intrinsic Mode Functions generated by Empirical Mode\nDecomposition. The data studied are optical turbulence parameter time series\nfrom a commercial long path length optical anemometer/scintillometer, measured\nover several hundred metres in outdoor environments.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0701238v1"
    },
    {
        "title": "Experimental evidence for phase synchronization transitions in human\n  cardio-respiratory system",
        "authors": [
            "Ronny Bartsch",
            "Jan W. Kantelhardt",
            "Thomas Penzel",
            "Shlomo Havlin"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Transitions in the dynamics of complex systems can be characterized by\nchanges in the synchronization behavior of their components. Taking the human\ncardio-respiratory system as an example and using an automated procedure for\nscreening the synchrograms of 112 healthy subjects we study the frequency and\nthe distribution of synchronization episodes under different physiological\nconditions that occur during sleep. We find that phase synchronization between\nheartbeat and breathing is significantly enhanced during non-rapid-eye-movement\n(non-REM) sleep (deep sleep and light sleep) and reduced during REM sleep. Our\nresults suggest that the synchronization is mainly due to a weak influence of\nthe breathing oscillator upon the heartbeat oscillator, which is disturbed in\nthe presence of long-term correlated noise, superimposed by the activity of\nhigher brain regions during REM sleep.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0701326v1"
    },
    {
        "title": "Yet on statistical properties of traded volume: correlation and mutual\n  information at different value magnitudes",
        "authors": [
            "Silvio M. Duarte Queiros",
            "Luis G. Moyano"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  In this article we analyse linear correlation and non-linear dependence of\ntraded volume, $v$, of the 30 constituents of Dow Jones Industrial Average at\ndifferent value scales. Specifically, we have raised $v$ to some real value\n$\\alpha $ or $\\beta $, which introduces a bias for small ($ \\alpha, \\beta <0$)\nor large ($\\alpha, \\beta >1$) values. Our results show that small values of $v$\nare regularly \\emph{anti-correlated} with values at other scales of traded\nvolume. This is consistent with the high liquidity of the 30 equities analysed\nand the asymmetric form of the multi-fractal spectrum for traded volume which\nhas supported the dynamical scenario presented by us.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0702185v1"
    },
    {
        "title": "A general approach to statistical modeling of physical laws:\n  nonparametric regression",
        "authors": [
            "I. Grabec"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Statistical modeling of experimental physical laws is based on the\nprobability density function of measured variables. It is expressed by\nexperimental data via a kernel estimator. The kernel is determined objectively\nby the scattering of data during calibration of experimental setup. A physical\nlaw, which relates measured variables, is optimally extracted from experimental\ndata by the conditional average estimator. It is derived directly from the\nkernel estimator and corresponds to a general nonparametric regression. The\nproposed method is demonstrated by the modeling of a return map of noisy\nchaotic data. In this example, the nonparametric regression is used to predict\na future value of chaotic time series from the present one. The mean predictor\nerror is used in the definition of predictor quality, while the redundancy is\nexpressed by the mean square distance between data points. Both statistics are\nused in a new definition of predictor cost function. From the minimum of the\npredictor cost function, a proper number of data in the model is estimated.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0089v1"
    },
    {
        "title": "Non-extensive thermodynamics of 1D systems with long-range interaction",
        "authors": [
            "S. S. Apostolov",
            "Z. A. Mayzelis",
            "O. V. Usatenko",
            "V. A. Yampol'skii"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  A new approach to non-extensive thermodynamical systems with non-additive\nenergy and entropy is proposed. The main idea of the paper is based on the\nstatistical matching of the thermodynamical systems with the additive\nmulti-step Markov chains. This general approach is applied to the Ising spin\nchain with long-range interaction between its elements. The asymptotical\nexpressions for the energy and entropy of the system are derived for the\nlimiting case of weak interaction. These thermodynamical quantities are found\nto be non-proportional to the length of the system (number of its particle).\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0887v1"
    },
    {
        "title": "Bibliometric statistical properties of the 100 largest European\n  universities: prevalent scaling rules in the science system",
        "authors": [
            "Anthony F. J. van Raan"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  For the 100 largest European universities we studied the statistical\nproperties of bibliometric indicators related to research performance, field\ncitation density and journal impact. We find a size-dependent cumulative\nadvantage for the impact of universities in terms of total number of citations.\nIn previous work a similar scaling rule was found at the level of research\ngroups. Therefore we conjecture that this scaling rule is a prevalent property\nof the science system. We observe that lower performance universities have a\nlarger size-dependent cumulative advantage for receiving citations than\ntop-performance universities. We also find that for the lower-performance\nuniversities the fraction of not-cited publications decreases considerably with\nsize. Generally, the higher the average journal impact of the publications of a\nuniversity, the lower the number of not-cited publications. We find that the\naverage research performance does not dilute with size. Large top-performance\nuniversities succeed in keeping a high performance over a broad range of\nactivities. This most probably is an indication of their scientific attractive\npower. Next we find that particularly for the lower-performance universities\nthe field citation density provides a strong cumulative advantage in citations\nper publication. The relation between number of citations and field citation\ndensity found in this study can be considered as a second basic scaling rule of\nthe science system. Top-performance universities publish in journals with\nsignificantly higher journal impact as compared to the lower performance\nuniversities. We find a significant decrease of the fraction of self-citations\nwith increasing research performance, average field citation density, and\naverage journal impact.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0889v1"
    },
    {
        "title": "Tail universalities in rank distributions as an algebraic problem: the\n  beta-like function",
        "authors": [
            "Gerardo G. Naumis",
            "Germinal Cocho"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Although power laws of the Zipf type have been used by many workers to fit\nrank distributions in different fields like in economy, geophysics, genetics,\nsoft-matter, networks etc., these fits usually fail at the tails. Some\ndistributions have been proposed to solve the problem, but unfortunately they\ndo not fit at the same time both ending tails. We show that many different data\nin rank laws, like in granular materials, codons, author impact in scientific\njournal, etc. are very well fitted by a beta-like function. Then we propose\nthat such universality is due to the fact that a system made from many\nsubsystems or choices, imply stretched exponential frequency-rank functions\nwhich qualitatively and quantitatively can be fitted with the proposed\nbeta-like function distribution in the limit of many random variables. We prove\nthis by transforming the problem into an algebraic one: finding the rank of\nsuccessive products of a given set of numbers.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.0551v1"
    },
    {
        "title": "Mykyta the Fox and networks of language",
        "authors": [
            "Yu. Holovatch",
            "V. Palchykov"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The results of quantitative analysis of word distribution in two fables in\nUkrainian by Ivan Franko: \"Mykyta the Fox\" and \"Abu-Kasym's slippers\" are\nreported. Our study consists of two parts: the analysis of frequency-rank\ndistributions and the application of complex networks theory. The analysis of\nfrequency-rank distributions shows that the text sizes are enough to observe\nstatistical properties. The power-law character of these distributions (Zipf's\nlaw) holds in the region of rank variable r=20 - 3000 with an exponent\n$\\alpha\\simeq 1$. This substantiates the choice of the above texts to analyse\ntypical properties of the language complex network on their basis. Besides, an\napplicability of the Simon model to describe non-asymptotic properties of word\ndistributions is evaluated.\n  In describing language as a complex network, usually the words are associated\nwith nodes, whereas one may give different meanings to the network links. This\nresults in different network representations. In the second part of the paper,\nwe give different representations of the language network and perform\ncomparative analysis of their characteristics. Our results demonstrate that the\nlanguage network of Ukrainian is a strongly correlated scale-free small world.\nEmpirical data obtained may be useful for theoretical description of language\nevolution.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.1298v1"
    },
    {
        "title": "On a generalised model for time-dependent variance with long-term memory",
        "authors": [
            "Silvio M. Duarte Queiros"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The ARCH process (R. F. Engle, 1982) constitutes a paradigmatic generator of\nstochastic time series with time-dependent variance like it appears on a wide\nbroad of systems besides economics in which ARCH was born. Although the ARCH\nprocess captures the so-called \"volatility clustering\" and the asymptotic\npower-law probability density distribution of the random variable, it is not\ncapable to reproduce further statistical properties of many of these time\nseries such as: the strong persistence of the instantaneous variance\ncharacterised by large values of the Hurst exponent (H > 0.8), and asymptotic\npower-law decay of the absolute values self-correlation function. By means of\nconsidering an effective return obtained from a correlation of past returns\nthat has a q-exponential form we are able to fix the limitations of the\noriginal model. Moreover, this improvement can be obtained through the correct\nchoice of a sole additional parameter, $q_{m}$. The assessment of its validity\nand usefulness is made by mimicking daily fluctuations of SP500 financial\nindex.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.3248v1"
    },
    {
        "title": "Augmented Sparse Reconstruction of Protein Signaling Networks",
        "authors": [
            "D. Napoletani",
            "T. Sauer",
            "D. C. Struppa",
            "E. Petricoin",
            "L. Liotta"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The problem of reconstructing and identifying intracellular protein signaling\nand biochemical networks is of critical importance in biology today. We sought\nto develop a mathematical approach to this problem using, as a test case, one\nof the most well-studied and clinically important signaling networks in biology\ntoday, the epidermal growth factor receptor (EGFR) driven signaling cascade.\nMore specifically, we suggest a method, augmented sparse reconstruction, for\nthe identification of links among nodes of ordinary differential equation (ODE)\nnetworks from a small set of trajectories with different initial conditions.\nOur method builds a system of representation by using a collection of integrals\nof all given trajectories and by attenuating block of terms in the\nrepresentation itself. The system of representation is then augmented with\nrandom vectors, and minimization of the 1-norm is used to find sparse\nrepresentations for the dynamical interactions of each node. Augmentation by\nrandom vectors is crucial, since sparsity alone is not able to handle the large\nerror-in-variables in the representation. Augmented sparse reconstruction\nallows to consider potentially very large spaces of models and it is able to\ndetect with high accuracy the few relevant links among nodes, even when\nmoderate noise is added to the measured trajectories. After showing the\nperformance of our method on a model of the EGFR protein network, we sketch\nbriefly the potential future therapeutic applications of this approach.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.3989v2"
    },
    {
        "title": "Comments on the Unified approach to the construction of Classical\n  confidence intervals",
        "authors": [
            "W. Wittek",
            "H. Bartko",
            "N. Galante",
            "T. Schweizer"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The paper comments on properties of the so-called \"Unified approach to the\nconstruction of classical confidence intervals\", in which confidence intervals\nare computed in a Neyman construction using the likelihood ratio as ordering\nquantity. In particular, two of the main results of a paper by Feldman and\nCousins (F&C) are discussed. It is shown that in the case of central intervals\nthe so-called flip-flopping problem, occuring in the specific scenario where\nthe experimenter decides to quote a standard upper limit or a confidence\ninterval depending on the measurement, can be easily avoided by choosing\nappropriate confidence levels for the standard upper limits and confidence\nintervals. In the F&C paper \"upper limit\" is defined as the upper edge of a\nconfidence interval, whose lower edge coincides with the physical limit. With\nthis definition of upper limit (F&C limit), in an approach which uses the\nlikelihood ratio as ordering quantity, two-sided confidence intervals\nautomatically change over to \"upper limits\" as the signal becomes weaker\n(Unified approach). In the present paper it is pointed out that this behaviour\nis not a special property of this approach, because approaches with other\nordering principles, like central intervals, symmetric intervals or\nhighest-probability intervals, exhibit the same behaviour. The Unified approach\nis presented in the F&C paper as a solution to the flip-flopping problem. This\nmight suggest that the F&C limit is a standard upper limit. In order to exclude\nany misunderstanding, it is proposed in the present paper to call the F&C limit\n\"upper edge of the confidence interval\", even if its lower edge coincides with\nthe physical limit.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.3622v3"
    },
    {
        "title": "Evaluating Local Community Methods in Networks",
        "authors": [
            "James P. Bagrow"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We present a new benchmarking procedure that is unambiguous and specific to\nlocal community-finding methods, allowing one to compare the accuracy of\nvarious methods. We apply this to new and existing algorithms. A simple class\nof synthetic benchmark networks is also developed, capable of testing\nproperties specific to these local methods.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.3880v2"
    },
    {
        "title": "Spatial Topology and its Structural Analysis based on the Concept of\n  Simplicial Complex",
        "authors": [
            "Bin Jiang",
            "Itzhak Omer"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  This paper introduces a model that identifies spatial relationships for a\nstructural analysis based on the concept of simplicial complex. The spatial\nrelationships are identified through overlapping two map layers, namely a\nprimary layer and a contextual layer. The identified spatial relationships are\nrepresented as a simplical complex, in which simplices and vertices\nrespectively represent two layers of objects. The model relies on the simplical\ncomplex for structural representation and analysis. To quantify structural\nproperties of individual primary objects (or equivalently simplices), and the\nsimplicial complex as a whole, we define a set of centrality measures by\nconsidering multidimensional chains of connectivity, i.e. the number of\ncontextual objects shared by a pair of primary objects. With the model, the\ninteraction and relationships with a geographic system are modeled from both\nlocal and global perspectives. The structural properties and modeling\ncapabilities are illustrated with a simple example and a case study applied to\nthe structural analysis of an urban system.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.0920v1"
    },
    {
        "title": "Probability Distributions in Complex Systems",
        "authors": [
            "D. Sornette"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We review briefly the concepts underlying complex systems and probability\ndistributions. The later are often taken as the first quantitative\ncharacteristics of complex systems, allowing one to detect the possible\noccurrence of regularities providing a step toward defining a classification of\nthe different levels of organization (the ``universality classes''). A rapid\nsurvey covers the Gaussian law, the power law and the stretched exponential\ndistributions. The fascination for power laws is then explained, starting from\nthe statistical physics approach to critical phenomena, out-of-equilibrium\nphase transitions, self-organized criticality, and ending with a large but not\nexhaustive list of mechanisms leading to power law distributions. A check-list\nfor testing and qualifying a power law distribution from your data is described\nin 7 steps. This essay enlarges the description of distributions by proposing\nthat ``kings'', i.e., events even beyond the extrapolation of the power law\ntail, may reveal an information which is complementary and perhaps sometimes\neven more important than the power law distribution. We conclude a list of\nfuture directions.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.2194v1"
    },
    {
        "title": "A fast multilevel algorithm for graph clustering and community detection",
        "authors": [
            "Hristo Djidjev"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  One of the most useful measures of cluster quality is the modularity of a\npartition, which measures the difference between the number of the edges\njoining vertices from the same cluster and the expected number of such edges in\na random (unstructured) graph. In this paper we show that the problem of\nfinding a partition maximizing the modularity of a given graph G can be reduced\nto a minimum weighted cut problem on a complete graph with the same vertices as\nG. We then show that the resulted minimum cut problem can be efficiently solved\nwith existing software for graph partitioning and that our algorithm finds\nclusterings of a better quality and much faster than the existing clustering\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.2387v1"
    },
    {
        "title": "Synchronized Collective Behavior via Low-cost Communication",
        "authors": [
            "Hai-Tao Zhang",
            "Michael ZhiQiang Chen",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  An important natural phenomenon surfaces that satisfactory synchronization of\nself-driven particles can be achieved via sharply reduced communication cost,\nespecially for high density particle groups with low external noise.\nStatistical numerical evidence illustrates that a highly efficient manner is to\ndistribute the communication messages as evenly as possible along the whole\ndynamic process, since it minimizes the communication redundancy. More\nsurprisingly, it is discovered that there exist some abnormal regions where\nmoderately decreasing the communication cost can even improve the\nsynchronization performance. A phase diagram on the noise-density parameter\nspace is given, where the dynamical behaviors can be divided into three\nqualitatively different phases: normal phase where better synchronization\ncorresponds to higher communication cost, abnormal phase where moderately\ndecreasing communication cost could even improve the synchronization, and the\ndisordered phase where no coherence among individuals is observed.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.3402v1"
    },
    {
        "title": "Detecting spatial patterns with the cumulant function. Part II: An\n  application to El Nino",
        "authors": [
            "Alberto Bernacchia",
            "Philippe Naveau",
            "Mathieu Vrac",
            "Pascal Yiou"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The spatial coherence of a measured variable (e.g. temperature or pressure)\nis often studied to determine the regions where this variable varies the most\nor to find teleconnections, i.e. correlations between specific regions. While\nusual methods to find spatial patterns, such as Principal Components Analysis\n(PCA), are constrained by linear symmetries, the dependence of variables such\nas temperature or pressure at different locations is generally nonlinear. In\nparticular, large deviations from the sample mean are expected to be strongly\naffected by such nonlinearities. Here we apply a newly developed nonlinear\ntechnique (Maxima of Cumulant Function, MCF) for the detection of typical\nspatial patterns that largely deviate from the mean. In order to test the\ntechnique and to introduce the methodology, we focus on the El Nino/Southern\nOscillation and its spatial patterns. We find nonsymmetric temperature patterns\ncorresponding to El Nino and La Nina, and we compare the results of MCF with\nother techniques, such as the symmetric solutions of PCA, and the nonsymmetric\nsolutions of Nonlinear PCA (NLPCA). We found that MCF solutions are more\nreliable than the NLPCA fits, and can capture mixtures of principal components.\nFinally, we apply Extreme Value Theory on the temporal variations extracted\nfrom our methodology. We find that the tails of the distribution of extreme\ntemperatures during La Nina episodes is bounded, while the tail during El Ninos\nis less likely to be bounded. This implies that the mean spatial patterns of\nthe two phases are asymmetric, as well as the behaviour of their extremes.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.3547v1"
    },
    {
        "title": "Towards journalometrical analysis of a scientific periodical: a case\n  study",
        "authors": [
            "O. Mryglod",
            "Yu. Holovatch"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  In this paper we use several approaches to analyse a scientific journal as a\ncomplex system and to make a possibly more complete description of its current\nstate and evolution. Methods of complex networks theory, statistics, and\nqueueing theory are used in this study. As a subject of the analysis we have\nchosen the journal ``Condensed Matter Physics''\n(http://www.icmp.lviv.ua/journal/). In particular, based on the statistical\ndata regarding the papers published in this journal since its foundation in\n1993 up to now we have composed the co-authorship network and extracted its\nmain quantitative characteristics. Further, we analyse the priorities of\nscientific trends reflected in the journal and its impact on the publications\nin other editions (the citation ratings). Moreover, to characterize an\nefficiency of the paper processing, we study the time dynamics of editorial\nprocessing in terms of queueing theory and human activity analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.3696v1"
    },
    {
        "title": "Brownian motion in a non-homogeneous force field and photonic force\n  microscope",
        "authors": [
            "Giorgio Volpe",
            "Giovanni Volpe",
            "Dmitri Petrov"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The Photonic Force Microscope (PFM) is an opto-mechanical technique based on\nan optical trap that can be assumed to probe forces in microscopic systems.\nThis technique has been used to measure forces in the range of pico- and\nfemto-Newton, assessing the mechanical properties of biomolecules as well as of\nother microscopic systems. For a correct use of the PFM, the force field to\nmeasure has to be invariable (homogeneous) on the scale of the Brownian motion\nof the trapped probe. This condition implicates that the force field must be\nconservative, excluding the possibility of a rotational component. However,\nthere are cases where these assumptions are not fulfilled Here, we show how to\nimprove the PFM technique in order to be able to deal with these cases. We\nintroduce the theory of this enhanced PFM and we propose a concrete analysis\nworkflow to reconstruct the force field from the experimental time-series of\nthe probe position. Furthermore, we experimentally verify some particularly\nimportant cases, namely the case of a conservative or rotational force-field.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.0492v1"
    },
    {
        "title": "Relaxational Singularities of Human Motor System at Aging Due to\n  Short-Range and Long-Range Time Correlations",
        "authors": [
            "Renat M. Yulmetyev",
            "David E. Valliancourt",
            "Fail M. Gafarov",
            "Sergey A. Demin",
            "Oleg Yu. Panischev",
            "Peter Hänggi"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  In this paper we study the relaxation singularities of human motor system at\naging. Our purpose is to examine the structure of force output variability as a\nfunction of human aging in the time and frequency domains. For analysis of\nexperimental data we have developed here the statistical theory of relaxation\nof force output fluctuation with taking into account the effects of two\nrelaxation channels. The first of them contains the contribution of short-range\ncorrelation whereas other relaxation component reflects the effect of\nlong-range correlation. The analysis of experimental data shows, that the\ngeneral behavior of relaxation processes at human aging is determined by a\ncomplicated combination and nonlinear interactions two above stated relaxation\nprocesses as a whole.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.1255v1"
    },
    {
        "title": "Mechanism for linear preferential attachment in growing networks",
        "authors": [
            "Xinping Xu",
            "Feng Liu",
            "Lianshou Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The network properties of a graph ensemble subject to the constraints imposed\nby the expected degree sequence are studied. It is found that the linear\npreferential attachment is a fundamental rule, as it keeps the maximal entropy\nin sparse growing networks. This provides theoretical evidence in support of\nthe linear preferential attachment widely exists in real networks and adopted\nas a crucial assumption in growing network models. Besides, in the sparse\nlimit, we develop a method to calculate the degree correlation and clustering\ncoefficient in our ensemble model, which is suitable for all kinds of sparse\nnetworks including the BA model, proposed by Barabasi and Albert.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.2598v1"
    },
    {
        "title": "On Shannon-Jaynes Entropy and Fisher Information",
        "authors": [
            "Vesselin I. Dimitrov"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The fundamentals of the Maximum Entropy principle as a rule for assigning and\nupdating probabilities are revisited. The Shannon-Jaynes relative entropy is\nvindicated as the optimal criterion for use with an updating rule. A\nconstructive rule is justified which assigns the probabilities least sensitive\nto coarse-graining. The implications of these developments for interpreting\nphysics laws as rules of inference upon incomplete information are briefly\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.2879v1"
    },
    {
        "title": "Pseudo-periodicity and 1/f noise from the sum of similar intermittent\n  signals",
        "authors": [
            "Giovanni Zanella"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The usual interpretation of noise is represented by a sum of many independent\ntwo-level elementary random signals with a distribution of relaxation times. In\nthis paper it is demonstrated that also the superposition of many similar\nsingle-sided two-level signals, with the same relaxation time, produces noise.\nThis is possible tanks to the coincidences among the signals which introduce\ncross-correlations and tune locally the resulting process in trains of\npseudo-periodic pulses. Computer simulations demonstrate the reliability of\nthis model, which permits to insert in an coherent framework other models\nsolving problems still open.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.3183v1"
    },
    {
        "title": "Stochastic solution of nonlinear and nonhomogeneous evolution problems\n  by a differential Kolmogorov equation",
        "authors": [
            "R. G. Keanini"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  A large class of physically important nonlinear and nonhomogeneous evolution\nproblems, characterized by advection-like and diffusion-like processes, can be\nusefully studied by a time-differential form of Kolmogorov's solution of the\nbackward-time Fokker-Planck equation. The differential solution embodies an\nintegral representation theorem by which any physical or mathematical entity\nsatisfying a generalized nonhomogeneous advection-diffusion equation can be\ncalculated incrementally in time. The utility of the approach for tackling\nnonlinear problems is illustrated via solution of the noise-free Burgers and\nrelated Kardar-Parisi-Zhang (KPZ) equations where it is shown that the\ndifferential Kolmogorov solution encompasses, and allows derivation of, the\nclassical Cole-Hopf and KPZ transformations and solutions. A second example,\nillustrating application of this approach to nonhomogeneous evolution problems,\nderives the Feynman-Kac formula appropriate to a Schrodinger-like equation.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.3202v1"
    },
    {
        "title": "Communities in networks - a continuous approach",
        "authors": [
            "Malgorzata J. Krawczyk",
            "Krzysztof Kulakowski"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  A system of differential equations is proposed designed as to identify\ncommunities in weighted networks. The input is a symmetric connectivity matrix\n$A_{ij}$. A priori information on the number of communities is not needed. To\nverify the dynamics, we prepared sets of separate, fully connected clusters. In\nthis case, the matrix $A$ has a block structure of zeros and units. A noise is\nintroduced as positive random numbers added to zeros and subtracted from units.\nThe task of the dynamics is to reproduce the initial block structure. In this\ntest, the system outperforms the modularity algorithm, if the number of\nclusters is larger than four.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.0923v5"
    },
    {
        "title": "The relativistic velocity addition law optimizes a forecast gambler's\n  profit",
        "authors": [
            "Edward W. Piotrowski",
            "Jerzy Luczka"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We extend the projective covariant bookmaker's bets model to the forecasting\ngamblers case. The probability of correctness of forecasts shifts probabilities\nof branching. The formula for the shift of probabilities leads to the velocity\naddition rule of the special theory of relativity. In the absence of\ninformation about bookmaker's wagers the stochastic logarithmic rates\ncompletely determines the optimal stakes of forecast gambler.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.4137v1"
    },
    {
        "title": "Community structure in directed networks",
        "authors": [
            "E. A. Leicht",
            "M. E. J. Newman"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We consider the problem of finding communities or modules in directed\nnetworks. The most common approach to this problem in the previous literature\nhas been simply to ignore edge direction and apply methods developed for\ncommunity discovery in undirected networks, but this approach discards\npotentially useful information contained in the edge directions. Here we show\nhow the widely used benefit function known as modularity can be generalized in\na principled fashion to incorporate the information contained in edge\ndirections. This in turn allows us to find communities by maximizing the\nmodularity over possible divisions of a network, which we do using an algorithm\nbased on the eigenvectors of the corresponding modularity matrix. This method\nis shown to give demonstrably better results than previous methods on a variety\nof test networks, both real and computer-generated.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.4500v1"
    },
    {
        "title": "An analysis of the abstracts presented at the annual meetings of the\n  Society for Neuroscience from 2001 to 2006",
        "authors": [
            "J. M. Lin",
            "J. W. Bohland",
            "P. Andrews",
            "G. Burns",
            "C. B. Allen",
            "P. P. Mitra"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We extracted and processed abstract data from the SFN annual meeting\nabstracts during the period 2001-2006, using techniques and software from\nnatural language processing, database management, and data visualization and\nanalysis. An important first step in the process was the application of data\ncleaning and disambiguation methods to construct a unified database, since the\ndata were too noisy to be of full utility in the raw form initially available.\nThe resulting co-author graph in 2006, for example, had 39,645 nodes (with an\nestimated 6% error rate in our disambiguation of similar author names) and\n13,979 abstracts, with an average of 1.5 abstracts per author, 4.3 authors per\nabstract, and 5.96 collaborators per author (including all authors on shared\nabstracts). Recent work in related areas has focused on reputational indices\nsuch as highly cited papers or scientists and journal impact factors, and to a\nlesser extent on creating visual maps of the knowledge space. In contrast,\nthere has been relatively less work on the demographics and community\nstructure, the dynamics of the field over time to examine major research trends\nand the structure of the sources of research funding. In this paper we examined\neach of these areas in order to gain an objective overview of contemporary\nneuroscience. Some interesting findings include a high geographical\nconcentration of neuroscience research in north eastern United States, a\nsurprisingly large transient population (60% of the authors appear in only one\nout of the six studied years), the central role played by the study of\nneurodegenerative disorders in the neuroscience community structure, and an\napparent growth of behavioral/systems neuroscience with a corresponding\nshrinkage of cellular/molecular neuroscience over the six year period.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.2523v2"
    },
    {
        "title": "The process of coevolutionary competitive exclusion: speciation,\n  multifractality and power-laws in correlation",
        "authors": [
            "Chen-Ping Zhu",
            "Tao Zhou",
            "Hui-Jie Yang",
            "Shi-Jie Xiong",
            "Zhi-Ming Gu",
            "Da-Ning Shi",
            "Da-Ren He",
            "Bing-Hong Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Competitive exclusion, a key principle of ecology, can be generalized to\nunderstand many other complex systems. Individuals under surviving pressure\ntend to be different from others, and correlations among them change\ncorrespondingly to the updating of their states. We show with numerical\nsimulation that these aptitudes can contribute to group formation or speciation\nin social fields. Moreover, they can lead to power-law topological correlations\nof complex networks. By coupling updating states of nodes with variation of\nconnections in a network, structural properties with power-laws and functions\nlike multifractality, spontaneous ranking and evolutionary branching of node\nstates can emerge out simultaneously from the present self-organized model of\ncoevolutionary process.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.2595v1"
    },
    {
        "title": "Analysis of Proton Radiography Images of Shock Melted/Damaged Tin",
        "authors": [
            "Hanna Makaruk",
            "Nikita A. Sakhanenko",
            "David B. Holtkamp",
            "Tiffany Hayes",
            "Joysree Aubrey"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Tin coupons were shock damaged/melted under identical conditions with a\ndiverging high explosive shock wave. Proton Radiography images and velocimetry\ndata from experiments with seven different tin coupons of varying thickness are\nanalyzed. Comparing experiments with identical samples allowed us to\ndistinguish between repeatable and random features. Shapes and velocities of\nthe main fragments are deterministic functions of the coupon thickness; random\ndifferences exist only at a small scale. Velocities of the leading layer and of\nthe main fragment differ by the same value independently of coupon thicknesses,\nwhich is likely related to the separation energy of metal layers.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.3736v1"
    },
    {
        "title": "Bi-clique Communities",
        "authors": [
            "Sune Lehmann",
            "Martin Schwartz",
            "Lars Kai Hansen"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We present a novel method for detecting communities in bipartite networks.\nBased on an extension of the $k$-clique community detection algorithm, we\ndemonstrate how modular structure in bipartite networks presents itself as\noverlapping bicliques. If bipartite information is available, the bi-clique\ncommunity detection algorithm retains all of the advantages of the $k$-clique\nalgorithm, but avoids discarding important structural information when\nperforming a one-mode projection of the network. Further, the bi-clique\ncommunity detection algorithm provides a new level of flexibility by\nincorporating independent clique thresholds for each of the non-overlapping\nnode sets in the bipartite network.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4867v2"
    },
    {
        "title": "Polyphase alternating codes",
        "authors": [
            "Markku Markkanen",
            "Juha Vierinen"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  This work introduces a method for constructing polyphase alternating codes in\nwhich the length of a code transmission cycle can be $p^m$ or $p-1$, where $p$\nis a prime number and $m$ is a positive integer. The relevant properties\nleading to the construction alternating codes and the algorithm for generating\nalternating codes is described. Examples of all practical and some not that\npractical polyphase code lengths are given.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.4528v1"
    },
    {
        "title": "A nonlinear Stein based estimator for multichannel image denoising",
        "authors": [
            "Caroline Chaux",
            "Laurent Duval",
            "Amel Benazza-Benyahia",
            "Jean-Christophe Pesquet"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The use of multicomponent images has become widespread with the improvement\nof multisensor systems having increased spatial and spectral resolutions.\nHowever, the observed images are often corrupted by an additive Gaussian noise.\nIn this paper, we are interested in multichannel image denoising based on a\nmultiscale representation of the images. A multivariate statistical approach is\nadopted to take into account both the spatial and the inter-component\ncorrelations existing between the different wavelet subbands. More precisely,\nwe propose a new parametric nonlinear estimator which generalizes many reported\ndenoising methods. The derivation of the optimal parameters is achieved by\napplying Stein's principle in the multivariate case. Experiments performed on\nmultispectral remote sensing images clearly indicate that our method\noutperforms conventional wavelet denoising techniques\n",
        "pdf_link": "http://arxiv.org/pdf/0712.2317v1"
    },
    {
        "title": "Nonlinear theory and tests of earthquake recurrence times",
        "authors": [
            "D. Sornette",
            "S. Utkin",
            "A. Saichev"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We develop an efficient numerical scheme to solve accurately the set of\nnonlinear integral equations derived previously in (Saichev and Sornette,\n2007), which describes the distribution of inter-event times in the framework\nof a general model of earthquake clustering with long memory. Detailed\ncomparisons between the linear and nonlinear versions of the theory and direct\nsynthetic catalogs show that the nonlinear theory provides an excellent fit to\nthe synthetic catalogs, while there are significant biases resulting from the\nuse of the linear approximation. We then address the suggestions proposed by\nsome authors to use the empirical distribution of inter-event times to obtain a\nbetter determination of the so-called clustering parameter. Our theory and\ntests against synthetic and empirical catalogs find a rather dramatic lack of\npower for the distribution of inter-event times to distinguish between quite\ndifferent sets of parameters, casting doubt on the usefulness of this\nstatistics for the specific purpose of identifying the clustering parameter.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.2335v1"
    },
    {
        "title": "Consensus of self-driven agents with avoidance of collisions",
        "authors": [
            "Liqian Peng",
            "Yang Zhao",
            "Baomei Tian",
            "Jue Zhang",
            "Bing-Hong Wang",
            "Hai-Tao Zhang",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In recent years, many efforts have been addressed on collision avoidance of\ncollectively moving agents. In this paper, we propose a modified version of the\nVicsek model with adaptive speed, which can guarantee the absence of\ncollisions. However, this strategy leads to an aggregated state with slowly\nmoving agents. We therefore further introduce a certain repulsion, which\nresults in both faster consensus and longer safe distance among agents, and\nthus provides a powerful mechanism for collective motions in biological and\ntechnological multi-agent systems.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.0379v3"
    },
    {
        "title": "Personal Recommendation via Modified Collaborative Filtering",
        "authors": [
            "Runran Liu",
            "Chunxiao Jia",
            "Tao Zhou",
            "Duo Sun",
            "Binghong Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In this paper, we propose a novel method to compute the similarity between\ncongeneric nodes in bipartite networks. Different from the standard Person\ncorrelation, we take into account the influence of node's degree. Substituting\nthis new definition of similarity for the standard Person correlation, we\npropose a modified collaborative filtering (MCF). Based on a benchmark\ndatabase, we demonstrate the great improvement of algorithmic accuracy for both\nuser-based MCF and object-based MCF.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.1333v4"
    },
    {
        "title": "Transmission code optimization method for incoherent scatter radar",
        "authors": [
            "Juha Vierinen",
            "Markku S. Lehtinen",
            "Mikko Orispaa",
            "Ilkka I. Virtanen"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  When statistical inversion of a lag profile is used to determine an\nincoherent scatter target, the posterior variance of the estimated target can\nbe used to determine how well a certain set of transmission codes perform. In\nthis work we present an incoherent scatter radar transmission code optimization\nsearch method suitable for different modulation types, including binary phase,\npolyphase and amplitude modulation. We find that the combination of amplitude\nand phase modulation provides better performance than traditional binary phase\ncoding, in some cases giving better accuracy than alternating codes.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.1343v1"
    },
    {
        "title": "Extending the definition of modularity to directed graphs with\n  overlapping communities",
        "authors": [
            "V. Nicosia",
            "G. Mangioni",
            "V. Carchiolo",
            "M. Malgeri"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Complex networks topologies present interesting and surprising properties,\nsuch as community structures, which can be exploited to optimize communication,\nto find new efficient and context-aware routing algorithms or simply to\nunderstand the dynamics and meaning of relationships among nodes. Complex\nnetworks are gaining more and more importance as a reference model and are a\npowerful interpretation tool for many different kinds of natural, biological\nand social networks, where directed relationships and contextual belonging of\nnodes to many different communities is a matter of fact. This paper starts from\nthe definition of modularity function, given by M. Newman to evaluate the\ngoodness of network community decompositions, and extends it to the more\ngeneral case of directed graphs with overlapping community structures.\nInteresting properties of the proposed extension are discussed, a method for\nfinding overlapping communities is proposed and results of its application to\nbenchmark case-studies are reported. We also propose a new dataset which could\nbe used as a reference benchmark for overlapping community structures\nidentification.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.1647v4"
    },
    {
        "title": "A dual assortative measure of community structure",
        "authors": [
            "Todd D. Kaplan",
            "Stephanie Forrest"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Current community detection algorithms operate by optimizing a statistic\ncalled modularity, which analyzes the distribution of positively weighted edges\nin a network. Modularity does not account for negatively weighted edges. This\npaper introduces a dual assortative modularity measure (DAMM) that incorporates\nboth positively and negatively weighted edges. We describe the the DAMM\nstatistic and illustrate its utility in a community detection algorithm. We\nevaluate the efficacy of the algorithm on both computer generated and\nreal-world networks, showing that DAMM broadens the domain of networks that can\nbe analyzed by community detection algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.3290v1"
    },
    {
        "title": "Measuring the Dynamical State of the Internet: Large Scale Network\n  Tomography via the ETOMIC Infrastructure",
        "authors": [
            "Gabor Simon",
            "Jozsef Steger",
            "Peter Haga Istvan Csabai",
            "Gabor Vattay"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In this paper we show how to go beyond the study of the topological\nproperties of the Internet, by measuring its dynamical state using special\nactive probing techniques and the methods of network tomography. We demonstrate\nthis approach by measuring the key state parameters of Internet paths, the\ncharacteristics of queueing delay, in a part of the European Internet. In the\npaper we describe in detail the ETOMIC measurement platform that was used to\nconduct the experiments, and the applied method of queueing delay tomography.\nThe main results of the paper are maps showing various spatial structure in the\ncharacteristics of queueing delay corresponding to the resolved part of the\nEuropean Internet. These maps reveal that the average queueing delay of network\nsegments spans more than two orders of magnitude, and that the distribution of\nthis quantity is very well fitted by the log-normal distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.4158v1"
    },
    {
        "title": "Empirical analysis on a keyword-based semantic system",
        "authors": [
            "Zike Zhang",
            "Linyuan Lv",
            "Jian-Guo Liu",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Keywords in scientific articles have found their significance in information\nfiltering and classification. In this article, we empirically investigated\nstatistical characteristics and evolutionary properties of keywords in a very\nfamous journal, namely Proceedings of the National Academy of Science of the\nUnited States of America (PNAS), including frequency distribution, temporal\nscaling behavior, and decay factor. The empirical results indicate that the\nkeyword frequency in PNAS approximately follows a Zipf's law with exponent\n0.86. In addition, there is a power-low correlation between the cumulative\nnumber of distinct keywords and the cumulative number of keyword occurrences.\nExtensive empirical analysis on some other journals' data is also presented,\nwith decaying trends of most popular keywords being monitored. Interestingly,\ntop journals from various subjects share very similar decaying tendency, while\nthe journals of low impact factors exhibit completely different behavior. Those\nempirical characters may shed some light on the in-depth understanding of\nsemantic evolutionary behaviors. In addition, the analysis of keyword-based\nsystem is helpful for the design of corresponding recommender systems.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.4163v4"
    },
    {
        "title": "Transfer matrix method to study electromagnetic shower",
        "authors": [
            "Ashok Razdan"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Transfer matrix method gives underlying dynamics of a multifractal. In the\npresent studies transfer matrix method is applied to multifractal properties of\nCherenkov image from which probabilities of electromagnetic components are\nobtained.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.0076v1"
    },
    {
        "title": "Street Hierarchies: A Minority of Streets Account for a Majority of\n  Traffic Flow",
        "authors": [
            "Bin Jiang"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Urban streets are hierarchically organized in the sense that a majority of\nstreets are trivial, while a minority of streets is vital. This hierarchy can\nbe simply, but elegantly, characterized by the 80/20 principle, i.e. 80 percent\nof streets are less connected (below the average), while 20 percent of streets\nare well connected (above the average); out of the 20 percent, there is 1\npercent of streets that are extremely well connected. This paper, using a\nEuropean city as an example, examined, at a much more detailed level, such\nstreet hierarchies from the perspective of geometric and topological\nproperties. Based on an empirical study, we further proved a previous\nconjecture that a minority of streets accounts for a majority of traffic flow;\nmore accurately, the 20 percent of top streets accommodate 80 percent of\ntraffic flow (20/80), and the 1 percent of top streets account for more than 20\npercent of traffic flow (1/20). Our study provides new evidence as to how a\ncity is (self-)organized, contributing to the understanding of cities and their\nevolution using increasingly available mobility geographic information.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.1284v1"
    },
    {
        "title": "Crowding at the Front of the Marathon Packs",
        "authors": [
            "Sanjib Sabhapandit",
            "Satya N. Majumdar",
            "S. Redner"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We study the crowding of near-extreme events in the time gaps between\nsuccessive finishers in major international marathons. Naively, one might\nexpect these gaps to become progressively larger for better-placing finishers.\nWhile such an increase does indeed occur from the middle of the finishing pack\ndown to approximately 20th place, the gaps saturate for the first 10-20\nfinishers. We give a probabilistic account of this feature. However, the data\nsuggests that the gaps have a weak maximum around the 10th place, a feature\nthat seems to have a sociological origin.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.1702v2"
    },
    {
        "title": "Differences between normal and shuffled texts: structural properties of\n  weighted networks",
        "authors": [
            "A. P. Masucci",
            "G. J. Rodgers"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In this paper we deal with the structural properties of weighted networks.\nStarting from an empirical analysis of a linguistic network, we analyse the\ndifferences between the statistical properties of a real and a shuffled network\nand we show that the scale free degree distribution and the scale free weight\ndistribution are induced by the scale free strength distribution, that is\nZipf's law. We test the result on a scientific collaboration network, that is a\nsocial network, and we define a measure, the vertex selectivity, that can\neasily distinguish a real network from a shuffled network. We prove, via an\nad-hoc stochastic growing network with second order correlations, that this\nmeasure can effectively capture the correlations within the topology of the\nnetwork.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.2798v2"
    },
    {
        "title": "Why Does Zipf's Law Break Down in Rank-Size Distribution of Cities?",
        "authors": [
            "Hiroto Kuninaka",
            "Mitsugu Matsushita"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We study rank-size distribution of cities in Japan on the basis of data\nanalysis. From the census data after World War II, we find that the rank-size\ndistribution of cities is composed of two parts, each of which has independent\npower exponent. In addition, the power exponent of the head part of the\ndistribution changes in time and Zipf's law holds only in a restricted period.\nWe show that Zipf's law broke down due to both of Showa and Heisei great\nmergers and recovered due to population growth in middle-sized cities after the\ngreat Showa merger.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.2810v3"
    },
    {
        "title": "BHP universality and gaussianity in sunspot numbers fluctuations",
        "authors": [
            "R. Gonçalves",
            "A. A. Pinto"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We analyze the famous Wolf's sunspot numbers. We discovered that the\ndistribution of the sunspot number fluctuations is a mixture of the BHP\ndistribution with the Gaussian distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.2880v4"
    },
    {
        "title": "Information Filtering via Self-Consistent Refinement",
        "authors": [
            "Jie Ren",
            "Tao Zhou",
            "Yi-Cheng Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Recommender systems are significant to help people deal with the world of\ninformation explosion and overload. In this Letter, we develop a general\nframework named self-consistent refinement and implement it be embedding two\nrepresentative recommendation algorithms: similarity-based and spectrum-based\nmethods. Numerical simulations on a benchmark data set demonstrate that the\npresent method converges fast and can provide quite better performance than the\nstandard methods.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.3748v1"
    },
    {
        "title": "Ants are not Conscious",
        "authors": [
            "Russell K. Standish"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Anthropic reasoning is a form of statistical reasoning based upon finding\noneself a member of a particular reference class of conscious beings. By\nconsidering empirical distribution functions defined over animal life on Earth,\nwe can deduce that the vast bulk of animal life is unlikely to be conscious.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.4121v3"
    },
    {
        "title": "Fitness, chance, and myths: an objective view on soccer results",
        "authors": [
            "Andreas Heuer",
            "Oliver Rubner"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We analyze the time series of soccer matches in a model-free way using data\nfor the German soccer league (Bundesliga). We argue that the goal difference is\na better measure for the overall fitness of a team than the number of points.\nIt is shown that the time evolution of the table during a season can be\ninterpreted as a random walk with an underlying constant drift. Variations of\nthe overall fitness mainly occur during the summer break but not during a\nseason. The fitness correlation shows a long-time decay on the scale of a\nquarter century. Some typical soccer myths are analyzed in detail. It is shown\nthat losing but no winning streaks exist. For this analysis ideas from\nmultidimensional NMR experiments have been borrowed. Furthermore, beyond the\ngeneral home advantage there is no statistically relevant indication of a\nteam-specific home fitness. Based on these insights a framework for a\nstatistical characterization of the results of a soccer league is introduced\nand some general consequences for the prediction of soccer results are\nformulated.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.0614v4"
    },
    {
        "title": "Conformal Invariance of Iso-height Lines in two-dimensional KPZ Surface",
        "authors": [
            "A. A. Saberi",
            "M. D. Niry",
            "S. M. Fazeli",
            "M. R. Rahimi Tabar",
            "S. Rouhani"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The statistics of the iso-height lines in (2+1)-dimensional\nKardar-Parisi-Zhang (KPZ) model is shown to be conformal invariant and\nequivalent to those of self-avoiding random walks. This leads to a rich variety\nof new exact analytical results for the KPZ dynamics. We present direct\nevidence that the iso-height lines can be described by the family of conformal\ninvariant curves called Schramm-Loewner evolution (or $SLE_\\kappa$) with\ndiffusivity $\\kappa=8/3$. It is shown that the absence of the non-linear term\nin the KPZ equation will change the diffusivity $\\kappa$ from 8/3 to 4,\nindicating that the iso-height lines of the Edwards-Wilkinson (EW) surface are\nalso conformally invariant, and belong to the universality class of the domain\nwalls in the O(2) spin model.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.1051v1"
    },
    {
        "title": "Some Considerations on Six Degrees of Separation from A Theoretical\n  Point of View",
        "authors": [
            "Norihito Toyota"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In this article we discuss six degrees of separation, which has been proposed\nby Milgram, from a theoretical point of view. Simply if one has $k$ friends,\nthe number $N$ of indirect friends goes up to $\\sim k^d$ in $d$ degrees of\nseparation. So it would easily come up to population of whole world. That,\nhowever, is unacceptable. Mainly because of nonzero clustering coefficient $C$,\n$N$ does not become $\\sim k^d$. In this article, we first discuss relations\nbetween six degrees of separation and the clustering coefficient in the small\nworld network proposed by Watt and Strogatz\\cite{Watt1},\\cite{Watt2}.\nEspecially, conditions that $(N)>$ (population of U.S.A or of the whole world)\narises in the WS model is explored by theoretical and numerical points of view.\nSecondly we introduce an index that represents velocity of propagation to the\nnumber of friends and obtain an analytical formula for it as a function of $C$,\n$K$, which is an average degree over all nodes, and some parameter $P$\nconcerned with network topology. Finally the index is calculated numerically to\nstudy the relation between $C$, $K$ and $P$ and $N$.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.2399v3"
    },
    {
        "title": "Use of the median in Physics and Astronomy",
        "authors": [
            "Jean-Michel Levy"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The sample median is often used in statistical analyses of physical or\nastronomical data wherein a central value must be found from samples polluted\nby elements which do not belong to the population of interest or when the\nunderlying probability law is such that the sample mean is useless for the\nstated purpose. Although it does not generally possesses the nice linearity\nproperties of the mean, the median has advantages of its own, some of which are\nexplored in this paper which elucidates analogies and differences between these\ntwo central value descriptors. Some elementary results are shown, most of which\nare certainly not new but not widely known either. It is observed that the\nmoment and the quantile approaches to the description of a probability\ndistribution are difficult to relate, but that when the quantile description is\nused, the sample median can be characterized very much in the same way as the\nsample mean; this opens the possibility of using it for estimation purposes\nbeyond what is usually done. In order to relate the two approaches, a\nderivation is given of the asymptotic joint distribution of the mean and the\nmedian for a general continuous probability law.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.0606v1"
    },
    {
        "title": "Self-organized Natural Roads for Predicting Traffic Flow: A Sensitivity\n  Study",
        "authors": [
            "Bin Jiang",
            "Sijian Zhao",
            "Junjun Yin"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In this paper, we extended road-based topological analysis to both nationwide\nand urban road networks, and concentrated on a sensitivity study with respect\nto the formation of self-organized natural roads based on the Gestalt principle\nof good continuity. Both Annual Average Daily Traffic (AADT) and Global\nPositioning System (GPS) data were used to correlate with a series of ranking\nmetrics including five centrality-based metrics and two PageRank metrics. It\nwas found that there exists a tipping point from segment-based to road-based\nnetwork topology in terms of correlation between ranking metrics and their\ntraffic. To our big surprise, (1) this correlation is significantly improved if\na selfish rather than utopian strategy is adopted in forming the self-organized\nnatural roads, and (2) point-based metrics assigned by summation into\nindividual roads tend to have a much better correlation with traffic flow than\nline-based metrics. These counter-intuitive surprising findings constitute\nemergent properties of self-organized natural roads, which are intelligent\nenough for predicting traffic flow, thus shedding substantial insights into the\nunderstanding of road networks and their traffic from the perspective of\ncomplex networks.\n  Keywords: topological analysis, traffic flow, phase transition, small world,\nscale free, tipping point\n",
        "pdf_link": "http://arxiv.org/pdf/0804.1630v3"
    },
    {
        "title": "An effective local routing strategy on the BA network",
        "authors": [
            "Yu-Jian Li",
            "Zhen-Dong Xi",
            "Chuan-Yang Yin",
            "Bing-Hong Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In this paper, We propose a effective routing strategy on the basis of the\nso-called nearest neighbor search strategy by introducing a preferential\ndelivering exponent alpha. we assume that the handling capacity of one vertex\nis proportional to its degree when the degree is smaller than a cut-off value\n$K$, and is infinite otherwise. It is found that by tuning the parameter alpha,\nthe scale-free network capacity measured by the order parameter is considerably\nenhanced compared to the normal nearest-neighbor strategy. Traffic dynamics\nboth near and far away from the critical generating rate R_c are discussed. We\nalso investigate R_c as functions of m (connectivity density), K (cutoff\nvalue). Due to the low cost of acquiring nearest-neighbor information and the\nstrongly improved network capacity, our strategy may be useful and reasonable\nfor the protocol designing of modern communication networks.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.4090v2"
    },
    {
        "title": "Orientation control by flow: Exact results and Langevin simulations",
        "authors": [
            "C. Tannous"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Rod shaped objects suspended in a flowing liquid might be orientated by the\nvelocity, nature of the liquid, the flow and the geometry of the channel\ncontaining the flow. Orientation settings might enhance or inhibit certain\nchemical reactions between the objects, other chemicals or with the walls of\nvessels holding the flowing suspension. The probability density function (PDF)\ndescribing the orientations of rod shaped objects in a flowing liquid satisfies\na Fokker-Planck equation whose solution is obtained analytically as well as\nnumerically from Langevin simulations for different flow parameters. The\nanalytical and numerical methods developed in the present work enable us to\ncalculate accurately the PDF for a range of the Peclet number $\\alpha$ covering\nseveral orders of magnitude, $10^{-4} \\le \\alpha \\le 10^{8}$. We apply these\nresults to the experimental determination of dichroism and birefringence of the\nsuspension as a function of $\\alpha$.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.4558v1"
    },
    {
        "title": "Kepler versus Akaike",
        "authors": [
            "Tomasz Stachowiak"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  I use the example of the Earth's orbit to illustrate the principle behind the\nAkaike Information Criterion, and refute the misconception that the criterion,\nby definition, discards more complex models in favour of simpler ones.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.2562v2"
    },
    {
        "title": "Testing long-term earthquake forecasts: likelihood methods and error\n  diagrams",
        "authors": [
            "Yan Y. Kagan"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We propose a new method to test the effectiveness of a spatial point process\nforecast based on a log-likelihood score for predicted point density and the\ninformation gain for events that actually occurred in the test period. The\nmethod largely avoids simulation use and allows us to calculate the information\nscore for each event or set of events as well as the standard error of each\nforecast. As the number of predicted events increases, the score distribution\napproaches the Gaussian law. The degree of its similarity to the Gaussian\ndistribution can be measured by the computed coefficients of skewness and\nkurtosis. To display the forecasted point density and the point events, we use\nan event concentration diagram or a variant of the Error Diagram (ED).\n  We demonstrate the application of the method by using our long-term forecast\nof seismicity in two western Pacific regions. We compare the ED for these\nregions with simplified diagrams based on two-segment approximations. Since the\nearthquakes in these regions are concentrated in narrow subduction belts, using\nthe forecast density as a template or baseline for the ED is a more convenient\ndisplay technique. We also show, using simulated event occurrence, that some\nproposed criteria for measuring forecast effectiveness at EDs would be strongly\nbiased for a small event number.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.2885v1"
    },
    {
        "title": "Empirical Tests of Zipf's law Mechanism In Open Source Linux\n  Distribution",
        "authors": [
            "T. Maillart",
            "D. Sornette",
            "S. Spaeth",
            "G. Von Krogh"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The evolution of open source software projects in Linux distributions offers\na remarkable example of a growing complex self-organizing adaptive system,\nexhibiting Zipf's law over four full decades. We present three tests of the\nusually assumed ingredients of stochastic growth models that have been\npreviously conjectured to be at the origin of Zipf's law: (i) the growth\nobserved between successive releases of the number of in-directed links of\npackages obeys Gibrat's law of proportional growth; (ii) the average growth\nincrement of the number of in-directed links of packages over a time interval\n$\\Delta t$ is proportional to $\\Delta t$, while its standard deviation is\nproportional to $\\sqrt{\\Delta t}$; (iii) the distribution of the number of\nin-directed links of new packages appearing in evolving versions of Debian\nLinux distributions has a tail thinner than Zipf's law, with an exponent which\nconverges to the Zipf's law value 1 as the time $\\Delta t$ between releases\nincreases.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.0014v1"
    },
    {
        "title": "Model based IVHM system for the solid rocket booster",
        "authors": [
            "D. G. Luchinsky",
            "V. V. Osipov",
            "V. N. Smelyanskiy",
            "D. A. Timucin",
            "S. Uckun"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We report progress in the development of a model-based hybrid probabilistic\napproach to an on-board IVHM for solid rocket boosters (SRBs) that can\naccommodate the abrupt changes of the model parameters in various nonlinear\ndynamical off-nominal regimes. The work is related to the ORION mission\nprogram. Specifically, a case breach fault for SRBs is considered that takes\ninto account burning a hole through the rocket case, as well as ablation of the\nnozzle throat under the action of hot gas flow. A high-fidelity model (HFM) of\nthe fault is developed in FLUENT in cylindrical symmetry. The results of the\nFLUENT simulations are shown to be in good agreement with quasi-stationary\napproximation and analytical solution of a system of one-dimensional partial\ndifferential equations (PDEs) for the gas flow in the combustion chamber and in\nthe hole through the rocket case.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.1519v1"
    },
    {
        "title": "Testing a priority-based queue model with Linux command histories",
        "authors": [
            "Seung Ki Baek",
            "Tae Young Kim",
            "Beom Jun Kim"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We study human dynamics by analyzing Linux history files. The goodness-of-fit\ntest shows that most of the collected datasets belong to the universality class\nsuggested in the literature by a variable-length queueing process based on\npriority. In order to check the validity of this model, we design two tests\nbased on mutual information between time intervals and a mathematical\nrelationship known as the arcsine law. Since the previously suggested queueing\nprocess fails to pass these tests, the result suggests that the modelling of\nhuman dynamics should properly consider the statistical dependency in the\ntemporal dimension.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.2097v1"
    },
    {
        "title": "Spectral Density of Sample Covariance Matrices of Colored Noise",
        "authors": [
            "Emil Dolezal",
            "Petr Seba"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We study the dependence of the spectral density of the covariance matrix\nensemble on the power spectrum of the underlying multivariate signal. The white\nnoise signal leads to the celebrated Marchenko-Pastur formula. We demonstrate\nresults for some colored noise signals.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.2378v2"
    },
    {
        "title": "Statistical switching kinetics in ferroelectrics",
        "authors": [
            "X. J. Lou"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  By assuming a more realistic nucleation and polarization reversal scenario we\nbuild a new statistical switching model for ferroelectrics, which is different\nfrom either the Kolmogorov-Avrami-Ishibashi (KAI) model or the\nNucleation-Limited-Switching (NLS) model. After incorporating a time-dependent\ndepolarization field this model gives a good description about the retardation\nbehavior in polycrystalline thin films at medium or low fields, which can not\nbe described by the traditional KAI model. This model predicts correctly n=1\nfor polycrystalline thin films at high Eappl or ceramic bulks in the ideal\ncase.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.2494v1"
    },
    {
        "title": "Negro and Danube are mirror rivers",
        "authors": [
            "R. Gonçalves",
            "A. A. Pinto"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We study the European river Danube and the South American river Negro daily\nwater levels. We present a fit for the Negro daily water level period and\nstandard deviation. Unexpectedly, we discover that the river Negro and Danube\nare mirror rivers in the sense that the daily water levels fluctuations\nhistograms are close to the BHP and reversed BHP, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.2988v1"
    },
    {
        "title": "Source separation techniques for characterising cosmic ray transients\n  from neutron monitor networks",
        "authors": [
            "T. Dudok de Wit",
            "A. A. Chilingarian",
            "G. G. Karapetyan"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The analysis of weak variations in the energetic particle flux, as detected\nby neutron or muon monitors, can often be considerably improved by analysing\ndata from monitor networks and thereby exploiting the spatial coherence of the\nflux. We present a statistical framework for carrying out such an analysis and\ndiscuss its physical interpretation. Two other applications are also presented:\nfilling data gaps and removing trends. This study focuses on the method and its\nvarious uses.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.4475v1"
    },
    {
        "title": "Information filtering based on transferring similarity",
        "authors": [
            "Duo Sun",
            "Tao Zhou",
            "Jian-Guo Liu",
            "Run-Ran Liu",
            "Chun-Xiao Jia",
            "Bing-Hong Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In this Brief Report, we propose a new index of user similarity, namely the\ntransferring similarity, which involves all high-order similarities between\nusers. Accordingly, we design a modified collaborative filtering algorithm,\nwhich provides remarkably higher accurate predictions than the standard\ncollaborative filtering. More interestingly, we find that the algorithmic\nperformance will approach its optimal value when the parameter, contained in\nthe definition of transferring similarity, gets close to its critical value,\nbefore which the series expansion of transferring similarity is convergent and\nafter which it is divergent. Our study is complementary to the one reported in\n[E. A. Leicht, P. Holme, and M. E. J. Newman, Phys. Rev. E {\\bf 73} 026120\n(2006)], and is relevant to the missing link prediction problem.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.4495v2"
    },
    {
        "title": "Data analysis recipes: Choosing the binning for a histogram",
        "authors": [
            "David W. Hogg"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Data points are placed in bins when a histogram is created, but there is\nalways a decision to be made about the number or width of the bins. This\ndecision is often made arbitrarily or subjectively, but it need not be. A\njackknife or leave-one-out cross-validation likelihood is defined and employed\nas a scalar objective function for optimization of the locations and widths of\nthe bins. The objective is justified as being related to the histogram's\nusefulness for predicting future data. The method works for data or histograms\nof any dimensionality.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.4820v1"
    },
    {
        "title": "Approximate Hamiltonian Statistics in One-dimensional Driven Dissipative\n  Many-Particle Systems",
        "authors": [
            "Martin Treiber",
            "Dirk Helbing"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  This contribution presents a derivation of the steady-state distribution of\nvelocities and distances of driven particles on a onedimensional periodic ring.\nWe will compare two different situations: (i) symmetrical interaction forces\nfulfilling Newton's law of \"actio = reactio\" and (ii) asymmetric, forwardly\ndirected interactions as, for example in vehicular traffic. Surprisingly, the\nsteady-state velocity and distance distributions for asymmetric interactions\nand driving terms agree with the equilibrium distributions of classical\nmany-particle systems with symmetrical interactions, if the system is large\nenough. This analytical result is confirmed by computer simulations and\nestablishes the possibility of approximating the steady state statistics in\ndriven many-particle systems by Hamiltonian systems. Our finding is also useful\nto understand the various departure time distributions of queueing systems as a\npossible effect of interactions among the elements in the respective queue [D.\nHelbing et al., Physica A 363, 62 (2006)].\n",
        "pdf_link": "http://arxiv.org/pdf/0809.0426v1"
    },
    {
        "title": "Mental States as Macrostates Emerging from EEG Dynamics",
        "authors": [
            "Carsten Allefeld",
            "Harald Atmanspacher",
            "Jiri Wackermann"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Correlations between psychological and physiological phenomena form the basis\nfor different medical and scientific disciplines, but the nature of this\nrelation has not yet been fully understood. One conceptual option is to\nunderstand the mental as \"emerging\" from neural processes in the specific sense\nthat psychology and physiology provide two different descriptions of the same\nsystem. Stating these descriptions in terms of coarser- and finer-grained\nsystem states (macro- and microstates), the two descriptions may be equally\nadequate if the coarse-graining preserves the possibility to obtain a dynamical\nrule for the system. To test the empirical viability of our approach, we\ndescribe an algorithm to obtain a specific form of such a coarse-graining from\ndata, and illustrate its operation using a simulated dynamical system. We then\napply the method to an electroencephalographic (EEG) recording, where we are\nable to identify macrostates from the physiological data that correspond to\nmental states of the subject.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.0479v3"
    },
    {
        "title": "Bayesian estimation for selective trace gas detection",
        "authors": [
            "John K. Stockton",
            "Ari K. Tuchman"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We present a Bayesian estimation analysis for a particular trace gas\ndetection technique with species separation provided by differential diffusion.\nThe proposed method collects a sample containing multiple gas species into a\ncommon volume, and then allows it to diffuse across a linear array of optical\nabsorption detectors, using, for example, high-finesse Fabry-Perot cavities.\nThe estimation procedure assumes that all gas parameters (e.g. diffusion\nconstants, optical cross sections) are known except for the number population\nof each species, which are determined from the time-of-flight absorption\nprofiles in each detector.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.0890v1"
    },
    {
        "title": "Generalized theory for node disruption in finite size complex networks",
        "authors": [
            "Bivas Mitra",
            "Niloy Ganguly",
            "Sujoy Ghose",
            "Fernando Peruani"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  After a failure or attack the structure of a complex network changes due to\nnode removal. Here, we show that the degree distribution of the distorted\nnetwork, under any node disturbances, can be easily computed through a simple\nformula. Based on this expression, we derive a general condition for the\nstability of non-correlated finite complex networks under any arbitrary attack.\nWe apply this formalism to derive an expression for the percolation threshold\n$f_c$ under a general attack of the form $f_k \\sim k^{\\gamma}$, where $f_k$\nstands for the probability of a node of degree $k$ of being removed during the\nattack. We show that $f_c$ of a finite network of size $N$ exhibits an additive\ncorrection which scales as $N^{-1}$ with respect to the classical result for\ninfinite networks.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0513v1"
    },
    {
        "title": "Introducing doubt in Bayesian model comparison",
        "authors": [
            "Glenn D Starkman",
            "Roberto Trotta",
            "Pascal M Vaudrevange"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  There are things we know, things we know we don't know, and then there are\nthings we don't know we don't know. In this paper we address the latter two\nissues in a Bayesian framework, introducing the notion of doubt to quantify the\ndegree of (dis)belief in a model given observational data in the absence of\nexplicit alternative models. We demonstrate how a properly calibrated doubt can\nlead to model discovery when the true model is unknown.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.2415v1"
    },
    {
        "title": "A model of underground ridership during the severe outbreaks of the SARS\n  epidemic in a modern city",
        "authors": [
            "Kuo-Ying Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The outbreaks of the severe acute respiratory syndrome (SARS) epidemic in\n2003 resulted in unprecedented impacts on people's daily life. One of the most\nsignificant impacts to people is the risk of contacting SARS while engaging\ndaily routine activity. In this work we use data from daily underground\nridership in Taipei and daily reported SARS cases in Taiwan to model the\ndynamics of the public underground usage during the wax and wane of the SARS\nperiod. We found that for each reported SARS case there is an immediate loss of\nabout 1200 underground ridership. These loss rates propagate to the following\ndays with an e-folding decay time of about 28 days, reflecting the public\nperception on the risk of contacting SARS disease when travelling with the\nunderground system. About 50% of daily ridership was lost during the peak of\nthe 2003 SARS period, compared with the loss of 80% daily ridership during the\nclosure of the underground system after Typhoon Nari, the loss of 50-70%\nridership due to the closure of the governmental offices and schools during\ntyphoon periods, and the loss of 60% daily ridership during Chinese New Year\nholidays.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.2464v1"
    },
    {
        "title": "Likelihood method and Fisher information in construction of physical\n  models",
        "authors": [
            "E. W. Piotrowski",
            "J. Sladkowski",
            "J. Syska",
            "S. Zajac"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The subjects of the paper are the likelihood method (LM) and the expected\nFisher information (FI) considered from the point od view of the construction\nof the physical models which originate in the statistical description of\nphenomena. The master equation case and structural information principle are\nderived. Then, the phenomenological description of the information transfer is\npresented. The extreme physical information (EPI) method is reviewed. As if\nmarginal, the statistical interpretation of the amplitude of the system is\ngiven. The formalism developed in this paper would be also applied in quantum\ninformation processing and quantum game theory.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.3554v1"
    },
    {
        "title": "The Fisher's information and quantum-classical field theory: classical\n  statistics similarity",
        "authors": [
            "Jacek Syska"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The classical statistics indication for the impossibility to derive quantum\nmechanics from classical mechanics is proved. The formalism of the statistical\nFisher information is used. Next the Fisher information as a tool of the\nconstruction of a self-consistent field theory, which joins the quantum theory\nand classical field theory, is proposed.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.3600v1"
    },
    {
        "title": "PDE-Foam - a probability-density estimation method using self-adapting\n  phase-space binning",
        "authors": [
            "Dominik Dannheim",
            "Tancredi Carli",
            "Karl-Johan Grahn",
            "Peter Speckmayer",
            "Alexander Voigt"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Probability Density Estimation (PDE) is a multivariate discrimination\ntechnique based on sampling signal and background densities defined by event\nsamples from data or Monte-Carlo (MC) simulations in a multi-dimensional phase\nspace. In this paper, we present a modification of the PDE method that uses a\nself-adapting binning method to divide the multi-dimensional phase space in a\nfinite number of hyper-rectangles (cells). The binning algorithm adjusts the\nsize and position of a predefined number of cells inside the multi-dimensional\nphase space, minimising the variance of the signal and background densities\ninside the cells. The implementation of the binning algorithm PDE-Foam is based\non the MC event-generation package Foam. We present performance results for\nrepresentative examples (toy models) and discuss the dependence of the obtained\nresults on the choice of parameters. The new PDE-Foam shows improved\nclassification capability for small training samples and reduced classification\ntime compared to the original PDE method based on range searching.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.0922v3"
    },
    {
        "title": "Detection of node group membership in networks with group overlap",
        "authors": [
            "Erin N. Sawardecker",
            "Marta Sales-Pardo",
            "Luís A. Nunes Amaral"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Most networks found in social and biochemical systems have modular\nstructures. An important question prompted by the modularity of these networks\nis whether nodes can be said to belong to a single group. If they cannot, we\nwould need to consider the role of \"overlapping communities.\" Despite some\nefforts in this direction, the problem of detecting overlapping groups remains\nunsolved because there is neither a formal definition of overlapping community,\nnor an ensemble of networks with which to test the performance of group\ndetection algorithms when nodes can belong to more than one group. Here, we\nintroduce an ensemble of networks with overlapping groups. We then apply three\ngroup identification methods--modularity maximization, k-clique percolation,\nand modularity-landscape surveying--to these networks. We find that the\nmodularity-landscape surveying method is the only one able to detect\nheterogeneities in node memberships, and that those heterogeneities are only\ndetectable when the overlap is small. Surprisingly, we find that the k-clique\npercolation method is unable to detect node membership for the overlapping\ncase.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.1243v1"
    },
    {
        "title": "Analysis of Discrete Signals with Stochastic Components using Flicker\n  Noise Spectroscopy",
        "authors": [
            "Serge F. Timashev",
            "Yuriy S. Polyakov"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The problem of information extraction from discrete stochastic time series,\nproduced with some finite sampling frequency, using flicker-noise spectroscopy,\na general framework for information extraction based on the analysis of the\ncorrelation links between signal irregularities and formulated for continuous\nsignals, is discussed. It is shown that the mathematical notions of Dirac and\nHeaviside functions used in the analysis of continuous signals may be\ninterpreted as high-frequency and low-frequency stochastic components,\nrespectively, in the case of discrete series. The analysis of\nelectroencephalogram measurements for a teenager with schizophrenic symptoms at\ntwo different sampling frequencies demonstrates that the \"power spectrum\" and\ndifference moment contain different information in the case of discrete\nsignals, which was formally proven for continuous signals. The sampling\ninterval itself is suggested as an additional parameter that should be included\nin general parameterization procedures for real signals.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2141v1"
    },
    {
        "title": "Applying Bayesian Neural Network to Determine Neutrino Incoming\n  Direction in Reactor Neutrino Experiments and Supernova Explosion Location by\n  Scintillator Detectors",
        "authors": [
            "Weiwei Xu",
            "Ye Xu",
            "Yixiong Meng",
            "Bin Wu"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In the paper, it is discussed by using Monte-Carlo simulation that the\nBayesian Neural Network(BNN) is applied to determine neutrino incoming\ndirection in reactor neutrino experiments and supernova explosion location by\nscintillator detectors. As a result, compared to the method in\nRef.\\cite{key-1}, the uncertainty on the measurement of the neutrino direction\nusing BNN is significantly improved. The uncertainty on the measurement of the\nreactor neutrino direction is about 1.0$^\\circ$ at the 68.3% C.L., and the one\nin the case of supernova neutrino is about 0.6$^\\circ$ at the 68.3% C.L..\nCompared to the method in Ref.\\cite{key-1}, the uncertainty attainable by using\nBNN reduces by a factor of about 20. And compared to the Super-Kamiokande\nexperiment(SK), it reduces by a factor of about 8.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2713v1"
    },
    {
        "title": "Spectral tripartitioning of networks",
        "authors": [
            "Thomas Richardson",
            "Peter J. Mucha",
            "Mason A. Porter"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We formulate a spectral graph-partitioning algorithm that uses the two\nleading eigenvectors of the matrix corresponding to a selected quality function\nto split a network into three communities in a single step. In so doing, we\nextend the recursive bipartitioning methods developed by Newman [Proc. Nat.\nAcad. Sci. 103, 8577 (2006); Phys. Rev. E 74, 036104 (2006)] to allow one to\nconsider the best available two-way and three-way divisions at each recursive\nstep. We illustrate the method using simple \"bucket brigade\" examples and then\napply the algorithm to examine the community structures of the coauthorship\ngraph of network scientists and of U. S. Congressional networks inferred from\nroll-call voting similarities.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2852v2"
    },
    {
        "title": "Generalized Exponential Function and some of its Applications to Complex\n  Systems",
        "authors": [
            "Alexandre Souto Martinez",
            "Rodrigo Silva Gonzalez",
            "Cesar Augusto Sangaletti Tercariol"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  From the integration of non-symmetrical hyperboles, a one-parameter\ngeneralization of the logarithmic function is obtained. Inverting this\nfunction, one obtains the generalized exponential function. We show that\nfunctions characterizing complex systems can be conveniently written in terms\nof this generalization of the exponential function. The gamma function is then\ngeneralized and we generalize the factorial operation. Also a very reliable\nrank distribution can be conveniently described by the generalized exponential\nfunction. Finally, we turn the attention to the generalization of one- and\ntwo-tail stretched exponential functions. One obtains, as particular cases, the\ngeneralized error function, the Zipf-Mandelbrot probability density function\n(pdf), the generalized gaussian and Laplace pdf. One can also obtain\nanalytically their cumulative functions and moments.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.3071v1"
    },
    {
        "title": "Nonadditive entropy: the concept and its use",
        "authors": [
            "Constantino Tsallis"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The entropic form $S_q$ is, for any $q \\neq 1$, {\\it nonadditive}. Indeed,\nfor two probabilistically independent subsystems, it satisfies\n$S_q(A+B)/k=[S_q(A)/k]+[S_q(B)/k]+(1-q)[S_q(A)/k][S_q(B)/k] \\ne\nS_q(A)/k+S_q(B)/k$. This form will turn out to be {\\it extensive} for an\nimportant class of nonlocal correlations, if $q$ is set equal to a special\nvalue different from unity, noted $q_{ent}$ (where $ent$ stands for $entropy$).\nIn other words, for such systems, we verify that $S_{q_{ent}}(N) \\propto N\n(N>>1)$, thus legitimating the use of the classical thermodynamical relations.\nStandard systems, for which $S_{BG}$ is extensive, obviously correspond to\n$q_{ent}=1$. Quite complex systems exist in the sense that, for them, no value\nof $q$ exists such that $S_q$ is extensive. Such systems are out of the present\nscope: they might need forms of entropy different from $S_q$, or perhaps --\nmore plainly -- they are just not susceptible at all for some sort of\nthermostatistical approach. Consistently with the results associated with\n$S_q$, the $q$-generalizations of the Central Limit Theorem and of its extended\nL\\'evy-Gnedenko form have been achieved. These recent theorems could of course\nbe the cause of the ubiquity of $q$-exponentials, $q$-Gaussians and related\nmathematical forms in natural, artificial and social systems. All of the above,\nas well as presently available experimental, observational and computational\nconfirmations -- in high energy physics and elsewhere --, are briefly reviewed.\nFinally, we address a confusion which is quite common in the literature, namely\nreferring to distinct physical mechanisms {\\it versus} distinct regimes of a\nsingle physical mechanism.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.4370v1"
    },
    {
        "title": "Predicting Missing Links via Local Information",
        "authors": [
            "Tao Zhou",
            "Linyuan Lu",
            "Yi-Cheng Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Missing link prediction of networks is of both theoretical interest and\npractical significance in modern science. In this paper, we empirically\ninvestigate a simple framework of link prediction on the basis of node\nsimilarity. We compare nine well-known local similarity measures on six real\nnetworks. The results indicate that the simplest measure, namely common\nneighbors, has the best overall performance, and the Adamic-Adar index performs\nthe second best. A new similarity measure, motivated by the resource allocation\nprocess taking place on networks, is proposed and shown to have higher\nprediction accuracy than common neighbors. It is found that many links are\nassigned same scores if only the information of the nearest neighbors is used.\nWe therefore design another new measure exploited information of the next\nnearest neighbors, which can remarkably enhance the prediction accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.0553v2"
    },
    {
        "title": "A Fast Chi-squared Technique For Period Search of Irregularly Sampled\n  Data",
        "authors": [
            "David M. Palmer"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  A new, computationally- and statistically-efficient algorithm, the Fast\n$\\chi^2$ algorithm, can find a periodic signal with harmonic content in\nirregularly-sampled data with non-uniform errors. The algorithm calculates the\nminimized $\\chi^2$ as a function of frequency at the desired number of\nharmonics, using Fast Fourier Transforms to provide $O (N \\log N)$ performance.\nThe code for a reference implementation is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.1913v1"
    },
    {
        "title": "The Network of Scientific Collaborations within the European Framework\n  Programme",
        "authors": [
            "Juan A. Almendral",
            "Joao G. Oliveira",
            "L. López",
            "J. F. F. Mendes",
            "Miguel A. F. Sanjuán"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We use the emergent field of Complex Networks to analyze the network of\nscientific collaborations between entities (universities, research\norganizations, industry related companies,...) which collaborate in the context\nof the so-called Framework Programme. We demonstrate here that it is a\nscale--free network with an accelerated growth, which implies that the creation\nof new collaborations is encouraged. Moreover, these collaborations possess\nhierarchical modularity. Likewise, we find that the information flow depends on\nthe size of the participants but not on geographical constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.3375v1"
    },
    {
        "title": "Assessment of Spatial-variant Noise Level in Real-time Dynamic MR Images\n  - a Random Matrix Approach",
        "authors": [
            "Yu Ding",
            "Yiu-Cho Chung",
            "Orlando P. Simonetti"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Accurate measurement of spatially variant noise in dynamic magnetic resonance\n(MR) images acquired using parallel imaging methods is problematic. We propose\na new method based on the random matrix theory to accurately assess the noise\nlevel. The method is validated using numerical simulation and in-vivo dynamic\ncardiac MR images.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.4027v2"
    },
    {
        "title": "An optimal method to combine results from different experiments",
        "authors": [
            "Theodore P. Hill",
            "Jack Miller"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  This article describes an optimal method (conflation) to consolidate data\nfrom different experiments, and illustrates the advantages of conflation by\ngraphical examples involving gaussian input distributions, and by a concrete\nnumerical example involving the values of lattice spacing of silicon crystals\nused in determination of the current values of Planck's constant and the\nAvogadro constant.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.4957v1"
    },
    {
        "title": "A characterization of the scientific impact of Brazilian institutions",
        "authors": [
            "Aristoklis D. Anastasiadis",
            "Marcelo P. de Albuquerque",
            "Marcio P. de Albuquerque"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this paper we studied the research activity of Brazilian Institutions for\nall sciences and also their performance in the area of physics between 1945 and\nDecember 2008. All the data come from the Web of Science database for this\nperiod. The analysis of the experimental data shows that, within a nonextensive\nthermostatistical formalism, the Tsallis \\emph{q}-exponential distribution\n$N(c)$ can constitute a new characterization of the research impact for\nBrazilian Institutions. The data examined in the present survey can be fitted\nsuccessfully by applying a universal curve namely, $N(c) \\propto 1/[1+(q-1)\nc/T]^{\\frac{1}{q-1}}$ with $q\\simeq 4/3$ for {\\it all} the available citations\n$c$, $T$ being an \"effective temperature\". The present analysis ultimately\nsuggests that via the \"effective temperature\" $T$, we can provide a new\nperformance metric for the impact level of the research activity in Brazil,\ntaking into account the number of the publications and their citations. This\nnew performance metric takes into account the \"quantity\" (number of\npublications) and the \"quality\" (number of citations) for different Brazilian\nInstitutions. In addition we analyzed the research performance of Brazil to\nshow how the scientific research activity changes with time, for instance\nbetween 1945 to 1985, then during the period 1986-1990, 1991-1995, and so on\nuntil the present. Finally, this work intends to show a new methodology that\ncan be used to analyze and compare institutions within a given country.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.0554v2"
    },
    {
        "title": "Detrended fluctuation analysis of power-law-correlated sequences with\n  random noises",
        "authors": [
            "Shin-ichi Tadaki"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Improvement in time resolution sometimes introduces short-range random noises\ninto temporal data sequences. These noises affect the results of power-spectrum\nanalyses and the Detrended Fluctuation Analysis (DFA). The DFA is one of useful\nmethods for analyzing long-range correlations in non-stationary sequences. The\neffects of noises are discussed based on artificial temporal sequences.\nShort-range noises prevent power-spectrum analyses from detecting long-range\ncorrelations. The DFA can extract long-range correlations from noisy time\nsequences. The DFA also gives the threshold time length, under which the noises\ndominate. For practical analyses, coarse-grained time sequences are shown to\nrecover long-range correlations.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.0678v1"
    },
    {
        "title": "Bridge Bounding: A Local Approach for Efficient Community Discovery in\n  Complex Networks",
        "authors": [
            "Symeon Papadopoulos",
            "Andre Skusa",
            "Athena Vakali",
            "Yiannis Kompatsiaris",
            "Nadine Wagner"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  The increasing importance of Web 2.0 applications during the last years has\ncreated significant interest in tools for analyzing and describing collective\nuser activities and emerging phenomena within the Web. Network structures have\nbeen widely employed in this context for modeling users, web resources and\nrelations between them. However, the amount of data produced by modern web\nsystems results in networks that are of unprecedented size and complexity, and\nare thus hard to interpret. To this end, community detection methods attempt to\nuncover natural groupings of web objects by analyzing the topology of their\ncontaining network. There are numerous techniques adopting a global perspective\nto the community detection problem, i.e. they operate on the complete network\nstructure, thus being computationally expensive and hard to apply in a\nstreaming manner. In order to add a local perspective to the study of the\nproblem, we present Bridge Bounding, a local methodology for community\ndetection, which explores the local network topology around a seed node in\norder to identify edges that act as boundaries to the local community. The\nproposed method can be integrated in an efficient global community detection\nscheme that compares favorably to the state of the art. As a case study, we\napply the method to explore the topic structure of the LYCOS iQ collaborative\nquestion/answering application by detecting communities in the networks created\nfrom the collective tagging activity of users.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.0871v1"
    },
    {
        "title": "Properties and application of the form $A\\cdot\n  exp(-(x-c)^2/(a(x-c)+2b^2))$ for investigation of ultra high energy cascades",
        "authors": [
            "A. A. Kirillov",
            "I. A. Kirillov"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  The form $A\\cdot exp(-(x-c)^2 /(a(x-c)+2b^2))$ is an asymmetric distribution\nintermediate between the normal and exponential distributions. Some specific\nproperties of the form are presented and methods of approximation are offered.\nAppropriate formulae and table are presented. The practical problems of\napproximation by the form are discussed with connection to the quality of\noriginal data. Application of the methods is illustrated by using in the\nproblem of calculating and studying distribution function of maximum ultra high\nenergy atmospheric showers. Relationship with exponential and normal\ndistributions makes usage of the form to be effective in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.2113v1"
    },
    {
        "title": "A data mining algorithm for automated characterisation of fluctuations\n  in multichannel timeseries",
        "authors": [
            "D. G. Pretty",
            "B. D. Blackwell"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We present a data mining technique for the analysis of multichannel\noscillatory timeseries data and show an application using poloidal arrays of\nmagnetic sensors installed in the H-1 heliac. The procedure is highly\nautomated, and scales well to large datasets. The timeseries data is split into\nshort time segments to provide time resolution, and each segment is represented\nby a singular value decomposition (SVD). By comparing power spectra of the\ntemporal singular vectors, singular values are grouped into subsets which\ndefine fluctuation structures. Thresholds for the normalised energy of the\nfluctuation structure and the normalised entropy of the SVD are used to filter\nthe dataset. We assume that distinct classes of fluctuations are localised in\nthe space of phase differences between each pair of nearest neighbour channels.\nAn expectation maximisation clustering algorithm is used to locate the distinct\nclasses of fluctuations, and a cluster tree mapping is used to visualise the\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.4478v1"
    },
    {
        "title": "Structure of shells in complex networks",
        "authors": [
            "Jia Shao",
            "Sergey V. Buldyrev",
            "Lidia A. Braunstein",
            "Shlomo Havlin",
            "H. Eugene Stanley"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In a network, we define shell $\\ell$ as the set of nodes at distance $\\ell$\nwith respect to a given node and define $r_\\ell$ as the fraction of nodes\noutside shell $\\ell$. In a transport process, information or disease usually\ndiffuses from a random node and reach nodes shell after shell. Thus,\nunderstanding the shell structure is crucial for the study of the transport\nproperty of networks. For a randomly connected network with given degree\ndistribution, we derive analytically the degree distribution and average degree\nof the nodes residing outside shell $\\ell$ as a function of $r_\\ell$. Further,\nwe find that $r_\\ell$ follows an iterative functional form\n$r_\\ell=\\phi(r_{\\ell-1})$, where $\\phi$ is expressed in terms of the generating\nfunction of the original degree distribution of the network. Our results can\nexplain the power-law distribution of the number of nodes $B_\\ell$ found in\nshells with $\\ell$ larger than the network diameter $d$, which is the average\ndistance between all pairs of nodes. For real world networks the theoretical\nprediction of $r_\\ell$ deviates from the empirical $r_\\ell$. We introduce a\nnetwork correlation function $c(r_\\ell)\\equiv r_{\\ell+1}/\\phi(r_\\ell)$ to\ncharacterize the correlations in the network, where $r_{\\ell+1}$ is the\nempirical value and $\\phi(r_\\ell)$ is the theoretical prediction. $c(r_\\ell)=1$\nindicates perfect agreement between empirical results and theory. We apply\n$c(r_\\ell)$ to several model and real world networks. We find that the networks\nfall into two distinct classes: (i) a class of {\\it poorly-connected} networks\nwith $c(r_\\ell)>1$, which have larger average distances compared with randomly\nconnected networks with the same degree distributions; and (ii) a class of {\\it\nwell-connected} networks with $c(r_\\ell)<1$.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.2070v1"
    },
    {
        "title": "Detecting network communities by propagating labels under constraints",
        "authors": [
            "Michael J. Barber",
            "John W. Clark"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We investigate the recently proposed label-propagation algorithm (LPA) for\nidentifying network communities. We reformulate the LPA as an equivalent\noptimization problem, giving an objective function whose maxima correspond to\ncommunity solutions. By considering properties of the objective function, we\nidentify conceptual and practical drawbacks of the label propagation approach,\nmost importantly the disparity between increasing the value of the objective\nfunction and improving the quality of communities found. To address the\ndrawbacks, we modify the objective function in the optimization problem,\nproducing a variety of algorithms that propagate labels subject to constraints;\nof particular interest is a variant that maximizes the modularity measure of\ncommunity quality. Performance properties and implementation details of the\nproposed algorithms are discussed. Bipartite as well as unipartite networks are\nconsidered.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.3138v2"
    },
    {
        "title": "Limits of Declustering Methods for Disentangling Exogenous from\n  Endogenous Events in Time Series with Foreshocks, Main shocks and Aftershocks",
        "authors": [
            "D. Sornette",
            "S. Utkin"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Many time series in natural and social sciences can be seen as resulting from\nan interplay between exogenous influences and an endogenous organization. We\nuse a simple (ETAS) model of events occurring sequentially, in which future\nevents are influenced (partially triggered) by past events to ask the question\nof how well can one disentangle the exogenous events from the endogenous ones.\nWe apply both model-dependant and model-independent stochastic declustering\nmethods to reconstruct the tree of ancestry and estimate key parameters. In\ncontrast with previously reported positive results, we have to conclude that\ndeclustered catalogs are rather unreliable for the synthetic catalogs that we\nhave investigated, which contains of the order of thousands of events, typical\nof realistic applications. The estimated rates of exogenous events suffer from\nlarge errors. The key branching ratio $n$, quantifying the fraction of events\nthat have been triggered by previous events, is also badly estimated in general\nfrom declustered catalogs. We find however that the errors tend to be smaller\nand perhaps acceptable in some cases for small triggering efficiency and\nbranching ratios. The high level of randomness together with the very long\nmemory makes the stochastic reconstruction of trees of ancestry and the\nestimation of the key parameters perhaps intrinsically unreliable for long\nmemory processes. For shorter memories (larger \"bare\" Omori exponent), the\nresults improve significantly.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.3217v1"
    },
    {
        "title": "Generalised cascades",
        "authors": [
            "Silvio M. Duarte Queiros"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this manuscript we give thought to the aftermath on the stable probability\ndensity function when standard multiplicative cascades are generalised cascades\nbased on the $q$-product of Borges that emerged in the context of non-extensive\nstatistical mechanics.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.3621v1"
    },
    {
        "title": "Random planar graphs and the London street network",
        "authors": [
            "A. P. Masucci",
            "D. Smith",
            "A. Crooks",
            "M. Batty"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this paper we analyse the street network of London both in its primary and\ndual representation. To understand its properties, we consider three idealised\nmodels based on a grid, a static random planar graph and a growing random\nplanar graph. Comparing the models and the street network, we find that the\nstreets of London form a self-organising system whose growth is characterised\nby a strict interaction between the metrical and informational space. In\nparticular, a principle of least effort appears to create a balance between the\nphysical and the mental effort required to navigate the city.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.5440v1"
    },
    {
        "title": "Stability Variances: A filter Approach",
        "authors": [
            "Alaa Makdissi",
            "François Vernotte",
            "Emeric De Clercq"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We analyze the Allan Variance estimator as the combination of Discrete-Time\nlinear filters. We apply this analysis to the different variants of the Allan\nvariance: the Overlapping Allan Variance, the Modified Allan variance, the\nHadamard Variance and the Overlapping Hadamard variance. Based on this analysis\nwe present a new method to compute a new estimator of the Allan Variance and\nits variants in the frequency domain. We show that the proposed frequency\ndomain equations are equivalent to extending the data by periodization in the\ntime domain. Like the Total Variance \\cite{totvar}, which is based on extending\nthe data manually in the time domain, our frequency domain variances estimators\nhave better statistics than the estimators of the classical variances in the\ntime domain. We demonstrate that the previous well-know equation that relates\nthe Allan Variance to the Power Spectrum Density (PSD) of continuous-time\nsignals is not valid for real world discrete-time measurements and we propose a\nnew equation that relates the Allan Variance to the PSD of the discrete-time\nsignals and that allows to compute the Allan variance and its different\nvariants in the frequency domain .\n",
        "pdf_link": "http://arxiv.org/pdf/0904.2660v1"
    },
    {
        "title": "Compressed Remote Sensing of Sparse Objects",
        "authors": [
            "Albert Fannjiang",
            "Pengchong Yan",
            "Thomas Strohmer"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  The linear inverse source and scattering problems are studied from the\nperspective of compressed sensing, in particular the idea that sufficient\nincoherence and sparsity guarantee uniqueness of the solution. By introducing\nthe sensor as well as target ensembles, the maximum number of recoverable\ntargets is proved to be at least proportional to the number of measurement data\nmodulo a log-square factor with overwhelming probability. Important\ncontributions of the analysis include the discoveries of the threshold\naperture, consistent with the classical Rayleigh criterion, and the decoherence\neffect induced by random antenna locations. The prediction of theorems are\nconfirmed by numerical simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.3994v2"
    },
    {
        "title": "Evaluation of pedodiversity and land use diversity in terms of the\n  Shannon Entropy",
        "authors": [
            "Tetsuo Yabuki",
            "Yumi Matsumura",
            "Yoko Nakatani"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Recently, the Shannon entropy, which was introduced originally as a measure\nof information amount, has been widely used as a useful index of various\ndiversities such as biodiversity and geodiversity. In this work we have\nevaluated the diversity of soil and land use, in both composition distributions\nand spatial distributions, in terms of the Shanonn entropy. Moreover we have\nalso proposed how to estimate the connection between the diversity of soil and\nland use in the spatial distribution using mutual entropy, and carried out the\nestimation of its connection.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.2821v1"
    },
    {
        "title": "Scaling and memory in recurrence intervals of Internet traffic",
        "authors": [
            "Shi-Min Cai",
            "Zhong-Qian Fu",
            "Tao Zhou",
            "Jun Gu",
            "Pei-Ling Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  By studying the statistics of recurrence intervals, $\\tau$, between\nvolatilities of Internet traffic rate changes exceeding a certain threshold\n$q$, we find that the probability distribution functions, $P_{q}(\\tau)$, for\nboth byte and packet flows, show scaling property as\n$P_{q}(\\tau)=\\frac{1}{\\overline{\\tau}}f(\\frac{\\tau}{\\overline{\\tau}})$. The\nscaling functions for both byte and packet flows obeys the same stretching\nexponential form, $f(x)=A\\texttt{exp}(-Bx^{\\beta})$, with $\\beta \\approx 0.45$.\nIn addition, we detect a strong memory effect that a short (or long) recurrence\ninterval tends to be followed by another short (or long) one. The detrended\nfluctuation analysis further demonstrates the presence of long-term correlation\nin recurrence intervals.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.3878v2"
    },
    {
        "title": "Upper Limits from Counting Experiments with Multiple Pipelines",
        "authors": [
            "Patrick J. Sutton"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In counting experiments, one can set an upper limit on the rate of a Poisson\nprocess based on a count of the number of events observed due to the process.\nIn some experiments, one makes several counts of the number of events, using\ndifferent instruments, different event detection algorithms, or observations\nover multiple time intervals. We demonstrate how to generalize the classical\nfrequentist upper limit calculation to the case where multiple counts of events\nare made over one or more time intervals using several (not necessarily\nindependent) procedures. We show how different choices of the rank ordering of\npossible outcomes in the space of counts correspond to applying different\nlevels of significance to the various measurements. We propose an ordering that\nis matched to the sensitivity of the different measurement procedures and show\nthat in typical cases it gives stronger upper limits than other choices. As an\nexample, we show how this method can be applied to searches for\ngravitational-wave bursts, where multiple burst-detection algorithms analyse\nthe same data set, and demonstrate how a single combined upper limit can be set\non the gravitational-wave burst rate.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.4089v2"
    },
    {
        "title": "A New Methodology for Generalizing Unweighted Network Measures",
        "authors": [
            "Sherief Abdallah"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Several important complex network measures that helped discovering common\npatterns across real-world networks ignore edge weights, an important\ninformation in real-world networks. We propose a new methodology for\ngeneralizing measures of unweighted networks through a generalization of the\ncardinality concept of a set of weights. The key observation here is that many\nmeasures of unweighted networks use the cardinality (the size) of some subset\nof edges in their computation. For example, the node degree is the number of\nedges incident to a node. We define the effective cardinality, a new metric\nthat quantifies how many edges are effectively being used, assuming that an\nedge's weight reflects the amount of interaction across that edge. We prove\nthat a generalized measure, using our method, reduces to the original\nunweighted measure if there is no disparity between weights, which ensures that\nthe laws that govern the original unweighted measure will also govern the\ngeneralized measure when the weights are equal. We also prove that our\ngeneralization ensures a partial ordering (among sets of weighted edges) that\nis consistent with the original unweighted measure, unlike previously developed\ngeneralizations. We illustrate the applicability of our method by generalizing\nfour unweighted network measures. As a case study, we analyze four real-world\nweighted networks using our generalized degree and clustering coefficient. The\nanalysis shows that the generalized degree distribution is consistent with the\npower-law hypothesis but with steeper decline and that there is a common\npattern governing the ratio between the generalized degree and the traditional\ndegree. The analysis also shows that nodes with more uniform weights tend to\ncluster with nodes that also have more uniform weights among themselves.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.4169v3"
    },
    {
        "title": "Syntax is from Mars while Semantics from Venus! Insights from Spectral\n  Analysis of Distributional Similarity Networks",
        "authors": [
            "Chris Biemann",
            "Monojit Choudhury",
            "Animesh Mukherjee"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We study the global topology of the syntactic and semantic distributional\nsimilarity networks for English through the technique of spectral analysis. We\nobserve that while the syntactic network has a hierarchical structure with\nstrong communities and their mixtures, the semantic network has several tightly\nknit communities along with a large core without any such well-defined\ncommunity structure.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.1467v1"
    },
    {
        "title": "Entropy correlation distance method applied to study correlations\n  between the Gross Domestic Product of rich countries",
        "authors": [
            "Marcel Ausloos",
            "Janusz Miskiewicz"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  The Theil index is much used in economy and finance; it looks like the\nShannon entropy, but pertains to event values rather than to their\nprobabilities. Any time series can be remapped through the Theil index.\nCorrelation coefficients can be evaluated between the new time series, thereby\nallowing to study their mutual statistical distance, - to be contrasted to the\nusual correlation distance measure for the primary time series. As an example\nthis entropy-like correlation distance method (ECDM) is applied to the Gross\nDomestic Product of 20 rich countries in order to test some economy\nglobalization process. Hierarchical distances allow to construct (i) a linear\nnetwork, (ii) a Locally Minimal Spanning Tree. The role of time averaging in\nfinite size windows is illustrated and discussed. It is also shown that the\nmean distance between the most developed countries, was decreasing since 1960\ntill 2000, - which we consider to be a proof of globalization of the economy\nfor these countries.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.2379v1"
    },
    {
        "title": "Computational time-reversal imaging with a small number of random and\n  noisy measurements",
        "authors": [
            "M. Andrecut"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Computational time reversal imaging can be used to locate the position of\nmultiple scatterers in a known background medium. The current methods for\ncomputational time reversal imaging are based on the null subspace projection\noperator, obtained through the singular value decomposition of the frequency\nresponse matrix. Here, we discuss the image recovery problem from a small\nnumber of random and noisy measurements, and we show that this problem is\nequivalent to a randomized approximation of the null subspace of the frequency\nresponse matrix.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.2600v1"
    },
    {
        "title": "Granularité laser et interférences de speckles",
        "authors": [
            "Boris Chenaud",
            "Pierre Valvin"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this paper we introduce experimentally the phenomenon of speckle and its\ninterferometric applications. With the popularization of CCD sensors and\nwebcams, it is now easy to acquire speckles patterns, to reduce them and\nexploit them. The material used here is what we could find easily in high\nschools. All Image processing mentioned in this article could be done using the\nfree software called IRIS. Keywords are essentially diffraction, interference\nphenomena and Fourier optics. After presenting the characteristics of speckles\nwe discuss the phenomenon of speckles interferences by analogy with the\nconventional 2 and N waves interferences. Finally, we apply interferometry to\nmeasure the angular separation between the components of a double star in\ndrawing heavily on the historical experience of Antoine Labeyrie.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.3612v1"
    },
    {
        "title": "Dragon-Kings, Black Swans and the Prediction of Crises",
        "authors": [
            "Didier Sornette"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We develop the concept of ``dragon-kings'' corresponding to meaningful\noutliers, which are found to coexist with power laws in the distributions of\nevent sizes under a broad range of conditions in a large variety of systems.\nThese dragon-kings reveal the existence of mechanisms of self-organization that\nare not apparent otherwise from the distribution of their smaller siblings. We\npresent a generic phase diagram to explain the generation of dragon-kings and\ndocument their presence in six different examples (distribution of city sizes,\ndistribution of acoustic emissions associated with material failure,\ndistribution of velocity increments in hydrodynamic turbulence, distribution of\nfinancial drawdowns, distribution of the energies of epileptic seizures in\nhumans and in model animals, distribution of the earthquake energies). We\nemphasize the importance of understanding dragon-kings as being often\nassociated with a neighborhood of what can be called equivalently a phase\ntransition, a bifurcation, a catastrophe (in the sense of Rene Thom), or a\ntipping point. The presence of a phase transition is crucial to learn how to\ndiagnose in advance the symptoms associated with a coming dragon-king. Several\nexamples of predictions using the derived log-periodic power law method are\ndiscussed, including material failure predictions and the forecasts of the end\nof financial bubbles.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.4290v1"
    },
    {
        "title": "Phase transition and information cascade in a voting model",
        "authors": [
            "Masato Hisakado",
            "Shintaro Mori"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We introduce a voting model that is similar to a Keynesian beauty contest and\nanalyze it from a mathematical point of view. There are two types of\nvoters-copycat and independent-and two candidates. Our voting model is a\nbinomial distribution (independent voters) doped in a beta binomial\ndistribution (copycat voters). We find that the phase transition in this system\nis at the upper limit of $t$, where $t$ is the time (or the number of the\nvotes). Our model contains three phases. If copycats constitute a majority or\neven half of the total voters, the voting rate converges more slowly than it\nwould in a binomial distribution. If independents constitute the majority of\nvoters, the voting rate converges at the same rate as it would in a binomial\ndistribution. We also study why it is difficult to estimate the conclusion of a\nKeynesian beauty contest when there is an information cascade.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.4818v2"
    },
    {
        "title": "Mutually-Antagonistic Interactions in Baseball Networks",
        "authors": [
            "Serguei Saavedra",
            "Scott Powers",
            "Trent McCotter",
            "Mason A. Porter",
            "Peter J. Mucha"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We formulate the head-to-head matchups between Major League Baseball pitchers\nand batters from 1954 to 2008 as a bipartite network of mutually-antagonistic\ninteractions. We consider both the full network and single-season networks,\nwhich exhibit interesting structural changes over time. We find interesting\nstructure in the network and examine their sensitivity to baseball's rule\nchanges. We then study a biased random walk on the matchup networks as a simple\nand transparent way to compare the performance of players who competed under\ndifferent conditions and to include information about which particular players\na given player has faced. We find that a player's position in the network does\nnot correlate with his success in the random walker ranking but instead has a\nsubstantial effect on its sensitivity to changes in his own aggregate\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.5241v2"
    },
    {
        "title": "Optimal transport on supply-demand networks",
        "authors": [
            "Yu-Han Chen",
            "Bing-Hong Wang",
            "Li-Chao Zhao",
            "Changsong Zhou",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Previously, transport networks are usually treated as homogeneous networks,\nthat is, every node has the same function, simultaneously providing and\nrequiring resources. However, some real networks, such as power grid and supply\nchain networks, show a far different scenario in which the nodes are classified\ninto two categories: the supply nodes provide some kinds of services, while the\ndemand nodes require them. In this paper, we propose a general transport model\nfor those supply-demand networks, associated with a criterion to quantify their\ntransport capacities. In a supply-demand network with heterogenous degree\ndistribution, its transport capacity strongly depends on the locations of\nsupply nodes. We therefore design a simulated annealing algorithm to find the\noptimal configuration of supply nodes, which remarkably enhances the transport\ncapacity, and outperforms the degree target algorithm, the betweenness target\nalgorithm, and the greedy method. This work provides a start point for\nsystematically analyzing and optimizing transport dynamics on supply-demand\nnetworks.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.1184v1"
    },
    {
        "title": "Origin of the Scaling Law in Human Mobility: Hierarchical Organization\n  of Traffic Systems",
        "authors": [
            "Xiaopu Han",
            "Qiang Hao",
            "Binghong Wang",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Uncovering the mechanism leading to the scaling law in human trajectories is\nof fundamental importance in understanding many spatiotemporal phenomena. We\npropose a hierarchical geographical model to mimic the real traffic system,\nupon which a random walker will generate a power-law travel displacement\ndistribution with exponent -2. When considering the inhomogeneities of cities'\nlocations and attractions, this model reproduces a power-law displacement\ndistribution with an exponential cutoff, as well as a scaling behavior in the\nprobability density of having traveled a certain distance at a certain time.\nOur results agree very well with the empirical observations reported in [D.\nBrockmann et al., Nature 439, 462 (2006)].\n",
        "pdf_link": "http://arxiv.org/pdf/0908.1221v2"
    },
    {
        "title": "B-Rank: A top N Recommendation Algorithm",
        "authors": [
            "Marcel Blattner"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this paper B-Rank, an efficient ranking algorithm for recommender systems,\nis proposed. B-Rank is based on a random walk model on hypergraphs. Depending\non the setup, B-Rank outperforms other state of the art algorithms in terms of\nprecision, recall (19% - 50%), and inter list diversity (20% - 60%). B-Rank\ncaptures well the difference between popular and niche objects. The proposed\nalgorithm produces very promising results for sparse and dense voting matrices.\nFurthermore, a recommendation list update algorithm is introduced,to cope with\nnew votes. This technique significantly reduces computational complexity. The\nimplementation of the algorithm is simple, since B-Rank needs no parameter\ntuning.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.2741v3"
    },
    {
        "title": "Quantifying Rational Belief",
        "authors": [
            "Ariel Caticha"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Some criticisms that have been raised against the Cox approach to probability\ntheory are addressed. Should we use a single real number to measure a degree of\nrational belief? Can beliefs be compared? Are the Cox axioms obvious? Are there\ncounterexamples to Cox? Rather than justifying Cox's choice of axioms we follow\na different path and derive the sum and product rules of probability theory as\nthe unique (up to regraduations) consistent representations of the Boolean AND\nand OR operations.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.3212v2"
    },
    {
        "title": "Anomalous diffusion in the dynamics of complex processes",
        "authors": [
            "Serge F. Timashev",
            "Yuriy S. Polyakov",
            "Pavel I. Misurkin",
            "Sergey G. Lakeev"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Anomalous diffusion, process in which the mean-squared displacement of system\nstates is a non-linear function of time, is usually identified in real\nstochastic processes by comparing experimental and theoretical displacements at\nrelatively small time intervals. This paper proposes an interpolation\nexpression for the identification of anomalous diffusion in complex signals for\nthe cases when the dynamics of the system under study reaches a steady state\n(large time intervals). This interpolation expression uses the chaotic\ndifference moment (transient structural function) of the second order as an\naverage characteristic of displacements. A general procedure for identifying\nanomalous diffusion and calculating its parameters in real stochastic signals,\nwhich includes the removal of the regular (low-frequency) components from the\nsource signal and the fitting of the chaotic part of the experimental\ndifference moment of the second order to the interpolation expression, is\npresented. The procedure was applied to the analysis of the dynamics of\nmagnetoencephalograms, blinking fluorescence of quantum dots, and X-ray\nemission from accreting objects. For all three applications, the interpolation\nwas able to adequately describe the chaotic part of the experimental difference\nmoment, which implies that anomalous diffusion manifests itself in these\nnatural signals. The results of this study make it possible to broaden the\nrange of complex natural processes in which anomalous diffusion can be\nidentified. The relation between the interpolation expression and a diffusion\nmodel, which is derived in the paper, allows one to simulate the chaotic\nprocesses in the open complex systems with anomalous diffusion.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.4118v2"
    },
    {
        "title": "Allan Variance Analysis as Useful Tool to Determine Noise in Various\n  Single-Molecule Setups",
        "authors": [
            "Fabian Czerwinski",
            "Andrew C. Richardson",
            "Christine Selhuber-Unkel",
            "Lene B. Oddershede"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  One limitation on the performance of optical traps is the noise inherently\npresent in every setup. Therefore, it is the desire of most experimentalists to\nminimize and possibly eliminate noise from their optical trapping experiments.\nA step in this direction is to quantify the actual noise in the system and to\nevaluate how much each particular component contributes to the overall noise.\nFor this purpose we present Allan variance analysis as a straightforward\nmethod. In particular, it allows for judging the impact of drift which gives\nrise to low-frequency noise, which is extremely difficult to pinpoint by other\nmethods. We show how to determine the optimal sampling time for calibration,\nthe optimal number of data points for a desired experiment, and we provide\nmeasurements of how much accuracy is gained by acquiring additional data\npoints. Allan variances of both micrometer-sized spheres and asymmetric\nnanometer-sized rods are considered.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.4484v1"
    },
    {
        "title": "Gibrat's law for cities: uniformly most powerful unbiased test of the\n  Pareto against the lognormal",
        "authors": [
            "Y. Malevergne",
            "V. Pisarenko",
            "D. Sornette"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We address the general problem of testing a power law distribution versus a\nlog-normal distribution in statistical data. This general problem is\nillustrated on the distribution of the 2000 US census of city sizes. We provide\ndefinitive results to close the debate between Eeckhout (2004, 2009) and Levy\n(2009) on the validity of Zipf's law, which is the special Pareto law with tail\nexponent 1, to describe the tail of the distribution of U.S. city sizes.\nBecause the origin of the disagreement between Eeckhout and Levy stems from the\nlimited power of their tests, we perform the {\\em uniformly most powerful\nunbiased test} for the null hypothesis of the Pareto distribution against the\nlognormal. The $p$-value and Hill's estimator as a function of city size lower\nthreshold confirm indubitably that the size distribution of the 1000 largest\ncities or so, which include more than half of the total U.S. population, is\nPareto, but we rule out that the tail exponent, estimated to be $1.4 \\pm 0.1$,\nis equal to 1. For larger ranks, the $p$-value becomes very small and Hill's\nestimator decays systematically with decreasing ranks, qualifying the lognormal\ndistribution as the better model for the set of smaller cities. These two\nresults reconcile the opposite views of Eeckhout (2004, 2009) and Levy (2009).\nWe explain how Gibrat's law of proportional growth underpins both the Pareto\nand lognormal distributions and stress the key ingredient at the origin of\ntheir difference in standard stochastic growth models of cities\n\\cite{Gabaix99,Eeckhout2004}.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.1281v1"
    },
    {
        "title": "Generalized priority-queue network dynamics: Impact of team and\n  hierarchy",
        "authors": [
            "Won-kuk Cho",
            "Byungjoon Min",
            "K. -I. Goh",
            "I. -M. Kim"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We study the effect of team and hierarchy on the waiting-time dynamics of\npriority-queue networks. To this end, we introduce generalized priority-queue\nnetwork models incorporating interaction rules based on team-execution and\nhierarchy in decision making, respectively. It is numerically found that the\nwaiting time distribution exhibits a power law for long waiting times in both\ncases, yet with different exponents depending on the team size and the position\nof queue nodes in the hierarchy, respectively. The observed power-law behaviors\nhave in many cases a corresponding single or pairwise-interacting queue\ndynamics, suggesting that the pairwise interaction may constitute a major\ndynamics consequence in the priority-queue networks. It is also found that the\nreciprocity of influence is a relevant factor for the priority-queue network\ndynamics\n",
        "pdf_link": "http://arxiv.org/pdf/0909.1461v1"
    },
    {
        "title": "Feasibility of 3D reconstructions from a single 2D diffraction\n  measurement",
        "authors": [
            "Pierre Thibault"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We comment on the recent manuscript by Raines et al. [arXiv:0905.0269v2] (now\npublished in Nature, vol. 463, p. 214-217, 2010), which suggests that in\ncertain conditions a single diffraction measurement may be sufficient to\nreconstruct the full three-dimensional density of a scatterer. We show that\npast literature contains the tools to assess rigorously the feasibility of this\napproach. We question the formulation of the reconstruction algorithm used by\nthe authors and we argue that the experimental data used as a demonstration is\nnot suitable for this method, and thus that the reconstruction is not valid.\n  This second version was produced for documentation purposes. In addition to\nthe minimally modified original comment, it includes in appendix a subsequent\nreply to one of the authors (J. Miao).\n",
        "pdf_link": "http://arxiv.org/pdf/0909.1643v2"
    },
    {
        "title": "Impact of sudden mass mortality on suicides",
        "authors": [
            "Bertrand M. Roehner"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We show that a large scale mass mortality results in increased numbers of\nsuicides. As a case in point, we consider the influenza epidemic of October\n1918 in the United States. In this month, suicides peaked at a level of over 4s\n(where s denotes the desaisonalized standard deviation of the suicide rate)\nwhich means that one would expect such a jump to occur merely by chance only\nonce in several centuries. The mechanism that we propose to explain this effect\nrelies on two steps (i) Mass mortalities break family bonds for instance\nbetween parents and children or husbands and wives. (ii) Increased numbers of\nsuicides then result from the well known fact that the severance of family\nbonds invariably produces more suicides.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.2425v1"
    },
    {
        "title": "Reconstructing the Traffic State by Fusion of Heterogeneous Data",
        "authors": [
            "Martin Treiber",
            "Arne Kesting",
            "R. Eddie Wilson"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We present an advanced interpolation method for estimating smooth\nspatiotemporal profiles for local highway traffic variables such as flow, speed\nand density. The method is based on stationary detector data as typically\ncollected by traffic control centres, and may be augmented by floating car data\nor other traffic information. The resulting profiles display transitions\nbetween free and congested traffic in great detail, as well as fine structures\nsuch as stop-and-go waves. We establish the accuracy and robustness of the\nmethod and demonstrate three potential applications: 1. compensation for gaps\nin data caused by detector failure; 2. separation of noise from dynamic traffic\ninformation; and 3. the fusion of floating car data with stationary detector\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.4467v1"
    },
    {
        "title": "Slepian functions and their use in signal estimation and spectral\n  analysis",
        "authors": [
            "Frederik J. Simons"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  It is a well-known fact that mathematical functions that are timelimited (or\nspacelimited) cannot be simultaneously bandlimited (in frequency). Yet the\nfinite precision of measurement and computation unavoidably bandlimits our\nobservation and modeling scientific data, and we often only have access to, or\nare only interested in, a study area that is temporally or spatially bounded.\nIn the geosciences we may be interested in spectrally modeling a time series\ndefined only on a certain interval, or we may want to characterize a specific\ngeographical area observed using an effectively bandlimited measurement device.\nIt is clear that analyzing and representing scientific data of this kind will\nbe facilitated if a basis of functions can be found that are \"spatiospectrally\"\nconcentrated, i.e. \"localized\" in both domains at the same time. Here, we give\na theoretical overview of one particular approach to this \"concentration\"\nproblem, as originally proposed for time series by Slepian and coworkers, in\nthe 1960s. We show how this framework leads to practical algorithms and\nstatistically performant methods for the analysis of signals and their power\nspectra in one and two dimensions, and on the surface of a sphere.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.5368v1"
    },
    {
        "title": "Pearson Walk with Shrinking Steps in Two Dimensions",
        "authors": [
            "C. A. Serino",
            "S. Redner"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We study the shrinking Pearson random walk in two dimensions and greater, in\nwhich the direction of the Nth is random and its length equals lambda^{N-1},\nwith lambda<1. As lambda increases past a critical value lambda_c, the endpoint\ndistribution in two dimensions, P(r), changes from having a global maximum away\nfrom the origin to being peaked at the origin. The probability distribution for\na single coordinate, P(x), undergoes a similar transition, but exhibits\nmultiple maxima on a fine length scale for lambda close to lambda_c. We\nnumerically determine P(r) and P(x) by applying a known algorithm that\naccurately inverts the exact Bessel function product form of the Fourier\ntransform for the probability distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.0852v3"
    },
    {
        "title": "Investigating Bimodal Clustering in Human Mobility",
        "authors": [
            "James P. Bagrow",
            "Tal Koren"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We apply a simple clustering algorithm to a large dataset of cellular\ntelecommunication records, reducing the complexity of mobile phone users' full\ntrajectories and allowing for simple statistics to characterize their\nproperties. For the case of two clusters, we quantify how clustered human\nmobility is, how much of a user's spatial dispersion is due to motion between\nclusters, and how spatially and temporally separated clusters are from one\nanother.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.0674v1"
    },
    {
        "title": "Community Structure of the Physical Review Citation Network",
        "authors": [
            "P. Chen",
            "S. Redner"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We investigate the community structure of physics subfields in the citation\nnetwork of all Physical Review publications between 1893 and August 2007. We\nfocus on well-cited publications (those receiving more than 100 citations), and\napply modularity maximization to uncover major communities that correspond to\nclearly-identifiable subfields of physics. While most of the links between\ncommunities connect those with obvious intellectual overlap, there sometimes\nexist unexpected connections between disparate fields due to the development of\na widely-applicable theoretical technique or by cross fertilization between\ntheory and experiment. We also examine communities decade by decade and also\nuncover a small number of significant links between communities that are widely\nseparated in time.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.0694v2"
    },
    {
        "title": "Community Structure in Time-Dependent, Multiscale, and Multiplex\n  Networks",
        "authors": [
            "Peter J. Mucha",
            "Thomas Richardson",
            "Kevin Macon",
            "Mason A. Porter",
            "Jukka-Pekka Onnela"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Network science is an interdisciplinary endeavor, with methods and\napplications drawn from across the natural, social, and information sciences. A\nprominent problem in network science is the algorithmic detection of\ntightly-connected groups of nodes known as communities. We developed a\ngeneralized framework of network quality functions that allowed us to study the\ncommunity structure of arbitrary multislice networks, which are combinations of\nindividual networks coupled through links that connect each node in one network\nslice to itself in other slices. This framework allows one to study community\nstructure in a very general setting encompassing networks that evolve over\ntime, have multiple types of links (multiplexity), and have multiple scales.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.1824v3"
    },
    {
        "title": "Approximate reconstruction of bandlimited functions for the integrate\n  and fire sampler",
        "authors": [
            "Hans G. Feichtinger",
            "José C. Príncipe",
            "José Luis Romero",
            "Alexander Singh Alvarado",
            "Gino Angelo Velasco"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this paper we study the reconstruction of a bandlimited signal from\nsamples generated by the integrate and fire model. This sampler allows us to\ntrade complexity in the reconstruction algorithms for simple hardware\nimplementations, and is specially convenient in situations where the sampling\ndevice is limited in terms of power, area and bandwidth.\n  Although perfect reconstruction for this sampler is impossible, we give a\ngeneral approximate reconstruction procedure and bound the corresponding error.\nWe also show the performance of the proposed algorithm through numerical\nsimulations.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.0766v1"
    },
    {
        "title": "An index to link scientific productivity with visibility",
        "authors": [
            "Ren Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  I here propose an index that links the number of papers a researcher has\npublished with impact factors (IFs) of the journals that publish these papers.\nA researcher is said to have an index z if totally z of his/her papers are\npublished in journals with IFs of at least z/2. The z-index, not meant to\nevaluate, compare and rank scientists, is a number that hopefully conveniently\nsummarizes the number of publications in journals with high IFs.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.3573v1"
    },
    {
        "title": "Link Prediction Based on Local Random Walk",
        "authors": [
            "Weiping Liu",
            "Linyuan Lu"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The problem of missing link prediction in complex networks has attracted much\nattention recently. Two difficulties in link prediction are the sparsity and\nhuge size of the target networks. Therefore, the design of an efficient and\neffective method is of both theoretical interests and practical significance.\nIn this Letter, we proposed a method based on local random walk, which can give\ncompetitively good prediction or even better prediction than other\nrandom-walk-based methods while has a lower computational complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.2467v1"
    },
    {
        "title": "Point Processes Modeling of Time Series Exhibiting Power-Law Statistics",
        "authors": [
            "B. Kaulakys",
            "M. Alaburda",
            "V. Gontis"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We consider stochastic point processes generating time series exhibiting\npower laws of spectrum and distribution density (Phys. Rev. E 71, 051105\n(2005)) and apply them for modeling the trading activity in the financial\nmarkets and for the frequencies of word occurrences in the language.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.2639v1"
    },
    {
        "title": "Dynamics of tournaments: the soccer case",
        "authors": [
            "H. V. Ribeiro",
            "R. S. Mendes",
            "L. C. Malacarne",
            "S. Picoli Jr.",
            "P. A. Santoro"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  A random walk-like model is considered to discuss statistical aspects of\ntournaments. The model is applied to soccer leagues with emphasis on the\nscores. This competitive system was computationally simulated and the results\nare compared with empirical data from the English, the German and the Spanish\nleagues and showed a good agreement with them. The present approach enabled us\nto characterize a diffusion where the scores are not normally distributed,\nhaving a short and asymmetric tail extending towards more positive values. We\nargue that this non-Gaussian behavior is related with the difference between\nthe teams and with the asymmetry of the scores system. In addition, we compared\ntwo tournament systems: the all-play-all and the elimination tournaments.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.2854v1"
    },
    {
        "title": "Effect of extreme data loss on long-range correlated and anti-correlated\n  signals quantified by detrended fluctuation analysis",
        "authors": [
            "Qianli D. Y. Ma",
            "Ronny P. Bartsch",
            "Pedro Bernaola-Galván",
            "Mitsuru Yoneyama",
            "Plamen Ch. Ivanov"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We investigate how extreme loss of data affects the scaling behavior of\nlong-range power-law correlated and anti-correlated signals applying the DFA\nmethod. We introduce a segmentation approach to generate surrogate signals by\nrandomly removing data segments from stationary signals with different types of\ncorrelations. These surrogate signals are characterized by: (i) the DFA scaling\nexponent $\\alpha$ of the original correlated signal, (ii) the percentage $p$ of\nthe data removed, (iii) the average length $\\mu$ of the removed (or remaining)\ndata segments, and (iv) the functional form of the distribution of the length\nof the removed (or remaining) data segments. We find that the {\\it global}\nscaling exponent of positively correlated signals remains practically unchanged\neven for extreme data loss of up to 90%. In contrast, the global scaling of\nanti-correlated signals changes to uncorrelated behavior even when a very small\nfraction of the data is lost. These observations are confirmed on the examples\nof human gait and commodity price fluctuations. We systematically study the\n{\\it local} scaling behavior of signals with missing data to reveal deviations\nacross scales. We find that for anti-correlated signals even 10% of data loss\nleads to deviations in the local scaling at large scales from the original\nanti-correlated towards uncorrelated behavior. In contrast, positively\ncorrelated signals show no observable changes in the local scaling for up to\n65% of data loss, while for larger percentage, the local scaling shows\noverestimated regions (with higher local exponent) at small scales, followed by\nunderestimated regions (with lower local exponent) at large scales. Finally, we\ninvestigate how the scaling is affected by the statistics of the remaining data\nsegments in comparison to the removed segments.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.3641v2"
    },
    {
        "title": "The Failure of the Ergodic Assumption",
        "authors": [
            "M. Ignaccolo",
            "M. Latka",
            "B. J. West"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The well established procedure of constructing phenomenological ensemble from\na single long time series is investigated. It is determined that a time series\ngenerated by a simple Uhlenbeck-Ornstein Langevin equation is mean ergodic.\nHowever the probability ensemble average yields a variance that is different\nfrom that determined using the phenomenological ensemble (time average). We\nconclude that the latter ensemble is often neither stationary nor ergodic and\nconsequently the probability ensemble averages can misrepresent the underlying\ndynamic process.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.4103v1"
    },
    {
        "title": "Soccer: is scoring goals a predictable Poissonian process?",
        "authors": [
            "Andreas Heuer",
            "Christian Mueller",
            "Oliver Rubner"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The non-scientific event of a soccer match is analysed on a strictly\nscientific level. The analysis is based on the recently introduced concept of a\nteam fitness (Eur. Phys. J. B 67, 445, 2009) and requires the use of\nfinite-size scaling. A uniquely defined function is derived which\nquantitatively predicts the expected average outcome of a soccer match in terms\nof the fitness of both teams. It is checked whether temporary fitness\nfluctuations of a team hamper the predictability of a soccer match.\n  To a very good approximation scoring goals during a match can be\ncharacterized as independent Poissonian processes with pre-determined\nexpectation values. Minor correlations give rise to an increase of the number\nof draws. The non-Poissonian overall goal distribution is just a consequence of\nthe fitness distribution among different teams. The limits of predictability of\nsoccer matches are quantified. Our model-free classification of the underlying\ningredients determining the outcome of soccer matches can be generalized to\ndifferent types of sports events.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.0797v2"
    },
    {
        "title": "Detecting highly overlapping community structure by greedy clique\n  expansion",
        "authors": [
            "Conrad Lee",
            "Fergal Reid",
            "Aaron McDaid",
            "Neil Hurley"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  In complex networks it is common for each node to belong to several\ncommunities, implying a highly overlapping community structure. Recent advances\nin benchmarking indicate that existing community assignment algorithms that are\ncapable of detecting overlapping communities perform well only when the extent\nof community overlap is kept to modest levels. To overcome this limitation, we\nintroduce a new community assignment algorithm called Greedy Clique Expansion\n(GCE). The algorithm identifies distinct cliques as seeds and expands these\nseeds by greedily optimizing a local fitness function. We perform extensive\nbenchmarks on synthetic data to demonstrate that GCE's good performance is\nrobust across diverse graph topologies. Significantly, GCE is the only\nalgorithm to perform well on these synthetic graphs, in which every node\nbelongs to multiple communities. Furthermore, when put to the task of\nidentifying functional modules in protein interaction data, and college dorm\nassignments in Facebook friendship data, we find that GCE performs\ncompetitively.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.1827v2"
    },
    {
        "title": "Zipf's Law Leads to Heaps' Law: Analyzing Their Relation in Finite-Size\n  Systems",
        "authors": [
            "Linyuan Lu",
            "Zi-Ke Zhang",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Background: Zipf's law and Heaps' law are observed in disparate complex\nsystems. Of particular interests, these two laws often appear together. Many\ntheoretical models and analyses are performed to understand their co-occurrence\nin real systems, but it still lacks a clear picture about their relation.\nMethodology/Principal Findings: We show that the Heaps' law can be considered\nas a derivative phenomenon if the system obeys the Zipf's law. Furthermore, we\nrefine the known approximate solution of the Heaps' exponent provided the\nZipf's exponent. We show that the approximate solution is indeed an asymptotic\nsolution for infinite systems, while in the finite-size system the Heaps'\nexponent is sensitive to the system size. Extensive empirical analysis on tens\nof disparate systems demonstrates that our refined results can better capture\nthe relation between the Zipf's and Heaps' exponents. Conclusions/Significance:\nThe present analysis provides a clear picture about the relation between the\nZipf's law and Heaps' law without the help of any specific stochastic model,\nnamely the Heaps' law is indeed a derivative phenomenon from Zipf's law. The\npresented numerical method gives considerably better estimation of the Heaps'\nexponent given the Zipf's exponent and the system size. Our analysis provides\nsome insights and implications of real complex systems, for example, one can\nnaturally obtained a better explanation of the accelerated growth of scale-free\nnetworks.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.3861v2"
    },
    {
        "title": "The Anderson-Darling test of fit for the power law distribution from\n  left censored samples",
        "authors": [
            "H. F. Coronel-Brizio",
            "A. R. Hernandez-Montoya"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Maximum likelihood estimation and a test of fit based on the Anderson-Darling\nstatistic is presented for the case of the power law distribution when the\nparameters are estimated from a left-censored sample. Expressions for the\nmaximum likelihood estimators and tables of asymptotic percentage points for\nthe A^2 statistic are given. The technique is illustrated for data from the Dow\nJones Industrial Average index, an example of high theoretical and practical\nimportance in Econophysics, Finance, Physics, Biology and, in general, in other\nrelated Sciences such as Complexity Sciences.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.0417v1"
    },
    {
        "title": "Markov Chain Analysis of Musical Dice Games",
        "authors": [
            "J. R. Dawin",
            "D. Volchenkov"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We have studied entropy, redundancy, complexity, and first passage times to\nnotes for 804 pieces of 29 composers. The successful understanding of tonal\nmusic calls for an experienced listener, as entropy dominates over redundancy\nin musical messages. First passage times to notes resolve tonality and feature\na composer. We also discuss the possible distances in space of musical dice\ngames and introduced the geodesic distance based on the Riemann structure\nassociated to the probability vectors (rows of the transition matrices).\n",
        "pdf_link": "http://arxiv.org/pdf/1004.4198v1"
    },
    {
        "title": "Missing and spurious interactions and the reconstruction of complex\n  networks",
        "authors": [
            "R. Guimera",
            "M. Sales-Pardo"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Network analysis is currently used in a myriad of contexts: from identifying\npotential drug targets to predicting the spread of epidemics and designing\nvaccination strategies, and from finding friends to uncovering criminal\nactivity. Despite the promise of the network approach, the reliability of\nnetwork data is a source of great concern in all fields where complex networks\nare studied. Here, we present a general mathematical and computational\nframework to deal with the problem of data reliability in complex networks. In\nparticular, we are able to reliably identify both missing and spurious\ninteractions in noisy network observations. Remarkably, our approach also\nenables us to obtain, from those noisy observations, network reconstructions\nthat yield estimates of the true network properties that are more accurate than\nthose provided by the observations themselves. Our approach has the potential\nto guide experiments, to better characterize network data sets, and to drive\nnew discoveries.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.4791v1"
    },
    {
        "title": "Filter banks and the \"Intensity Analysis\" of EMG",
        "authors": [
            "Frank Borg"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Vinzenz von Tscharner (2000) has presented an interesting mathematical method\nfor analyzing EMG-data called \"intensity analysis\" (EMG = electromyography).\nBasically the method is a sort of bandpassing of the signal. The central idea\nof the method is to describe the \"power\" (or \"intensity\") of a non-stationary\nEMG signal as a function both of time and of frequency. The connection with\nwavelet theory is that the filter is constructed by rescaling a given mother\nwavelet using a special array of scales (center frequencies) with non-constant\nrelative bandwidth. Some aspects of the method may seem a bit ad hoc and we\nhave therefore undertaken a closer mathematical investigation, showing the\nconnection with the conventional wavelet analysis and giving a somewhat\nsimplified formulation of the method using Morlet wavelets. It is pointed out\nthat the \"intensity analysis\" method is related to the concept of an equalizer.\nIn order to illustrate the method we apply it to nonstationary EMG-signals of a\ndynamic leg-extension force-velcity tests. (Data provided by Taija Finni,\nUniversity of Jyv\\\"askyl\\\"a.)\n",
        "pdf_link": "http://arxiv.org/pdf/1005.0696v1"
    },
    {
        "title": "Quantifying long-range correlations in complex networks beyond nearest\n  neighbors",
        "authors": [
            "Diego Rybski",
            "Hernán D. Rozenfeld",
            "Jürgen P. Kropp"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We propose a fluctuation analysis to quantify spatial correlations in complex\nnetworks. The approach considers the sequences of degrees along shortest paths\nin the networks and quantifies the fluctuations in analogy to time series. In\nthis work, the Barabasi-Albert (BA) model, the Cayley tree at the percolation\ntransition, a fractal network model, and examples of real-world networks are\nstudied. While the fluctuation functions for the BA model show exponential\ndecay, in the case of the Cayley tree and the fractal network model the\nfluctuation functions display a power-law behavior. The fractal network model\ncomprises long-range anti-correlations. The results suggest that the\nfluctuation exponent provides complementary information to the fractal\ndimension.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.3680v1"
    },
    {
        "title": "How to Combine Independent Data Sets for the Same Quantity",
        "authors": [
            "Theodore P. Hill",
            "Jack Miller"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  This paper describes a recent mathematical method called conflation for\nconsolidating data from independent experiments that are designed to measure\nthe same quantity, such as Planck's constant or the mass of the top quark.\nConflation is easy to calculate and visualize, and minimizes the maximum loss\nin Shannon information in consolidating several independent distributions into\na single distribution. In order to benefit the experimentalist with a much more\ntransparent presentation than the previous mathematical treatise, the main\nbasic properties of conflation are derived in the special case of normal\n(Gaussian) data. Included are examples of applications to real data from\nmeasurements of the fundamental physical constants and from measurements in\nhigh energy physics, and the conflation operation is generalized to weighted\nconflation for situations when the underlying experiments are not uniformly\nreliable.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.4978v3"
    },
    {
        "title": "A Better Definition of the Kilogram",
        "authors": [
            "Theodore P. Hill",
            "Ronald F. Fox",
            "Jack Miller"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  This article reviews several recent proposed redefinitions of the kilogram,\nand compares them with respect to practical realizations, uncertainties\n(estimated standard deviations), and educational aspects.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.5139v2"
    },
    {
        "title": "Algorithm for Sector Spectra Calculation from Images Registered by the\n  Spectral Airglow Temperature Imager",
        "authors": [
            "Atanas Marinov Atanassov"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The Spectral Airglow Temperature Imager is an instrument, specially designed\nfor investigation of the wave processes in the Mesosphere-Lower Thermosphere.\nIn order to determine the kinematic parameters of a wave, the values of a\nphysical quantity in different space points and their changes in the time\nshould be known. As a result of the possibilities of the SATI instrument for\nspace scanning, different parts of the images (sectors of spectrograms)\ncorrespond to the respective mesopause areas (where the radiation is\ngenerated). Algorithms for sector spectra calculation are proposed. In contrast\nto the original algorithms where twelve sectors with angles of 30 degrees are\nonly determined now sectors with arbitrary orientation and angles are\ncalculated. An algorithm is presented for sector calculation based on pixel\ndivision into sub pixels. A comparative results are shown.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.3468v2"
    },
    {
        "title": "NetzCope: A Tool for Displaying and Analyzing Complex Networks",
        "authors": [
            "Michael J. Barber",
            "Ludwig Streit",
            "Oleg Strogan"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Networks are a natural and popular mechanism for the representation and\ninvestigation of a broad class of systems. But extracting information from a\nnetwork can present significant challenges. We present NetzCope, a software\napplication for the display and analysis of networks. Its key features include\nthe visualization of networks in two or three dimensions, the organization of\nvertices to reveal structural similarity, and the detection and visualization\nof network communities by modularity maximization.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.4294v2"
    },
    {
        "title": "Taxonomies of Networks",
        "authors": [
            "Jukka-Pekka Onnela",
            "Daniel J. Fenn",
            "Stephen Reid",
            "Mason A. Porter",
            "Peter J. Mucha",
            "Mark D. Fricker",
            "Nick S. Jones"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The study of networks has grown into a substantial interdisciplinary\nendeavour that encompasses myriad disciplines in the natural, social, and\ninformation sciences. Here we introduce a framework for constructing taxonomies\nof networks based on their structural similarities. These networks can arise\nfrom any of numerous sources: they can be empirical or synthetic, they can\narise from multiple realizations of a single process, empirical or synthetic,\nor they can represent entirely different systems in different disciplines.\nSince the mesoscopic properties of networks are hypothesized to be important\nfor network function, we base our comparisons on summaries of network community\nstructures. While we use a specific method for uncovering network communities,\nmuch of the introduced framework is independent of that choice. After\nintroducing the framework, we apply it to construct a taxonomy for 746\nindividual networks and demonstrate that our approach usefully identifies\nsimilar networks. We also construct taxonomies within individual categories of\nnetworks, and in each case we expose non-trivial structure. For example we\ncreate taxonomies for similarity networks constructed from both political\nvoting data and financial data. We also construct network taxonomies to compare\nthe social structures of 100 Facebook networks and the growth structures\nproduced by different types of fungi.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.5731v3"
    },
    {
        "title": "How to compare diffusion processes assessed by single-particle tracking\n  and pulsed field gradient nuclear magnetic resonance",
        "authors": [
            "Michael Bauer",
            "Rustem Valiullin",
            "Günter Radons",
            "Jörg Kärger"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Heterogeneous diffusion processes occur in many different fields such as\ntransport in living cells or diffusion in porous media. A characterization of\nthe transport parameters of such processes can be achieved by ensemble-based\nmethods, such as pulsed field gradient nuclear magnetic resonance (PFG NMR), or\nby trajectory-based methods obtained from single-particle tracking (SPT)\nexperiments. In this paper, we study the general relationship between both\nmethods and its application to heterogeneous systems. We derive analytical\nexpressions for the distribution of diffusivities from SPT and further relate\nit to NMR spin-echo diffusion attenuation functions. To exemplify the\napplicability of this approach, we employ a well-established two-region\nexchange model, which has widely been used in the context of PFG NMR studies of\nmultiphase systems subjected to interphase molecular exchange processes. This\ntype of systems, which can also describe a layered liquid with layer-dependent\nself-diffusion coefficients, has also recently gained attention in SPT\nexperiments. We reformulate the results of the two-region exchange model in\nterms of SPT-observables and compare its predictions to that obtained using the\nexact transformation which we derived.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.5953v2"
    },
    {
        "title": "Human activity as the decision-based queueing process: statistical data\n  analysis of waiting times in scientific journals",
        "authors": [
            "O. Mryglod",
            "Yu. Holovatch"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We consider the editorial processing of papers in scientific journals as a\nhuman activity process based on the decision making. A functional form of the\nprobability distributions of random variables describing a human dynamics is\nstudied using classical approaches of mass service systems theory, physics of\ncritical phenomena and statistical methods of data analysis. Our additional\ngoal is to corroborate the scientometrical application of the results obtained.\nKeywords: data analysis, statistics, mass service systems, human activity,\nscientometrics\n",
        "pdf_link": "http://arxiv.org/pdf/1007.0945v1"
    },
    {
        "title": "Optimal random search for a single hidden target",
        "authors": [
            "Joseph Snider"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  A single target is hidden at a location chosen from a predetermined\nprobability distribution. Then, a searcher must find a second probability\ndistribution from which random search points are sampled such that the target\nis found in the minimum number of trials. Here it will be shown that if the\nsearcher must get very close to the target to find it, then the best search\ndistribution is proportional to the square root of the target distribution. For\na Gaussian target distribution, the optimum search distribution is\napproximately a Gaussian with a standard deviation that varies inversely with\nhow close the searcher must be to the target to find it. For a network, where\nthe searcher randomly samples nodes and looks for the fixed target along edges,\nthe optimum is to either sample a node with probability proportional to the\nsquare root of the out degree plus one or not at all.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.3774v2"
    },
    {
        "title": "Lower Bounds on Mutual Information",
        "authors": [
            "David V. Foster",
            "Peter Grassberger"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We correct claims about lower bounds on mutual information (MI) between\nreal-valued random variables made in A. Kraskov {\\it et al.}, Phys. Rev. E {\\bf\n69}, 066138 (2004). We show that non-trivial lower bounds on MI in terms of\nlinear correlations depend on the marginal (single variable) distributions.\nThis is so in spite of the invariance of MI under reparametrizations, because\nlinear correlations are not invariant under them. The simplest bounds are\nobtained for Gaussians, but the most interesting ones for practical purposes\nare obtained for uniform marginal distributions. The latter can be enforced in\ngeneral by using the ranks of the individual variables instead of their actual\nvalues, in which case one obtains bounds on MI in terms of Spearman correlation\ncoefficients. We show with gene expression data that these bounds are in\ngeneral non-trivial, and the degree of their (non-)saturation yields valuable\ninsight.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.0817v1"
    },
    {
        "title": "Identification of Overlapping Communities by Locally Calculating\n  Community-Changing Resolution Levels",
        "authors": [
            "Frank Havemann",
            "Michael Heinz",
            "Alexander Struck",
            "Jochen Gläser"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  An algorithm for the detection of overlapping natural communities in networks\nwas proposed by Lancichinetti, Fortunato, and Kertesz (LFK) last year. The LFK\nalgorithm constructs natural communities of (in principle) all nodes of a graph\nby maximising the local fitness of communities. The resulting modules can\noverlap. The generation of communities can easily be repeated for many values\nof resolution; thus allowing different views on the network at different\nresolutions. We implemented the main idea of the LFK algorithm---to generate\nnatural communities of each node of a network---in a different way. We start\nwith a value of the resolution parameter that is high enough for each node to\nbe its own natural community. As soon as the resolution is reduced, each node\nacquires other nodes as members of its community, i.e. natural communities\ngrow. For each community found at a certain resolution level we calculate the\nnext lower resolution where a node is added. After adding a node to a community\nof a seed node we check whether it is also the natural community of a node that\nwe have already analysed. In this case, we can stop expanding the seed node's\ncommunity. We tested our algorithm on a small benchmark graph and on a network\nof about 500 papers in information science (weighted with the Salton index of\nbibliographic coupling). In our tests, this approach results in characteristic\nranges of resolution where a large resolution change does not lead to a growth\nof the natural community. Such stable modules were also obtained by applying\nthe LFK algorithm but since we determine communities for all resolution values\nin one run, our approach is faster than the LFK reference. And our algorithm\nreveals the hierarchical structure of the graph more easily.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.1004v1"
    },
    {
        "title": "Interpretations of Negative Probabilities",
        "authors": [
            "Mark Burgin"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  In this paper, we give a frequency interpretation of negative probability, as\nwell as of extended probability, demonstrating that to a great extent, these\nnew types of probabilities, behave as conventional probabilities. Extended\nprobability comprises both conventional probability and negative probability.\nThe frequency interpretation of negative probabilities gives supportive\nevidence to the axiomatic system built in (Burgin, 2009; arXiv:0912.4767) for\nextended probability as it is demonstrated in this paper that frequency\nprobabilities satisfy all axioms of extended probability.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.1287v1"
    },
    {
        "title": "Bertrand's paradox: a physical solution",
        "authors": [
            "P. Di Porto",
            "B. Crosignani",
            "A. Ciattoni",
            "H. C. Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We present a conclusive answer to Bertrand's paradox, a long standing open\nissue in the basic physical interpretation of probability. The paradox deals\nwith the existence of mutually inconsistent results when looking for the\nprobability that a chord, drawn at random in a circle, is longer than the side\nof an inscribed equilateral triangle. We obtain a unique solution by\nsubstituting chord drawing with the throwing of a straw of finite length L on a\ncircle of radius R, thus providing a satisfactory operative definition of the\nassociated experiment. The obtained probability turns out to be a function of\nthe ratio L/R, as intuitively expected.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.1878v1"
    },
    {
        "title": "Noise Corruption of Empirical Mode Decomposition and Its Effect on\n  Instantaneous Frequency",
        "authors": [
            "Daniel N. Kaslovsky",
            "Francois G. Meyer"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Huang's Empirical Mode Decomposition (EMD) is an algorithm for analyzing\nnonstationary data that provides a localized time-frequency representation by\ndecomposing the data into adaptively defined modes. EMD can be used to estimate\na signal's instantaneous frequency (IF) but suffers from poor performance in\nthe presence of noise. To produce a meaningful IF, each mode of the\ndecomposition must be nearly monochromatic, a condition that is not guaranteed\nby the algorithm and fails to be met when the signal is corrupted by noise. In\nthis work, the extraction of modes containing both signal and noise is\nidentified as the cause of poor IF estimation. The specific mechanism by which\nsuch \"transition\" modes are extracted is detailed and builds on the observation\nof Flandrin and Goncalves that EMD acts in a filter bank manner when analyzing\npure noise. The mechanism is shown to be dependent on spectral leak between\nmodes and the phase of the underlying signal. These ideas are developed through\nthe use of simple signals and are tested on a synthetic seismic waveform.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.4176v1"
    },
    {
        "title": "Bayesian theory of systematic measurement deviations",
        "authors": [
            "Michael Krystek"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Concerning systematic effects, the recommendation given in the GUM is to\ncorrect for them, but unfortunately no detailed information is available, how\nto do this. This publication will show, how systematic measurement deviations\ncan be handled correctly based on the Bayesian probability theory. After a\nshort overview about useful methods and tools, like the product rule of\nprobability theory, Bayes' theorem, the principle of maximum entropy, and the\nmarginalisation equation, an outline of a method to handle systematic\nmeasurement deviations is introduced. Finally some simple examples of practical\ninterest are given, in order to demonstrate the applicability of the suggested\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.0942v3"
    },
    {
        "title": "Scaling of Geographic Space from the Perspective of City and Field\n  Blocks and Using Volunteered Geographic Information",
        "authors": [
            "Bin Jiang",
            "Xintao Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Scaling of geographic space refers to the fact that for a large geographic\narea its small constituents or units are much more common than the large ones.\nThis paper develops a novel perspective to the scaling of geographic space\nusing large street networks involving both cities and countryside. Given a\nstreet network of an entire country, we decompose the street network into\nindividual blocks, each of which forms a minimum ring or cycle such as city\nblocks and field blocks. The block sizes demonstrate the scaling property,\ni.e., far more small blocks than large ones. Interestingly, we find that the\nmean of all the block sizes can easily separate between small and large blocks-\na high percentage (e.g., 90%) of smaller ones and a low percentage (e.g., 10%)\nof larger ones. Based on this regularity, termed as the head/tail division\nrule, we propose an approach to delineating city boundaries by grouping the\nsmaller blocks. The extracted city sizes for the three largest European\ncountries (France, Germany and UK) exhibit power law distributions. We further\ndefine the concept of border number as a topological distance of a block far\nfrom the outmost border to map the center(s) of the country and the city. We\ndraw an analogy between a country and a city (or geographic space in general)\nwith a complex organism like the human body or the human brain to further\nelaborate on the power of this block perspective in reflecting the structure or\npatterns of geographic space.\n  Keywords: Power law distribution, scaling of geographic space, data-intensive\ngeospatial computing, street networks\n",
        "pdf_link": "http://arxiv.org/pdf/1009.3635v5"
    },
    {
        "title": "Measuring Urban Sprawl Based on Massive Street Nodes and the Novel\n  Concept of Natural Cities",
        "authors": [
            "Tao Jia",
            "Bin Jiang"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  In this paper, we develop a novel approach to measuring urban sprawl based on\nstreet nodes and naturally defined urban boundaries, both extracted from\nmassive volunteered geographic information OpenStreetMap databases through some\ndata-intensive computing processes. The street nodes are defined as street\nintersections and ends, while the naturally defined urban boundaries constitute\nwhat we call natural cities. We find that the street nodes are significantly\ncorrelated with population of cities. Based on this finding, we set street\nnodes as a proxy of population to measure urban sprawl. We further find that\nstreet nodes bear a significant linear relationship with city areal extents. In\nthe plot with the x axis representing city areal extents, and the y axis street\nnodes, sprawling cities are located below the regression line. We verified the\napproach using urban areas and population from the US census, and then applied\nthe approach to three European countries: France, Germany, and the United\nKingdom for the categorization of natural cities into three classes: sprawling,\ncompact, and normal. This categorization sets a uniform standard for cross\ncomparing sprawling levels across an entire country.\n  Keywords: Street networks, openstreetmap, volunteered geographic information,\nGIS\n",
        "pdf_link": "http://arxiv.org/pdf/1010.0541v2"
    },
    {
        "title": "Maximum entropy generation in open systems: the Fourth Law?",
        "authors": [
            "Umberto Lucia"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  This paper develops an analytical and rigorous formulation of the maximum\nentropy generation principle. The result is suggested as the Fourth Law of\nThermodynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.3989v1"
    },
    {
        "title": "Icequakes coupled with surface displacements for predicting glacier\n  break-off",
        "authors": [
            "Jerome Faillettaz",
            "Didier Sornette",
            "Martin Funk"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  A hanging glacier at the east face of Weisshorn (Switzerland) broke off in\n2005. We were able to monitor and measure surface motion and icequake activity\nfor 25 days up to three days prior to the break-off. The analysis of seismic\nwaves generated by the glacier during the rupture maturation process revealed\nfour types of precursory signals of the imminent catastrophic rupture: (i) an\nincrease in seismic activity within the glacier, (ii) a decrease in the waiting\ntime between two successive icequakes, (iii) a change in the size-frequency\ndistribution of icequake energy, and (iv) a modification in the structure of\nthe waiting time distributions between two successive icequakes. Morevover, it\nwas possible to demonstrate the existence of a correlation between the seismic\nactivity and the log-periodic oscillations of the surface velocities\nsuperimposed on the global acceleration of the glacier during the rupture\nmaturation. Analysis of the seismic activity led us to the identification of\ntwo regimes: a stable phase with diffuse damage, and an unstable and dangerous\nphase characterized by a hierarchical cascade of rupture instabilities where\nlarge icequakes are triggered.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.4781v1"
    },
    {
        "title": "Application of a Quantum Ensemble Model to Linguistic Analysis",
        "authors": [
            "Andrij Rovenchak",
            "Solomija Buk"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  A new set of parameters to describe the word frequency behavior of texts is\nproposed. The analogy between the word frequency distribution and the\nBose-distribution is suggested and the notion of \"temperature\" is introduced\nfor this case. The calculations are made for English, Ukrainian, and the\nGuinean Maninka languages. The correlation between in-deep language structure\n(the level of analyticity) and the defined parameters is shown to exist.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.5076v1"
    },
    {
        "title": "Earthquake-like patterns of acoustic emission in crumpled plastic sheets",
        "authors": [
            "R. S. Mendes",
            "L. C. Malacarne",
            "R. P. B. Santos",
            "H. V. Ribeiro",
            "S. Picoli Jr"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We report remarkable similarities in the output signal of two distinct\nout-of- equilibrium physical systems - earthquakes and the intermittent\nacoustic noise emitted by crum- pled plastic sheets - Biaxially Oriented\nPolypropylene (BOPP) films. We show that both signals share several statistical\nproperties including the distribution of energy, distribution of energy in-\ncrements for distinct time scales, distribution of return intervals and\ncorrelations in the magnitude and sign of energy increments. This analogy is\nconsistent with the concept of universality in com- plex systems and could\nprovide some insight on the mechanisms behind the complex behavior of\nearthquakes.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.5402v1"
    },
    {
        "title": "On hypothesis testing, trials factor, hypertests and the BumpHunter",
        "authors": [
            "Georgios Choudalakis"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  A detailed presentation of hypothesis testing is given. The \"look elsewhere\"\neffect is illustrated, and a treatment of the trials factor is proposed with\nthe introduction of hypothesis hypertests. An example of such a hypertest is\npresented, named BumpHunter, which is used in the recent ATLAS dijet resonance\nsearch, and in an earlier version in the CDF Global Search, to look for exotic\nphenomena in high energy physics. As a demonstration, the BumpHunter is used to\naddress Problem 1 of the Banff Challenge.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.0390v2"
    },
    {
        "title": "Composite gravitational-wave detection of compact binary coalescence",
        "authors": [
            "Kipp Cannon",
            "Chad Hanna",
            "Drew Keppel",
            "Antony C. Searle"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The detection of gravitational waves from compact binaries relies on a\ncomputationally burdensome processing of gravitational-wave detector data. The\nparameter space of compact-binary-coalescence gravitational waves is large and\noptimal detection strategies often require nearly redundant calculations.\nPreviously, it has been shown that singular value decomposition of search\nfilters removes redundancy. Here we will demonstrate the use of singular value\ndecomposition for a composite detection statistic. This can greatly improve the\nprospects for a computationally feasible rapid detection scheme across a large\ncompact binary parameter space.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.0584v1"
    },
    {
        "title": "Phase retrieval from single biomolecule diffraction pattern",
        "authors": [
            "Shiro Ikeda",
            "Hidetoshi Kono"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In this paper, we propose the SPR (sparse phase retrieval) method, which is a\nnew phase retrieval method for coherent x-ray diffraction imaging (CXDI).\nConventional phase retrieval methods effectively solve the problem for high\nsignal-to-noise ratio measurements, but would not be sufficient for single\nbiomolecular imaging which is expected to be realized with femto-second x-ray\nfree electron laser pulses. The SPR method is based on the Bayesian statistics.\nIt does not need to set the object boundary constraint that is required by the\ncommonly used hybrid input-output (HIO) method, instead a prior distribution is\ndefined with an exponential distribution and used for the estimation.\nSimulation results demonstrate that the proposed method reconstructs the\nelectron density under a noisy condition even some central pixels are masked.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.1442v5"
    },
    {
        "title": "Identification of earthquake precursors in the hydrogeochemical and\n  geoacoustic data for the Kamchatka peninsula by flicker-noise spectroscopy",
        "authors": [
            "G. V. Ryabinin",
            "Yu. S. Polyakov",
            "V. A. Gavrilov",
            "S. F. Timashev"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  A phenomenological systems approach for identifying potential precursors in\nmultiple signals of different types for the same local seismically active\nregion is proposed based on the assumption that a large earthquake may be\npreceded by a system reconfiguration (preparation) at different time and space\nscales. A nonstationarity factor introduced within the framework of\nflicker-noise spectroscopy, a statistical physics approach to the analysis of\ntime series, is used as the dimensionless criterion for detecting qualitative\n(precursory) changes within relatively short time intervals in arbitrary\nsignals. Nonstationarity factors for chlorine-ion concentration variations in\nthe underground water of two boreholes on the Kamchatka peninsula and\ngeacoustic emissions in a deep borehole within the same seismic zone are\nstudied together in the time frame around a large earthquake on October 8,\n2001. It is shown that nonstationarity factor spikes (potential precursors)\ntake place in the interval from 70 to 50 days before the earthquake for the\nhydrogeochemical data and at 29 and 6 days in advance for the geoacoustic data.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.1473v1"
    },
    {
        "title": "On the interpolation of univariate distributions",
        "authors": [
            "Hans P. Dembinski"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  This note discusses an interpolation technique for univariate distributions.\nIn other words, the question is how to obtain a good approximation for f(x|a)\nif a0 < a < a1 is a control variable and f(x|a0) and f(x|a1) are known. The\ntechnique presented here is based on the interpolation of the quantile\nfunction, i.e. the inverse of the cumulative density function.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.3266v2"
    },
    {
        "title": "Collective Motion of Inelastic Particles between Two Oscillating Walls",
        "authors": [
            "Fei Fang Chung",
            "Sy-Sang Liaw",
            "Wei Chun Chang"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  This study theoretically considers the motion of N identical inelastic\nparticles between two oscillating walls. The particles' average energy\nincreases abruptly at certain critical filling fractions, wherein the system\nchanges into a solid-like phase with particles clustered in their compact form.\nMolecular dynamics simulations of the system show that the critical filling\nfraction is a decreasing function of vibration amplitude independent of\nvibration frequency, which is consistent with previous experimental results.\nThis study considers the entire group of particles as a giant pseudo-particle\nwith an effective size and an effective coefficient of restitution. The\nN-particles system is then analytically treated as a one-particle problem. The\ncritical filling fraction's dependence on vibration amplitude can be explained\nas a necessary condition for a stable resonant solution. The fluctuation to the\nsystem's mean flow energy is also studied to show the relation between the\ngranular temperature and the system phase.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.5438v1"
    },
    {
        "title": "Efficient routing strategies in scale-free networks with limited\n  bandwidth",
        "authors": [
            "Ming Tang",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We study the traffic dynamics in complex networks where each link is assigned\na limited and identical bandwidth. Although the first-in-first-out (FIFO)\nqueuing rule is widely applied in the routing protocol of information packets,\nhere we argue that if we drop this rule, the overall throughput of the network\ncan be remarkably enhanced. We proposed some efficient routing strategies that\ndo not strictly obey the FIFO rule. Comparing with the routine shortest path\nstrategy, the throughput for both Barab\\'asi-Albert (BA) networks and the real\nInternet, the throughput can be improved more than five times. We calculate the\ntheoretical limitation of the throughput. In BA networks, our proposed strategy\ncan achieve 88% of the theoretical optimum, yet for the real Internet, it is\nabout 12%, implying that we have a huge space to further improve the routing\nstrategy for the real Internet. Finally we discuss possibly promising ways to\ndesign more efficient routing strategies for the Internet.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.1770v1"
    },
    {
        "title": "The Nuclear Science References (NSR) Database and Web Retrieval System",
        "authors": [
            "B. Pritychenko",
            "E. Betak",
            "M. A. Kellett",
            "B. Singh",
            "J. Totans"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The Nuclear Science References (NSR) database together with its associated\nWeb interface, is the world's only comprehensive source of easily accessible\nlow- and intermediate-energy nuclear physics bibliographic information for more\nthan 200,000 articles since the beginning of nuclear science. The\nweekly-updated NSR database provides essential support for nuclear data\nevaluation, compilation and research activities. The principles of the database\nand Web application development and maintenance are described. Examples of\nnuclear structure, reaction and decay applications are specifically included.\nThe complete NSR database is freely available at the websites of the National\nNuclear Data Center http://www.nndc.bnl.gov/nsr and the International Atomic\nEnergy Agency http://www-nds.iaea.org/nsr.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.2806v2"
    },
    {
        "title": "Quantum Informatics View of Statistical Data Processing",
        "authors": [
            "Yu. I. Bogdanov",
            "N. A. Bogdanova"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Application of root density estimator to problems of statistical data\nanalysis is demonstrated. Four sets of basis functions based on\nChebyshev-Hermite, Laguerre, Kravchuk and Charlier polynomials are considered.\nThe sets may be used for numerical analysis in problems of reconstructing\nstatistical distributions by experimental data. Examples of numerical modeling\nare given.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.4140v1"
    },
    {
        "title": "Weighted Radial Variation for Node Feature Classification",
        "authors": [
            "C. Andris"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Connections created from a node-edge matrix have been traditionally difficult\nto visualize and analyze because of the number of flows to be rendered in a\nlimited feature or cartographic space. Because analyzing connectivity patterns\nis useful for understanding the complex dynamics of human and information flow\nthat connect non-adjacent space, techniques that allow for visual data mining\nor static representations of system dynamics are a growing field of research.\nHere, we create a Weighted Radial Variation (WRV) technique to classify a set\nof nodes based on the configuration of their radially-emanating vector flows.\nEach entity's vector is syncopated in terms of cardinality, direction, length,\nand flow magnitude. The WRV process unravels each star-like entity's individual\nflow vectors on a 0-360{\\deg} spectrum, to form a unique signal whose\ndistribution depends on the flow presence at each step around the entity, and\nis further characterized by flow distance and magnitude. The signals are\nprocessed with an unsupervised classification method that clusters entities\nwith similar signatures in order to provide a typology for each node in the\nsystem of spatial flows. We use a case study of U.S. county-to-county human\nincoming and outgoing migration data to test our method.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.4873v1"
    },
    {
        "title": "Information filtering via preferential diffusion",
        "authors": [
            "Linyuan Lu",
            "Weiping Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Recommender systems have shown great potential to address information\noverload problem, namely to help users in finding interesting and relevant\nobjects within a huge information space. Some physical dynamics, including heat\nconduction process and mass or energy diffusion on networks, have recently\nfound applications in personalized recommendation. Most of the previous studies\nfocus overwhelmingly on recommendation accuracy as the only important factor,\nwhile overlook the significance of diversity and novelty which indeed provide\nthe vitality of the system. In this paper, we propose a recommendation\nalgorithm based on the preferential diffusion process on user-object bipartite\nnetwork. Numerical analyses on two benchmark datasets, MovieLens and Netflix,\nindicate that our method outperforms the state-of-the-art methods.\nSpecifically, it can not only provide more accurate recommendations, but also\ngenerate more diverse and novel recommendations by accurately recommending\nunpopular objects.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.5499v1"
    },
    {
        "title": "Incorporating Nuisance Parameters in Likelihoods for Multisource Spectra",
        "authors": [
            "J. S. Conway"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We describe here the general mathematical approach to constructing\nlikelihoods for fitting observed spectra in one or more dimensions with\nmultiple sources, including the effects of systematic uncertainties represented\nas nuisance parameters, when the likelihood is to be maximized with respect to\nthese parameters. We consider three types of nuisance parameters: simple\nmultiplicative factors, source spectra \"morphing\" parameters, and parameters\nrepresenting statistical uncertainties in the predicted source spectra.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.0354v1"
    },
    {
        "title": "Do firms share the same functional form of their growth rate\n  distribution? A new statistical test",
        "authors": [
            "Josè T. Lunardi",
            "Salvatore Miccichè",
            "Fabrizio Lillo",
            "Rosario N. Mantegna",
            "Mauro Gallegati"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We introduce a new statistical test of the hypothesis that a balanced panel\nof firms have the same growth rate distribution or, more generally, that they\nshare the same functional form of growth rate distribution. We applied the test\nto European Union and US publicly quoted manufacturing firms data, considering\nfunctional forms belonging to the Subbotin family of distributions. While our\nhypotheses are rejected for the vast majority of sets at the sector level, we\ncannot rejected them at the subsector level, indicating that homogenous panels\nof firms could be described by a common functional form of growth rate\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.2234v1"
    },
    {
        "title": "Adaptive mosaic image representation for image processing",
        "authors": [
            "Evgenia Gelman"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Method for a mosaic image representation (MIR) is proposed for a selective\ntreatment of image fragments of different transition frequency. MIR method is\nbased on piecewise-constant image approximation on a non-uniform orthogonal\ngrid constructed by the following recurrent multigrid algorithm. A sequence of\nnested uniform grids is built, such that each cell of a current grid is\nsubdivided into four smaller cells for the next grid designing. In each grid\nthe cells are selected, where the color intensity function can be approximated\nby its average value with a given precision (thereafter 'good' cells). After\nreplacing colors of good cells by their approximating constants the\nreconstructed image looks like a mosaic composed of one-colored cells.\nMultigrid algorithm results in the stratification of the image space into\nregions of different transition frequency. Sizes of these regions depend on the\nfew tuning precision parameters that characterizes adaptability of the method\nto the image fragments of different non-homogeneity degree. The method is found\nefficient for prominent contour (skeleton) extraction, edge detection as well\nas for the Lossy Compression of single images and video sequence of images.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.2356v1"
    },
    {
        "title": "A program for the Bayesian Neural Network in the ROOT framework",
        "authors": [
            "Jiahang Zhong",
            "Run-Sheng Huang",
            "Shih-Chang Lee"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We present a Bayesian Neural Network algorithm implemented in the TMVA\npackage, within the ROOT framework. Comparing to the conventional utilization\nof Neural Network as discriminator, this new implementation has more advantages\nas a non-parametric regression tool, particularly for fitting probabilities. It\nprovides functionalities including cost function selection, complexity control\nand uncertainty estimation. An example of such application in High Energy\nPhysics is shown. The algorithm is available with ROOT release later than 5.29.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.2854v2"
    },
    {
        "title": "Statistical Inference for Valued-Edge Networks: Generalized Exponential\n  Random Graph Models",
        "authors": [
            "Bruce A. Desmarais",
            "Skyler J. Cranmer"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Across the sciences, the statistical analysis of networks is central to the\nproduction of knowledge on relational phenomena. Because of their ability to\nmodel the structural generation of networks, exponential random graph models\nare a ubiquitous means of analysis. However, they are limited by an inability\nto model networks with valued edges. We solve this problem by introducing a\nclass of generalized exponential random graph models capable of modeling\nnetworks whose edges are valued, thus greatly expanding the scope of networks\napplied researchers can subject to statistical analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.4615v1"
    },
    {
        "title": "Quantum Bayesian implementation",
        "authors": [
            "Haoyang Wu"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Bayesian implementation concerns decision making problems when agents have\nincomplete information. This paper proposes that the traditional sufficient\nconditions for Bayesian implementation shall be amended by virtue of a quantum\nBayesian mechanism. In addition, by using an algorithmic Bayesian mechanism,\nthis amendment holds in the macro world.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.0471v7"
    },
    {
        "title": "Law of the leading digits and the ideological struggle for numbers",
        "authors": [
            "Tariq Ahmad Mir"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Benford's law states that the occurrence of significant digits in many data\nsets is not uniform but tends to follow a logarithmic distribution such that\nthe smaller digits appear as first significant digits more frequently than the\nlarger ones. We investigate here numerical data on the country-wise adherent\ndistribution of seven major world religions i.e. Christianity, Islam, Buddhism,\nHinduism, Sikhism, Judaism and Baha'ism to see if the proportion of the leading\ndigits occurring in the distribution conforms to Benford's law. We find that\nthe adherent data of all the religions, except Christianity, excellently does\nconform to Benford's law. Furthermore, unlike the adherent data on\nChristianity, the significant digit distribution of the three major Christian\ndenominations i.e. Catholicism, Protestantism and Orthodoxy obeys the law. Thus\nin spite of their complexity general laws can be established for the evolution\nof the religious groups.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.3948v4"
    },
    {
        "title": "MaxEnt power spectrum estimation using the Fourier transform for\n  irregularly sampled data applied to a record of stellar luminosity",
        "authors": [
            "Robert W. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The principle of maximum entropy is applied to the spectral analysis of a\ndata signal with general variance matrix and containing gaps in the record. The\nrole of the entropic regularizer is to prevent one from overestimating\nstructure in the spectrum when faced with imperfect data. Several arguments are\npresented suggesting that the arbitrary prefactor should not be introduced to\nthe entropy term. The introduction of that factor is not required when a\ncontinuous Poisson distribution is used for the amplitude coefficients. We\ncompare the formalism for when the variance of the data is known explicitly to\nthat for when the variance is known only to lie in some finite range. The\nresult of including the entropic measure factor is to suggest a spectrum\nconsistent with the variance of the data which has less structure than that\ngiven by the forward transform. An application of the methodology to example\ndata is demonstrated.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.0446v2"
    },
    {
        "title": "Unfolding algorithms and tests using RooUnfold",
        "authors": [
            "Tim Adye"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The RooUnfold package provides a common framework to evaluate and use\ndifferent unfolding algorithms, side-by-side. It currently provides\nimplementations or interfaces for the Iterative Bayes, Singular Value\nDecomposition, and TUnfold methods, as well as bin-by-bin and matrix inversion\nreference methods. Common tools provide covariance matrix evaluation and\nmulti-dimensional unfolding. A test suite allows comparisons of the performance\nof the algorithms under different truth and measurement models. Here I outline\nthe package, the unfolding methods, and some experience of their use.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.1160v1"
    },
    {
        "title": "Some ways of combining optimum interval upper limits",
        "authors": [
            "S. Yellin"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  When backgrounds are not well enough controlled to measure the value of some\nphysical parameter, one may still obtain an upper limit on the parameter. A\nsingle experiment may have several detectors, each of which can alone be used\nto derive an upper limit from the set of detected events, and the experiment\ncan be run during multiple periods. There can also be more than one experiment\nwhich produces an upper limit. Six methods are discussed for producing a\ncombined upper limit. These methods all assume use of either the optimum\ninterval method or the maximum gap method for finding an upper limit in the\nface of poorly known background.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.2928v2"
    },
    {
        "title": "A Bayesian approach to evaluate confidence intervals in counting\n  experiments with background",
        "authors": [
            "F. Loparco",
            "M. N. Mazziotta"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In this paper we propose a procedure to evaluate Bayesian confidence\nintervals in counting experiments where both signal and background fluctuations\nare described by the Poisson statistics. The results obtained when the method\nis applied to the calculation of upper limits will also be illustrated.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.3041v1"
    },
    {
        "title": "Power-Constrained Limits",
        "authors": [
            "Glen Cowan",
            "Kyle Cranmer",
            "Eilam Gross",
            "Ofer Vitells"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We propose a method for setting limits that avoids excluding parameter values\nfor which the sensitivity falls below a specified threshold. These\n\"power-constrained\" limits (PCL) address the issue that motivated the widely\nused CLs procedure, but do so in a way that makes more transparent the\nproperties of the statistical test to which each value of the parameter is\nsubjected. A case of particular interest is for upper limits on parameters that\nare proportional to the cross section of a process whose existence is not yet\nestablished. The basic idea of the power constraint can easily be applied,\nhowever, to other types of limits.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.3166v1"
    },
    {
        "title": "Deconvolution of a linear combination of Gaussian kernels by an\n  inhomogeneous Fredholm integral equation of second kind and applications to\n  image processing",
        "authors": [
            "Waldemar Ulmer"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Scatter processes of photons lead to blurring of images. Multiple scatter can\nusually be described by one Gaussian convolution kernel. This can be a crude\napproximation and we need a linear combination of 2/3 Gaussian kernels to\naccount for tails.If image structures are recorded by appropriate measurements,\nthese structures are always blurred. The ideal image (source function without\nany blurring) is subjected to Gaussian convolutions to yield a blurred image,\nwhich is recorded by a detector array. The inverse problem of this procedure is\nthe determination of the ideal source image from really determined image. If\nthe scatter parameters are known, we are able to calculate the idealistic\nsource structure by a deconvolution. We shall extend it to linear combinations\nof two/three Gaussian convolution kernels in order to found applications to\naforementioned image processing, where a single Gaussian kernel would be crude.\nIn this communication, we shall derive a new deconvolution method for a linear\ncombination of 2/3 Gaussian kernels with different rms values, namely the\nformulation of an inhomogeneous Fredholm integral equation of second kind and\nthe related Liouville - Neumann series (LNS) to calculate solutions in every\ndesired order. The LNS solution provides the source function rho in terms of\nthe Fredholm kernel Kf. We can verify some advantages of LNS in image\nprocessing. Applications of the LNS solution are inverse problems (2 or 3\nGaussian kernels) of image processing in CBCT and IMRT detector arrays of\nportal imaging. A particular advantage of LNS is given, if the scatter\nfunctions depend on x,y,z. This fact implies that the scatter functions can be\nscaled according to the electron density provided by image reconstruction\nprocedures.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.3401v3"
    },
    {
        "title": "Sparse Random Approximation and Lossy Compression",
        "authors": [
            "M. Andrecut"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We discuss a method for sparse signal approximation, which is based on the\ncorrelation of the target signal with a pseudo-random signal, and uses a\nmodification of the greedy matching pursuit algorithm. We show that this\napproach provides an efficient encoding-decoding method, which can be used also\nfor lossy compression and encryption purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.4936v1"
    },
    {
        "title": "An Iterative, Dynamically Stabilized(IDS) Method of Data Unfolding",
        "authors": [
            "Bogdan Malaescu"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We describe an iterative unfolding method for experimental data, making use\nof a regularization function. The use of this function allows one to build an\nimproved normalization procedure for Monte Carlo spectra, unbiased by the\npresence of possible new structures in data. We unfold, in a dynamically stable\nway, data spectra which can be strongly affected by fluctuations in the\nbackground subtraction and simultaneously reconstruct structures which were not\ninitially simulated.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.3107v1"
    },
    {
        "title": "A new family of solvable Pearson-Dirichlet random walks",
        "authors": [
            "G. Le Caer"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  A n-step Pearson-Gamma random walk in Rd starts at the origin and consists of\nn independent steps with gamma distributed lengths and uniform orientations.\nThe gamma distribution of each step length has a shape parameter q>0.\nConstrained random walks of n steps in Rd are obtained from the latter walks by\nimposing that the sum of the step lengths is equal to a fixed value. When the\nlatter is chosen, without loss of generality, to be equal to 1, then the\nconstrained step lengths have a Dirichlet distribution whose parameters are all\nequal to q. The density of the endpoint position of a n- step planar walk of\nthis type (n\\geq2), with q=d=2, was shown recently to be a weighted mixture of\n1+floor(n/2) endpoint densities of planar Pearson-Dirichlet walks with q=1\n(Stochastics, 82: 201, 2010). The previous result is generalized to any walk\nspace dimension and any number of steps n\\geq2 when the parameter of the\nPearson-Dirichlet random walk is q=d>1. The endpoint density is a weighted\nmixture of 1+floor(n/2) densities with simple forms, equivalently expressed as\na product of a power and a Gauss hypergeometric function. The weights are\nproducts of factors which depend both on d and n and Bessel numbers independent\nof d.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.5070v1"
    },
    {
        "title": "Entry Dispersion Analysis for the Hayabusa Spacecraft using Ground Based\n  Optical Observation",
        "authors": [
            "T. Yamaguchi",
            "M. Yoshikawa",
            "M. Yagi",
            "D. J. Tholen"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Hayabusa asteroid explorer successfully released the sample capsule to\nAustralia on June 13, 2010. Since the Earth reentry phase of sample return was\ncritical, many backup plans for predicting the landing location were prepared.\nThis paper investigates the reentry dispersion using ground based optical\nobservation as a backup observation for radiometric observation. Several\nscenarios are calculated and compared for the reentry phase of the Hayabusa to\nevaluate the navigation accuracy of the ground-based observation. The optical\nobservation doesn't require any active reaction from a spacecraft, thus these\nresults show that optical observations could be a steady backup strategy even\nif a spacecraft had some trouble. We also evaluate the landing dispersion of\nthe Hayabusa only with the optical observation.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.5835v1"
    },
    {
        "title": "Median Algorithm for Sector Spectra Calculation from Images Registered\n  by the Spectral Airglow Temperature Imager",
        "authors": [
            "Atanas Marinov Atanassov"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The Spectral Airglow Temperature Imager is an instrument, specially designed\nfor investigation of the wave processes in the Mesosphere-Lower Thermosphere.\nIn order to determine the kinematic parameters of a wave, the values of a\nphysical quantity in different space points and their changes in the time\nshould be known. As a result of the possibilities of the SATI instrument for\nspace scanning, different parts of the images (sectors of spectrograms)\ncorrespond to the respective mesopause areas (where the radiation is\ngenerated). An approach is proposed for sector spectra determination from SATI\nimages based on ordered statistics instead of meaning. Comparative results are\nshown.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.1987v1"
    },
    {
        "title": "Random matrix approach in search for weak signals immersed in background\n  noise",
        "authors": [
            "D. Grech",
            "J. Miskiewicz"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We present new, original and alternative method for searching signals coded\nin noisy data. The method is based on the properties of random matrix\neigenvalue spectra. First, we describe general ideas and support them with\nresults of numerical simulations for basic periodic signals immersed in\nartificial stochastic noise. Then, the main effort is put to examine the\nstrength of a new method in investigation of data content taken from the real\nastrophysical NAUTILUS detector, searching for the presence of gravitational\nwaves. Our method discovers some previously unknown problems with data\naggregation in this experiment. We provide also the results of new method\napplied to the entire respond signal from ground based detectors in future\nexperimental activities with reduced background noise level. We indicate good\nperformance of our method what makes it a positive predictor for further\napplications in many areas.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.2789v1"
    },
    {
        "title": "Log-Normal Distribution of Single Molecule Fluorescence Bursts in\n  Micro/Nano-Fluidic Channels",
        "authors": [
            "Lazar L. Kish",
            "Jun Kameoka",
            "Claes G. Granqvist",
            "Laszlo B. Kish"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The width and shape of photon burst histograms pose significant limitations\nto the identification of single molecules in micro/nano-fluidic channels, and\nthe nature of these histograms is not fully understood. To reach a deeper\nunderstanding, we performed computer simulations based on a Gaussian beam\nintensity profile with various fluidic channel diameters and assuming (i) a\ndeterministic (noise-free) case, (ii) photon emission/absorption noise, and\n(iii) photon noise with diffusion. Photon noise in narrow channels yields a\nGaussian burst distribution while additional strong diffusion produces skewed\nhistograms. We use the fluctuating residence time picture [Phys. Rev. Lett. 80,\n2386-2388 (1998)] and conclude that the skewness of the photon number\ndistribution is caused by the longitudinal diffusive component of the motion of\nthe molecules as they traverse the laser beam. In the case of strong diffusion\nin narrow channels, this effect leads to a log-normal distribution. We show\nthat the same effect can transform the separate peaks of the photon burst\nhistograms of multiple molecule mixtures into a single log-normal shape.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.4360v6"
    },
    {
        "title": "Application of Self-Organizing Map to Stellar Spectral Classifications",
        "authors": [
            "Bazarghan Mahdi"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We present an automatic, fast, accurate and robust method of classifying\nastronomical objects. The Self Organizing Map (SOM) as an unsupervised\nArtificial Neural Network (ANN) algorithm is used for classification of stellar\nspectra of stars. The SOM is used to make clusters of different spectral\nclasses of Jacoby, Hunter and Christian (JHC) library. This ANN technique needs\nno training examples and the stellar spectral data sets are directly fed to the\nnetwork for the classification. The JHC library contains 161 spectra out of\nwhich, 158 spectra are selected for the classification. These 158 spectra are\ninput vectors to the network and mapped into a two dimensional output grid. The\ninput vectors close to each other are mapped into the same or neighboring\nneurons in the output space. So, the similar objects are making clusters in the\noutput map and making it easy to analyze high dimensional data.\n  After running the SOM algorithm on 158 stellar spectra, with 2799 data points\neach, the output map is analyzed and found that, there are 7 clusters in the\noutput map corresponding to O to M stellar type. But, there are 12\nmisclassifications out of 158 and all of them are misclassified into the\nneighborhood of correct clusters which gives a success rate of about 92.4%.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.0514v1"
    },
    {
        "title": "Priors for New Physics",
        "authors": [
            "Maurizio Pierini",
            "Harrison B. Prosper",
            "Sezen Sekmen",
            "Maria Spiropulu"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The interpretation of data in terms of multi-parameter models of new physics,\nusing the Bayesian approach, requires the construction of multi-parameter\npriors. We propose a construction that uses elements of Bayesian reference\nanalysis. Our idea is to initiate the chain of inference with the reference\nprior for a likelihood function that depends on a single parameter of interest\nthat is a function of the parameters of the physics model. The reference\nposterior density of the parameter of interest induces on the parameter space\nof the physics model a class of posterior densities. We propose to continue the\nchain of inference with a particular density from this class, namely, the one\nfor which indistinguishable models are equiprobable and use it as the prior for\nsubsequent analysis. We illustrate our method by applying it to the constrained\nminimal supersymmetric Standard Model and two non-universal variants of it.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.0523v1"
    },
    {
        "title": "Time domain maximum likelihood parameter estimation in LISA Pathfinder\n  Data Analysis",
        "authors": [
            "G. Congedo",
            "L. Ferraioli",
            "M. Hueller",
            "F. De Marchi",
            "S. Vitale",
            "M. Armano",
            "M. Hewitson",
            "M. Nofrarias"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  LISA is the upcoming space-based Gravitational Wave telescope. LISA\nPathfinder, to be launched in the coming years, will prove and verify the\ndetection principle of the fundamental Doppler link of LISA on a flight\nhardware identical in design to that of LISA. LISA Pathfinder will collect a\npicture of all noise disturbances possibly affecting LISA, achieving the\nunprecedented pureness of geodesic motion necessary for the detection of\ngravitational waves. The first steps of both missions will crucially depend on\na very precise calibration of the key system parameters. Moreover, robust\nparameters estimation is of fundamental importance in the correct assessment of\nthe residual force noise, an essential part of the data processing for LISA. In\nthis paper we present a maximum likelihood parameter estimation technique in\ntime domain being devised for this calibration and show its proficiency on\nsimulated data and validation through Monte Carlo realizations of independent\nnoise runs. We discuss its robustness to non-standard scenarios possibly\narising during the real-life mission, as well as its independence to the\ninitial guess and non-gaussianities. Furthermore, we apply the same technique\nto data produced in mission-like fashion during operational exercises with a\nrealistic simulator provided by ESA.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.0862v1"
    },
    {
        "title": "Statistical methods used in ATLAS for exclusion and discovery",
        "authors": [
            "Diego Casadei"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The statistical methods used by the ATLAS Collaboration for setting upper\nlimits or establishing a discovery are reviewed, as they are fundamental\ningredients in the search for new phenomena. The analyses published so far\nadopted different approaches, choosing a frequentist or a Bayesian or a hybrid\nfrequentist-Bayesian method to perform a search for new physics and set upper\nlimits. In this note, after the introduction of the necessary basic concepts of\nstatistical hypothesis testing, a few recommendations are made about the\npreferred approaches to be followed in future analyses.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.2288v1"
    },
    {
        "title": "Path lengths in turbulence",
        "authors": [
            "Nicholas T. Ouellette",
            "Eberhard Bodenschatz",
            "Haitao Xu"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  By tracking tracer particles at high speeds and for long times, we study the\ngeometric statistics of Lagrangian trajectories in an intensely turbulent\nlaboratory flow. In particular, we consider the distinction between the\ndisplacement of particles from their initial positions and the total distance\nthey travel. The difference of these two quantities shows power-law scaling in\nthe inertial range. By comparing them with simulations of a chaotic but\nnon-turbulent flow and a Lagrangian Stochastic model, we suggest that our\nresults are a signature of turbulence.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.2824v1"
    },
    {
        "title": "Are Percolation Transitions always Sharpened by Making Networks\n  Interdependent?",
        "authors": [
            "Seung-Woo Son",
            "Peter Grassberger",
            "Maya Paczuski"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We study a model for coupled networks introduced recently by Buldyrev et al.,\nNature 464, 1025 (2010), where each node has to be connected to others via two\ntypes of links to be viable. Removing a critical fraction of nodes leads to a\npercolation transition that has been claimed to be more abrupt than that for\nuncoupled networks. Indeed, it was found to be discontinuous in all cases\nstudied. Using an efficient new algorithm we verify that the transition is\ndiscontinuous for coupled Erdos-Renyi networks, but find it to be continuous\nfor fully interdependent diluted lattices. In 2 and 3 dimension, the order\nparameter exponent $\\beta$ is larger than in ordinary percolation, showing that\nthe transition is less sharp, i.e. further from discontinuity, than for\nisolated networks. Possible consequences for spatially embedded networks are\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.3863v3"
    },
    {
        "title": "Reference analysis of the signal + background model in counting\n  experiments",
        "authors": [
            "Diego Casadei"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The model representing two independent Poisson processes, labelled as\n\"signal\" and \"background\" and both contributing at the same time to the total\nnumber of counted events, is considered from a Bayesian point of view. This is\na widely used model for the searches of rare or exotic events in presence of\nsome background source, as for example in the searches performed by the\nhigh-energy physics experiments. In the assumption of some prior knowledge\nabout the background yield, a reference prior is obtained for the signal alone\nand its properties are studied. Finally, the properties of the full solution,\nthe marginal reference posterior, are illustrated with few examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.4270v5"
    },
    {
        "title": "Spectral properties of correlation functions of fields with arbitrary\n  position dependence in restricted geometries from the ballistic to the\n  diffusive regimes",
        "authors": [
            "C. M. Swank",
            "A. Petukhov",
            "R. Golub"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The transition between ballistic and diffusive motion poses difficult\nproblems in several fields of physics. In this work we show how to calculate\nthe spectra of the correlation functions between fields of arbitrary spatial\ndependence as seen by particles moving through the fields in regions bounded by\nspecularly reflecting walls valid for diffusive and ballistic motion as well as\nthe transition region in between for motion in 2 and 3 dimensions.\n  Applications to relaxation in nmr are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.4701v1"
    },
    {
        "title": "Deterministic multidimensional growth model for small-world networks",
        "authors": [
            "Aoyuan Peng",
            "Lianming Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We proposed a deterministic multidimensional growth model for small-world\nnetworks. The model can characterize the distinguishing properties of many\nreal-life networks with geometric space structure. Our results show the model\npossesses small-world effect: larger clustering coefficient and smaller\ncharacteristic path length. We also obtain some accurate results for its\nproperties including degree distribution, clustering coefficient and network\ndiameter and discuss them. It is also worth noting that we get an accurate\nanalytical expression for calculating the characteristic path length. We verify\nnumerically and experimentally these main features.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5450v1"
    },
    {
        "title": "A Pattern Measure",
        "authors": [
            "Allen Klinger",
            "Nikos A. Salingaros"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In this paper we propose numerical measures for evaluating the aesthetic\ninterest of simple patterns. The patterns consist of elements (symbols, pixels,\netc.) in regular square arrays. The measures depend on two characteristics of\nthe patterns: the number of different types of element, and the number of\nsymmetries in their arrangement. We define two complementary composite measures\nL and C for the degree of pattern in a design, and compute them here for 2x2\nand 6x6 arrays. The results distinguish simple from high-variation cases. We\nsuspect that the measure L corresponds to the degree that human beings\nintuitively feel a design to be \"interesting\", so this model would aid in\nquantifying the visual connection of two- dimensional designs with viewers. The\nother composite measure C based on these numerical properties characterizes the\nextent of randomness of an array. Combining symbol variety with symmetry\ncalculations allows us to employ hierarchical scaling to count the relative\nimpact of different levels of scale. By identifying substructures we can\ndistinguish between organized patterns and disorganized complexity. The\nmeasures described here are related to verbal descriptors derived from work by\npsychologists on responses to visual environments.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5508v1"
    },
    {
        "title": "Compression Waves and Phase Plots: Simulations",
        "authors": [
            "Daniel Orlikowski",
            "Roger Minich"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Compression wave analysis started nearly 50 years ago with Fowles.[1]\nCoperthwaite and Williams [2] gave a method that helps identify simple and\nsteady waves. We have been developing a method that gives describes the\nnon-isentropic character of compression waves, in general.[3] One result of\nthat work is a simple analysis tool. Our method helps clearly identify when a\ncompression wave is a simple wave, a steady wave (shock), and when the\ncompression wave is in transition. This affects the analysis of compression\nwave experiments and the resulting extraction of the high-pressure equation of\nstate.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5617v1"
    },
    {
        "title": "Percolation of partially interdependent networks under targeted attack",
        "authors": [
            "Gao-Gao Dong",
            "Jian-Xi Gao",
            "Li-Xin Tian",
            "Rui-Jin Du",
            "Ying-Huan He"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The study of interdependent networks, and in particular the robustness on\nnetworks, has attracted considerable attention. Recent studies mainly assume\nthat the dependence is fully interdependent. However, targeted attack for\npartially interdependent networks simultaneously has the characteristics of\ngenerality in real world. In this letter, the comprehensive percolation of\ngeneralized framework of partially interdependent networks under targeted\nattack is analyzed. As $\\alpha=0$ and $\\alpha=1$, the percolation law is\npresented. Especially, when $a=b=k$, $p_{1}=p_{2}=p$, $q_{A}=q_{B}=q$, the\nfirst and second lines of phase transition coincide with each other. The\ncorresponding phase transition diagram and the critical line between the first\nand the second phase transition are found. We find that the tendency of\ncritical line is monotone decreasing with parameter $p_{1}$. However, for\ndifferent $\\alpha$, the tendency of critical line is monotone increasing with\n$\\alpha$. In a larger sense, our findings have potential application for\ndesigning networks with strong robustness and can regulate the robustness of\nsome current networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5788v1"
    },
    {
        "title": "Partial Wave Analysis using Graphics Cards",
        "authors": [
            "Niklaus Berger"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Partial wave analysis is a key technique in hadron spectroscopy. The use of\nunbinned likelihood fits on large statistics data samples and ever more complex\nphysics models makes this analysis technique computationally very expensive.\nParallel computing techniques, in particular the use of graphics processing\nunits, are a powerful means to speed up analyses; in the contexts of the BES\nIII, Compass and GlueX experiments, parallel analysis frameworks have been\ncreated. They provide both fits that are faster by more than two orders of\nmagnitude than legacy code and environments to quickly program and run an\nanalysis. This in turn allows the physicists to focus on the many difficult\nopen problems pertaining to partial wave analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5882v1"
    },
    {
        "title": "Exact solutions for mass-dependent irreversible aggregations",
        "authors": [
            "Seung-Woo Son",
            "Claire Christensen",
            "Golnoosh Bizhani",
            "Peter Grassberger",
            "Maya Paczuski"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We consider the mass-dependent aggregation process (k+1)X -> X, given a fixed\nnumber of unit mass particles in the initial state. One cluster is chosen\nproportional to its mass and is merged into one either with k-neighbors in one\ndimension, or -- in the well-mixed case -- with k other clusters picked\nrandomly. We find the same combinatorial exact solutions for the probability to\nfind any given configuration of particles on a ring or line, and in the\nwell-mixed case. The mass distribution of a single cluster exhibits scaling\nlaws and the finite size scaling form is given. The relation to the classical\nsum kernel of irreversible aggregation is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.0045v1"
    },
    {
        "title": "Feldman-Cousins Confidence Levels - Toy MC Method",
        "authors": [
            "Till Moritz Karbach"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In particle physics, the likelihood ratio ordering principle is frequently\nused to determine confidence regions. This method has statistical properties\nthat are superior to that of other confidence regions. But it often requires\nintensive computations involving thousands of toy Monte Carlo datasets. The\noriginal paper by Feldman and Cousins contains a recipe to perform the toy MC\ncomputation. In this note, we explain their recipe in a more algorithmic way,\nshow its connection to 1-CL plots, and apply it to simple Gaussian situations\nwith boundaries.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.0714v1"
    },
    {
        "title": "Wavelets and wavelet-like transforms on the sphere and their application\n  to geophysical data inversion",
        "authors": [
            "Frederik J. Simons",
            "Ignace Loris",
            "Eugene Brevdo",
            "Ingrid C. Daubechies"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Many flexible parameterizations exist to represent data on the sphere. In\naddition to the venerable spherical harmonics, we have the Slepian basis,\nharmonic splines, wavelets and wavelet-like Slepian frames. In this paper we\nfocus on the latter two: spherical wavelets developed for geophysical\napplications on the cubed sphere, and the Slepian \"tree\", a new construction\nthat combines a quadratic concentration measure with wavelet-like\nmultiresolution. We discuss the basic features of these mathematical tools, and\nillustrate their applicability in parameterizing large-scale global geophysical\n(inverse) problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.1718v1"
    },
    {
        "title": "Correlations of record events as a test for heavy-tailed distributions",
        "authors": [
            "J. Franke",
            "G. Wergen",
            "J. Krug"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  A record is an entry in a time series that is larger or smaller than all\nprevious entries. If the time series consists of independent, identically\ndistributed random variables with a superimposed linear trend, record events\nare positively (negatively) correlated when the tail of the distribution is\nheavier (lighter) than exponential. Here we use these correlations to detect\nheavy-tailed behavior in small sets of independent random variables. The method\nconsists of converting random subsets of the data into time series with a\ntunable linear drift and computing the resulting record correlations.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.2061v2"
    },
    {
        "title": "Random Walk Picture of Basketball Scoring",
        "authors": [
            "Alan Gabel",
            "S. Redner"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We present evidence, based on play-by-play data from all 6087 games from the\n2006/07--2009/10 seasons of the National Basketball Association (NBA), that\nbasketball scoring is well described by a weakly-biased continuous-time random\nwalk. The time between successive scoring events follows an exponential\ndistribution, with little memory between different scoring intervals. Using\nthis random-walk picture that is augmented by features idiosyncratic to\nbasketball, we account for a wide variety of statistical properties of scoring,\nsuch as the distribution of the score difference between opponents and the\nfraction of game time that one team is in the lead. By further including the\nheterogeneity of team strengths, we build a computational model that accounts\nfor essentially all statistical features of game scoring data and season\nwin/loss records of each team.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.2825v2"
    },
    {
        "title": "Percolation Theory on Interdependent Networks Based on Epidemic\n  Spreading",
        "authors": [
            "Seung-Woo Son",
            "Golnoosh Bizhani",
            "Claire Christensen",
            "Peter Grassberger",
            "Maya Paczuski"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We consider percolation on interdependent locally treelike networks, recently\nintroduced by Buldyrev et al., Nature 464, 1025 (2010), and demonstrate that\nthe problem can be simplified conceptually by deleting all references to\ncascades of failures. Such cascades do exist, but their explicit treatment just\ncomplicates the theory -- which is a straightforward extension of the usual\nepidemic spreading theory on a single network. Our method has the added\nbenefits that it is directly formulated in terms of an order parameter and its\nmodular structure can be easily extended to other problems, e.g. to any number\nof interdependent networks, or to networks with dependency links.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.4447v1"
    },
    {
        "title": "A possible statistical mechanism of anomalous neutrino velocity in OPERA\n  experiment?",
        "authors": [
            "Robert Alicki"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The set of kinetic equations describing the process of conversion of a beam\nof protons into mesons and then to neutrinos is solved. The asymptotic\nevolution of the density profile of neutrinos is essentially the same as that\nobtained in the previous version of the note for a simple model of uniformly\ndamped wave-packet. It shows again that the recently reported \"superluminal\nneutrinos\" could be considered as a purely statistical effect due to the fact\nthat the detected neutrinos represent a biased sample of initial protons.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.5727v2"
    },
    {
        "title": "Editorial process in scientific journals: analysis and modeling",
        "authors": [
            "O. Mryglod",
            "Yu. Holovatch",
            "I. Mryglod"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The editorial handling of papers in scientific journals as a human activity\nprocess is considered. Using recently proposed approaches of human dynamics\ntheory we examine the probability distributions of random variables reflecting\nthe temporal characteristics of studied processes. The first part of this paper\ncontains our results of analysis of the real data about papers published in\nscientific journals. The second part is devoted to modeling of time-series\nconnected with editorial work. The purpose of our work is to present new object\nthat can be studied in terms of human dynamics theory and to corroborate the\nscientometrical application of the results obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.6211v1"
    },
    {
        "title": "A non-parametric peak finder algorithm and its application in searches\n  for new physics",
        "authors": [
            "S. Chekanov",
            "M. Erickson"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We have developed an algorithm for non-parametric fitting and extraction of\nstatistically significant peaks in the presence of statistical and systematic\nuncertainties. Applications of this algorithm for analysis of high-energy\ncollision data are discussed. In particular, we illustrate how to use this\nalgorithm in general searches for new physics in invariant-mass spectra using\npp Monte Carlo simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.3772v2"
    },
    {
        "title": "Information-theoretic bound on the energy cost of stochastic simulation",
        "authors": [
            "Karoline Wiesner",
            "Mile Gu",
            "Elisabeth Rieper",
            "Vlatko Vedral"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Physical systems are often simulated using a stochastic computation where\ndifferent final states result from identical initial states. Here, we derive\nthe minimum energy cost of simulating a complex data set of a general physical\nsystem with a stochastic computation. We show that the cost is proportional to\nthe difference between two information-theoretic measures of complexity of the\ndata - the statistical complexity and the predictive information. We derive the\ndifference as the amount of information erased during the computation. Finally,\nwe illustrate the physics of information by implementing the stochastic\ncomputation as a Gedankenexperiment of a Szilard-type engine. The results\ncreate a new link between thermodynamics, information theory, and complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.4217v1"
    },
    {
        "title": "Causal information approach to partial conditioning in multivariate data\n  sets",
        "authors": [
            "Daniele Marinazzo",
            "Mario Pellicoro",
            "Sebastiano Stramaglia"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  When evaluating causal influence from one time series to another in a\nmultivariate dataset it is necessary to take into account the conditioning\neffect of the other variables. In the presence of many variables, and possibly\nof a reduced number of samples, full conditioning can lead to computational and\nnumerical problems. In this paper we address the problem of partial\nconditioning to a limited subset of variables, in the framework of information\ntheory. The proposed approach is tested on simulated datasets and on an example\nof intracranial EEG recording from an epileptic subject. We show that, in many\ninstances, conditioning on a small number of variables, chosen as the most\ninformative ones for the driver node, leads to results very close to those\nobtained with a fully multivariate analysis, and even better in the presence of\na small number of samples. This is particularly relevant when the pattern of\ncausalities is sparse.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.0680v2"
    },
    {
        "title": "A new FSA approach for in situ $γ$-ray spectroscopy",
        "authors": [
            "A. Caciolli",
            "M. Baldoncini",
            "G. P. Bezzon",
            "C. Broggini",
            "G. P. Buso",
            "I. Callegari",
            "T. Colonna",
            "G. Fiorentini",
            "E. Guastaldi",
            "F. Mantovani",
            "G. Massa",
            "R. Menegazzo",
            "L. Mou",
            "C. Rossi Alvarez",
            "M. Shyti",
            "A. Zanon",
            "G. Xhixha"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  An increasing demand of environmental radioactivity monitoring comes both\nfrom the scientific community and from the society. This requires accurate,\nreliable and fast response preferably from portable radiation detectors. Thanks\nto recent improvements in the technology, $\\gamma$-spectroscopy with sodium\niodide scintillators has been proved to be an excellent tool for in-situ\nmeasurements for the identification and quantitative determination of\n$\\gamma$-ray emitting radioisotopes, reducing time and costs. Both for\ngeological and civil purposes not only $^{40}$K, $^{238}$U, and $^{232}$Th have\nto be measured, but there is also a growing interest to determine the\nabundances of anthropic elements, like $^{137}$Cs and $^{131}$I, which are used\nto monitor the effect of nuclear accidents or other human activities.\n  The Full Spectrum Analysis (FSA) approach has been chosen to analyze the\n$\\gamma$-spectra. The Non Negative Least Square (NNLS) and the energy\ncalibration adjustment have been implemented in this method for the first time\nin order to correct the intrinsic problem related with the $\\chi ^2$\nminimization which could lead to artifacts and non physical results in the\nanalysis.\n  A new calibration procedure has been developed for the FSA method by using in\nsitu $\\gamma$-spectra instead of calibration pad spectra. Finally, the new\nmethod has been validated by acquiring $\\gamma$-spectra with a 10.16 cm x 10.16\ncm sodium iodide detector in 80 different sites in the Ombrone basin, in\nTuscany. The results from the FSA method have been compared with the laboratory\nmeasurements by using HPGe detectors on soil samples collected in the different\nsites, showing a satisfactory agreement between them. In particular, the\n$^{137}$Cs isotopes has been implemented in the analysis since it has been\nfound not negligible during the in-situ measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.1560v1"
    },
    {
        "title": "Optimized differential energy loss estimation for tracker detectors",
        "authors": [
            "Ferenc Sikler",
            "Sandor Szeles"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The estimation of differential energy loss for charged particles in tracker\ndetectors is studied. The robust truncated mean method can be generalized to\nthe linear combination of the energy deposit measurements. The optimized\nweights in case of arithmetic and geometric means are obtained using a detailed\nsimulation. The results show better particle separation power for both\nsemiconductor and gaseous detectors.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.2491v1"
    },
    {
        "title": "Universal scaling in sports ranking",
        "authors": [
            "Weibing Deng",
            "Wei Li",
            "Xu Cai",
            "Alain Bulou",
            "Qiuping A. Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Ranking is a ubiquitous phenomenon in the human society. By clicking the web\npages of Forbes, you may find all kinds of rankings, such as world's most\npowerful people, world's richest people, top-paid tennis stars, and so on and\nso forth. Herewith, we study a specific kind, sports ranking systems in which\nplayers' scores and prize money are calculated based on their performances in\nattending various tournaments. A typical example is tennis. It is found that\nthe distributions of both scores and prize money follow universal power laws,\nwith exponents nearly identical for most sports fields. In order to understand\nthe origin of this universal scaling we focus on the tennis ranking systems. By\nchecking the data we find that, for any pair of players, the probability that\nthe higher-ranked player will top the lower-ranked opponent is proportional to\nthe rank difference between the pair. Such a dependence can be well fitted to a\nsigmoidal function. By using this feature, we propose a simple toy model which\ncan simulate the competition of players in different tournaments. The\nsimulations yield results consistent with the empirical findings. Extensive\nstudies indicate the model is robust with respect to the modifications of the\nminor parts.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.2919v1"
    },
    {
        "title": "A simple energy loss model and its applications for silicon detectors",
        "authors": [
            "Ferenc Sikler"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The energy loss of charged particles in silicon can be approximated by a\nsimple analytical model. With help of measured charge deposits in individual\nchannels of hit clusters their position and energy can be estimated. Deposits\nbelow threshold and saturated values are treated properly, resulting in a wider\ndynamic range. The proposed method gives improvements on both hit position and\nenergy residuals. The model is successfully applied to track differential\nenergy loss estimation and to detector gain calibration tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.3213v1"
    },
    {
        "title": "The Critical Coupling Likelihood Method: A new approach for seamless\n  integration of environmental and operating conditions of gravitational wave\n  detectors into gravitational wave searches",
        "authors": [
            "Cesar A. Costa",
            "Cristina V. Torres"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Any search effort for gravitational waves (GW) using interferometric\ndetectors like LIGO needs to be able to identify if and when noise is coupling\ninto the detector's output signal. The Critical Coupling Likelihood (CCL)\nmethod has been developed to characterize potential noise coupling and in the\nfuture aid GW search efforts. By testing two hypotheses about pairs of\nchannels, CCL is able to identify undesirable coupled instrumental noise from\npotential GW candidates. Our preliminary results show that CCL can associate up\nto $\\sim 80%$ of observed artifacts with $SNR \\geq 8$, to local noise sources,\nwhile reducing the duty cycle of the instrument by $\\lesssim 15%$. An approach\nlike CCL will become increasingly important as GW research moves into the\nAdvanced LIGO era, going from the first GW detection to GW astronomy.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.4516v4"
    },
    {
        "title": "Approximated maximum likelihood estimation in multifractal random walks",
        "authors": [
            "Ola Løvsletten",
            "Martin Rypdal"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We present an approximated maximum likelihood method for the multifractal\nrandom walk processes of [E. Bacry et al., Phys. Rev. E 64, 026103 (2001)]. The\nlikelihood is computed using a Laplace approximation and a truncation in the\ndependency structure for the latent volatility. The procedure is implemented as\na package in the R computer language. Its performance is tested on synthetic\ndata and compared to an inference approach based on the generalized method of\nmoments. The method is applied to estimate parameters for various financial\nstock indices.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.0105v2"
    },
    {
        "title": "Analysis of fractional Gaussian noises using level crossing method",
        "authors": [
            "M. Vahabi",
            "G. R. Jafari",
            "M. Sadegh Movahed"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The so-called level crossing analysis has been used to investigate the\nempirical data set. But there is a lack of interpretation for what is reflected\nby the level crossing results. The fractional Gaussian noise as a well-defined\nstochastic series could be a suitable benchmark to make the level crossing\nfindings more sense. In this article, we calculated the average frequency of\nupcrossing for a wide range of fractional Gaussian noises from logarithmic\n(zero Hurst exponent, H=0), to Gaussian, H=1, ($0<H<1$). By introducing the\nrelative change of the total numbers of upcrossings for original data with\nrespect to so-called shuffled one, $\\mathcal{R}$, an empirical function for the\nHurst exponent versus $\\mathcal{R}$ has been established. Finally to make the\nconcept more obvious, we applied this approach to some financial series.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.1502v1"
    },
    {
        "title": "Information Filtering via Implicit Trust-based Network",
        "authors": [
            "Zhao-Guo Xuan",
            "Zhan Li",
            "Jian-Guo Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Based on the user-item bipartite network, collaborative filtering (CF)\nrecommender systems predict users' interests according to their history\ncollections, which is a promising way to solve the information exploration\nproblem. However, CF algorithm encounters cold start and sparsity problems. The\ntrust-based CF algorithm is implemented by collecting the users' trust\nstatements, which is time-consuming and must use users' private friendship\ninformation. In this paper, we present a novel measurement to calculate users'\nimplicit trust-based correlation by taking into account their average ratings,\nrating ranges, and the number of common rated items. By applying the similar\nidea to the items, a item-based CF algorithm is constructed. The simulation\nresults on three benchmark data sets show that the performances of both\nuser-based and item-based algorithms could be enhanced greatly. Finally, a\nhybrid algorithm is constructed by integrating the user-based and item-based\nalgorithms, the simulation results indicate that hybrid algorithm outperforms\nthe state-of-the-art methods. Specifically, it can not only provide more\naccurate recommendations, but also alleviate the cold start problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2388v1"
    },
    {
        "title": "Information filtering via biased heat conduction",
        "authors": [
            "Jian-Guo Liu",
            "Tao Zhou",
            "Qiang Guo"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Heat conduction process has recently found its application in personalized\nrecommendation [T. Zhou \\emph{et al.}, PNAS 107, 4511 (2010)], which is of high\ndiversity but low accuracy. By decreasing the temperatures of small-degree\nobjects, we present an improved algorithm, called biased heat conduction (BHC),\nwhich could simultaneously enhance the accuracy and diversity. Extensive\nexperimental analyses demonstrate that the accuracy on MovieLens, Netflix and\nDelicious datasets could be improved by 43.5%, 55.4% and 19.2% compared with\nthe standard heat conduction algorithm, and the diversity is also increased or\napproximately unchanged. Further statistical analyses suggest that the present\nalgorithm could simultaneously identify users' mainstream and special tastes,\nresulting in better performance than the standard heat conduction algorithm.\nThis work provides a creditable way for highly efficient information filtering.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2392v1"
    },
    {
        "title": "Error Bars for Distributions of Numbers of Events",
        "authors": [
            "Ritu Aggarwal",
            "Allen Caldwell"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The common practice for displaying error bars on distributions of numbers of\nevents is confusing and can lead to incorrect conclusions. A proposal is made\nfor a different style of presentation that more directly indicates the level of\nagreement between expectations and observations.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2593v3"
    },
    {
        "title": "Ricean Shadowed Statistical Characterization of Shallow Water Acoustic\n  Channels for Wireless Communications",
        "authors": [
            "F. Ruiz-Vega",
            "M. C. Clemente",
            "P. Otero",
            "J. F. Paris"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In this letter, the statistical behaviour of the shallow water acoustic\nchannel for wireless communications is shown to be well characterized by the\nRicean shadowed distribution, which has never been proposed for communication\npurposes on this type of channel. This characterization is clearly motivated\nfrom statistical and physical perspectives and has both theoretical and\npractical advantages compared to previously proposed models.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.4410v1"
    },
    {
        "title": "Use of Singular-Value Decomposition in Gravitational-Wave Data Analysis",
        "authors": [
            "Drew Keppel"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Singular-value decomposition is a powerful technique that has been used in\nthe analysis of matrices in many fields. In this paper, we summarize how it has\nbeen applied to the analysis of gravitational-wave data. These include\nproducing basis waveforms for matched filtering, decreasing the computational\ncost of searching for many waveforms, improving parameter estimation, and\nproviding a method of waveform interpolation.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.1739v1"
    },
    {
        "title": "Shape analysis using fractal dimension: a curvature based approach",
        "authors": [
            "André R. Backes",
            "João B. Florindo",
            "Odemir M. Bruno"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The present work shows a novel fractal dimension method for shape analysis.\nThe proposed technique extracts descriptors from the shape by applying a\nmultiscale approach to the calculus of the fractal dimension of that shape. The\nfractal dimension is obtained by the application of the curvature scale-space\ntechnique to the original shape. Through the application of a multiscale\ntransform to the dimension calculus, it is obtained a set of numbers\n(descriptors) capable of describing with a high precision the shape in\nanalysis. The obtained descriptors are validated in a classification process.\nThe results demonstrate that the novel technique provides descriptors highly\nreliable, confirming the precision of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3118v1"
    },
    {
        "title": "Fractal Descriptors Based on Fourier Spectrum Applied to Texture\n  Analysis",
        "authors": [
            "João Batista Florindo",
            "Odemir Martinez Bruno"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  This work proposes the development and study of a novel technique for the\ngeneration of fractal descriptors used in texture analysis. The novel\ndescriptors are obtained from a multiscale transform applied to the Fourier\ntechnique of fractal dimension calculus. The power spectrum of the Fourier\ntransform of the image is plotted against the frequency in a log- log scale and\na multiscale transform is applied to this curve. The obtained values are taken\nas the fractal descriptors of the image. The validation of the propose is\nperformed by the use of the descriptors for the classification of a dataset of\ntexture images whose real classes are previously known. The classification\nprecision is compared to other fractal descriptors known in the literature. The\nresults confirm the efficiency of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.4597v1"
    },
    {
        "title": "The profile likelihood ratio and the look elsewhere effect in high\n  energy physics",
        "authors": [
            "Gioacchino Ranucci"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The experimental issue of the search for new particles of unknown mass poses\nthe challenge of exploring a wide interval to look for the usual signatures\nrepresented by excess of events above the background. A side effect of such a\nbroad range quest is that the significance calculations valid for signals of\nknown location are no more applicable when such an information is missing. This\ncircumstance is commonly termed in high energy physics applications as the look\nelsewhere effect. How it concretely manifests in a specific problem of signal\nsearch depends upon the particular strategy adopted to unravel the sought-after\nsignal from the underlying background. In this respect an increasingly popular\nmethod is the profile likelihood ratio, especially because of its asymptotic\nbehavior dictated by one of the most famous statistic result, the Wilks'\ntheorem. This work is centered on the description of the look elsewhere effect\nin the framework of the profile likelihood methodology, in particular proposing\na conjecture about the distribution of the likelihood ratio under the null\nhypothesis of absence of the signal over the searched mass interval, a\ncondition which is known to violate the conditions of the Wilks' theorem.\nSeveral Monte Carlo tests to support the conjecture are presented, as well.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.4604v1"
    },
    {
        "title": "Order Effects in Sequential Measurements of Non-Commuting Psychological\n  Observables",
        "authors": [
            "Harald Atmanspacher",
            "Hartmann Roemer"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Sequential measurements of non-commuting observables produce order effects\nthat are well-known in quantum physics. But their conceptual basis, a\nsignificant measurement interaction, is relevant for far more general\nsituations. We argue that non-commutativity is ubiquitous in psychology where\nalmost every interaction with a mental system changes that system in an\nuncontrollable fashion. Psychological order effects for sequential measurements\nare therefore to be expected as a rule. In this paper we focus on the\ntheoretical basis of such effects. We classify several families of order\neffects theoretically, relate them to psychological observations, and predict\neffects yet to be discovered empirically. We assess the complexity, related to\nthe predictive power, of particular (Hilbert space) models of order effects and\ndiscuss possible limitations of such models.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.4685v2"
    },
    {
        "title": "Solving the accuracy-diversity dilemma via directed random walks",
        "authors": [
            "Jian-Guo Liu",
            "Kerui Shi",
            "Qiang Guo"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Random walks have been successfully used to measure user or object\nsimilarities in collaborative filtering (CF) recommender systems, which is of\nhigh accuracy but low diversity. A key challenge of CF system is that the\nreliably accurate results are obtained with the help of peers' recommendation,\nbut the most useful individual recommendations are hard to be found among\ndiverse niche objects. In this paper we investigate the direction effect of the\nrandom walk on user similarity measurements and find that the user similarity,\ncalculated by directed random walks, is reverse to the initial node's degree.\nSince the ratio of small-degree users to large-degree users is very large in\nreal data sets, the large-degree users' selections are recommended extensively\nby traditional CF algorithms. By tuning the user similarity direction from\nneighbors to the target user, we introduce a new algorithm specifically to\naddress the challenge of diversity of CF and show how it can be used to solve\nthe accuracy-diversity dilemma. Without relying on any context-specific\ninformation, we are able to obtain accurate and diverse recommendations, which\noutperforms the state-of-the-art CF methods. This work suggests that the random\nwalk direction is an important factor to improve the personalized\nrecommendation performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.6278v1"
    },
    {
        "title": "10 Years of Object-Oriented Analysis on H1",
        "authors": [
            "Paul Laycock"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Over a decade ago, the H1 Collaboration decided to embrace the\nobject-oriented paradigm and completely redesign its data analysis model and\ndata storage format. The event data model, based on the RooT framework,\nconsists of three layers - tracks and calorimeter clusters, identified\nparticles and finally event summary data - with a singleton class providing\nunified access. This original solution was then augmented with a fourth layer\ncontaining user-defined objects.\n  This contribution will summarise the history of the solutions used, from\nmodifications to the original design, to the evolution of the high-level\nend-user analysis object framework which is used by H1 today. Several important\nissues are addressed - the portability of expert knowledge to increase the\nefficiency of data analysis, the flexibility of the framework to incorporate\nnew analyses, the performance and ease of use, and lessons learned for future\nprojects.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.2646v1"
    },
    {
        "title": "Forward Tracking in the ILD Detector",
        "authors": [
            "Robin Glattauer",
            "Rudolf Frühwirth",
            "Jakob Lettenbichler",
            "Winfried Mitaroff"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The reconstruction software for ILD is currently subject to a major revision,\naiming at improving its accuracy, speed, efficiency and maintainability in time\nfor the upcoming DBD Report. This requires replacing old code by novel methods\nfor track search and fit, together with modern standards for interfaces and\ntools. Track reconstruction in the \"forward region\", defined by the silicon\nForward Tracking Detector (FTD), relies heavily on a powerful stand-alone track\nsearch. The new software makes use of a Cellular Automaton, a Kalman filter,\nand a Hopfield Neural Network. We give an overview of the project, its methods\nand merits.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.2761v1"
    },
    {
        "title": "Wind speed modeled as an indexed semi-Markov process",
        "authors": [
            "Guglielmo D'Amico",
            "Filippo Petroni",
            "Flavio Prattico"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The increasing interest in renewable energy, particularly in wind, has given\nrise to the necessity of accurate models for the generation of good synthetic\nwind speed data. Markov chains are often used with this purpose but better\nmodels are needed to reproduce the statistical properties of wind speed data.\nIn a previous paper we showed that semi-Markov processes are more appropriate\nfor this purpose but to reach an accurate reproduction of real data features\nhigh order model should be used. In this work we introduce an indexed\nsemi-Markov process that is able to fit real data. We downloaded a database,\nfreely available from the web, in which are included wind speed data taken from\nL.S.I. -Lastem station (Italy) and sampled every 10 minutes. We then generate\nsynthetic time series for wind speed by means of Monte Carlo simulations. The\ntime lagged autocorrelation is then used to compare statistical properties of\nthe proposed model with those of real data and also with a synthetic time\nseries generated though a simple semi-Markov process.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.3535v3"
    },
    {
        "title": "Nonlinear Laplacian spectral analysis: Capturing intermittent and\n  low-frequency spatiotemporal patterns in high-dimensional data",
        "authors": [
            "Dimitrios Giannakis",
            "Andrew J. Majda"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We present a technique for spatiotemporal data analysis called nonlinear\nLaplacian spectral analysis (NLSA), which generalizes singular spectrum\nanalysis (SSA) to take into account the nonlinear manifold structure of complex\ndata sets. The key principle underlying NLSA is that the functions used to\nrepresent temporal patterns should exhibit a degree of smoothness on the\nnonlinear data manifold M; a constraint absent from classical SSA. NLSA\nenforces such a notion of smoothness by requiring that temporal patterns belong\nin low-dimensional Hilbert spaces V_l spanned by the leading l Laplace-Beltrami\neigenfunctions on M. These eigenfunctions can be evaluated efficiently in high\nambient-space dimensions using sparse graph-theoretic algorithms. Moreover,\nthey provide orthonormal bases to expand a family of linear maps, whose\nsingular value decomposition leads to sets of spatiotemporal patterns at\nprogressively finer resolution on the data manifold. The Riemannian measure of\nM and an adaptive graph kernel width enhances the capability of NLSA to detect\nimportant nonlinear processes, including intermittency and rare events. The\nminimum dimension of V_l required to capture these features while avoiding\noverfitting is estimated here using spectral entropy criteria.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.6103v2"
    },
    {
        "title": "Can a Lamb Reach a Haven Before Being Eaten by Diffusing Lions?",
        "authors": [
            "Alan Gabel",
            "Satya N. Majumdar",
            "Nagendra K. Panduranga",
            "S. Redner"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We study the survival of a single diffusing lamb on the positive half line in\nthe presence of N diffusing lions that all start at the same position L to the\nright of the lamb and a haven at x=0. If the lamb reaches this haven before\nmeeting any lion, the lamb survives. We investigate the survival probability of\nthe lamb, S_N(x,L), as a function of N and the respective initial positions of\nthe lamb and the lions, x and L. We determine S_N(x,L) analytically for the\nspecial cases of N=1 and N--->oo. For large but finite N, we determine the\nunusual asymptotic form whose leading behavior is S_N(z)\\simN^{-z^2}, with\nz=x/L. Simulations of the capture process very slowly converge to this\nasymptotic prediction as N reaches 10^{500}.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.2985v2"
    },
    {
        "title": "Topology of Fracture Networks",
        "authors": [
            "Christian André Andresen",
            "Alex Hansen",
            "Romain Le Goc",
            "Philippe Davy",
            "Sigmund Mongstad Hope"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We propose a mapping from fracture systems consisting of intersecting\nfracture sheets in three dimensions to an abstract network consisting of nodes\nand links. This makes it possible to analyze fracture systems with the methods\ndeveloped within modern network theory. We test the mapping for two-dimensional\ngeological fracture outcrops and find that the equivalent networks are\nsmall-world and dissasortative. By anlayzing the Discrete Fracture Network\nmodel, which is used to generate artifical fracture networks, we also find\nsmall world networks. However, the networks turn out to be assortative.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.4510v3"
    },
    {
        "title": "Semi-blind Sparse Image Reconstruction with Application to MRFM",
        "authors": [
            "Se Un Park",
            "Nicolas Dobigeon",
            "Alfred O. Hero"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We propose a solution to the image deconvolution problem where the\nconvolution kernel or point spread function (PSF) is assumed to be only\npartially known. Small perturbations generated from the model are exploited to\nproduce a few principal components explaining the PSF uncertainty in a high\ndimensional space. Unlike recent developments on blind deconvolution of natural\nimages, we assume the image is sparse in the pixel basis, a natural sparsity\narising in magnetic resonance force microscopy (MRFM). Our approach adopts a\nBayesian Metropolis-within-Gibbs sampling framework. The performance of our\nBayesian semi-blind algorithm for sparse images is superior to previously\nproposed semi-blind algorithms such as the alternating minimization (AM)\nalgorithm and blind algorithms developed for natural images. We illustrate our\nmyopic algorithm on real MRFM tobacco virus data.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.4723v1"
    },
    {
        "title": "Imagiro: an implementation of Bayesian iterative unfolding for high\n  energy physics",
        "authors": [
            "Benjamin Wynne"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Unfolding of reconstructed event properties to identify the true features of\ncollider events is a complementary method to the established practice of\ndetector calibration, and is particularly relevant to large, composite particle\ndetectors such as those at the Large Hadron Collider. The behaviour of the\ndetector is simulated and used to create a mapping between the true properties\nof events and their reconstructed equivalents. Unfolding attempts to invert\nthis mapping for use in correcting measurements.\n  Imagiro is a new software package providing Bayesian iterative unfolding with\nsystematic and statistical error estimation. The software is designed to\nsimplify the user experience with automatic self-testing and the calculation of\noptimal parameters. Methods are provided for loading data and producing plotted\nresults in the widely used ROOT format.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.4981v1"
    },
    {
        "title": "Impact of the Great East Japan Earthquake on Hotel Industry in Pacific\n  Tohoku Prefectures: From spatio-temporal dependence of hotel availability",
        "authors": [
            "Aki-Hiro Sato"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  This paper investigates the impact of the Great Japan Earthquake (and\nsubsequent tsunami turmoil) on socio-economic activities by using data on hotel\nopportunities collected from an electronic hotel booking service. A method to\nestimate both primary and secondary regional effects of a natural disaster on\nhuman behavior is proposed. It is confirmed that temporal variation in the\nregional share of available hotels before and after a natural disaster may be\nan indicator to measure the socio-economic impact at each district.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.0433v1"
    },
    {
        "title": "Patterns of Regional Travel Behavior: An Analysis of Japanese Hotel\n  Reservation Data",
        "authors": [
            "Aki-Hiro Sato"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  This study considers the availability of room opportunities collected from a\nJapanese hotel booking site. We empirically analyze the daily number of room\nopportunities for four areas. To determine the migration trends of travelers,\nwe discuss a finite mixture of Poisson distributions and the EM-algorithm as\nits parameter estimation method. We further propose a method to infer the\nprobability of opportunities existing for each observation. We characterize\ndemand-supply situations by means of relationship between the averaged room\nprices and the probability of opportunity existing.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.0442v1"
    },
    {
        "title": "Inhomogeneity of the phase space of the damped harmonic oscillator under\n  Levy noise",
        "authors": [
            "Zhan Cao",
            "Yu-Feng Wang",
            "Hong-Gang Luo"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The damped harmonic oscillator under symmetric L\\'{e}vy white noise shows\ninhomogeneous phase space, which is in contrast to the homogeneous one of the\nsame oscillator under the Gaussian white noise, as shown in a recent paper [I.\nM. Sokolov, W. Ebeling, and B. Dybiec, Phys. Rev. E \\textbf{83}, 041118\n(2011)]. The inhomogeneity of the phase space shows certain correlation between\nthe coordinate and the velocity of the damped oscillator under symmetric\nL\\'{e}vy white noise. In the present work we further explore the physical\norigin of these distinguished features and find that it is due to the\ncombination of the damped effect and heavy tail of the noise. We demonstrate\ndirectly this in the reduced coordinate $\\tilde{x}$ versus velocity $\\tilde{v}$\nplots and identify the physics of the anti-association of the coordinate and\nvelocity.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.0593v2"
    },
    {
        "title": "Prediction of Catastrophes: an experimental model",
        "authors": [
            "Randall D. Peters",
            "Martine Le Berre",
            "Yves Pomeau"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Catastrophes of all kinds can be roughly defined as short duration-large\namplitude events following and followed by long periods of \"ripening\". Major\nearthquakes surely belong to the class of 'catastrophic' events. Because of the\nspace-time scales involved, an experimental approach is often difficult, not to\nsay impossible, however desirable it could be. Described in this article is a\n\"laboratory\" setup that yields data of a type that is amenable to theoretical\nmethods of prediction. Observations are made of a critical slowing down in the\nnoisy signal of a solder wire creeping under constant stress. This effect is\nshown to be a fair signal of the forthcoming catastrophe in both of two\ndynamical models. The first is an \"abstract\" model in which a time dependent\nquantity drifts slowly but makes quick jumps from time to time. The second is a\nrealistic physical model for the collective motion of dislocations (the\nAnanthakrishna set of equations for creep). Hope thus exists that similar\nchanges in the response to noise could forewarn catastrophes in other\nsituations, where such precursor effects should manifest early enough.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.1551v1"
    },
    {
        "title": "Moving Taylor Bayesian Regression for nonparametric multidimensional\n  function estimation with possibly correlated errors",
        "authors": [
            "Jobst Heitzig"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We present a nonparametric method for estimating the value and several\nderivatives of an unknown, sufficiently smooth real-valued function of\nreal-valued arguments from a finite sample of points, where both the function\narguments and the corresponding values are known only up to measurement errors\nhaving some assumed distribution and correlation structure. The method, Moving\nTaylor Bayesian Regression (MOTABAR), uses Bayesian updating to find the\nposterior mean of the coefficients of a Taylor polynomial of the function at a\nmoving position of interest. When measurement errors are neglected, MOTABAR\nbecomes a multivariate interpolation method. It contains several well-known\nregression and interpolation methods as special or limit cases. We demonstrate\nthe performance of MOTABAR using the reconstruction of the Lorenz attractor\nfrom noisy observations as an example.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.2841v1"
    },
    {
        "title": "Superluminal neutrinos and quantum cross-correlation theory of neutrino\n  source location",
        "authors": [
            "V. D. Rusov",
            "V. I. Vysotskii",
            "V. A. Tarasov",
            "I. V. Sharph",
            "V. P. Smolyar",
            "T. N. Zelentsova",
            "K. K. Merkotan",
            "E. P. Linnik",
            "M. E. Beglaryan"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Based on the developed cross-correlation theory for remote location of\nneutrino source with two-detector setup for neutrino detection the modification\nof arrangement of the OPERA experiment is suggested. Within the framework of\ncomputing experiment based on the OPERA experimental data we show that the use\nof this theory makes it possible not only to determine with high accuracy the\ndelay time between neutrino signals but to eliminate the errors of blind\nanalysis by which all necessary time corrections for determination of signal\n\"technologically unremovable\" delay time between CERN and GSL are performed.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.5168v1"
    },
    {
        "title": "A Not-So-Fundamental Limitation on Studying Complex Systems with\n  Statistics: Comment on Rabin (2011)",
        "authors": [
            "Drew M. Thomas"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Although living organisms are affected by many interrelated and unidentified\nvariables, this complexity does not automatically impose a fundamental\nlimitation on statistical inference. Nor need one invoke such complexity as an\nexplanation of the \"Truth Wears Off\" or \"decline\" effect; similar \"decline\"\neffects occur with far simpler systems studied in physics. Selective reporting\nand publication bias, and scientists' biases in favour of reporting\neye-catching results (in general) or conforming to others' results (in physics)\nbetter explain this feature of the \"Truth Wears Off\" effect than Rabin's\nsuggested limitation on statistical inference.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.6096v2"
    },
    {
        "title": "Statistical Uncertainty in Line Shift and Width Interpretation",
        "authors": [
            "I H Hutchinson"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  A general mathematical derivation is given of the uncertainty arising from\nphoton statistics in spectroscopic determination of line shift and width, in\nthe presence of a background spectrum.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.6436v2"
    },
    {
        "title": "The effects of environmental disturbances on tumor growth",
        "authors": [
            "Ning Xing Wang",
            "Xiao Miao Zhang",
            "Xiao Bing Han"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  In this study, the analytic expressions of the steady probability\ndistribution of tumor cells were established based on the steady state solution\nto the corresponding Fokker-Planck equation. Then, the effects of two\nuncorrelated white noises on tumor cell growth were investigated. It was found\nthat the predation rate plays the main role in determining whether or not the\nnoise is favorable for tumor growth.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.0173v1"
    },
    {
        "title": "Hamiltonian Monte Carlo with Reduced Momentum Flips",
        "authors": [
            "Jascha Sohl-Dickstein"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Hamiltonian Monte Carlo (or hybrid Monte Carlo) with partial momentum\nrefreshment explores the state space more slowly than it otherwise would due to\nthe momentum reversals which occur on proposal rejection. These cause\ntrajectories to double back on themselves, leading to random walk behavior on\ntimescales longer than the typical rejection time, and leading to slower\nmixing. I present a technique by which the number of momentum reversals can be\nreduced. This is accomplished by maintaining the net exchange of probability\nbetween states with opposite momenta, but reducing the rate of exchange in both\ndirections such that it is 0 in one direction. An experiment illustrates these\nreduced momentum flips accelerating mixing for a particular distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.1939v1"
    },
    {
        "title": "Texture Analysis And Characterization Using Probability Fractal\n  Descriptors",
        "authors": [
            "J. B. Florindo",
            "O. M. Bruno"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  A gray-level image texture descriptors based on fractal dimension estimation\nis proposed in this work. The proposed method estimates the fractal dimension\nusing probability (Voss) method. The descriptors are computed applying a\nmultiscale transform to the fractal dimension curves of the texture image. The\nproposed texture descriptor method is evaluated in a classification task of\nwell known benchmark texture datasets. The results show the great performance\nof the proposed method as a tool for texture images analysis and\ncharacterization.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.2821v1"
    },
    {
        "title": "Data analysis recipes: Probability calculus for inference",
        "authors": [
            "David W. Hogg"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  In this pedagogical text aimed at those wanting to start thinking about or\nbrush up on probabilistic inference, I review the rules by which probability\ndistribution functions can (and cannot) be combined. I connect these rules to\nthe operations performed in probabilistic data analysis. Dimensional analysis\nis emphasized as a valuable tool for helping to construct non-wrong\nprobabilistic statements. The applications of probability calculus in\nconstructing likelihoods, marginalized likelihoods, posterior probabilities,\nand posterior predictions are all discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.4446v1"
    },
    {
        "title": "A sampling algorithm to estimate the effect of fluctuations in particle\n  physics data",
        "authors": [
            "Federico Colecchia"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Background properties in experimental particle physics are typically\nestimated using large data sets. However, different events can exhibit\ndifferent features because of the quantum mechanical nature of the underlying\nphysics processes. While signal and background fractions in a given data set\ncan be evaluated using a maximum likelihood estimator, the shapes of the\ncorresponding distributions are traditionally obtained using high-statistics\ncontrol samples, which normally neglects the effect of fluctuations. On the\nother hand, if it was possible to subtract background using templates that take\nfluctuations into account, this would be expected to improve the resolution of\nthe observables of interest, and to reduce systematics depending on the\nanalysis. This study is an initial step in this direction. We propose a novel\nalgorithm inspired by the Gibbs sampler that makes it possible to estimate the\nshapes of signal and background probability density functions from a given\ncollection of particles, using control sample templates as initial conditions\nand refining them to take into account the effect of fluctuations. Results on\nMonte Carlo data are presented, and the prospects for future development are\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.5886v3"
    },
    {
        "title": "TUnfold: an algorithm for correcting migration effects in high energy\n  physics",
        "authors": [
            "Stefan Schmitt"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  TUnfold is a tool for correcting migration and background effects in high\nenergy physics for multi-dimensional distributions. It is based on a least\nsquare fit with Tikhonov regularisation and an optional area constraint. For\ndetermining the strength of the regularisation parameter, the L-curve method\nand scans of global correlation coefficients are implemented. The algorithm\nsupports background subtraction and error propagation of statistical and\nsystematic uncertainties, in particular those originating from limited\nknowledge of the response matrix. The program is interfaced to the ROOT\nanalysis framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.6201v4"
    },
    {
        "title": "Turbulence modeling by time-series methods",
        "authors": [
            "Vincenzo Ferrazzano",
            "Claudia Klüppelberg"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  A general model for stationary, time-wise turbulent velocity is presented and\ndiscussed. This approach, inspired by modeling ideas of Barndorff-Nielsen and\nSchimgel, is coherent with the K41 hypothesis of local isotropy, and it allows\nus to separate second-order statistics from higher order ones. The model can be\nmotivated by Taylor's hypothesis and a relation between time and spatial\nspectra. Second order statistics are used to separate the deterministic kernel\nfunction and the weakly stationary driving noise. A non-parametric estimation\nmethod for the turbulence intermittency is suggested.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.6614v1"
    },
    {
        "title": "On the scaling ranges of detrended fluctuation analysis for long-memory\n  correlated short series of data",
        "authors": [
            "Dariusz Grech",
            "Zygmunt Mazur"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We examine the scaling regime for the detrended fluctuation analysis (DFA) -\nthe most popular method used to detect the presence of long memory in data and\nthe fractal structure of time series. First, the scaling range for DFA is\nstudied for uncorrelated data as a function of length $L$ of time series and\nregression line coefficient $R^2$ at various confidence levels. Next, an\nanalysis of artificial short series with long memory is performed. In both\ncases the scaling range $\\lambda$ is found to change linearly -- both with $L$\nand $R^2$. We show how this dependence can be generalized to a simple unified\nmodel describing the relation $\\lambda=\\lambda(L, R^2, H)$ where $H$ ($1/2\\leq\nH \\leq 1$) stands for the Hurst exponent of long range autocorrelated data. Our\nfindings should be useful in all applications of DFA technique, particularly\nfor instantaneous (local) DFA where enormous number of short time series has to\nbe examined at once, without possibility for preliminary check of the scaling\nrange of each series separately.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.1007v1"
    },
    {
        "title": "Percolation of Partially Interdependent Scale-free Networks",
        "authors": [
            "Di Zhou",
            "Jianxi Gao",
            "H. Eugene Stanley",
            "Shlomo Havlin"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We study the percolation behavior of two interdependent scale-free (SF)\nnetworks under random failure of 1-$p$ fraction of nodes. Our results are based\non numerical solutions of analytical expressions and simulations. We find that\nas the coupling strength between the two networks $q$ reduces from 1 (fully\ncoupled) to 0 (no coupling), there exist two critical coupling strengths $q_1$\nand $q_2$, which separate three different regions with different behavior of\nthe giant component as a function of $p$. (i) For $q \\geq q_1$, an abrupt\ncollapse transition occurs at $p=p_c$. (ii) For $q_2<q<q_1$, the giant\ncomponent has a hybrid transition combined of both, abrupt decrease at a\ncertain $p=p^{jump}_c$ followed by a smooth decrease to zero for $p <\np^{jump}_c$ as $p$ decreases to zero. (iii) For $q \\leq q_2$, the giant\ncomponent has a continuous second-order transition (at $p=p_c$). We find that\n$(a)$ for $\\lambda \\leq 3$, $q_1 \\equiv 1$; and for $\\lambda > 3$, $q_1$\ndecreases with increasing $\\lambda$. $(b)$ In the hybrid transition, at the\n$q_2 < q < q_1$ region, the mutual giant component $P_{\\infty}$ jumps\ndiscontinuously at $p=p^{jump}_c$ to a very small but non-zero value, and when\nreducing $p$, $P_{\\infty}$ continuously approaches to 0 at $p_c = 0$ for\n$\\lambda < 3$ and at $p_c > 0$ for $\\lambda > 3$. Thus, the known theoretical\n$p_c=0$ for a single network with $\\lambda \\leqslant 3$ is expected to be valid\nalso for strictly partial interdependent networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2427v2"
    },
    {
        "title": "First and second order semi-Markov chains for wind speed modeling",
        "authors": [
            "Guglielmo D'Amico",
            "Filippo Petroni",
            "Flavio Prattico"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The increasing interest in renewable energy, particularly in wind, has given\nrise to the necessity of accurate models for the generation of good synthetic\nwind speed data. Markov chains are often used with this purpose but better\nmodels are needed to reproduce the statistical properties of wind speed data.\nWe downloaded a database, freely available from the web, in which are included\nwind speed data taken from L.S.I. -Lastem station (Italy) and sampled every 10\nminutes. With the aim of reproducing the statistical properties of this data we\npropose the use of three semi-Markov models. We generate synthetic time series\nfor wind speed by means of Monte Carlo simulations. The time lagged\nautocorrelation is then used to compare statistical properties of the proposed\nmodels with those of real data and also with a synthetic time series generated\nthough a simple Markov chain.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2452v3"
    },
    {
        "title": "Deterministic ants in labirynth -- information gained by map sharing",
        "authors": [
            "Janusz Malinowski",
            "Krzysztof Kułakowski"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  A few of ant robots are dropped to a labirynth, formed by a square lattice\nwith a small number of nodes removed. Ants move according to a deterministic\nalgorithm designed to explore all corridors. Each ant remembers the shape of\ncorridors which she has visited. Once two ants met, they share the information\nacquired. We evaluate how the time of getting a complete information by an ant\ndepends on the number of ants, and how the length known by an ant depends on\ntime. Numerical results are presented in the form of scaling relations.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2460v1"
    },
    {
        "title": "On the relation between frequentist and Bayesian approaches for the case\n  of Poisson statistics",
        "authors": [
            "Sergey Bitioukov",
            "Nikolai Krasnikov"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We propose modified frequentist definition for the determination of\nconfidence intervals for the case of Poisson statistics. Namely, we require\nthat 1-\\beta' \\geq \\sum_{n=o}^{n_{obs}+k} P(n|\\lambda) \\geq \\alpha'. We show\nthat this definition is equivalent to the Bayesian method with prior\n\\pi(\\lambda) \\sim \\lambda^{k}. We also propose modified frequentist definition\nfor the case of nonzero background.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.3991v1"
    },
    {
        "title": "Rounding Effects in Record Statistics",
        "authors": [
            "G. Wergen",
            "D. Volovik",
            "S. Redner",
            "J. Krug"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We analyze record-breaking events in time series of continuous random\nvariables that are subsequently discretized by rounding down to integer\nmultiples of a discretization scale $\\Delta>0$. Rounding leads to ties of an\nexisting record, thereby reducing the number of new records. For an infinite\nnumber of random variables that are drawn from distributions with a finite\nupper limit, the number of discrete records is finite, while for distributions\nwith a thinner than exponential upper tail, fewer discrete records arise\ncompared to continuous variables. In the latter case the record sequence\nbecomes highly regular at long times.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.4432v2"
    },
    {
        "title": "Quantitative model validation techniques: new insights",
        "authors": [
            "You Ling",
            "Sankaran Mahadevan"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  This paper develops new insights into quantitative methods for the validation\nof computational model prediction. Four types of methods are investigated,\nnamely classical and Bayesian hypothesis testing, a reliability-based method,\nand an area metric-based method. Traditional Bayesian hypothesis testing is\nextended based on interval hypotheses on distribution parameters and equality\nhypotheses on probability distributions, in order to validate models with\ndeterministic/stochastic output for given inputs. Two types of validation\nexperiments are considered - fully characterized (all the model/experimental\ninputs are measured and reported as point values) and partially characterized\n(some of the model/experimental inputs are not measured or are reported as\nintervals). Bayesian hypothesis testing can minimize the risk in model\nselection by properly choosing the model acceptance threshold, and its results\ncan be used in model averaging to avoid Type I/II errors. It is shown that\nBayesian interval hypothesis testing, the reliability-based method, and the\narea metric-based method can account for the existence of directional bias,\nwhere the mean predictions of a numerical model may be consistently below or\nabove the corresponding experimental observations. It is also found that under\nsome specific conditions, the Bayes factor metric in Bayesian equality\nhypothesis testing and the reliability-based metric can both be mathematically\nrelated to the p-value metric in classical hypothesis testing. Numerical\nstudies are conducted to apply the above validation methods to gas damping\nprediction for radio frequency (RF) microelectromechanical system (MEMS)\nswitches. The model of interest is a general polynomial chaos (gPC) surrogate\nmodel constructed based on expensive runs of a physics-based simulation model,\nand validation data are collected from fully characterized experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.5014v1"
    },
    {
        "title": "Calibration of multi-physics computational models using Bayesian\n  networks",
        "authors": [
            "You Ling",
            "Joshua Mullins",
            "Sankaran Mahadevan"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  This paper develops a Bayesian network-based method for the calibration of\nmulti-physics models, integrating various sources of uncertainty with\ninformation from computational models and experimental data. We adopt the\nKennedy and O'Hagan (KOH) framework for model calibration under uncertainty,\nand develop extensions to multi-physics models and various scenarios of\navailable data. Both aleatoric uncertainty (due to natural variability) and\nepistemic uncertainty (due to lack of information, including data uncertainty\nand model uncertainty) are accounted for in the calibration process.\nChallenging aspects of Bayesian calibration for multi-physics models are\ninvestigated, including: (1) calibration with different forms of experimental\ndata (e.g., interval data and time series data), (2) determination of the\nidentifiability of model parameters when the analytical expression of model is\nknown or unknown, (3) calibration of multiple physics models sharing common\nparameters, which enables efficient use of data especially when the\nexperimental resources are limited. A first-order Taylor series expansion-based\nmethod is proposed to determine which model parameters are identifiable.\nFollowing the KOH framework, a probabilistic discrepancy function is estimated\nand added to the prediction of the calibrated model, attempting to account for\nmodel uncertainty. This discrepancy function is modeled as a Gaussian process\nwhen sufficient data are available for multiple model input combinations, and\nis modeled as a random variable when the available data are limited. The\noverall approach is illustrated using two application examples related to\nmicroelectromechanical system (MEMS) devices: (1) calibration of a dielectric\ncharging model with time-series data, and (2) calibration of two physics models\n(pull-in voltage and creep) using measurements of different physical quantities\nin different devices.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.5015v1"
    },
    {
        "title": "Limited Urban Growth: London's Street Network Dynamics since the 18th\n  Century",
        "authors": [
            "A. Paolo Masucci",
            "Kiril Stanilov",
            "Michael Batty"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We investigate the growth dynamics of Greater London defined by the\nadministrative boundary of the Greater London Authority, based on the evolution\nof its street network during the last two centuries. This is done by employing\na unique dataset, consisting of the planar graph representation of nine time\nslices of Greater London's road network spanning 224 years, from 1786 to 2010.\nWithin this time-frame, we address the concept of the metropolitan area or city\nin physical terms, in that urban evolution reveals observable transitions in\nthe distribution of relevant geometrical properties. Given that London has a\nhard boundary enforced by its long-standing green belt, we show that its street\nnetwork dynamics can be described as a fractal space-filling phenomena up to a\ncapacitated limit, whence its growth can be predicted with a striking level of\naccuracy. This observation is confirmed by the analytical calculation of key\ntopological properties of the planar graph, such as the topological growth of\nthe network and its average connectivity. This study thus represents an example\nof a strong violation of Gibrat's law. In particular, we are able to show\nanalytically how London evolves from a more loop-like structure, typical of\nplanned cities, toward a more tree-like structure, typical of self-organized\ncities. These observations are relevant to the discourse on sustainable urban\nplanning with respect to the control of urban sprawl in many large cities,\nwhich have developed under the conditions of spatial constraints imposed by\ngreen belts and hard urban boundaries.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.5298v2"
    },
    {
        "title": "Calculating particle correlators with the account of detector efficiency",
        "authors": [
            "Y. A. Kulchitsky",
            "R. Lednicky",
            "F. Rimondi",
            "N. A. Rusakovich",
            "P. V. Tsiareshka"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The formulae for correlators K2 and K3 of a given particle observable (e.g.\nenergy or transverse momentum) accounting for the track reconstruction\nefficiency are presented. Similar to the case of an ideal detector, the\ncorrelators can be expressed through the event-by-event fluctuation of the\nobservable single event mean and the observable pseudo-central moments.\nHowever, on the contrary to the ideal case, this splitting does not allow for a\nsubstantial reduction of the computation time.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.0309v4"
    },
    {
        "title": "Non-Smooth Variational Data Assimilation with Sparse Priors",
        "authors": [
            "Ardeshir M. Ebtehaj",
            "Efi Foufoula-Georgiou",
            "Sara Q. Zhang",
            "Arthur Y. Hou"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  This paper proposes an extension to the classical 3D variational data\nassimilation approach by explicitly incorporating as a prior information, the\ntransform-domain sparsity observed in a large class of geophysical signals. In\nparticular, the proposed framework extends the maximum likelihood estimation of\nthe analysis state to the maximum a posteriori estimator, from a Bayesian\nperspective. The promise of the methodology is demonstrated via application to\na 1D synthetic example.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.0454v1"
    },
    {
        "title": "An algorithm for quantifying dependence in multivariate data sets",
        "authors": [
            "Michael Feindt",
            "Michael Prim"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We describe an algorithm to quantify dependence in a multivariate data set.\nThe algorithm is able to identify any linear and non-linear dependence in the\ndata set by performing a hypothesis test for two variables being independent.\nAs a result we obtain a reliable measure of dependence.\n  In high energy physics understanding dependencies is especially important in\nmultidimensional maximum likelihood analyses. We therefore describe the problem\nof a multidimensional maximum likelihood analysis applied on a multivariate\ndata set with variables that are dependent on each other. We review common\nprocedures used in high energy physics and show that general dependence is not\nthe same as linear correlation and discuss their limitations in practical\napplication.\n  Finally we present the tool CAT, which is able to perform all reviewed\nmethods in a fully automatic mode and creates an analysis report document with\nnumeric results and visual review.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.0981v2"
    },
    {
        "title": "Spectra for the product of Gaussian noises",
        "authors": [
            "L. B. Kish",
            "R. Mingesz",
            "Z. Gingl",
            "C. G. Granqvist"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Products of Gaussian noises often emerge as the result of non-linear\ndetection techniques or as a parasitic effect, and their proper handling is\nimportant in many practical applications, including in fluctuation-enhanced\nsensing, indoor air or environmental quality monitoring, etc. We use Rice's\nrandom phase oscillator formalism to calculate the power density spectra\nvariance for the product of two Gaussian band-limited white noises with\nzero-mean and the same bandwidth W. The ensuing noise spectrum is found to\ndecrease linearly from zero frequency to 2W, and it is zero for frequencies\ngreater than 2W. Analogous calculations performed for the square of a single\nGaussian noise confirm earlier results. The spectrum at non-zero frequencies,\nand the variance of the square of a noise, is amplified by a factor two as a\nconsequence of correlation effects between frequency products. Our analytic\nresults is corroborated by computer simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.1881v1"
    },
    {
        "title": "Performability analysis of the second order semi-Markov chains: an\n  application to wind energy production",
        "authors": [
            "Guglielmo D'Amico",
            "Filippo Petroni",
            "Flavio Prattico"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  In this paper a general second order semi-Markov reward model is presented.\nEquations for the higher order moments of the reward process are presented for\nthe first time and applied to wind energy production. The application is\nexecuted by considering a database, freely available from the web, that\nincludes wind speed data taken from L.S.I. - Lastem station (Italy) and sampled\nevery 10 minutes. We compute the expectation and the variance of the total\nenergy produced by using the commercial blade Aircon HAWT - 10 kW.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.2277v2"
    },
    {
        "title": "Towards the perfect prediction of soccer matches",
        "authors": [
            "Andreas Heuer",
            "Oliver Rubner"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We present a systematic approach to the prediction of soccer matches. First,\nwe show that the information about chances for goals is by far more informative\nthan about the actual results. Second, we present a multivariate regression\napproach and show how the prediction quality increases with increasing\ninformation content. This prediction quality can be explicitly expressed in\nterms of just two parameters. Third, by disentangling the systematic and random\ncomponents of soccer matches we can identify the optimum level of\npredictability. These concepts are exemplified for the German Bundesliga.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.4561v1"
    },
    {
        "title": "Reliability measures of second order semi-Markov chain applied to wind\n  energy production",
        "authors": [
            "Guglielmo D'Amico",
            "Filippo Petroni",
            "Flavio Prattico"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  In this paper we consider the problem of wind energy production by using a\nsecond order semi-Markov chain in state and duration as a model of wind speed.\nThe model used in this paper is based on our previous work where we have showed\nthe ability of second order semi-Markov process in reproducing statistical\nfeatures of wind speed. Here we briefly present the mathematical model and\ndescribe the data and technical characteristics of a commercial wind turbine\n(Aircon HAWT-10kW). We show how, by using our model, it is possible to compute\nsome of the main dependability measures such as reliability, availability and\nmaintainability functions. We compare, by means of Monte Carlo simulations, the\nresults of the model with real energy production obtained from data available\nin the Lastem station (Italy) and sampled every 10 minutes. The computation of\nthe dependability measures is a crucial point in the planning and development\nof a wind farm. Through our model, we show how the values of this quantity can\nbe obtained both analytically and computationally.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.6038v2"
    },
    {
        "title": "Anomalous Diffusion and Long-range Correlations in the Score Evolution\n  of the Game of Cricket",
        "authors": [
            "H. V. Ribeiro",
            "Satyam Mukherjee",
            "Xiao Han T. Zeng"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We investigate the time evolution of the scores of the second most popular\nsport in world: the game of cricket. By analyzing the scores event-by-event of\nmore than two thousand matches, we point out that the score dynamics is an\nanomalous diffusive process. Our analysis reveals that the variance of the\nprocess is described by a power-law dependence with a super-diffusive exponent,\nthat the scores are statistically self-similar following a universal Gaussian\ndistribution, and that there are long-range correlations in the score\nevolution. We employ a generalized Langevin equation with a power-law\ncorrelated noise that describe all the empirical findings very well. These\nobservations suggest that competition among agents may be a mechanism leading\nto anomalous diffusion and long-range correlation.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.1204v1"
    },
    {
        "title": "Comment on \"Dynamic Opinion Model and Invasion Percolation\"",
        "authors": [
            "A. Sattari",
            "M. Paczuski",
            "P. Grassberger"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  In J. Shao et al., PRL 103, 108701 (2009) the authors claim that a model with\nmajority rule coarsening exhibits in d=2 a percolation transition in the\nuniversality class of invasion percolation with trapping. In the present\ncomment we give compelling evidence, including high statistics simulations on\nmuch larger lattices, that this is not correct. and that the model is trivially\nin the ordinary percolation universality class.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.1778v1"
    },
    {
        "title": "Microstructure identification via detrended fluctuation analysis of\n  ultrasound signals",
        "authors": [
            "Paulo G. Normando",
            "Romao S. Nascimento",
            "Elineudo P. Moura",
            "Andre P. Vieira"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We describe an algorithm for simulating ultrasound propagation in random\none-dimensional media, mimicking different microstructures by choosing physical\nproperties such as domain sizes and mass densities from probability\ndistributions. By combining a detrended fluctuation analysis (DFA) of the\nsimulated ultrasound signals with tools from the pattern-recognition\nliterature, we build a Gaussian classifier which is able to associate each\nultrasound signal with its corresponding microstructure with a very high\nsuccess rate. Furthermore, we also show that DFA data can be used to train a\nmultilayer perceptron which estimates numerical values of physical properties\nassociated with distinct microstructures.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.2070v2"
    },
    {
        "title": "From the time series to the complex networks: the parametric (dynamical)\n  natural visibility graph",
        "authors": [
            "I. V. Bezsudnov",
            "A. A. Snarskii"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We present the modification of natural visibility graph (NVG) algorithm used\nfor the mapping of the time series to the complex networks (graphs). We propose\nthe parametric natural visibility graph (PNVG) algorithm. The PNVG consists of\nNVG links, which satisfy an additional constraint determined by a newly\nintroduced continuous parameter - the view angle. The alteration ofview angle\nmodifiesthe PNVG and its properties such as the average node degree, average\nlink length of the graph as well as cluster quantity of built graph etc.\nWecalculated and analyzed different PNVG properties depending on the view angle\nfor different types of the time series such as the random (uncorrelated,\ncorrelated and fractal) and cardiac rhythm time series for healthy and ill\npatients. Investigation of different PNVG properties shows that the view angle\ngives a new approach to characterize the structure of the time series that are\ninvisible in the conventional version of the algorithm. It is also shown that\nthe PNVG approach allows to distinguish, identify and describe in detail\nvarious time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.6365v2"
    },
    {
        "title": "Maximum likelihood reconstruction for Ising models with asynchronous\n  updates",
        "authors": [
            "Hong-Li Zeng",
            "Mikko Alava",
            "Erik Aurell",
            "John Hertz",
            "Yasser Roudi"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We describe how the couplings in an asynchronous kinetic Ising model can be\ninferred. We consider two cases, one in which we know both the spin history and\nthe update times and one in which we only know the spin history. For the first\ncase, we show that one can average over all possible choices of update times to\nobtain a learning rule that depends only on spin correlations and can also be\nderived from the equations of motion for the correlations. For the second case,\nthe same rule can be derived within a further decoupling approximation. We\nstudy all methods numerically for fully asymmetric Sherrington-Kirkpatrick\nmodels, varying the data length, system size, temperature, and external field.\nGood convergence is observed in accordance with the theoretical expectations.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.2401v3"
    },
    {
        "title": "Improvements and considerations for size distribution retrieval from\n  small-angle scattering data by Monte-Carlo methods",
        "authors": [
            "Brian Richard Pauw",
            "Jan-Skov Pedersen",
            "Samuel Tardif",
            "Masaki Takata",
            "Bo Brummersted Iversen"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Monte-Carlo (MC) methods, based on random updates and the trial-and-error\nprinciple, are well suited to retrieve particle size distributions from\nsmall-angle scattering patterns of dilute solutions of scatterers. The size\nsensitivity of size determination methods in relation to the range of\nscattering vectors covered by the data is discussed. Improvements are presented\nto existing MC methods in which the particle shape is assumed to be known. A\ndiscussion of the problems with the ambiguous convergence criteria of the MC\nmethods are given and a convergence criterion is proposed, which also allows\nthe determination of uncertainties on the determined size distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.5304v1"
    },
    {
        "title": "The optimal division between sample and background measurement time for\n  photon counting experiments",
        "authors": [
            "Brian Richard Pauw",
            "Samuel Tardif"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Usually, equal time is given to measuring the background and the sample, or\neven a longer background measurement is taken as it has so few counts. While\nthis seems the right thing to do, the relative error after background\nsubtraction improves when more time is spent counting the measurement with the\nhighest amount of scattering. As the available measurement time is always\nlimited, a good division must be found between measuring the background and\nsample, so that the uncertainty of the background-subtracted intensity is as\nlow as possible.\n  Herein outlined is the method to determine how best to divide measurement\ntime between a sample and the background, in order to minimize the relative\nuncertainty. Also given is the relative reduction in uncertainty to be gained\nfrom the considered division. It is particularly useful in the case of scanning\ndiffractometers, including the likes of Bonse-Hart cameras, where the\nmeasurement time division for each point can be optimized depending on the\nsignal-to-noise ratio.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.5329v1"
    },
    {
        "title": "Asymptotic distribution for two-sided tests with lower and upper\n  boundaries on the parameter of interest",
        "authors": [
            "Glen Cowan",
            "Kyle Cranmer",
            "Eilam Gross",
            "Ofer Vitells"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We present the asymptotic distribution for two-sided tests based on the\nprofile likelihood ratio with lower and upper boundaries on the parameter of\ninterest. This situation is relevant for branching ratios and the elements of\nunitary matrices such as the CKM matrix.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.6948v1"
    },
    {
        "title": "Effect of detrending on multifractal characteristics",
        "authors": [
            "P. Oświęcimka",
            "S. Drożdż",
            "J. Kwapień",
            "A. Z. Górski"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Different variants of MFDFA technique are applied in order to investigate\nvarious (artificial and real-world) time series. Our analysis shows that the\ncalculated singularity spectra are very sensitive to the order of the\ndetrending polynomial used within the MFDFA method. The relation between the\nwidth of the multifractal spectrum (as well as the Hurst exponent) and the\norder of the polynomial used in calculation is evident. Furthermore, type of\nthis relation itself depends on the kind of analyzed signal. Therefore, such an\nanalysis can give us some extra information about the correlative structure of\nthe time series being studied.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.0354v1"
    },
    {
        "title": "The Greek Public Debt Path: From Zero to Infinity",
        "authors": [
            "Dimitris Sardelis"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The aim of the present article is to treat the Greek public debt issue\nstrictly as a curve fitting problem. Thus, based on Eurostat data and using the\nMathematica technical computing software, an exponential function that best\nfits the data is determined modelling how the Greek public debt expands with\ntime. Exploring the main features of this best fit model, it is concluded that\nthe Greek public debt cannot possibly be serviced in the long run unless a\nradical growth is implemented and/or part of the debt is written off.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.1679v1"
    },
    {
        "title": "Forecasting the underlying potential governing the time series of a\n  dynamical system",
        "authors": [
            "V. N. Livina",
            "G. Lohmann",
            "M. Mudelsee",
            "T. M. Lenton"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We introduce a technique of time series analysis, potential forecasting,\nwhich is based on dynamical propagation of the probability density of time\nseries. We employ polynomial coefficients of the orthogonal approximation of\nthe empirical probability distribution and extrapolate them in order to\nforecast the future probability distribution of data. The method is tested on\nartificial data, used for hindcasting observed climate data, and then applied\nto forecast Arctic sea-ice time series. The proposed methodology completes a\nframework for `potential analysis' of tipping points which altogether serves\nanticipating, detecting and forecasting non-linear changes including\nbifurcations using several independent techniques of time series analysis.\n  Although being applied to climatological series in the present paper, the\nmethod is very general and can be used to forecast dynamics in time series of\nany origin.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.4090v2"
    },
    {
        "title": "Can we define a best estimator in simple 1-D cases ?",
        "authors": [
            "Eric Lantz",
            "Francois Vernotte"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  With a small number of measurements, the three most well known estimators of\na single parameter give very different results when estimating a scale\nparameter. A transformation of this scale parameter to a location parameter by\nusing logarithms of the data rends the three estimators equivalent in the\nabsence of any a priori information.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.4307v2"
    },
    {
        "title": "On the scaling range of power-laws originated from fluctuation analysis",
        "authors": [
            "Grech Dariusz",
            "Mazur Zygmunt"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We extend our previous study of scaling range properties done for detrended\nfluctuation analysis (DFA) \\cite{former_paper} to other techniques of\nfluctuation analysis (FA). The new technique called Modified Detrended Moving\nAverage Analysis (MDMA) is introduced and its scaling range properties are\nexamined and compared with those of detrended moving average analysis (DMA) and\nDFA. It is shown that contrary to DFA, DMA and MDMA techniques exhibit power\nlaw dependence of the scaling range with respect to the length of the searched\nsignal and with respect to the accuracy $R^2$ of the fit to the considered\nscaling law imposed by DMA or MDMA schemes. This power law dependence is\nsatisfied for both uncorrelated and autocorrelated data. We find also a simple\ngeneralization of this power law relation for series with different level of\nautocorrelations measured in terms of the Hurst exponent. Basic relations\nbetween scaling ranges for different techniques are also discussed. Our\nfindings should be particularly useful for local FA in e.g., econophysics,\nfinances or physiology, where the huge number of short time series has to be\nexamined at once and wherever the preliminary check of the scaling range regime\nfor each of the series separately is neither effective nor possible.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.5070v1"
    },
    {
        "title": "Maxallent: Maximizers of all Entropies and Uncertainty of Uncertainty",
        "authors": [
            "A. N. Gorban"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The entropy maximum approach (Maxent) was developed as a minimization of the\nsubjective uncertainty measured by the Boltzmann--Gibbs--Shannon entropy. Many\nnew entropies have been invented in the second half of the 20th century. Now\nthere exists a rich choice of entropies for fitting needs. This diversity of\nentropies gave rise to a Maxent \"anarchism\". Maxent approach is now the\nconditional maximization of an appropriate entropy for the evaluation of the\nprobability distribution when our information is partial and incomplete. The\nrich choice of non-classical entropies causes a new problem: which entropy is\nbetter for a given class of applications? We understand entropy as a measure of\nuncertainty which increases in Markov processes. In this work, we describe the\nmost general ordering of the distribution space, with respect to which all\ncontinuous-time Markov processes are monotonic (the Markov order). For\ninference, this approach results in a set of conditionally \"most random\"\ndistributions. Each distribution from this set is a maximizer of its own\nentropy. This \"uncertainty of uncertainty\" is unavoidable in analysis of\nnon-equilibrium systems. Surprisingly, the constructive description of this set\nof maximizers is possible. Two decomposition theorems for Markov processes\nprovide a tool for this description.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.5142v4"
    },
    {
        "title": "Multifractal age? Multifractal analysis of cardiac interbeat intervals\n  in assessing of healthy aging",
        "authors": [
            "Danuta Makowiec",
            "Stanisaw Kryszewski",
            "Joanna Wdowczyk-Szulc",
            "Marta Zarczynska-Buchowiecka",
            "Marcin Gruchal",
            "Andrzej Rynkiewicz"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  24-hour Holter recordings of 124 healthy people at different age are studied.\nThe nocturnal signals of young people reveal the presence of the multiplicative\nstructure. This structure is significantly weaker in diurnal signals and\nbecomes less evident for elderly people. Multifractal analysis allows us to\npropose qualitative and quantitative methods to estimate the advancement of the\naging process for healthy humans.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.1019v1"
    },
    {
        "title": "Network representation of cardiac interbeat intervals for monitoring\n  restitution of autonomic control for heart transplant patients",
        "authors": [
            "Danuta Makowiec",
            "Stanisław Kryszewski",
            "Beata Graff",
            "Joanna Danuta Makowiec",
            "Stanislaw Kryszewski",
            "Beata Graff",
            "Joanna Wdowczyk-Szulc",
            "Marta Zarczynska- Buchnowiecka",
            "Marcin Gruchala",
            "Andrzej Rynkiewicz"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The aim is to present the ability of a network of transitions as a nonlinear\ntool providing a graphical representation of a time series. This representation\nis used for cardiac RR-intervals in follow-up observation of changes in heart\nrhythm of patients recovering after heart transplant.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.1023v1"
    },
    {
        "title": "Classification of missing values in spatial data using spin models",
        "authors": [
            "Milan Žukovič",
            "Dionissios T. Hristopulos"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  A problem of current interest is the estimation of spatially distributed\nprocesses at locations where measurements are missing. Linear interpolation\nmethods rely on the Gaussian assumption, which is often unrealistic in\npractice, or normalizing transformations, which are successful only for mild\ndeviations from the Gaussian behavior. We propose to address the problem of\nmissing values estimation on two-dimensional grids by means of spatial\nclassification methods based on spin (Ising, Potts, clock) models. The \"spin\"\nvariables provide an interval discretization of the process values, and the\nspatial correlations are captured in terms of interactions between the spins.\nThe spins at the unmeasured locations are classified by means of the \"energy\nmatching\" principle: the correlation energy of the entire grid (including\nprediction sites) is estimated from the sample-based correlations. We\ninvestigate the performance of the spin classifiers in terms of computational\nspeed, misclassification rate, class histogram and spatial correlations\nreproduction using simulated realizations of spatial random fields, real\nrainfall data, and a digital test image. We also compare the spin-based methods\nwith standard classifiers such as the k-nearest neighbor, the fuzzy k-nearest\nneighbor, and the Support Vector Machine. We find that the spin-based\nclassifiers provide competitive choices.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.1464v1"
    },
    {
        "title": "On sampling and modeling complex systems",
        "authors": [
            "Matteo Marsili",
            "Iacopo Mastromatteo",
            "Yasser Roudi"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The study of complex systems is limited by the fact that only few variables\nare accessible for modeling and sampling, which are not necessarily the most\nrelevant ones to explain the systems behavior. In addition, empirical data\ntypically under sample the space of possible states. We study a generic\nframework where a complex system is seen as a system of many interacting\ndegrees of freedom, which are known only in part, that optimize a given\nfunction. We show that the underlying distribution with respect to the known\nvariables has the Boltzmann form, with a temperature that depends on the number\nof unknown variables. In particular, when the unknown part of the objective\nfunction decays faster than exponential, the temperature decreases as the\nnumber of variables increases. We show in the representative case of the\nGaussian distribution, that models are predictable only when the number of\nrelevant variables is less than a critical threshold. As a further consequence,\nwe show that the information that a sample contains on the behavior of the\nsystem is quantified by the entropy of the frequency with which different\nstates occur. This allows us to characterize the properties of maximally\ninformative samples: in the under-sampling regime, the most informative\nfrequency size distributions have power law behavior and Zipf's law emerges at\nthe crossover between the under sampled regime and the regime where the sample\ncontains enough statistics to make inference on the behavior of the system.\nThese ideas are illustrated in some applications, showing that they can be used\nto identify relevant variables or to select most informative representations of\ndata, e.g. in data clustering.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.3622v4"
    },
    {
        "title": "Record-breaking statistics for random walks in the presence of\n  measurement error and noise",
        "authors": [
            "Yaniv Edery",
            "Alexander B. Kostinski",
            "Satya N. Majumdar",
            "Brian Berkowitz"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We address the question of distance record-setting by a random walker in the\npresence of measurement error, $\\delta$, and additive noise, $\\gamma$ and show\nthat the mean number of (upper) records up to $n$ steps still grows universally\nas $< R_n> \\sim n^{1/2}$ for large $n$ for all jump distributions, including\nL\\'evy flights, and for all $\\delta$ and $\\gamma$. In contrast to the universal\ngrowth exponent of 1/2, the pace of record setting, measured by the pre-factor\nof $n^{1/2}$, depends on $\\delta$ and $\\gamma$. In the absence of noise\n($\\gamma=0$), the pre-factor $S(\\delta)$ is evaluated explicitly for arbitrary\njump distributions and it decreases monotonically with increasing $\\delta$\nwhereas, in case of perfect measurement $(\\delta=0)$, the corresponding\npre-factor $T(\\gamma)$ increases with $\\gamma$. Our analytical results are\nsupported by extensive numerical simulations and qualitatively similar results\nare found in two and three dimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.0627v1"
    },
    {
        "title": "Exact test for Markov order",
        "authors": [
            "Shawn D. Pethel",
            "Daniel W. Hahs"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We describe an exact test of the null hypothesis that a Markov chain is nth\norder versus the alternate hypothesis that it is $(n+1)$-th order. The\nprocedure does not rely on asymptotic properties, but instead builds up the\ntest statistic distribution via surrogate data and is valid for any sample\nsize. Surrogate data are generated using a novel algorithm that guarantees, per\nshot, a uniform sampling from the set of sequences that exactly match the nth\norder properties of the observed data.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.1500v1"
    },
    {
        "title": "Asymptotic Formula for a General Double-Bounded Custom-Sided Likelihood\n  Based Test Statistic",
        "authors": [
            "Will Buttinger"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  This paper presents the asymptotic distributions of a general\nlikelihood-based test statistic, derived using results of Wilks and Wald. The\ngeneral form of the test statistic incorporates the test statistics and\nassociated asymptotic formulae previously derived by Cowan, Cranmer, Gross and\nVitells, which are seen to be special cases of the likelihood-based test\nstatistic described here.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.2799v1"
    },
    {
        "title": "Four Decades of Computing in Subnuclear Physics - from Bubble Chamber to\n  LHC",
        "authors": [
            "Jürgen Knobloch"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  This manuscript addresses selected aspects of computing for the\nreconstruction and simulation of particle interactions in subnuclear physics.\nBased on personal experience with experiments at DESY and at CERN, I cover the\nevolution of computing hardware and software from the era of track chambers\nwhere interactions were recorded on photographic film up to the LHC experiments\nwith their multi-million electronic channels.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.2974v1"
    },
    {
        "title": "A model-free characterization of recurrences in stationary time series",
        "authors": [
            "Rémy Chicheportiche",
            "Anirban Chakraborti"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Study of recurrences in earthquakes, climate, financial time-series, etc. is\ncrucial to better forecast disasters and limit their consequences. However,\nalmost all the previous phenomenological studies involved only a long-ranged\nautocorrelation function, or disregarded the multi-scaling properties induced\nby potential higher order dependencies. Consequently, they missed the facts\nthat non-linear dependences do impact both the statistics and dynamics of\nrecurrence times, and that scaling arguments for the unconditional distribution\nmay not be applicable. We argue that copulas is the correct model-free\nframework to study non-linear dependencies in time series and related concepts\nlike recurrences. Fitting and/or simulating the intertemporal distribution of\nrecurrence intervals is very much system specific, and cannot actually benefit\nfrom universal features, in contrast to the previous claims. This has important\nimplications in epilepsy prognosis and financial risk management applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.3704v2"
    },
    {
        "title": "Digital Cavities and Their Potential Applications",
        "authors": [
            "Khadga Karki",
            "Magne Torbjornsson",
            "Julia. R. Widom",
            "Andrew H. Marcus",
            "Tonu Pullerits"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The concept of a digital cavity is presented. The functionality of a tunable\nradio-frequency/microwave cavity with unrestricted Q-factor is implemented. The\ntheoretical aspects of the cavity and its potential applications in high\nresolution spectroscopy and synchronization of clocks together with examples in\nsignal processing and data acquisition are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.6044v1"
    },
    {
        "title": "The balancing act of template bank construction: inspiral waveform\n  template banks for gravitational-wave detectors and optimizations at fixed\n  computational cost",
        "authors": [
            "Drew Keppel"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Gravitational-wave searches for signals from inspiralling compact binaries\nhave relied on matched filtering banks of waveforms (called template banks) to\ntry to extract the signal waveforms from the detector data. These template\nbanks have been constructed using four main considerations, the region of\nparameter space of interest, the sensitivity of the detector, the matched\nfiltering bandwidth, and the sensitivity one is willing to lose due to the\ngranularity of template placement, the latter of which is governed by the\nminimal match. In this work we describe how the choice of the lower frequency\ncutoff, the lower end of the matched filter frequency band, can be optimized\nfor detection. We also show how the minimal match can be optimally chosen in\nthe case of limited computational resources. These techniques are applied to\nsearches for binary neutron star signals that have been previously performed\nwhen analyzing Initial LIGO and Virgo data and will be performed analyzing\nAdvanced LIGO and Advanced Virgo data using the expected detector sensitivity.\nBy following the algorithms put forward here, the volume sensitivity of these\nsearches is predicted to improve without increasing the computational cost of\nperforming the search.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.2005v1"
    },
    {
        "title": "Variational Semi-blind Sparse Deconvolution with Orthogonal Kernel Bases\n  and its Application to MRFM",
        "authors": [
            "Se Un Park",
            "Nicolas Dobigeon",
            "Alfred O. Hero"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We present a variational Bayesian method of joint image reconstruction and\npoint spread function (PSF) estimation when the PSF of the imaging device is\nonly partially known. To solve this semi-blind deconvolution problem, prior\ndistributions are specified for the PSF and the 3D image. Joint image\nreconstruction and PSF estimation is then performed within a Bayesian\nframework, using a variational algorithm to estimate the posterior\ndistribution. The image prior distribution imposes an explicit atomic measure\nthat corresponds to image sparsity. Importantly, the proposed Bayesian\ndeconvolution algorithm does not require hand tuning. Simulation results\nclearly demonstrate that the semi-blind deconvolution algorithm compares\nfavorably with previous Markov chain Monte Carlo (MCMC) version of myopic\nsparse reconstruction. It significantly outperforms mismatched non-blind\nalgorithms that rely on the assumption of the perfect knowledge of the PSF. The\nalgorithm is illustrated on real data from magnetic resonance force microscopy\n(MRFM).\n",
        "pdf_link": "http://arxiv.org/pdf/1303.3866v1"
    },
    {
        "title": "Dimensional analysis using toric ideals",
        "authors": [
            "M. A. Atherton",
            "R. A. Bates",
            "H. P. Wynn"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Classical dimensional analysis is one of the cornerstones of qualitative\nphysics and is also used in the analysis of engineering systems, for example in\nengineering design. The basic power product relationship in dimensional\nanalysis is identical to one way of defining toric ideals in algebraic\ngeometry, a large and growing field. This paper exploits the toric\nrepresentation to provide a method for automatic dimensional analysis for\nengineering systems. In particular all \"primitive\", invariants for a particular\nproblem, in a well defined sense, can be found using such methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.6659v1"
    },
    {
        "title": "Super-linear scaling of offsprings at criticality in branching processes",
        "authors": [
            "A. Saichev",
            "D. Sornette"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  For any branching process, we demonstrate that the typical total number\n$r_{\\rm mp}(\\nu \\tau)$ of events triggered over all generations within any\nsufficiently large time window $\\tau$ exhibits, at criticality, a super-linear\ndependence $r_{\\rm mp}(\\nu \\tau) \\sim (\\nu \\tau)^\\gamma$ (with $\\gamma >1$) on\nthe total number $\\nu \\tau$ of the immigrants arriving at the Poisson rate\n$\\nu$. In branching processes in which immigrants (or sources) are\ncharacterized by fertilities distributed according to an asymptotic power law\ntail with tail exponent $1 < \\gamma \\leqslant 2$, the exponent of the\nsuper-linear law for $r_{\\rm mp}(\\nu \\tau)$ is identical to the exponent\n$\\gamma$ of the distribution of fertilities. For $\\gamma>2$ and for standard\nbranching processes without power law distribution of fertilities, $r_{\\rm\nmp}(\\nu \\tau) \\sim (\\nu \\tau)^2$. This novel scaling law replaces and tames the\ndivergence $\\nu \\tau/(1-n)$ of the mean total number ${\\bar R}_t(\\tau)$ of\nevents, as the branching ratio (defined as the average number of triggered\nevents of first generation per source) tends to 1. The derivation uses the\nformalism of generating probability functions. The corresponding prediction is\nconfirmed by numerical calculations and an heuristic derivation enlightens its\nunderlying mechanism. We also show that ${\\bar R}_t(\\tau)$ is always linear in\n$\\nu \\tau$ even at criticality ($n=1$). Our results thus illustrate the\nfundamental difference between the mean total number, which is controlled by a\nfew extremely rare realizations, and the typical behavior represented by\n$r_{\\rm mp}(\\nu \\tau)$.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.0684v1"
    },
    {
        "title": "Predicting the variance of a measurement with 1/f noise",
        "authors": [
            "Benjamin Lenoir"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Measurement devices always add noise to the signal of interest and it is\nnecessary to evaluate the variance of the results. This article focuses on\nstationary random processes whose Power Spectrum Density is a power law of\nfrequency. For flicker noise, behaving as $1/f$ and which is present in many\ndifferent phenomena, the usual way to compute the variance leads to infinite\nvalues. This article proposes an alternative definition of the variance which\ntakes into account the fact that measurement devises need to be calibrated.\nThis new variance, which depends on the calibration duration, the measurement\nduration and the duration between the calibration and the measurement, allows\navoiding infinite values when computing the variance of a measurement.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.3989v1"
    },
    {
        "title": "Characterization of the degree of Musical non-Markovianity",
        "authors": [
            "Maria Mannone",
            "Giuseppe Compagno"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Recently, as an aid for musical analysis, in computational musicology\nmathematical and informatics tools have been developed to quantitatively\ncharacterize some aspects of musical compositions. To a musical composition can\nbe attributed by ear a certain amount of memory. This results associated to\nrepetitions and similarities of the patterns in musical scores. To higher\nvariations, a lower amount of memory is perceived. However musical memory of a\nscore has never been quantitatively defined. Here we aim to give such a measure\nfollowing an approach similar to that used in Physics to quantify the memory\n(non-Markovianity) of open quantum systems. We have applied this measure to\nsome existing musical compositions, showing that the results obtained via this\nquantifier agree with what one expects by ear. The Musical non-Markovianity\nquantifier can thus be used as a new tool that can aid quantitative musical\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.0229v2"
    },
    {
        "title": "Network topology reconstructed from derivative-variable correlations",
        "authors": [
            "Zoran Levnajić",
            "Arkady Pikovsky"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  A method of network reconstruction from the dynamical time series is\nintroduced, relying on the concept of derivative-variable correlation. Using a\ntunable observable as a parameter, the reconstruction of any network with known\ninteraction functions is formulated via simple matrix equation. We suggest a\nprocedure aimed at optimizing the reconstruction from the time series of length\ncomparable to the characteristic dynamical time scale. Our method also provides\na reliable precision estimate. We illustrate the method's implementation via\nelementary dynamical models, and demonstrate its robustness to both model and\nobservation errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.2462v2"
    },
    {
        "title": "A spatiospectral localization approach to estimating potential fields on\n  the surface of a sphere from noisy, incomplete data taken at satellite\n  altitudes",
        "authors": [
            "Frederik J. Simons",
            "F. A. Dahlen"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Satellites mapping the spatial variations of the gravitational or magnetic\nfields of the Earth or other planets ideally fly on polar orbits, uniformly\ncovering the entire globe. Thus, potential fields on the sphere are usually\nexpressed in spherical harmonics, basis functions with global support. For\nvarious reasons, however, inclined orbits are favorable. These leave a \"polar\ngap\": an antipodal pair of axisymmetric polar caps without any data coverage,\ntypically smaller than 10 degrees in diameter for terrestrial gravitational\nproblems, but 20 degrees or more in some planetary magnetic configurations. The\nestimation of spherical harmonic field coefficients from an incompletely\nsampled sphere is prone to error, since the spherical harmonics are not\northogonal over the partial domain of the cut sphere. Although approaches based\non wavelets have gained in popularity in the last decade, we present a method\nfor localized spherical analysis that is firmly rooted in spherical harmonics.\nWe construct a basis of bandlimited spherical functions that have the majority\nof their energy concentrated in a subdomain of the unit sphere by solving\nSlepian's (1960) concentration problem in spherical geometry, and use them for\nthe geodetic problem at hand. Most of this work has been published by us\nelsewhere. Here, we highlight the connection of the \"spherical Slepian basis\"\nto wavelets by showing their asymptotic self-similarity, and focus on the\ncomputational considerations of calculating concentrated basis functions on\nirregularly shaped domains.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3255v1"
    },
    {
        "title": "Multiplicative Modeling of Children's Growth and Its Statistical\n  Properties",
        "authors": [
            "Hiroto Kuninaka",
            "Mitsugu Matsushita"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We develop a numerical growth model which can predict the statistical\nproperties of the height distribution of Japanese children. Our previous\nstudies have clarified that the height distribution of schoolchildren shows a\ntransition from the lognormal to the normal distribution during puberty period.\nIn this paper, we demonstrate by our simulations that the transition occurs\nowing to the variability of the onset of the puberty period.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3732v4"
    },
    {
        "title": "Temporal fluctuation scaling in nonstationary counting processes",
        "authors": [
            "Shinsuke Koyama"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The fluctuation scaling law has universally been observed in a wide variety\nof phenomena. For counting processes describing the number of events occurred\nduring time intervals, it is expressed as a power function relationship between\nthe variance and the mean of the event count per unit time, the characteristic\nexponent of which is obtained theoretically in the limit of long duration of\ncounting windows. Here I show that the scaling law effectively appears even in\na short timescale in which only a few events occur. Consequently, the counting\nstatistics of nonstationary event sequences are shown to exhibit the scaling\nlaw as well as the dynamics at temporal resolution of this timescale. I also\npropose a method to extract in a systematic manner the characteristic scaling\nexponent from nonstationary data.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.6703v1"
    },
    {
        "title": "A program for SAXS data processing and analysis",
        "authors": [
            "Zhi-hong Li"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  A computer program for small angle X-ray scattering (SAXS) data processing\nand analysis named S.exe written in Intel Visual Fortran has been developed.\nThis paper briefly introduces its main theory and function.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.0358v1"
    },
    {
        "title": "Memory and long-range correlations in chess games",
        "authors": [
            "Ana L. Schaigorodsky",
            "Juan I. Perotti",
            "Orlando V. Billoni"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  In this paper we report the existence of long-range memory in the opening\nmoves of a chronologically ordered set of chess games using an extensive chess\ndatabase. We used two mapping rules to build discrete time series and analyzed\nthem using two methods for detecting long-range correlations; rescaled range\nanalysis and detrented fluctuation analysis. We found that long-range memory is\nrelated to the level of the players. When the database is filtered according to\nplayer levels we found differences in the persistence of the different subsets.\nFor high level players, correlations are stronger at long time scales; whereas\nin intermediate and low level players they reach the maximum value at shorter\ntime scales. This can be interpreted as a signature of the different strategies\nused by players with different levels of expertise. These results are robust\nagainst the assignation rules and the method employed in the analysis of the\ntime series.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.0729v3"
    },
    {
        "title": "On the multifractal effects generated by monofractal signals",
        "authors": [
            "Dariusz Grech",
            "Grzegorz Pamuła"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We study quantitatively the level of false multifractal signal one may\nencounter while analyzing multifractal phenomena in time series within\nmultifractal detrended fluctuation analysis (MF-DFA). The investigated effect\nappears as a result of finite length of used data series and is additionally\namplified by the long-term memory the data eventually may contain. We provide\nthe detailed quantitative description of such apparent multifractal background\nsignal as a threshold in spread of generalized Hurst exponent values $\\Delta h$\nor a threshold in the width of multifractal spectrum $\\Delta \\alpha$ below\nwhich multifractal properties of the system are only apparent, i.e. do not\nexist, despite $\\Delta\\alpha\\neq0$ or $\\Delta h\\neq 0$. We find this effect\nquite important for shorter or persistent series and we argue it is linear with\nrespect to autocorrelation exponent $\\gamma$. Its strength decays according to\npower law with respect to the length of time series. The influence of basic\nlinear and nonlinear transformations applied to initial data in finite time\nseries with various level of long memory is also investigated. This provides\nadditional set of semi-analytical results. The obtained formulas are\nsignificant in any interdisciplinary application of multifractality, including\nphysics, financial data analysis or physiology, because they allow to separate\nthe 'true' multifractal phenomena from the apparent (artificial) multifractal\neffects. They should be a helpful tool of the first choice to decide whether we\ndo in particular case with the signal with real multiscaling properties or not.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.2014v2"
    },
    {
        "title": "Combining upper limits with a Bayesian approach",
        "authors": [
            "Liu Yang",
            "Kai Zhu",
            "Yongsheng Zhu",
            "Hao Cai"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We discuss how to determine and combine upper limits based on observed events\nand estimated backgrounds with a Bayesian method, when insignificant signals\nare observed in independent measurements. In addition to some general features\ndeduced from the analytical formulae, systematic numerical results are obtained\nby a C$++$ program (CULBA) for low-count experiments, which can be used as a\nreference to combine two upper limits.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.3804v4"
    },
    {
        "title": "Information and treatment of unknown correlations in the combination of\n  measurements using the BLUE method",
        "authors": [
            "Andrea Valassi",
            "Roberto Chierici"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We discuss the effect of large positive correlations in the combinations of\nseveral measurements of a single physical quantity using the Best Linear\nUnbiased Estimate (BLUE) method. We suggest a new approach for comparing the\nrelative weights of the different measurements in their contributions to the\ncombined knowledge about the unknown parameter, using the well-established\nconcept of Fisher information. We argue, in particular, that one We discuss the\neffect of large positive correlations in the combinations of several\nmeasurements of a single physical quantity using the Best Linear Unbiased\nEstimate (BLUE) method. We suggest a new approach for comparing the relative\nweights of the different measurements in their contributions to the combined\nknowledge about the unknown parameter, using the well-established concept of\nFisher information. We argue, in particular, that one contribution to\ninformation comes from the collective interplay of the measurements through\ntheir correlations and that this contribution cannot be attributed to any of\nthe individual measurements alone. We show that negative coefficients in the\nBLUE weighted average invariably indicate the presence of a regime of high\ncorrelations, where the effect of further increasing some of these correlations\nis that of reducing the error on the combined estimate. In these regimes, we\nstress that assuming fully correlated systematic uncertainties is not a truly\nconservative choice, and that the correlations provided as input to BLUE\ncombinations need to be assessed with extreme care instead. In situations where\nthe precise evaluation of these correlations is impractical, or even\nimpossible, we provide tools to help experimental physicists perform more\nconservative combinations.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.4003v3"
    },
    {
        "title": "Applications of the Beta Distribution Part 1: Transformation Group\n  Approach",
        "authors": [
            "Robert W. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  A transformation group approach to the prior for the parameters of the beta\ndistribution is suggested which accounts for finite sets of data by imposing a\nlimit to the range of parameter values under consideration. The relationship\nbetween the beta distribution and the Poisson and gamma distributions in the\ncontinuum is explored, with an emphasis on the decomposition of the model into\nseparate estimates for size and shape. Use of the beta distribution in\nclassification and prediction problems is discussed, and the effect of the\nprior on the analysis of some well known examples from statistical genetics is\nexamined.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.6437v2"
    },
    {
        "title": "Oscillating Entropy",
        "authors": [
            "E. Canessa"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The log-periodic equation for the entropy $S = - (k/a) \\sum_{i=1}^{N} p_{i}\n\\sin(a \\ln p_{i})$, based on the forgotten Sharma-Taneja entropy measure, is\nstudied for the first time with $N$ the total number of system states and\n$p_{i}$ the associated probabilities. It is argued that an oscillatory regime\nfor $S$ could in principle be understood in terms of a linear time-dependent\nbehavior for the associated probabilities in analogy with a spring system of\nfrequency $w$ gaining momentum from the surroundings. The physical meaning for\nthe production of entropy given by the parameter $a$ relates the angle $\\omega\nt$. We discuss its properties, and make a connection with the non-extensive\nTsallis, R\\'{e}nyi, Boltzmann-Gibbs and Shannon entropies as special limiting\ncases for systems with constant mass. Our non-trivial form of entropy displays\npeculiar concavity and addition properties. Log-periodicity in $S$, concavity\nlost, and non-additivity are manifested by increasing the value of the\ncoefficient $a$, which sets the variations with respect to the behavior of the\nmonotonic Gibbs entropy function. We have considered systems with linearly\nincreasing mass and have explained how \"oscillating entropy\" in such systems\nmay appear.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.6681v2"
    },
    {
        "title": "The bold/timorous walker on the trek from home",
        "authors": [
            "Maurizio Serva"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We study a one-dimensional random walk with memory. The behavior of the\nwalker is modified with respect to the simple symmetric random walk (SSRW) only\nwhen he is at the maximum distance ever reached from his starting point (home).\nIn this case, having the choice to move farther or to move closer, he decides\nwith different probabilities. If the probability of a forward step is higher\nthen the probability of a backward step, the walker is bold, otherwise he is\ntimorous. We investigate the asymptotic properties of this bold/timorous random\nwalk (BTRW) showing that the scaling behavior vary continuously from\nsubdiffusive (timorous) to superdiffusive (bold). The scaling exponents are\nfully determined with a new mathematical approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.1560v2"
    },
    {
        "title": "Detrended Cross-Correlation Analysis Consistently Extended to\n  Multifractality",
        "authors": [
            "Paweł Oświȩcimka",
            "Stanisław Drożdż",
            "Marcin Forczek",
            "Stanisław Jadach",
            "Jarosław Kwapień"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We propose a novel algorithm - Multifractal Cross-Correlation Analysis\n(MFCCA) - that constitutes a consistent extension of the Detrended\nCross-Correlation Analysis (DCCA) and is able to properly identify and quantify\nsubtle characteristics of multifractal cross-correlations between two time\nseries. Our motivation for introducing this algorithm is that the already\nexisting methods like MF-DXA have at best serious limitations for most of the\nsignals describing complex natural processes and often indicate multifractal\ncross-correlations when there are none. The principal component of the present\nextension is proper incorporation of the sign of fluctuations to their\ngeneralized moments. Furthermore, we present a broad analysis of the model\nfractal stochastic processes as well as of the real-world signals and show that\nMFCCA is a robust and selective tool at the same time, and therefore allows for\na reliable quantification of the cross-correlative structure of analyzed\nprocesses. In particular, it allows one to identify the boundaries of the\nmultifractal scaling and to analyze a relation between the generalized Hurst\nexponent and the multifractal scaling parameter $\\lambda_q$. This relation\nprovides information about character of potential multifractality in\ncross-correlations and thus enables a deeper insight into dynamics of the\nanalyzed processes than allowed by any other related method available so far.\nBy using examples of time series from stock market, we show that financial\nfluctuations typically cross-correlate multifractally only for relatively large\nfluctuations, whereas small fluctuations remain mutually independent even at\nmaximum of such cross-correlations. Finally, we indicate possible utility of\nMFCCA to study effects of the time-lagged cross-correlations.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.6148v2"
    },
    {
        "title": "Analysis of multichannel measurements of rare processes with uncertain\n  expected background and acceptance",
        "authors": [
            "I. B. Smirnov"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  A typical experiment in high energy physics is considered. The result of the\nexperiment is assumed to be a histogram consisting of bins or channels with\nnumbers of corresponding registered events. The expected background and\nexpected signal shape or acceptance are measured in separate auxiliary\nexperiments, or calculated by the Monte Carlo method with finite sample size,\nand hence with finite precision. An especially complex situation occurs when\nthe expected background in some of the channels happens to be zero due to\neither a fluctuation of the auxiliary measurement (or simulation) or because it\nis truly zero. Different statistical methods give different confidence\nintervals for the full signal rate and different significances of the\nsignal+background hypothesis versus the pure background hypothesis. Detailed\nanalysis and numerical tests are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.0531v5"
    },
    {
        "title": "Statistics of weighted Poisson events and its applications",
        "authors": [
            "G. Bohm",
            "G. Zech"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The statistics of the sum of random weights where the number of weights is\nPoisson distributed has important applications in nuclear physics, particle\nphysics and astrophysics. Events are frequently weighted according to their\nacceptance or relevance to a certain type of reaction. The sum is described by\nthe compound Poisson distribution (CPD) which is shortly reviewed. It is shown\nthat the CPD can be approximated by a scaled Poisson distribution (SPD). The\nSPD is applied to parameter estimation in situations where the data are\ndistorted by resolution effects. It performs considerably better than the\nnormal approximation that is usually used. A special Poisson bootstrap\ntechnique is presented which permits to derive confidence limits for\nobservations following the CPD.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.1287v1"
    },
    {
        "title": "On the distinguishability of histograms",
        "authors": [
            "S. Bityukov",
            "N. Krasnikov",
            "A. Nikitenko",
            "V. Smirnova"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We consider an approach for testing the hypothesis that two realizations of\nthe random variables in the form of histograms are taken from the same\nstatistical population (i.e. two histograms are drawn from the same\ndistribution). The approach is based on the notion \"significance of deviation\".\nThis approach allows to estimate the statistical difference between two\nhistograms using multi-dimensional test statistics. The distinguishability of\nhistograms is estimated with the help of the construction a number of clones\n(rehistograms) of the observed histograms.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.4649v2"
    },
    {
        "title": "Effect of Correlations Between Model Parameters and Nuisance Parameters\n  When Model Parameters are Fit to Data",
        "authors": [
            "Byron Roe"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The effect of correlations between model parameters and nuisance parameters\nis discussed, in the context of fitting model parameters to data. Modifications\nto the usual $\\chi^2$ method are required. Fake data studies, as used at\npresent, will not be optimum. Problems will occur for applications of the\nMaltoni-Schwetz \\cite{ms} theorem. Neutrino oscillations are used as examples,\nbut the problems discussed here are general ones, which are often not\naddressed.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.6146v1"
    },
    {
        "title": "Newtonian Dynamics from the principle of Maximum Caliber",
        "authors": [
            "Diego González",
            "Sergio Davis",
            "Gonzalo Gutiérrez"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The foundations of Statistical Mechanics can be recovered almost in their\nentirety from the Principle of Maximum Entropy. In this work we show that its\nnon-equilibrium generalization, the Principle of Maximum Caliber (Jaynes,\n1980), when applied to the unknown trajectory followed by a particle, leads to\nNewton's second law under two quite intuitive assumptions (the expected square\ndisplacement in one step and the spatial probability distribution of the\nparticle are known at all times). Our derivation explicitly highlights the role\nof mass as an emergent measure of the fluctuations in velocity (inertia) and\nthe origin of potential energy as a manifestation of spatial correlations.\nAccording to our findings, the application of Newton's equations is not limited\nto mechanical systems, and therefore could be used in modelling ecological,\nfinancial and biological systems, among others.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.1382v1"
    },
    {
        "title": "A cyclic time-dependent Markov process to model daily patterns in wind\n  turbine power production",
        "authors": [
            "Teresa Scholz",
            "Vitor V. Lopes",
            "Ana Estanqueiro"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Wind energy is becoming a top contributor to the renewable energy mix, which\nraises potential reliability issues for the grid due to the fluctuating nature\nof its source. To achieve adequate reserve commitment and to promote market\nparticipation, it is necessary to provide models that can capture daily\npatterns in wind power production. This paper presents a cyclic inhomogeneous\nMarkov process, which is based on a three-dimensional state-space (wind power,\nspeed and direction). Each time-dependent transition probability is expressed\nas a Bernstein polynomial. The model parameters are estimated by solving a\nconstrained optimization problem: The objective function combines two maximum\nlikelihood estimators, one to ensure that the Markov process long-term behavior\nreproduces the data accurately and another to capture daily fluctuations. A\nconvex formulation for the overall optimization problem is presented and its\napplicability demonstrated through the analysis of a case-study. The proposed\nmodel is capable of reproducing the diurnal patterns of a three-year dataset\ncollected from a wind turbine located in a mountainous region in Portugal. In\naddition, it is shown how to compute persistence statistics directly from the\nMarkov process transition matrices. Based on the case-study, the power\nproduction persistence through the daily cycle is analysed and discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.3073v1"
    },
    {
        "title": "ROAn, a ROOT based Analysis Framework",
        "authors": [
            "Thomas Lauf",
            "Robert Andritschke"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The ROOT based Offline and Online Analysis (ROAn) framework was developed to\nperform data analysis on data from Depleted P-channel Field Effect Transistor\n(DePFET) detectors, a type of active pixel sensors developed at the MPI\nHalbleiterlabor (HLL). ROAn is highly flexible and extensible, thanks to ROOT's\nfeatures like run-time type information and reflection. ROAn provides an\nanalysis program which allows to perform configurable step-by-step analysis on\narbitrary data, an associated suite of algorithms focused on DePFET data\nanalysis, and a viewer program for displaying and processing online or offline\ndetector data streams. The analysis program encapsulates the applied algorithms\nin objects called steps which produce analysis results. The dependency between\nresults and thus the order of calculation is resolved automatically by the\nprogram. To optimize algorithms for studying detector effects, analysis\nparameters are often changed. Such changes of input parameters are detected in\nsubsequent analysis runs and only the necessary recalculations are triggered.\nThis saves time and simultaneously keeps the results consistent. The viewer\nprogram offers a configurable Graphical User Interface (GUI) and process chain,\nwhich allows the user to adapt the program to different tasks such as offline\nviewing of file data, online monitoring of running detector systems, or\nperforming online data analysis (histogramming, calibration, etc.). Because of\nits modular design, ROAn can be extended easily, e.g. be adapted to new\ndetector types and analysis processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.4848v1"
    },
    {
        "title": "Error Assessment in Modeling with Fractal Brownian Motions",
        "authors": [
            "Bingqiang Qiao",
            "Siming Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  To model a given time series $F(t)$ with fractal Brownian motions (fBms), it\nis necessary to have appropriate error assessment for related quantities.\nUsually the fractal dimension $D$ is derived from the Hurst exponent $H$ via\nthe relation $D=2-H$, and the Hurst exponent can be evaluated by analyzing the\ndependence of the rescaled range $\\langle|F(t+\\tau)-F(t)|\\rangle$ on the time\nspan $\\tau$. For fBms, the error of the rescaled range not only depends on data\nsampling but also varies with $H$ due to the presence of long term memory. This\nerror for a given time series then can not be assessed without knowing the\nfractal dimension. We carry out extensive numerical simulations to explore the\nerror of rescaled range of fBms and find that for $0<H<0.5$, $|F(t+\\tau)-F(t)|$\ncan be treated as independent for time spans without overlap; for $0.5<H<1$,\nthe long term memory makes $|F(t+\\tau)-F(t)|$ correlated and an approximate\nmethod is given to evaluate the error of $\\langle|F(t+\\tau)-F(t)|\\rangle$. The\nerror and fractal dimension can then be determined self-consistently in the\nmodeling of a time series with fBms.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.7699v1"
    },
    {
        "title": "Statistical properties of position-dependent ball-passing networks in\n  football games",
        "authors": [
            "Takuma Narizuka",
            "Ken Yamamoto",
            "Yoshihiro Yamazaki"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Statistical properties of position-dependent ball-passing networks in real\nfootball games are examined. We find that the networks have the small-world\nproperty, and their degree distributions are fitted well by a truncated gamma\ndistribution function. In order to reproduce these properties of networks, a\nmodel based on a Markov chain is proposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.0641v2"
    },
    {
        "title": "Toward particle-level filtering of individual collision events at the\n  Large Hadron Collider and beyond",
        "authors": [
            "Federico Colecchia"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Low-energy strong interactions are a major source of background at hadron\ncolliders, and methods of subtracting the associated energy flow are well\nestablished in the field. Traditional approaches treat the contamination as\ndiffuse, and estimate background energy levels either by averaging over large\ndata sets or by restricting to given kinematic regions inside individual\ncollision events. On the other hand, more recent techniques take into account\nthe discrete nature of background, most notably by exploiting the presence of\nsubstructure inside hard jets, i.e. inside collections of particles originating\nfrom scattered hard quarks and gluons. However, none of the existing methods\nsubtract background at the level of individual particles inside events. We\nillustrate the use of an algorithm that can enable particle-by-particle\nbackground discrimination at the Large Hadron Collider, and we envisage this as\nthe basis for a novel event filtering procedure upstream of the official jet\nreconstruction pipelines. Our hope is that this new technique will improve\nphysics analysis when used in combination with state-of-the-art algorithms in\nhigh-luminosity hadron collider environments.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.1156v2"
    },
    {
        "title": "Performance of multifractal detrended fluctuation analysis on short time\n  series",
        "authors": [
            "Juan Luis Lopez",
            "Jesus Guillermo Contreras"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The performance of the multifractal detrended analysis on short time series\nis evaluated for synthetic samples of several mono- and multifractal models.\nThe reconstruction of the generalized Hurst exponents is used to determine the\nrange of applicability of the method and the precision of its results as a\nfunction of the decreasing length of the series. As an application the series\nof the daily exchange rate between the U.S. dollar and the euro is studied.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.2278v1"
    },
    {
        "title": "Weak ergodicity breaking in an anomalous diffusion process of mixed\n  origins",
        "authors": [
            "Felix Thiel",
            "Igor M. Sokolov"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The ergodicity breaking parameter is a measure for the heterogeneity among\ndifferent trajectories of one ensemble. In this report this parameter is\ncalculated for fractional Brownian motion with a random change of time scale,\noften called \"subordination\". We proceed to show that this quantity is the same\nas the known CTRW case.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.3061v2"
    },
    {
        "title": "Computational Assignment of Chemical Shifts for Protein Residues",
        "authors": [
            "Lars A. Bratholm"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Fast and accurate protein structure prediction is one of the major challenges\nin structural biology, biotechnology and molecular biomedicine. These fields\nrequire 3D protein structures for rational design of proteins with improved or\nnovel properties. X-ray crystallography is the most common approach even with\nits low success rate, but lately NMR based approaches have gained popularity.\nThe general approach involves a set of distance restraints used to guide a\nstructure prediction, but simple NMR triple-resonance experiments often provide\nenough structural information to predict the structure of small proteins.\nPrevious protein folding simulations that have utilised experimental data have\nweighted the experimental data and physical force field terms more or less\narbitrarily, and the method is thus not generally applicable to new proteins.\nFurthermore a complete and near error-free assignment of chemical shifts\nobtained by the NMR experiments is needed, due to the static, or deterministic,\nassignment. In this thesis I present Chemshift, a module for handling chemical\nshift assignments, implemented in the protein structure determination program\nPhaistos. This module treats both the assignment of experimental data, as well\nas the weighing compared to physical terms, in a probabilistic framework where\nno data is discarded. Provided a partial assignment of NMR peaks, the module is\nable to improve the assignment with the intension to utilise this in the\nprotein folding with little bias.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.3186v1"
    },
    {
        "title": "Scaled Brownian motion as a mean field model for continuous time random\n  walks",
        "authors": [
            "Felix Thiel",
            "Igor M. Sokolov"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We consider scaled Brownian motion (sBm), a random process described by a\ndiffusion equation with explicitly time-dependent diffusion coefficient $D(t) =\nD_0 t^{\\alpha - 1}$ (Batchelor's equation) which, for $\\alpha < 1$, is often\nused for fitting experimental data for subdiffusion of unclear genesis. We show\nthat this process is a close relative of subdiffusive continuous-time random\nwalks and describes the motion of the center of mass of a cloud of independent\nwalkers. It shares with subdiffusive CTRW its non-stationary and non-ergodic\nproperties. The non-ergodicity of sBm does not however go hand in hand with\nstrong difference between its different realizations: its heterogeneity\n(\"ergodicity breaking\") parameter tends to zero for long trajectories.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.3455v1"
    },
    {
        "title": "Copulas and time series with long-ranged dependences",
        "authors": [
            "Rémy Chicheportiche",
            "Anirban Chakraborti"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We review ideas on temporal dependences and recurrences in discrete time\nseries from several areas of natural and social sciences. We revisit existing\nstudies and redefine the relevant observables in the language of copulas (joint\nlaws of the ranks). We propose that copulas provide an appropriate mathematical\nframework to study non-linear time dependences and related concepts - like\naftershocks, Omori law, recurrences, waiting times. We also critically argue\nusing this global approach that previous phenomenological attempts involving\nonly a long-ranged autocorrelation function lacked complexity in that they were\nessentially mono-scale.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.5101v1"
    },
    {
        "title": "Texture descriptor combining fractal dimension and artificial crawlers",
        "authors": [
            "Wesley Nunes Gonçalves",
            "Bruno Brandoli Machado",
            "Odemir Martinez Bruno"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Texture is an important visual attribute used to describe images. There are\nmany methods available for texture analysis. However, they do not capture the\ndetails richness of the image surface. In this paper, we propose a new method\nto describe textures using the artificial crawler model. This model assumes\nthat each agent can interact with the environment and each other. Since this\nswarm system alone does not achieve a good discrimination, we developed a new\nmethod to increase the discriminatory power of artificial crawlers, together\nwith the fractal dimension theory. Here, we estimated the fractal dimension by\nthe Bouligand-Minkowski method due to its precision in quantifying structural\nproperties of images. We validate our method on two texture datasets and the\nexperimental results reveal that our method leads to highly discriminative\ntextural features. The results indicate that our method can be used in\ndifferent texture applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.5290v1"
    },
    {
        "title": "Reliability measures for indexed semi-Markov chains applied to wind\n  energy production",
        "authors": [
            "Guglielmo D'Amico",
            "Filippo Petroni",
            "Flavio Prattico"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The computation of the dependability measures is a crucial point in the\nplanning and development of a wind farm. In this paper we address the issue of\nenergy production by wind turbine by using an indexed semi-Markov chain as a\nmodel of wind speed. We present the mathematical model, we describe the data\nand technical characteristics of a commercial wind turbine (Aircon HAWT-10kW).\nWe show how to compute some of the main dependability measures such as\nreliability, availability and maintainability functions. We compare the results\nof the model with real energy production obtained from data available in the\nLastem station (Italy) and sampled every 10 minutes.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.6585v1"
    },
    {
        "title": "Systematic Physics Constrained Parameter Estimation of Stochastic\n  Differential Equations",
        "authors": [
            "Daniel Peavoy",
            "Christian L. E. Franzke",
            "Gareth O. Roberts"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  A systematic Bayesian framework is developed for physics constrained\nparameter inference ofstochastic differential equations (SDE) from partial\nobservations. The physical constraints arederived for stochastic climate models\nbut are applicable for many fluid systems. A condition isderived for global\nstability of stochastic climate models based on energy conservation.\nStochasticclimate models are globally stable when a quadratic form, which is\nrelated to the cubic nonlinearoperator, is negative definite. A new algorithm\nfor the efficient sampling of such negative definite matrices is developed and\nalso for imputing unobserved data which improve the accuracy of theparameter\nestimates. The performance of this framework is evaluated on two conceptual\nclimatemodels.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.1881v3"
    },
    {
        "title": "Long-range spatial correlations and fluctuation statistics of lightning\n  activity rates in Brazil",
        "authors": [
            "H. V. Ribeiro",
            "F. J. Antonio",
            "L. G. A. Alves",
            "E. K. Lenzi",
            "R. S. Mendes"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We report on a statistical analysis of the lightning activity rates in all\nBrazilian cities. We find out that the average of lightning activity rates\nexhibit a dependence on the latitude of the cities, displaying one peak around\nthe Tropic of Capricorn and another one just before the Equator. We verify that\nthe standard deviation of these rates is almost a constant function of the\nlatitude and that the distribution of the fluctuations surrounding the average\ntendency is quite well described by a Gumbel distribution, which thus connects\nthese rates to extreme processes. We also investigate the behavior of the\nlightning activity rates versus the longitude of the cities. For this case, the\naverage rates exhibit an approximate plateau for a wide range of longitude\nvalues, the standard deviation is an approximate constant function of\nlongitude, and the fluctuations are described by a Laplace distribution. We\nfurther characterize the spatial correlation of the lightning activity rates\nbetween pairs of cities, where our results show that the spatial correlation\nfunction decays very slowly with the distance between the cities and that for\nintermediate distances the correlation exhibits an approximate logarithmic\ndecay. Finally, we propose to model this last behavior within the framework of\nthe Edwards-Wilkinson equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.2015v1"
    },
    {
        "title": "Critical assessment of the evidence for striped nanoparticles",
        "authors": [
            "Julian Stirling",
            "Ioannis Lekkas",
            "Adam Sweetman",
            "Predrag Djuranovic",
            "Quanmin Guo",
            "Josef Granwehr",
            "Raphaël Lévy",
            "Philip Moriarty"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  There is now a significant body of literature in which it is claimed that\nstripes form in the ligand shell of suitably functionalised Au nanoparticles.\nThis stripe morphology has been proposed to strongly affect the physicochemical\nand biochemical properties of the particles. We critique the published evidence\nfor striped nanoparticles in detail, with a particular focus on the\ninterpretation of scanning tunnelling microscopy (STM) data (as this is the\nonly technique which ostensibly provides direct evidence for the presence of\nstripes). Through a combination of an exhaustive re-analysis of the original\ndata with new experimental measurements of a simple control sample comprising\nentirely unfunctionalised particles, we conclusively show that all of the STM\nevidence for striped nanoparticles published to date can instead be explained\nby a combination of well-known instrumental artefacts, strong observer bias,\nand/or improper data acquisition/analysis protocols. We also critically\nre-examine the evidence for the presence of ligand stripes which has been\nclaimed to have been found from transmission electron microscopy, nuclear\nmagnetic resonance spectroscopy, small angle neutron scattering experiments,\nand computer simulations. Although these data can indeed be interpreted in\nterms of stripe formation, we show that, just as for the STM measurements,\nrather more mundane interpretations can account for the reported results.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.6812v1"
    },
    {
        "title": "Investigating complex networks with inverse models: analytical aspects\n  of spatial leakage and connectivity estimation",
        "authors": [
            "Vincent Wens"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Network theory and inverse modeling are two standard tools of applied\nphysics, whose combination is needed when studying the dynamical organization\nof spatially distributed systems from indirect measurements. However, the\nassociated connectivity estimation may be affected by spatial leakage, an\nartifact of inverse modeling that limits the interpretability of network\nanalysis. This paper investigates general analytical aspects pertaining to this\nissue. First, the existence of spatial leakage is derived from the topological\nstructure of inverse operators. Then, the geometry of spatial leakage is\nmodeled and used to define a geometric correction scheme, which limits spatial\nleakage effects in connectivity estimation. Finally, this new approach for\nnetwork analysis is compared analytically to existing methods based on linear\nregressions, which are shown to yield biased coupling estimates.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.0444v4"
    },
    {
        "title": "Generalization of the Poisson distribution for the case of changing\n  probabilities of consecutive events",
        "authors": [
            "E. A. Kushnirenko"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  In this paper the generalization of the Poisson distribution is derived for\nthe case when each consecutive event changes event rate. A simple formula for\nthe probability of observing of a given number of events for the selected\nperiod of time is derived for a given set of rates. Application of this\ndistribution in high-energy physics calculations is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.0718v1"
    },
    {
        "title": "Neutrino mass and Extreme Value Distributions in $β$-decay",
        "authors": [
            "J. G. Esteve",
            "F. Falceto"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We propose a new application of the Extreme Value Theory for distributions\nwith compact support. The novelty of our proposal is the use of these tools to\nestimate the neutrino mass from the energy spectrum of electrons in\n$\\beta$-decay. In this way the dependence of the result on the mass of the\nneutrino is considerably enhanced increasing the sensitivity of the experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.1644v2"
    },
    {
        "title": "Bursting transition in a linear self-exciting point process",
        "authors": [
            "Tomokatsu Onaga",
            "Shigeru Shinomoto"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Self-exciting point processes describe the manner in which every event\nfacilitates the occurrence of succeeding events. By increasing excitability,\nthe event occurrences start to exhibit bursts even in the absence of external\nstimuli. We revealed that the transition is uniquely determined by the average\nnumber of events added by a single event, $1-1/\\sqrt{2} \\approx 0.2929$,\nindependently of the temporal excitation profile. We further extended the\ntheory to multi-dimensional processes, to be able to incite or inhibit bursting\nin networks of agents.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.5186v1"
    },
    {
        "title": "Sensitivity of Yield Optimized Superoscillations",
        "authors": [
            "Moshe Schwartz",
            "Ehud Perlsman"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Super oscillating signals are band limited signals that oscillate in some\nregion faster than their largest Fourier component. Such signals have many\nobvious scientific and technological applications, yet their practical use is\nstrongly limited by the fact that an overwhelming proportion of the energy goes\ninto that part of the signal, which is not superoscillating. In a recent\narticle the problem of optimization of such signals has been studied. In that\narticle the concept of superoscillation yield is defined as the ratio of the\nenergy in the super oscillations to the total energy of the signal, given the\nrange in time and frequency of the superoscillations, which is imposed by\nforcing the signal to interpolate among a set of predetermined points. The\noptimization of the superoscillation yield consists of obtaining the Fourier\ncoefficients of the low frequency components of which the signal consists, that\nmaximize the yield under the interpolation constraint. Since in practical\napplications it is impossible to determine the Fourier coefficients with\ninfinite precision, it is necessary to answer two questions. The first is how\nis the superoscillating nature of the signal affected by random small\ndeviations in those Fourier coefficients and the second is how is the yield\naffected? These are the questions addressed in the present article. Limits on\nthe necessary precision are obtained. Those limits seem not to be impractical.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.3089v1"
    },
    {
        "title": "On the combination of correlated estimates of a physics observable",
        "authors": [
            "Richard Nisius"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The combination of a number of correlated estimates of a given observable is\nfrequently performed using the Best Linear Unbiased Estimate (BLUE) method.\nMost features of such a combination can already be seen by analysing the\nspecial case of a pair of estimates from two correlated estimators of the\nobservable. Two important parameters of this combination are the weight of the\nless precise estimate and the ratio of uncertainties of the combined result and\nthe more precise estimate. Derivatives of these quantities are derived with\nrespect to the correlation and the ratio of uncertainties of the two estimates.\n  The impact of using either absolute or relative uncertainties in the BLUE\ncombination is investigated on a number of examples including Peelle's\nPertinent Puzzle. Using an example, a critical assessment is performed of\nsuggested methods to deal with the fact that both the correlation and the ratio\nof uncertainties of a pair of estimates are typically only known with some\nuncertainty. Finally, a proposal is made to decide on the usefulness of a\ncombination and to perform it. The proposal is based on possible improvements\nwith respect to the most precise estimate by including additional estimates.\nThis procedure can be applied to the general case of several observables.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.4016v2"
    },
    {
        "title": "Exact detection of direct links in networks of interacting dynamical\n  units",
        "authors": [
            "Nicolás Rubido",
            "Arturo C. Martí",
            "Ezequiel Bianco-Martínez",
            "Celso Grebogi",
            "Murilo S. Baptista",
            "Cristina Masoller"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The inference of an underlying network topology from local observations of a\ncomplex system composed of interacting units is usually attempted by using\nstatistical similarity measures, such as Cross-Correlation (CC) and Mutual\nInformation (MI). The possible existence of a direct link between different\nunits is, however, hindered within the time-series measurements. Here we show\nthat, for the class of systems studied, when an abrupt change in the ordered\nset of CC or MI values exists, it is possible to infer, without errors, the\nunderlying network topology from the time-series measurements, even in the\npresence of observational noise, non-identical units, and coupling\nheterogeneity. We find that a necessary condition for the discontinuity to\noccur is that the dynamics of the coupled units is partially coherent, i.e.,\nneither complete disorder nor globally synchronous patterns are present. We\ncritically compare the inference methods based on CC and MI, in terms of how\neffective, robust, and reliable they are, and conclude that, in general, MI\noutperforms CC in robustness and reliability. Our findings could be relevant\nfor the construction and interpretation of functional networks, such as those\nconstructed from brain or climate data.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.4839v3"
    },
    {
        "title": "Dynamical localization and eigenstate localization in trap models",
        "authors": [
            "Franziska Flegel",
            "Igor M. Sokolov"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The one-dimensional random trap model with a power-law distribution of mean\nsojourn times exhibits a phenomenon of dynamical localization in the case where\ndiffusion is anomalous: The probability to find two independent walkers at the\nsame site, as given by the participation ratio, stays constant and high in a\nbroad domain of intermediate times. This phenomenon is absent in dimensions two\nand higher. In finite lattices of all dimensions the participation ratio\nfinally equilibrates to a different final value. We numerically investigate\ntwo-particle properties in a random trap model in one and in three dimensions,\nusing a method based on spectral decomposition of the transition rate matrix.\nThe method delivers a very effective computational scheme producing numerically\nexact results for the averages over thermal histories and initial conditions in\na given landscape realization. Only a single averaging procedure over disorder\nrealizations is necessary. The behavior of the participation ratio is compared\nto other measures of localization, as for example to the states' gyration\nradius, according to which the dynamically localized states are extended. This\nmeans that although the particles are found at the same site with a high\nprobability, the typical distance between them grows. Moreover the final\nequilibrium state is extended both with respect to its gyration radius and to\nits Lyapunov exponent. In addition, we show that the phenomenon of dynamical\nlocalization is only marginally connected with the spectrum of the transition\nrate matrix, and is dominated by the properties of its eigenfunctions which\ndiffer significantly in dimensions one and three.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.7082v2"
    },
    {
        "title": "Modelling and analysis of turbulent datasets using ARMA processes",
        "authors": [
            "Davide Faranda",
            "Flavio Maria Emanuele Pons",
            "Bérèngere Dubrulle",
            "François Daviaud",
            "Brice Saint-Michel",
            "Éric Herbert",
            "Pierre-Philippe Cortet"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We introduce a novel way to extract information from turbulent datasets by\napplying an ARMA statistical analysis. Such analysis goes well beyond the\nanalysis of the mean flow and of the fluctuations and links the behavior of the\nrecorded time series to a discrete version of a stochastic differential\nequation which is able to describe the correlation structure in the dataset. We\nintroduce a new intermittency parameter $\\Upsilon$ that measures the difference\nbetween the resulting analysis and the Obukhov model of turbulence, the\nsimplest stochastic model reproducing both Richardson law and the Kolmogorov\nspectrum. We test the method on datasets measured in a von K\\'arm\\'an swirling\nflow experiment. We found that the ARMA analysis is well correlated with\nspatial structures of the flow, and can discriminate between two different\nflows with comparable mean velocities, obtained by changing the forcing.\nMoreover, we show that the intermittency parameter is highest in regions where\nshear layer vortices are present, thereby establishing a link between\nintermittency corrections and coherent structures. We show that some salient\nfeatures of the analysis are preserved when considering global instead of local\nobservables. Finally we analyze flow configurations with multistability\nfeatures where the ARMA technique is efficient in discriminating different\nstability branches of the system.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.0547v2"
    },
    {
        "title": "Hamiltonian formalism and path entropy maximization",
        "authors": [
            "Sergio Davis",
            "Diego González"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Maximization of the path information entropy is a clear prescription for\nconstructing models in non-equilibrium statistical mechanics. Here it is shown\nthat, following this prescription under the assumption of arbitrary\ninstantaneous constraints on position and velocity, a Lagrangian emerges which\ndetermines the most probable trajectory. Deviations from the probability\nmaximum can be consistently described as slices in time by a Hamiltonian,\naccording to a nonlinear Langevin equation and its associated Fokker-Planck\nequation. The connections unveiled between the maximization of path entropy and\nthe Langevin/Fokker-Planck equations imply that missing information about the\nphase space coordinate never decreases in time, a purely\ninformation-theoretical version of the Second Law of Thermodynamics. All of\nthese results are independent of any physical assumptions, and thus valid for\nany generalized coordinate as a function of time, or any other parameter. This\nreinforces the view that the Second Law is a fundamental property of plausible\ninference.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.3249v3"
    },
    {
        "title": "Negative probabilities and counter-factual reasoning in quantum\n  cognition",
        "authors": [
            "J. Acacio de Barros",
            "Gary Oas"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  In this paper we discuss quantum-like decision-making experiments using\nnegative probabilities. We do so by showing how the two-slit experiment, in the\nsimplified version of the Mach-Zehnder interferometer, can be described by this\nformalism. We show that negative probabilities impose constraints to what types\nof counter-factual reasoning we can make with respect to (quantum) internal\nrepresentations of the decision maker.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.3921v2"
    },
    {
        "title": "Evaluation of the training objectives with surface electromyography",
        "authors": [
            "Paulina Trybek",
            "Michal Nowakowski",
            "Lukasz Machura"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  In this work the multifractal analysis of the kinesiological surface\nelectromyographic signal is proposed. The goal was to investigate the level of\nneuromuscular activation during complex movements on the laparoscopic trainer.\nThe basic issue of this work concerns the changes observed in the signal\nobtained from the complete beginner in the field of using laparoscopic tools\nand the same person subjected to the series of training. To quantify the\ncomplexity of the kinesiological sEMG, the nonlinear analysis technique, namely\nthe MultiFractal Detrended Fluctuation Analysis was adopted. The analysis was\nbased on the parameters describing the multifractal spectrum -- Hurst exponent\nand the spectrum width. The statistically significant differences for a\nselected group of muscles at the different states (before and after training)\nare presented. Additionally, as the base case, the relaxation state was\nconsidered and compared with the working states.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.4336v5"
    },
    {
        "title": "Zero-one-only process: a correlated random walk with a stochastic\n  ratchet",
        "authors": [
            "Seung Ki Baek",
            "Hawoong Jeong",
            "Seung-Woo Son",
            "Beom Jun Kim"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The investigation of random walks is central to a variety of stochastic\nprocesses in physics, chemistry, and biology. To describe a transport\nphenomenon, we study a variant of the one-dimensional persistent random walk,\nwhich we call a zero-one-only process. It makes a step in the same direction as\nthe previous step with probability $p$, and stops to change the direction with\n$1-p$. By using the generating-function method, we calculate its characteristic\nquantities such as the statistical moments and probability of the first return.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.4756v1"
    },
    {
        "title": "Evolution of Force Networks in Dense Particulate Media",
        "authors": [
            "Miroslav Kramar",
            "Arnaud Goullet",
            "Lou Kondic",
            "Konstantin Mischaikow"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We introduce novel sets of measures with the goal of describing dynamical\nproperties of force networks in dense particulate systems. The presented\napproach is based on persistent homology and allows for extracting precise,\nquantitative measures that describe the evolution of geometric features of the\ninterparticle forces, without necessarily considering the details related to\nindividual contacts between particles. The networks considered emerge from\ndiscrete element simulations of two dimensional particulate systems consisting\nof compressible frictional circular disks. We quantify the evolution of the\nnetworks for slowly compressed systems undergoing jamming transition. The main\nfindings include uncovering significant but localized changes of force networks\nfor unjammed systems, global (system-wide) changes as the systems evolve\nthrough jamming, to be followed by significantly less dramatic evolution for\nthe jammed states. We consider both connected components, related in loose\nsense to force chains, and loops, and find that both measures provide a\nsignificant insight into the evolution of force networks. In addition to\nnormal, we consider also tangential forces between the particles and find that\nthey evolve in the consistent manner. Consideration of both frictional and\nfrictionless systems leads us to the conclusion that friction plays a\nsignificant role in determining the dynamical properties of the considered\nnetworks. We find that the proposed approach describes the considered networks\nin a precise yet tractable manner, allowing to identify novel features which\ncould be difficult or impossible to describe using other approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.6279v1"
    },
    {
        "title": "Validating Predictions of Unobserved Quantities",
        "authors": [
            "Todd A. Oliver",
            "Gabriel Terejanu",
            "Christopher S. Simmons",
            "Robert D. Moser"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The ultimate purpose of most computational models is to make predictions,\ncommonly in support of some decision-making process (e.g., for design or\noperation of some system). The quantities that need to be predicted (the\nquantities of interest or QoIs) are generally not experimentally observable\nbefore the prediction, since otherwise no prediction would be needed. Assessing\nthe validity of such extrapolative predictions, which is critical to informed\ndecision-making, is challenging. In classical approaches to validation, model\noutputs for observed quantities are compared to observations to determine if\nthey are consistent. By itself, this consistency only ensures that the model\ncan predict the observed quantities under the conditions of the observations.\nThis limitation dramatically reduces the utility of the validation effort for\ndecision making because it implies nothing about predictions of unobserved QoIs\nor for scenarios outside of the range of observations. However, there is no\nagreement in the scientific community today regarding best practices for\nvalidation of extrapolative predictions made using computational models. The\npurpose of this paper is to propose and explore a validation and predictive\nassessment process that supports extrapolative predictions for models with\nknown sources of error. The process includes stochastic modeling, calibration,\nvalidation, and predictive assessment phases where representations of known\nsources of uncertainty and error are built, informed, and tested. The proposed\nmethodology is applied to an illustrative extrapolation problem involving a\nmisspecified nonlinear oscillator.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.7555v1"
    },
    {
        "title": "Spatially-Aware Temporal Anomaly Mapping of Gamma Spectra",
        "authors": [
            "Alex Reinhart",
            "Alex Athey",
            "Steven Biegalski"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  For security, environmental, and regulatory purposes it is useful to\ncontinuously monitor wide areas for unexpected changes in radioactivity. We\nreport on a temporal anomaly detection algorithm which uses mobile detectors to\nbuild a spatial map of background spectra, allowing sensitive detection of any\nanomalies through many days or months of monitoring. We adapt\npreviously-developed anomaly detection methods, which compare spectral shape\nrather than count rate, to function with limited background data, allowing\nsensitive detection of small changes in spectral shape from day to day. To\ndemonstrate this technique we collected daily observations over the period of\nsix weeks on a 0.33 square mile research campus and performed source injection\nsimulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.1135v1"
    },
    {
        "title": "Generalizations related to hypothesis testing with the Posterior\n  distribution of the Likelihood Ratio",
        "authors": [
            "I. Smith",
            "A. Ferrari"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The Posterior distribution of the Likelihood Ratio (PLR) is proposed by\nDempster in 1974 for significance testing in the simple vs composite hypotheses\ncase. In this hypotheses test case, classical frequentist and Bayesian\nhypotheses tests are irreconcilable, as emphasized by Lindley's paradox, Berger\n& Selke in 1987 and many others. However, Dempster shows that the PLR (with\ninner threshold 1) is equal to the frequentist p-value in the simple Gaussian\ncase. In 1997, Aitkin extends this result by adding a nuisance parameter and\nshowing its asymptotic validity under more general distributions. Here we\nextend the reconciliation between the PLR and a frequentist p-value for a\nfinite sample, through a framework analogous to the Stein's theorem frame in\nwhich a credible (Bayesian) domain is equal to a confidence (frequentist)\ndomain.\n  This general reconciliation result only concerns simple vs composite\nhypotheses testing. The measures proposed by Aitkin in 2010 and Evans in 1997\nhave interesting properties and extend Dempster's PLR but only by adding a\nnuisance parameter. Here we propose two extensions of the PLR concept to the\ngeneral composite vs composite hypotheses test. The first extension can be\ndefined for improper priors as soon as the posterior is proper. The second\nextension appears from a new Bayesian-type Neyman-Pearson lemma and emphasizes,\nfrom a Bayesian perspective, the role of the LR as a discrepancy variable for\nhypothesis testing.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.1023v1"
    },
    {
        "title": "Fourier-Domain Fixed Point Algorithms with Coded Diffraction Patterns",
        "authors": [
            "Albert Fannjiang"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Fourier-domain Difference Map (FDM) for phase retrieval with two oversampled\ncoded diffraction patterns are proposed. FDM is a 3-parameter family of fixed\npoint algorithms including Fourier-domain Hybrid-Projection-Reflection (FHPR)\nand Douglas-Rachford (FDR) algorithm. For generic complex objects without any\nobject constraint, FDM yields a unique fixed point, after proper projection\nback to the object domain, which is the true solution to the phase retrieval\nproblem up to a global phase factor.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.2742v2"
    },
    {
        "title": "CCCP Algorithms to Minimize the Bethe free energy of 3-SAT Problem",
        "authors": [
            "Yusupjan Habibulla"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The k-sat problem is a prototypical constraint satisfaction problem. There\nare many algorithms to study k-sat problem, BP algorithm is famous one of them.\nBut BP algorithm does not converge when $\\alpha$(constraint density)is bigger\nthan some threshold value. In this paper we use CCCP (Concave Convex Procedure)\nalgorithm to study 3-sat problem and we get better results than BP algorithm\nthat CCCP algorithm still converges when BP algorithm does not converge. Our\nwork almost builds on recent results by Yuille \\cite{Yuille2002} who apply the\nCCCP algorithm to Bethe and Kikuchi free energies and obtained two algorithms\non 2D and 3D spin glasses. Our implementation of CCCP algorithm on 3-sat\nproblem is some different from his implementation and we have some different\nviews about CCCP algorithm's some properties. Some difference of these maybe\nbecause of CCCP algorithm have different properties and implementation process\non different problem and some others of these are related to the CCCP algorithm\nitself. Our work indicates that CCCP algorithm has more learning and inference\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.4041v2"
    },
    {
        "title": "Constructing directed networks from multivariate time series using\n  linear modelling technique",
        "authors": [
            "Toshihiro Tanizawa",
            "Tomomichi Nakamura",
            "Fumihiko Taya",
            "Michael Small"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We describe a method to construct directed networks from multivariate time\nseries which has several advantages over the widely accepted methods. This\nmethod is based on an information theoretic reduction of linear\n(auto-regressive) models. The models are called reduced auto-regressive (RAR)\nmodels. The procedure of the proposed method is composed of three steps: (i)\neach time series is treated as a basic node of a network, (ii) multivariate RAR\nmodels are built and the constituent information in the models is summarized,\nand (iii) nodes are connected with a directed link based on that summary\ninformation. The proposed method is demonstrated for numerical data generated\nby known systems, and applied to several actual time series of special\ninterest. Although the proposed method can identify connectivity, there are\nthree points to keep in mind: (1) the proposed method cannot always identify\nnonlinear relationships among components, (2) as constructing RAR models is\nNP-hard, the network constructed by the proposed method might be near-optimal\nnetwork when we cannot perform an exhaustive search, and (3) it is difficult to\nconstruct appropriate networks when the observational noise is large.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.5247v3"
    },
    {
        "title": "Benford distributions in NMR",
        "authors": [
            "Gaurav Bhole",
            "Abhishek Shukla",
            "T. S. Mahesh"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Benford's Law is an empirical law which predicts the frequency of significant\ndigits in databases corresponding to various phenomena, natural or artificial.\nAlthough counter intuitive at the first sight, it predicts a higher occurrence\nof digit 1, and decreasing occurrences to other larger digits. Here we report\nthe Benford analysis of various NMR databases and draw several interesting\ninferences. We observe that, in general, NMR signals follow Benford\ndistribution in time-domain as well as in frequency domain. Our survey included\nNMR signals of various nuclear species in a wide variety of molecules in\ndifferent phases, namely liquid, liquid-crystalline, and solid. We also studied\nthe dependence of Benford distribution on NMR parameters such as signal to\nnoise ratio, number of scans, pulse angles, and apodization. In this process we\nalso find that, under certain circumstances, the Benford analysis can\ndistinguish a genuine spectrum from a visually identical simulated spectrum.\nFurther we find that chemical-shift databases and amplitudes of certain radio\nfrequency pulses generated using optimal control techniques also satisfy\nBenford's law to a good extent.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.7077v1"
    },
    {
        "title": "A Bayesian Approach for Parameter Estimation and Prediction using a\n  Computationally Intensive Model",
        "authors": [
            "Dave Higdon",
            "Jordan D. McDonnell",
            "Nicolas Schunck",
            "Jason Sarich",
            "Stefan M. Wild"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Bayesian methods have been very successful in quantifying uncertainty in\nphysics-based problems in parameter estimation and prediction. In these cases,\nphysical measurements y are modeled as the best fit of a physics-based model\n$\\eta(\\theta)$ where $\\theta$ denotes the uncertain, best input setting. Hence\nthe statistical model is of the form $y = \\eta(\\theta) + \\epsilon$, where\n$\\epsilon$ accounts for measurement, and possibly other error sources. When\nnon-linearity is present in $\\eta(\\cdot)$, the resulting posterior distribution\nfor the unknown parameters in the Bayesian formulation is typically complex and\nnon-standard, requiring computationally demanding computational approaches such\nas Markov chain Monte Carlo (MCMC) to produce multivariate draws from the\nposterior. While quite generally applicable, MCMC requires thousands, or even\nmillions of evaluations of the physics model $\\eta(\\cdot)$. This is problematic\nif the model takes hours or days to evaluate. To overcome this computational\nbottleneck, we present an approach adapted from Bayesian model calibration.\nThis approach combines output from an ensemble of computational model runs with\nphysical measurements, within a statistical formulation, to carry out\ninference. A key component of this approach is a statistical response surface,\nor emulator, estimated from the ensemble of model runs. We demonstrate this\napproach with a case study in estimating parameters for a density functional\ntheory (DFT) model, using experimental mass/binding energy measurements from a\ncollection of atomic nuclei. We also demonstrate how this approach produces\nuncertainties in predictions for recent mass measurements obtained at Argonne\nNational Laboratory (ANL).\n",
        "pdf_link": "http://arxiv.org/pdf/1407.3017v2"
    },
    {
        "title": "Forecasting future oil production in Norway and the UK: a general\n  improved methodology",
        "authors": [
            "Lucas Fievet",
            "Zalàn Forrò",
            "Peter Cauwels",
            "Didier Sornette"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We present a new Monte-Carlo methodology to forecast the crude oil production\nof Norway and the U.K. based on a two-step process, (i) the nonlinear\nextrapolation of the current/past performances of individual oil fields and\n(ii) a stochastic model of the frequency of future oil field discoveries.\nCompared with the standard methodology that tends to underestimate remaining\noil reserves, our method gives a better description of future oil production,\nas validated by our back-tests starting in 2008. Specifically, we predict\nremaining reserves extractable until 2030 to be 188 +/- 10 million barrels for\nNorway and 98 +/- 10 million barrels for the UK, which are respectively 45% and\n66% above the predictions using the standard methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.3652v1"
    },
    {
        "title": "Identification of cross and autocorrelations in time series within an\n  approach based on Wigner eigenspectrum of random matrices",
        "authors": [
            "Michal Sawa",
            "Dariusz Grech"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We present an original and novel method based on random matrix approach that\nenables to distinguish the respective role of temporal autocorrelations inside\ngiven time series and cross correlations between various time series. The\nproposed algorithm is based on properties of Wigner eigenspectrum of random\nmatrices instead of commonly used Wishart eigenspectrum methodology. The\nproposed approach is then qualitatively and quantitatively applied to financial\ndata in stocks building WIG (Warsaw Stock Exchange Index).\n",
        "pdf_link": "http://arxiv.org/pdf/1407.4702v1"
    },
    {
        "title": "Mantid - Data Analysis and Visualization Package for Neutron Scattering\n  and $μSR$ Experiments",
        "authors": [
            "O. Arnold",
            "J. C. Bilheux",
            "J. M. Borreguero",
            "A. Buts",
            "S. I. Campbell",
            "L. Chapon",
            "M. Doucet",
            "N. Draper",
            "R. Ferraz Leal",
            "M. A. Gigg",
            "V. E. Lynch",
            "A. Markvardsen",
            "D. J. Mikkelson",
            "R. L. Mikkelson",
            "R. Miller",
            "K. Palmen",
            "P. Parker",
            "G. Passos",
            "T. G. Perring",
            "P. F. Peterson",
            "S. Ren",
            "M. A. Reuter",
            "A. T. Savici",
            "J. W. Taylor",
            "R. J. Taylor",
            "R. Tolchenov",
            "W. Zhou",
            "J. Zikovsky"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The Mantid framework is a software solution developed for the analysis and\nvisualization of neutron scattering and muon spin measurements. The framework\nis jointly developed by software engineers and scientists at the ISIS Neutron\nand Muon Facility and the Oak Ridge National Laboratory. The objectives,\nfunctionality and novel design aspects of Mantid are described.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.5860v1"
    },
    {
        "title": "Stochastic processes via the pathway model",
        "authors": [
            "A. M. Mathai",
            "H. J. Haubold"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  After collecting data from observations or experiments, the next step is to\nbuild an appropriate mathematical or stochastic model to describe the data so\nthat further studies can be done with the help of the models. In this article,\nthe input-output type mechanism is considered first, where reaction, diffusion,\nreaction-diffusion, and production-destruction type physical situations can fit\nin. Then techniques are described to produce thicker or thinner tails (power\nlaw behavior) in stochastic models. Then the pathway idea is described where\none can switch to different functional forms of the probability density\nfunction) through a parameter called the pathway parameter.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.2768v1"
    },
    {
        "title": "A new time-frequency method to reveal quantum dynamics of atomic\n  hydrogen in intense laser pulses: Synchrosqueezing Transform",
        "authors": [
            "Yae-lin Sheu",
            "Liang-Yan Hsu",
            "Hau-tieng Wu",
            "Peng-Cheng Li",
            "Shih-I Chu"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  This study introduces a new adaptive time-frequency (TF) analysis technique,\nsynchrosqueezing transform (SST), to explore the dynamics of a laser-driven\nhydrogen atom at an {\\it ab initio} level, upon which we have demonstrated its\nversatility as a new viable venue for further exploring quantum dynamics. For a\nsignal composed of oscillatory components which can be characterized by\ninstantaneous frequency, the SST enables rendering the decomposed signal based\non the phase information inherited in the linear TF representation with\nmathematical support. Compared with the classical type TF methods, the SST\nclearly depicts several intrinsic quantum dynamical processes such as selection\nrules, AC Stark effects, and high harmonic generation.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.2926v1"
    },
    {
        "title": "JFIT: a framework to obtain combined experimental results through joint\n  fits",
        "authors": [
            "Eli Ben-Haim",
            "René Brun",
            "Bertrand Echenard",
            "Thomas E. Latham"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  A master-worker architecture is presented for obtaining combined experimental\nresults through joint fits of datasets from several experiments. The design of\nthe architecture allows such joint fits to be performed keeping the data\nseparated, in its original format, and using independent fitting environments.\nThis allows the benefits of joint fits, such as ensuring that correlations are\ncorrectly taken into account and better determination of nuisance parameters,\nto be harnessed without the need to reformat data samples or to rewrite\nexisting fitting code. The Jfit framework is a C++ implementation of this idea\nin the Laura++ package, using dedicated classes of the ROOT package. We present\nthe Jfit framework, give instructions for its use, and demonstrate its\nfunctionalities with concrete examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.5080v4"
    },
    {
        "title": "Finite sample properties of power-law cross-correlations estimators",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We study finite sample properties of estimators of power-law\ncross-correlations -- detrended cross-correlation analysis (DCCA), height\ncross-correlation analysis (HXA) and detrending moving-average\ncross-correlation analysis (DMCA) -- with a special focus on short-term memory\nbias as well as power-law coherency. Presented broad Monte Carlo simulation\nstudy focuses on different time series lengths, specific methods' parameter\nsetting, and memory strength. We find that each method is best suited for\ndifferent time series dynamics so that there is no clear winner between the\nthree. The method selection should be then made based on observed dynamic\nproperties of the analyzed series.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.6857v1"
    },
    {
        "title": "Whole fusion-fission process with Langevin approach and compared with\n  analytical solution for barrier passage",
        "authors": [
            "Jie Han",
            "Jing-Dong Bao"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We investigate time-dependent probability for a Brownian particle passing\nover the barrier to stay at a metastable potential pocket against escaping over\nthe barrier. This is related to whole fusion-fission dynamical process and can\nbe called the reverse Kramers problem. By the passing probability over the\nsaddle point of inverse harmonic potential multiplying the exponential decay\nfactor of a particle in the metastable potential, we present an approximate\nexpression for the modified passing probability over the barrier, in which the\neffect of reflection boundary of potential is taken into account. Our\nanalytical result and Langevin Monte-Carlo simulation show that the probability\npassing and against escaping over the barrier is a non-monotonous function of\ntime and its maximal value is less than the stationary result of passing\nprobability over the saddle point of inverse harmonic potential.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.2705v1"
    },
    {
        "title": "The perils of thresholding",
        "authors": [
            "Francesc Font-Clos",
            "Gunnar Pruessner",
            "Anna Deluca",
            "Nicholas R. Moloney"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The thresholding of time series of activity or intensity is frequently used\nto define and differentiate events. This is either implicit, for example due to\nresolution limits, or explicit, in order to filter certain small scale physics\nfrom the supposed true asymptotic events. Thresholding the birth-death process,\nhowever, introduces a scaling region into the event size distribution, which is\ncharacterised by an exponent that is unrelated to the actual asymptote and is\nrather an artefact of thresholding. As a result, numerical fits of simulation\ndata produce a range of exponents, with the true asymptote visible only in the\ntail of the distribution. This tail is increasingly difficult to sample as the\nthreshold is increased. In the present case, the exponents and the spurious\nnature of the scaling region can be determined analytically, thus demonstrating\nthe way in which thresholding conceals the true asymptote. The analysis also\nsuggests a procedure for detecting the influence of the threshold by means of a\ndata collapse involving the threshold-imposed scale.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.6048v1"
    },
    {
        "title": "Interpolation between multi-dimensional histograms using a new\n  non-linear moment morphing method",
        "authors": [
            "Max Baak",
            "Stefan Gadatsch",
            "Robert Harrington",
            "Wouter Verkerke"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  A prescription is presented for the interpolation between multi-dimensional\ndistribution templates based on one or multiple model parameters. The technique\nuses a linear combination of templates, each created using fixed values of the\nmodel's parameters and transformed according to a specific procedure, to model\na non-linear dependency on model parameters and the dependency between them. By\nconstruction the technique scales well with the number of input templates used,\nwhich is a useful feature in modern day particle physics, where a large number\nof templates is often required to model the impact of systematic uncertainties.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.7388v1"
    },
    {
        "title": "Heuristic algorithm for 1D and 2D unfolding",
        "authors": [
            "Yordan Karadzhov"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  A very simple heuristic approach to the unfolding problem will be described.\nAn iterative algorithm starts with an empty histogram and every iteration aims\nto add one entry to this histogram. The entry to be added is selected according\nto a criteria which includes a $\\chi^2$ test and a regularization. After a\nrelatively small number of iterations (500 - 1000) the growing reconstructed\ndistribution converges to the true distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1375v1"
    },
    {
        "title": "Measuring the performance of sensors that report uncertainty",
        "authors": [
            "A. D. Martin",
            "T. C. A. Molteno",
            "M. Parry"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We provide methods to validate and compare sensor outputs, or inference\nalgorithms applied to sensor data, by adapting statistical scoring rules. The\nreported output should either be in the form of a prediction interval or of a\nparameter estimate with corresponding uncertainty. Using knowledge of the\n`true' parameter values, scoring rules provide a method of ranking different\nsensors or algorithms for accuracy and precision. As an example, we apply the\nscoring rules to the inferred masses of cattle from ground force data and draw\nconclusions on which rules are most meaningful and in which way.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.4354v1"
    },
    {
        "title": "Given enough choice, simple local rules percolate discontinuously",
        "authors": [
            "Alex Waagen",
            "Raissa M. D'Souza"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  There is still much to discover about the mechanisms and nature of\ndiscontinuous percolation transitions. Much of the past work considers graph\nevolution algorithms known as Achlioptas processes in which a single edge is\nadded to the graph from a set of $k$ randomly chosen candidate edges at each\ntimestep until a giant component emerges. Several Achlioptas processes seem to\nyield a discontinuous percolation transition, but it was proven by Riordan and\nWarnke that the transition must be continuous in the thermodynamic limit.\nHowever, they also proved that if the number $k(n)$ of candidate edges\nincreases with the number of nodes, then the percolation transition may be\ndiscontinuous. Here we attempt to find the simplest such process which yields a\ndiscontinuous transition in the thermodynamic limit. We introduce a process\nwhich considers only the degree of candidate edges and not component size. We\ncalculate the critical point $t_{c}=(1-\\theta(\\frac{1}{k}))n$ and rigorously\nshow that the critical window is of size $O(\\frac{n}{k(n)})$. If $k(n)$ grows\nvery slowly, for example $k(n)=\\log n$, the critical window is barely sublinear\nand hence the phase transition is discontinuous but appears continuous in\nfinite systems. We also present arguments that Achlioptas processes with\nbounded size rules will always have continuous percolation transitions even\nwith infinite choice.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.4527v1"
    },
    {
        "title": "Kernel density estimation of a multidimensional efficiency profile",
        "authors": [
            "Anton Poluektov"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Kernel density estimation is a convenient way to estimate the probability\ndensity of a distribution given the sample of data points. However, it has\ncertain drawbacks: proper description of the density using narrow kernels needs\nlarge data samples, whereas if the kernel width is large, boundaries and narrow\nstructures tend to be smeared. Here, an approach to correct for such effects,\nis proposed that uses an approximate density to describe narrow structures and\nboundaries. The approach is shown to be well suited for the description of the\nefficiency shape over a multidimensional phase space in a typical particle\nphysics analysis. An example is given for the five-dimensional phase space of\nthe $\\Lambda_b^0\\to D^0p\\pi$ decay.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.5528v1"
    },
    {
        "title": "Hubble's Law Implies Benford's Law for Distances to Galaxies",
        "authors": [
            "Ronald F. Fox",
            "Theodore P. Hill"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  A recent article by Alexopoulos and Leontsinis presented empirical evidence\nthat the first digits of the distances to galaxies are a reasonably good fit to\nthe probabilities predicted by Benford's law, the well known logarithmic\nstatistical distribution of significant digits. The purpose of the present\narticle is to give a theoretical explanation, based on Hubble's law and\nmathematical properties of Benford's law, why galaxy distances might be\nexpected to follow Benford's law. The new galaxy-distance law derived here,\nwhich is robust with respect to change of scale and base, to additive and\nmultiplicative computational or observational errors, and to variability of the\nHubble constant in both time and space, predicts that conformity to Benford's\nlaw will improve as more data on distances to galaxies becomes available.\nConversely, with the logical derivation of this law presented here, the recent\nempirical observations may be viewed as independent evidence of the validity of\nHubble's law.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.1536v5"
    },
    {
        "title": "McSAS: A package for extracting quantitative form-free distributions",
        "authors": [
            "Ingo Breßler",
            "Brian R. Pauw",
            "Andreas Thünemann"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  A reliable and user-friendly characterisation of nano-objects in a target\nmaterial is presented here in the form of a software data analysis package for\ninterpreting small-angle X-ray scattering (SAXS) patterns. When provided with\ndata on absolute scale with reasonable uncertainty estimates, the software\noutputs (size) distributions in absolute volume fractions complete with\nuncertainty estimates and minimum evidence limits, and outputs all distribution\nmodes of a user definable range of one or more model parameters. A multitude of\nmodels are included, including prolate and oblate nanoparticles, core-shell\nobjects, polymer models (Gaussian chain and Kholodenko worm) and a model for\ndensely packed spheres (using the LMA-PY approximations). The McSAS software\ncan furthermore be integrated as part of an automated reduction and analysis\nprocedure in laboratory instruments or at synchrotron beamlines.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.1900v1"
    },
    {
        "title": "Enhancing fractal descriptors on images by combining boundary and\n  interior of Minkowski dilation",
        "authors": [
            "Marcos W. S. Oliveira",
            "Dalcimar Casanova",
            "João B. Florindo",
            "Odemir Martinez Bruno"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  This work proposes to obtain novel fractal descriptors from gray-level\ntexture images by combining information from interior and boundary measures of\nthe Minkowski dilation applied to the texture surface. At first, the image is\nconverted into a surface where the height of each point is the gray intensity\nof the respective pixel in that position in the image. Thus, this surface is\nmorphologically dilated by spheres. The radius of such spheres is ranged within\nan interval and the volume and the external area of the dilated structure are\ncomputed for each radius. The final descriptors are given by such measures\nconcatenated and subject to a canonical transform to reduce the dimensionality.\nThe proposal is an enhancement to the classical Bouligand-Minkowski fractal\ndescriptors, where only the volume (interior) information is considered. As\ndifferent structures may have the same volume, but not the same area, the\nproposal yields to more rich descriptors as confirmed by results on the\nclassification of benchmark databases.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.7880v1"
    },
    {
        "title": "Noise-Produced Patterns in Images Constructed from Magnetic Flux Leakage\n  Data",
        "authors": [
            "Anastasiya V. Pimenova",
            "Denis S. Goldobin",
            "Jeremy Levesley",
            "Peter Elkington",
            "Mark Bacciarelli"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Magnetic flux leakage measurements help identify the position, size and shape\nof corrosion-related defects in steel casings used to protect boreholes drilled\ninto oil and gas reservoirs. Images constructed from magnetic flux leakage data\ncontain patterns related to noise inherent in the method. We investigate the\npatterns and their scaling properties for the case of delta-correlated input\nnoise, and consider the implications for the method's ability to resolve\ndefects. The analytical evaluation of the noise-produced patterns is made\npossible by model reduction facilitated by large-scale approximation. With\nappropriate modification, the approach can be employed to analyze\nnoise-produced patterns in other situations where the data of interest are not\nmeasured directly, but are related to the measured data by a complex linear\ntransform involving integrations with respect to spatial coordinates.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.03081v1"
    },
    {
        "title": "Tomographic X-ray data of a walnut",
        "authors": [
            "Keijo Hämäläinen",
            "Lauri Harhanen",
            "Aki Kallonen",
            "Antti Kujanpää",
            "Esa Niemi",
            "Samuli Siltanen"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  This is the documentation of the tomographic X-ray data of a walnut made\navailable at http://www.fips.fi/dataset.php . The data can be freely used for\nscientific purposes with appropriate references to the data and to this\ndocument in arXiv. The data set consists of (1) the X-ray sinogram of a single\n2D slice of the walnut with three different resolutions and (2) the\ncorresponding measurement matrices modeling the linear operation of the X-ray\ntransform. Each of these sinograms was obtained from a measured 120-projection\nfan-beam sinogram by down-sampling and taking logarithms. The original\n(measured) sinogram is also provided in its original form and resolution. In\naddition, a larger set of 1200 projections of the same walnut was measured and\na high-resolution filtered back-projection reconstruction was computed from\nthis data; both the sinogram and the FBP reconstruction are included in the\ndata set, the latter serving as a ground truth reconstruction.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.04064v1"
    },
    {
        "title": "Compounding approach for univariate time series with non-stationary\n  variances",
        "authors": [
            "Rudi Schäfer",
            "Sonja Barkhofen",
            "Thomas Guhr",
            "Hans-Jürgen Stöckmann",
            "Ulrich Kuhl"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  A defining feature of non-stationary systems is the time dependence of their\nstatistical parameters. Measured time series may exhibit Gaussian statistics on\nshort time horizons, due to the central limit theorem. The sample statistics\nfor long time horizons, however, averages over the time-dependent parameters.\nTo model the long-term statistical behavior, we compound the local distribution\nwith the distribution of its parameters. Here we consider two concrete, but\ndiverse examples of such non-stationary systems, the turbulent air flow of a\nfan and a time series of foreign exchange rates. Our main focus is to\nempirically determine the appropriate parameter distribution for the\ncompounding approach. To this end we have to estimate the parameter\ndistribution for univariate time series in a highly non-stationary situation.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.02177v1"
    },
    {
        "title": "Detecting and interpreting distortions in hierarchical organization of\n  complex time series",
        "authors": [
            "Stanisław Drożdż",
            "Paweł Oświęcimka"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Hierarchical organization is a cornerstone of complexity and multifractality\nconstitutes its central quantifying concept. For model uniform cascades the\ncorresponding singularity spectra are symmetric while those extracted from\nempirical data are often asymmetric. Using the selected time series\nrepresenting such diverse phenomena like price changes and inter-transaction\ntimes in the financial markets, sentence length variability in the narrative\ntexts, Missouri River discharge and Sunspot Number variability as examples, we\nshow that the resulting singularity spectra appear strongly asymmetric, more\noften left-sided but in some cases also right-sided. We present a unified view\non the origin of such effects and indicate that they may be crucially\ninformative for identifying composition of the time series. One particularly\nintriguing case of this later kind of asymmetry is detected in the daily\nreported Sunspot Number variability. This signals that either the commonly used\nfamous Wolf formula distorts the real dynamics in expressing the largest\nSunspot Numbers or, if not, that their dynamics is governed by a somewhat\ndifferent mechanism.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.02405v1"
    },
    {
        "title": "Comment on 'Anomalous diffusion induced by enhancement of memory'",
        "authors": [
            "Rüdiger Kürsten"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  In a recent paper [2] the author introduced and investigated a random walk\nmodel similar to a model introduced in [1]. In these models the increment of\nthe random walk depends on the complete past of the process. In this note I\nwill point out that the models considered in [1] and [2] can be mapped onto\neach other one to one. They can be defined on a common probability space and\nhence all expectation values of the model [2] with parameter p are equal to the\nones of [1] with a corresponding parameter $\\tilde{p}$.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.03302v1"
    },
    {
        "title": "A more accurate measurement of the $^{28}$Si lattice parameter",
        "authors": [
            "Enrico Massa",
            "Carlo Paolo Sasso",
            "Giovanni Mana",
            "Carlo Palmisano"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  In 2011, a discrepancy between the values of the Planck constant measured by\ncounting Si atoms and by comparing mechanical and electrical powers prompted a\nreview, among others, of the measurement of the spacing of $^{28}$Si {220}\nlattice planes, either to confirm the measured value and its uncertainty or to\nidentify errors. This exercise confirmed the result of the previous measurement\nand yields the additional value $d_{220}=192014711.98(34)$ am having a reduced\nuncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.06136v2"
    },
    {
        "title": "Analysis of the ensemble Kalman filter for marginal and joint posteriors",
        "authors": [
            "Matthias Morzfeld",
            "Daniel Hodyss"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The ensemble Kalman filter (EnKF) is widely used to sample a probability\ndensity function (pdf) generated by a stochastic model conditioned by noisy\ndata. This pdf can be either a joint posterior that describes the evolution of\nthe state of the system in time, conditioned on all the data up to the present,\nor a particular marginal of this posterior. We show that the EnKF collapses in\nthe same way and under even broader conditions as a particle filter when it\nsamples the joint posterior. However, this does not imply that EnKF collapses\nwhen it samples the marginal posterior. We we show that a localized and\ninflated EnKF can efficiently sample this marginal, and argue that the marginal\nposterior is often the more useful pdf in geophysics. This explains the wide\napplicability of EnKF in this field. We further investigate the typical tuning\nof EnKF, in which one attempts to match the mean square error (MSE) to the\nmarginal posterior variance, and show that sampling error may be huge, even if\nthe MSE is moderate.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.06769v4"
    },
    {
        "title": "Practical Statistics for the LHC",
        "authors": [
            "Kyle Cranmer"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  This document is a pedagogical introduction to statistics for particle\nphysics. Emphasis is placed on the terminology, concepts, and methods being\nused at the Large Hadron Collider. The document addresses both the statistical\ntests applied to a model of the data and the modeling itself.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.07622v1"
    },
    {
        "title": "A Cook Book of Structure Functions",
        "authors": [
            "Aglae Kellerer"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The structure function is a useful quantity to characterize wavefront\ndistortions. We derive expressions for the structure functions of the averaged\nwavefront phase and slopes. The expressions are valid within the inertial range\nof atmospheric turbulence, and are meant to serve as engineering formulae when\nreconstructing profiles of the atmospheric turbulence, specifically in the\ncontext of atmospheric profiling instruments (e.g. SLODAR and S-DIMM+) and\nmulti-conjugate adaptive optical systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.00320v1"
    },
    {
        "title": "Deviation from power law of the global earthquake seismic moment\n  distribution",
        "authors": [
            "Isabel Serra",
            "Álvaro Corral"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The distribution of seismic moment is of capital interest to evaluate\nearthquake hazard, in particular regarding the most extreme events. We make use\nof likelihood-ratio tests to compare the simple Gutenberg-Richter power-law\ndistribution with two statistical models that incorporate an exponential tail:\nthe so-called tapered Gutenberg-Richter and the truncated gamma, when fitted to\nthe global CMT earthquake catalog. The outcome is that the truncated gamma\nmodel outperforms the other two models. If simulated samples of the truncated\ngamma are reshuffled in order to mimic the time occurrence of the order\nstatistics of the empirical data, this model turns out to be able to explain\nthe empirical data both before and after the great Sumatra-Andaman earthquake\nof 2004.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.00378v2"
    },
    {
        "title": "Learning about probabilistic inference and forecasting by playing with\n  multivariate normal distributions",
        "authors": [
            "Giulio D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The properties of the normal distribution under linear transformation, as\nwell the easy way to compute the covariance matrix of marginals and\nconditionals, offer a unique opportunity to get an insight about several\naspects of uncertainties in measurements. The way to build the overall\ncovariance matrix in a few, but conceptually relevant cases is illustrated:\nseveral observations made with (possibly) different instruments measuring the\nsame quantity; effect of systematics (although limited to offset, in order to\nstick to linear models) on the determination of the 'true value', as well in\nthe prediction of future observations; correlations which arise when different\nquantities are measured with the same instrument affected by an offset\nuncertainty; inferences and predictions based on averages; inference about\nconstrained values; fits under some assumptions (linear models with known\nstandard deviations). Many numerical examples are provided, exploiting the\nability of the R language to handle large matrices and to produce high quality\nplots. Some of the results are framed in the general problem of 'propagation of\nevidence', crucial in analyzing graphical models of knowledge.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.02065v1"
    },
    {
        "title": "Natural units and the vector space of physical values",
        "authors": [
            "Gerrit Ansmann"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We explore the mathematical foundations of the vector space of physical\ndimensions introduced in A. Maksymowicz, Am. J. Phys. 44, 1976, and extend this\nformalism to the vector space of physical values. As different unit systems\ncorrespond to different bases of this vector space, our formalism may find use\nfor introducing the concept of natural units and transforming physical values\nbetween unit systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.02332v1"
    },
    {
        "title": "Automated weighing by sequential inference in dynamic environments",
        "authors": [
            "A. D. Martin",
            "T. C. A. Molteno"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We demonstrate sequential mass inference of a suspended bag of milk powder\nfrom simulated measurements of the vertical force component at the pivot while\nthe bag is being filled. We compare the predictions of various sequential\ninference methods both with and without a physics model to capture the system\ndynamics. We find that non-augmented and augmented-state unscented Kalman\nfilters (UKFs) in conjunction with a physics model of a pendulum of varying\nmass and length provide rapid and accurate predictions of the milk powder mass\nas a function of time. The UKFs outperform the other method tested - a particle\nfilter. Moreover, inference methods which incorporate a physics model\noutperform equivalent algorithms which do not.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.06009v1"
    },
    {
        "title": "Combined estimation for multi-measurements of branching ratio",
        "authors": [
            "Xiao-Xia Liu",
            "Xiao-Rui Lyu",
            "Yong-Sheng Zhu"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  A maximum likelihood method is used to deal with the combined estimation of\nmulti-measurements of a branching ratio, where each result can be presented as\nan upper limit. The joint likelihood function is constructed using observed\nspectra of all measurements and the combined estimate of the branching ratio is\nobtained by maximizing the joint likelihood function. The Bayesian credible\ninterval, or upper limit of the combined branching ratio, is given in cases\nboth with and without inclusion of systematic error.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.01278v1"
    },
    {
        "title": "On the ambiguity of determination of interfering resonances parameters",
        "authors": [
            "V. M. Malyshev"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The general form of solutions for parameters of interfering Breit-Wigner\nresonances is found. The number of solutions is determined by the properties of\nroots of corresponding characteristic equation and does not exceed $2^{N-1}$,\nwhere $N$ is the number of resonances. For resonances of more complicated form,\nprovided that their amplitudes satisfy certain conditions, for any $N\\ge2$\nmultiple solutions also exist.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.01509v1"
    },
    {
        "title": "Search for a Lorentz invariant velocity distribution of a relativistic\n  gas",
        "authors": [
            "Evaldo M. F. Curado",
            "Felipe T. L. Germani",
            "Ivano Damiao Soares"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We examine numerically and analytically the problem of the relativistic\nvelocity distribution in a 1-dim relativistic gas in thermal equilibrium. Our\nderivation is based on the special theory of relativity, the central limit\ntheorem and the Lobachevsky structure of the velocity space of the theory,\nwhere the rapidity variable plays a crucial role. For v^2/c^2 << 1 and 1/\\beta\n= k_B T/ m_0 c^2 << 1 the distribution tends to the Maxwell-Boltzmann\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.03409v2"
    },
    {
        "title": "Model selection for amplitude analysis",
        "authors": [
            "Baptiste Guegan",
            "John Hardin",
            "Justin Stevens",
            "Mike Williams"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Model complexity in amplitude analyses is often a priori under-constrained\nsince the underlying theory permits a large number of possible amplitudes to\ncontribute to most physical processes. The use of an overly complex model\nresults in reduced predictive power and worse resolution on unknown parameters\nof interest. Therefore, it is common to reduce the complexity by removing from\nconsideration some subset of the allowed amplitudes. This paper studies a\nmethod for limiting model complexity from the data sample itself through\nregularization during regression in the context of a multivariate (Dalitz-plot)\nanalysis. The regularization technique applied greatly improves the\nperformance. An outline of how to obtain the significance of a resonance in a\nmultivariate amplitude analysis is also provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.05133v2"
    },
    {
        "title": "A highly specific test for periodicity",
        "authors": [
            "Gerrit Ansmann"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We present a method that allows to distinguish between nearly periodic and\nstrictly periodic time series. To this purpose, we employ a conservative\ncriterion for periodicity, namely that the time series can be interpolated by a\nperiodic function whose local extrema are also present in the time series. Our\nmethod is intended for the analysis of time series generated by deterministic\ntime-continuous dynamical systems, where it can help telling periodic dynamics\nfrom chaotic or transient ones. We empirically investigate our method's\nperformance and compare it to an approach based on marker events (or Poincar\\'e\nsections). We demonstrate that our method is capable of detecting small\ndeviations from periodicity and outperforms the marker-event-based approach in\ntypical situations. Our method requires no adjustment of parameters to the\nindividual time series, yields the period length with a precision that exceeds\nthe sampling rate, and its runtime grows asymptotically linear with the length\nof the time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.01287v4"
    },
    {
        "title": "Allan variance computed in space domain: Application to InSAR data to\n  characterize noise and geophysical signal",
        "authors": [
            "Olivier Cavalié",
            "François Vernotte"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The Allan variance was introduced fifty years ago for analyzing the stability\nof frequency standards. Beside its metrological interest, it is also an\nestimator of the large trends of the power spectral density (PSD) of frequency\ndeviation. For instance, the Allan variance is able to discriminate different\ntypes of noise characterized by different power laws in the PSD. But, it was\nalso used in other fields: accelerometry, geophysics, geodesy, astrophysics and\neven finances! However, it seems that up to now, it has been exclusively\napplied for time series analysis. We propose here to use the Allan variance\nonto spatial data.\n  Interferometric synthetic aperture radar (InSAR) is used in geophysics to\nimage ground displacements in space (over the SAR image spatial coverage) and\nin time thank to the regular SAR image acquisitions by dedicated satellites.\nThe main limitation of the technique is the atmospheric disturbances that\naffect the radar signal while traveling from the sensor to the ground and back.\nIn this paper, we propose to use the Allan variance for analyzing spatial data\nfrom InSAR measurements. The Allan variance was computed in XY mode as well as\nin radial mode for detecting different types of behavior for different\nspace-scales, in the same way as the different types of noise versus the\nintegration time in the classical time and frequency application. We found that\nradial AVAR is the more appropriate way to have an estimator insensitive to the\nspatial axis and we applied it on SAR data acquired over eastern Turkey for the\nperiod 2003-2011. Space AVAR allowed to well characterize noise features,\nclassically found in InSAR such as phase decorrelation producing white noise or\natmospheric delays, behaving like a random walk signal. We finally applied the\nspace AVAR to an InSAR time series to detect when the geophysical signal, here\nthe ground motion, emerges from the noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.01322v1"
    },
    {
        "title": "Non-parametric causal inference for bivariate time series",
        "authors": [
            "James M. McCracken",
            "Robert S. Weigel"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We introduce new quantities for exploratory causal inference between\nbivariate time series. The quantities, called penchants and leanings, are\ncomputationally straightforward to apply, follow directly from assumptions of\nprobabilistic causality, do not depend on any assumed models for the time\nseries generating process, and do not rely on any embedding procedures; these\nfeatures may provide a clearer interpretation of the results than those from\nexisting time series causality tools. The penchant and leaning are computed\nbased on a structured method for computing probabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.06823v2"
    },
    {
        "title": "A conjugate subgradient algorithm with adaptive preconditioning for\n  LASSO minimization",
        "authors": [
            "Alessandro Mirone",
            "Pierre Paleo"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  This paper describes a new efficient conjugate subgradient algorithm which\nminimizes a convex function containing a least squares fidelity term and an\nabsolute value regularization term. This method is successfully applied to the\ninversion of ill-conditioned linear problems, in particular for computed\ntomography with the dictionary learning method. A comparison with other\nstate-of-art methods shows a significant reduction of the number of iterations,\nwhich makes this algorithm appealing for practical use.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.07730v2"
    },
    {
        "title": "Detrended fluctuation analysis made flexible to detect range of\n  cross-correlated fluctuations",
        "authors": [
            "Jaroslaw Kwapien",
            "Pawel Oswiecimka",
            "Stanislaw Drozdz"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The detrended cross-correlation coefficient $\\rho_{\\rm DCCA}$ has recently\nbeen proposed to quantify the strength of cross-correlations on different\ntemporal scales in bivariate, non-stationary time series. It is based on the\ndetrended cross-correlation and detrended fluctuation analyses (DCCA and DFA,\nrespectively) and can be viewed as an analogue of the Pearson coefficient in\nthe case of the fluctuation analysis. The coefficient $\\rho_{\\rm DCCA}$ works\nwell in many practical situations but by construction its applicability is\nlimited to detection of whether two signals are generally cross-correlated,\nwithout possibility to obtain information on the amplitude of fluctuations that\nare responsible for those cross-correlations. In order to introduce some\nrelated flexibility, here we propose an extension of $\\rho_{\\rm DCCA}$ that\nexploits the multifractal versions of DFA and DCCA: MFDFA and MFCCA,\nrespectively. The resulting new coefficient $\\rho_q$ not only is able to\nquantify the strength of correlations, but also it allows one to identify the\nrange of detrended fluctuation amplitudes that are correlated in two signals\nunder study. We show how the coefficient $\\rho_q$ works in practical situations\nby applying it to stochastic time series representing processes with long\nmemory: autoregressive and multiplicative ones. Such processes are often used\nto model signals recorded from complex systems and complex physical phenomena\nlike turbulence, so we are convinced that this new measure can successfully be\napplied in time series analysis. In particular, we present an example of such\napplication to highly complex empirical data from financial markets. The\npresent formulation can straightforwardly be extended to multivariate data in\nterms of the $q$-dependent counterpart of the correlation matrices and then to\nthe network representation.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.08692v2"
    },
    {
        "title": "Chi-square Fitting When Overall Normalization is a Fit Parameter",
        "authors": [
            "Byron Roe"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The problem of fitting an event distribution when the total expected number\nof events is not fixed, keeps appearing in experimental studies. In a\nchi-square fit, if overall normalization is one of the parameters parameters to\nbe fit, the fitted curve may be seriously low with respect to the data points,\nsometimes below all of them. This problem and the solution for it are well\nknown within the statistics community, but, apparently, not well known among\nsome of the physics community. The purpose of this note is didactic, to explain\nthe cause of the problem and the easy and elegant solution. The solution is to\nuse maximum likelihood instead of chi-square. The essential difference between\nthe two approaches is that maximum likelihood uses the normalization of each\nterm in the chi-square assuming it is a normal distribution, 1/sqrt(2 pi\nsigma-square). In addition, the normalization is applied to the theoretical\nexpectation not to the data. In the present note we illustrate what goes wrong\nand how maximum likelihood fixes the problem in a very simple toy example which\nillustrates the problem clearly and is the appropriate physics model for event\nhistograms. We then note how a simple modification to the chi-square method\ngives a result identical to the maximum likelihood method.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.09077v1"
    },
    {
        "title": "Bounds of memory strength for power-law series",
        "authors": [
            "Fangjian Guo",
            "Dan Yang",
            "Zimo Yang",
            "Zhi-Dan Zhao",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Many time series produced by complex systems are empirically found to follow\npower-law distributions with different exponents $\\alpha$. By permuting the\nindependently drawn samples from a power-law distribution, we present\nnon-trivial bounds on the memory strength (1st-order autocorrelation) as a\nfunction of $\\alpha$, which are markedly different from the ordinary $\\pm 1$\nbounds for Gaussian or uniform distributions. When $1 < \\alpha \\leq 3$, as\n$\\alpha$ grows bigger, the upper bound increases from 0 to +1 while the lower\nbound remains 0; when $\\alpha > 3$, the upper bound remains +1 while the lower\nbound descends below 0. Theoretical bounds agree well with numerical\nsimulations. Based on the posts on Twitter, ratings of MovieLens, calling\nrecords of the mobile operator Orange, and browsing behavior of Taobao, we find\nthat empirical power-law distributed data produced by human activities obey\nsuch constraints. The present findings explain some observed constraints in\nbursty time series and scale-free networks, and challenge the validity of\nmeasures like autocorrelation and assortativity coefficient in heterogeneous\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.09096v6"
    },
    {
        "title": "Estimation of the TQ-complexity of chaotic sequences",
        "authors": [
            "A. V. Makarenko"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  A new approach is proposed to the quantitative estimation of the complexity\nof multidimensional discrete sequences in terms of the shapes of their\ntrajectories in the extended space of states. This approach is based on the\nstudy of the structural properties of sequences and is suitable for estimating\nthe complexity of both chaotic and stochastic sequences. It is constructed on\nthe method, proposed earlier by the author, of symbolic CTQ-analysis of\nmultidimensional discrete sequences and mappings. The algorithm proposed\nmanipulates not only the frequency of occurrence of symbols, but also takes\ninto account their sequence order. An example (financial time series) is given\nthat demonstrates the application of the tools developed.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.09103v1"
    },
    {
        "title": "Extending the square root method to account for additive forecast noise\n  in ensemble methods",
        "authors": [
            "Patrick N. Raanes",
            "Alberto Carrassi",
            "Laurent Bertino"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  A square root approach is considered for the problem of accounting for model\nnoise in the forecast step of the ensemble Kalman filter (EnKF) and related\nalgorithms. The primary aim is to replace the method of simulated,\npseudo-random, additive noise so as to eliminate the associated sampling\nerrors. The core method is based on the analysis step of ensemble square root\nfilters, and consists in the deterministic computation of a transform matrix.\nThe theoretical advantages regarding dynamical consistency are surveyed,\napplying equally well to the square root method in the analysis step. A\nfundamental problem due to the limited size of the ensemble subspace is\ndiscussed, and novel solutions that complement the core method are suggested\nand studied. Benchmarks from twin experiments with simple, low-order dynamics\nindicate improved performance over standard approaches such as additive,\nsimulated noise and multiplicative inflation.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.06201v1"
    },
    {
        "title": "Detecting changes in maps of gamma spectra with Kolmogorov-Smirnov tests",
        "authors": [
            "Alex Reinhart",
            "Valérie Ventura",
            "Alex Athey"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Various security, regulatory, and consequence management agencies are\ninterested in continuously monitoring wide areas for unexpected changes in\nradioactivity. Existing detection systems are designed to search for\nradioactive sources but are not suited to repeat mapping and change detection.\nUsing a set of daily spectral observations collected at the Pickle Research\nCampus, we improved on the prior Spectral Comparison Ratio Anomaly Mapping\n(SCRAM) algorithm and developed a new method based on two-sample\nKolmogorov-Smirnov tests to detect sudden spectral changes. We also designed\nsimulations and visualizations of statistical power to compare methods and\nguide deployment scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.06954v2"
    },
    {
        "title": "Fractional Mellin Transform -- A possible application in CFT",
        "authors": [
            "R. A. Treumann",
            "W. Baumjohann"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We propose a fractional variant of Mellin's transform which may find an\napplication in the Conformal Field Theory. Its advantage is the presence of an\narbitrary parameter which may substantially simplify calculations and help\nadjusting convergence.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.04579v1"
    },
    {
        "title": "ROOT - A C++ Framework for Petabyte Data Storage, Statistical Analysis\n  and Visualization",
        "authors": [
            "Ilka Antcheva",
            "Maarten Ballintijn",
            "Bertrand Bellenot",
            "Marek Biskup",
            "Rene Brun",
            "Nenad Buncic",
            "Philippe Canal",
            "Diego Casadei",
            "Olivier Couet",
            "Valery Fine",
            "Leandro Franco",
            "Gerardo Ganis",
            "Andrei Gheata",
            "David Gonzalez Maline",
            "Masaharu Goto",
            "Jan Iwaszkiewicz",
            "Anna Kreshuk",
            "Diego Marcos Segura",
            "Richard Maunder",
            "Lorenzo Moneta",
            "Axel Naumann",
            "Eddy Offermann",
            "Valeriy Onuchin",
            "Suzanne Panacek",
            "Fons Rademakers",
            "Paul Russo",
            "Matevz Tadel"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  ROOT is an object-oriented C++ framework conceived in the high-energy physics\n(HEP) community, designed for storing and analyzing petabytes of data in an\nefficient way. Any instance of a C++ class can be stored into a ROOT file in a\nmachine-independent compressed binary format. In ROOT the TTree object\ncontainer is optimized for statistical data analysis over very large data sets\nby using vertical data storage techniques. These containers can span a large\nnumber of files on local disks, the web, or a number of different shared file\nsystems. In order to analyze this data, the user can chose out of a wide set of\nmathematical and statistical functions, including linear algebra classes,\nnumerical algorithms such as integration and minimization, and various methods\nfor performing regression analysis (fitting). In particular, ROOT offers\npackages for complex data modeling and fitting, as well as multivariate\nclassification based on machine learning techniques. A central piece in these\nanalysis tools are the histogram classes which provide binning of one- and\nmulti-dimensional data. Results can be saved in high-quality graphical formats\nlike Postscript and PDF or in bitmap formats like JPG or GIF. The result can\nalso be stored into ROOT macros that allow a full recreation and rework of the\ngraphics. Users typically create their analysis macros step by step, making use\nof the interactive C++ interpreter CINT, while running over small data samples.\nOnce the development is finished, they can run these macros at full compiled\nspeed over large data sets, using on-the-fly compilation, or by creating a\nstand-alone batch program. Finally, if processing farms are available, the user\ncan reduce the execution time of intrinsically parallel tasks - e.g. data\nmining in HEP - by using PROOF, which will take care of optimally distributing\nthe work over the available resources in a transparent way.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.07749v1"
    },
    {
        "title": "Stochastic modeling and performance monitoring of wind farm power\n  production",
        "authors": [
            "Patrick Milan",
            "Matthias Wächter",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We present a new stochastic approach to describe and remodel the conversion\nprocess of a wind farm at a sampling frequency of 1Hz. When conditioning on\nvarious wind direction sectors, the dynamics of the conversion process appear\nas a fluctuating trajectory around an average IEC-like power curve, see section\nII. Our approach is to consider the wind farm as a dynamical system that can be\ndescribed as a stochastic drift/diffusion model, where a drift coefficient\ndescribes the attraction towards the power curve and a diffusion coefficient\nquantifies additional turbulent fluctuations. These stochastic coefficients are\ninserted into a Langevin equation that, once properly adapted to our particular\nsystem, models a synthetic signal of power output for any given wind\nspeed/direction signals, see section III. When combined with a pre-model for\nturbulent wind fluctuations, the stochastic approach models the power output of\nthe wind farm at a sampling frequency of 1Hz using only ten-minute average\nvalues of wind speed and directions. The stochastic signals generated are\ncompared to the measured signal, and show a good statistical agreement,\nincluding a proper reproduction of the intermittent, gusty features measured.\nIn parallel, a second application for performance monitoring is introduced in\nsection IV. The drift coefficient can be used as a sensitive measure of the\nglobal wind farm performance. When monitoring the wind farm as a whole, the\ndrift coefficient registers some significant deviation from normal operation if\none of twelve wind turbines is shut down during less than 4% of the time. Also,\nintermittent anomalies can be detected more rapidly than when using ten-minute\naveraging methods. Finally, a probabilistic description of the conversion\nprocess is proposed and modeled in appendix A, that can in turn be used to\nfurther improve the estimation of the stochastic coefficients.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.03061v1"
    },
    {
        "title": "Stochastic Bifurcations induced by correlated Noise in a Birhythmic van\n  der Pol System",
        "authors": [
            "R. Mbakob Yonkeu",
            "R. Yamapi",
            "G. Filatrella",
            "C. Tchawoua"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We investigate the effects of exponentially correlated noise on birhythmic\nvan der Pol type oscillators. The analytical results are obtained applying the\nquasi-harmonic assumption to the Langevin equation to derive an approximated\nFokker-Planck equation. This approach allows to analytically derive the\nprobability distributions as well as the activation energies associated to\nswitching between coexisting attractors. The stationary probability density\nfunction of the van der Pol oscillator reveals the influence of the correlation\ntime on the dynamics. Stochastic bifurcations are discussed through a\nqualitative change of the stationary probability distribution, which indicates\nthat noise intensity and correlation time can be treated as bifurcation\nparameters. Comparing the analytical and numerical results, we find good\nagreement both when the frequencies of the attractors are about equal or when\nthey are markedly different.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.07290v1"
    },
    {
        "title": "Time reversibility from visibility graphs of non-stationary processes",
        "authors": [
            "Lucas Lacasa",
            "Ryan Flanagan"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Visibility algorithms are a family of methods to map time series into\nnetworks, with the aim of describing the structure of time series and their\nunderlying dynamical properties in graph-theoretical terms. Here we explore\nsome properties of both natural and horizontal visibility graphs associated to\nseveral non-stationary processes, and we pay particular attention to their\ncapacity to assess time irreversibility. Non-stationary signals are\n(infinitely) irreversible by definition (independently of whether the process\nis Markovian or producing entropy at a positive rate), and thus the link\nbetween entropy production and time series irreversibility has only been\nexplored in non-equilibrium stationary states. Here we show that the visibility\nformalism naturally induces a new working definition of time irreversibility,\nwhich allows to quantify several degrees of irreversibility for stationary and\nnon-stationary series, yielding finite values that can be used to efficiently\nassess the presence of memory and off-equilibrium dynamics in non-stationary\nprocesses without needs to differentiate or detrend them. We provide rigorous\nresults complemented by extensive numerical simulations on several classes of\nstochastic processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.01318v1"
    },
    {
        "title": "Characterization of river flow fluctuations via horizontal visibility\n  graphs",
        "authors": [
            "A. C. Braga",
            "L. G. A. Alves",
            "L. S. Costa",
            "A. A. Ribeiro",
            "M. M. A. de Jesus",
            "A. A. Tateishi",
            "H. V. Ribeiro"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We report on a large-scale characterization of river discharges by employing\nthe network framework of the horizontal visibility graph. By mapping daily time\nseries from 141 different stations of 53 Brazilian rivers into complex\nnetworks, we present an useful approach for investigating the dynamics of river\nflows. We verified that the degree distributions of these networks were well\ndescribed by exponential functions, where the characteristic exponents are\nalmost always larger than the value obtained for random time series. The\nfaster-than-random decay of the degree distributions is an another evidence\nthat the fluctuation dynamics underlying the river discharges has a long-range\ncorrelated nature. We further investigated the evolution of the river\ndischarges by tracking the values of the characteristic exponents (of the\ndegree distribution) and the global clustering coefficients of the networks\nover the years. We show that the river discharges in several stations have\nevolved to become more or less correlated (and displaying more or less complex\ninternal network structures) over the years, a behavior that could be related\nto changes in the climate system and other man-made phenomena.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.07009v2"
    },
    {
        "title": "Big Data Is not just a New Type, but a New Paradigm",
        "authors": [
            "Bin Jiang"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  This paper is a first draft of the introduction to the special issue on\nvolunteered geographic information published in Computers, Environment and\nUrban Systems (2015, 53, 1-122). In this short paper, I put georeferenced big\ndata (hereafter, big data) such as tweets locations in comparison with small\ndata such as census data in terms of data characteristics, and further argued\nthat big data differs fundamentally from small data in terms of data analytics,\nboth geometrially and statistically. I would like to thank my colleague Dr.\nJean-Claude Thill, who expanded the draft towards a broader scope.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.01242v1"
    },
    {
        "title": "Negative Probabilities and Contextuality",
        "authors": [
            "J. Acacio de Barros",
            "Janne Kujala",
            "Gary Oas"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  There has been a growing interest, both in physics and psychology, in\nunderstanding contextuality in experimentally observed quantities. Different\napproaches have been proposed to deal with contextual systems, and a promising\none is contextuality-by-default, put forth by Dzhafarov and Kujala. The goal of\nthis paper is to present a tutorial on a different approach: negative\nprobabilities. We do so by presenting the overall theory of negative\nprobabilities in a way that is consistent with contextuality-by-default and by\nexamining with this theory some simple examples where contextuality appears,\nboth in physics and psychology.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.02823v1"
    },
    {
        "title": "Hyperspectral Unmixing in Presence of Endmember Variability,\n  Nonlinearity or Mismodelling Effects",
        "authors": [
            "Abderrahim Halimi",
            "Paul Honeine",
            "Jose Bioucas-Dias"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  This paper presents three hyperspectral mixture models jointly with Bayesian\nalgorithms for supervised hyperspectral unmixing. Based on the residual\ncomponent analysis model, the proposed general formulation assumes the linear\nmodel to be corrupted by an additive term whose expression can be adapted to\naccount for nonlinearities (NL), endmember variability (EV), or mismodelling\neffects (ME). The NL effect is introduced by considering a polynomial\nexpression that is related to bilinear models. The proposed new formulation of\nEV accounts for shape and scale endmember changes while enforcing a smooth\nspectral/spatial variation. The ME formulation takes into account the effect of\noutliers and copes with some types of EV and NL. The known constraints on the\nparameter of each observation model are modeled via suitable priors. The\nposterior distribution associated with each Bayesian model is optimized using a\ncoordinate descent algorithm which allows the computation of the maximum a\nposteriori estimator of the unknown model parameters. The proposed mixture and\nBayesian models and their estimation algorithms are validated on both synthetic\nand real images showing competitive results regarding the quality of the\ninferences and the computational complexity when compared to the\nstate-of-the-art algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.05698v2"
    },
    {
        "title": "Algorithms for Identification of Nearly-Coincident Events in\n  Calorimetric Sensors",
        "authors": [
            "B. Alpert",
            "E. Ferri",
            "D. Bennett",
            "M. Faverzani",
            "J. Fowler",
            "A. Giachero",
            "J. Hays-Wehle",
            "M. Maino",
            "A. Nucciotti",
            "A. Puiu",
            "D. Swetz",
            "J. Ullom"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  For experiments with high arrival rates, reliable identification of\nnearly-coincident events can be crucial. For calorimetric measurements to\ndirectly measure the neutrino mass such as HOLMES, unidentified pulse pile-ups\nare expected to be a leading source of experimental error. Although Wiener\nfiltering can be used to recognize pile-up, it suffers errors due to\npulse-shape variation from detector nonlinearity, readout dependence on\nsub-sample arrival times, and stability issues from the ill-posed deconvolution\nproblem of recovering Dirac delta-functions from smooth data. Due to these\nfactors, we have developed a processing method that exploits singular value\ndecomposition to (1) separate single-pulse records from piled-up records in\ntraining data and (2) construct a model of single-pulse records that accounts\nfor varying pulse shape with amplitude, arrival time, and baseline level,\nsuitable for detecting nearly-coincident events. We show that the resulting\nprocessing advances can reduce the required performance specifications of the\ndetectors and readout system or, equivalently, enable larger sensor arrays and\nbetter constraints on the neutrino mass.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.01608v1"
    },
    {
        "title": "Complex Network Approach to Fractional Time Series",
        "authors": [
            "Pouya Manshour"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  In order to extract correlation information inherited in stochastic time\nseries, the visibility graph algorithm has been recently proposed, by which a\ntime series can be mapped onto a complex network. We demonstrate that the\nvisibility algorithm is not an appropriate one to study the correlation aspects\nof a time series. We then employ the horizontal visibility algorithm, as a much\nsimpler one, to map fractional processes onto complex networks. The degree\ndistributions are shown to have parabolic exponential forms with Hurst\ndependent fitting parameter. Further, we take into account other topological\nproperties such as maximum eigenvalue of the adjacency matrix and the degree\nassortativity, and show that such topological quantities can also be used to\npredict the Hurst exponent, with an exception for anti-persistent fractional\nGaussian noises. To solve this problem, we take into account the Spearman\ncorrelation coefficient between nodes' degrees and their corresponding data\nvalues in the original time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.08205v2"
    },
    {
        "title": "On causality of extreme events",
        "authors": [
            "Massimiliano Zanin"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Multiple metrics have been developed to detect causality relations between\ndata describing the elements constituting complex systems, all of them\nconsidering their evolution through time. Here we propose a metric able to\ndetect causality within static data sets, by analysing how extreme events in\none element correspond to the appearance of extreme events in a second one. The\nmetric is able to detect non- linear causalities; to analyse both\ncross-sectional and longitudinal data sets; and to discriminate between real\ncausalities and correlations caused by confounding factors. We validate the\nmetric through synthetic data, dynamical and chaotic systems, and data\nrepresenting the human brain activity in a cognitive task. We further show how\nthe proposed metric is able to outperform classical causality metrics, provided\nnon-linear relationships are present and large enough data sets are available.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.07054v3"
    },
    {
        "title": "Dynamic system classifier",
        "authors": [
            "Daniel Pumpe",
            "Maksim Greiner",
            "Ewald Müller",
            "Torsten A. Enßlin"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Stochastic differential equations describe well many physical, biological and\nsociological systems, despite the simplification often made in their\nderivation. Here the usage of simple stochastic differential equations to\ncharacterize and classify complex dynamical systems is proposed within a\nBayesian framework. To this end, we develop a dynamic system classifier (DSC).\nThe DSC first abstracts training data of a system in terms of time dependent\ncoefficients of the descriptive stochastic differential equation. Thereby the\nDSC identifies unique correlation structures within the training data. For\ndefiniteness we restrict the presentation of DSC to oscillation processes with\na time dependent frequency {\\omega}(t) and damping factor {\\gamma}(t). Although\nreal systems might be more complex, this simple oscillator captures many\ncharacteristic features. The {\\omega} and {\\gamma} timelines represent the\nabstract system characterization and permit the construction of efficient\nsignal classifiers. Numerical experiments show that such classifiers perform\nwell even in the low signal-to-noise regime.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.07901v2"
    },
    {
        "title": "Big Data Processing in Complex Hierarchical Network Systems",
        "authors": [
            "Olexandr Polishchuk",
            "Dmytro Polishchuk",
            "Maria Tyutyunnyk",
            "Mykhailo Yadzhak"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  This article covers the problem of processing of Big Data that describe\nprocess of complex networks and network systems operation. It also introduces\nthe notion of hierarchical network systems combination into associations and\nconglomerates alongside with complex networks combination into multiplexes. The\nanalysis is provided for methods of global network structures study depending\non the purpose of the research. Also the main types of information flows in\ncomplex hierarchical network systems being the basic components of associations\nand conglomerates are covered. Approaches are proposed for creation of\nefficient computing environments, distributed computations organization and\ninformation processing methods parallelization at different levels of system\nhierarchy.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.00633v3"
    },
    {
        "title": "The Laura++ Dalitz plot fitter",
        "authors": [
            "Thomas Latham"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The Laura++ software package is designed for performing fits of amplitude\nmodels to data from decays of spin-0 particles into final states containing\nthree spin-0 particles - so-called Dalitz-plot analysis. An overview of the\namplitude formalism and its software implementation is given as well as\nsummaries of recently added features and planned future developments.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.00752v1"
    },
    {
        "title": "Photographic dataset: random peppercorns",
        "authors": [
            "Teemu Helenius",
            "Samuli Siltanen"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  This is a photographic dataset collected for testing image processing\nalgorithms. The idea is to have sets of different but statistically similar\nimages. In this work the images show randomly distributed peppercorns. The\ndataset is made available at www.fips.fi/photographic_dataset.php .\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01046v1"
    },
    {
        "title": "Modelling, controlling, predicting blackouts",
        "authors": [
            "Chengwei Wang",
            "Celso Grebogi",
            "Murilo S. Baptista"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The electric power system is one of the cornerstones of modern society. One\nof its most serious malfunctions is the blackout, a catastrophic event that may\ndisrupt a substantial portion of the system, playing havoc to human life and\ncausing great economic losses. Thus, understanding the mechanisms leading to\nblackouts and creating a reliable and resilient power grid has been a major\nissue, attracting the attention of scientists, engineers and stakeholders. In\nthis paper, we study the blackout problem in power grids by considering a\npractical phase-oscillator model. This model allows one to simultaneously\nconsider different types of power sources (e.g., traditional AC power plants\nand renewable power sources connected by DC/AC inverters) and different types\nof loads (e.g., consumers connected to distribution networks and consumers\ndirectly connected to power plants). We propose two new control strategies\nbased on our model, one for traditional power grids, and another one for smart\ngrids. The control strategies show the efficient function of the fast-response\nenergy storage systems in preventing and predicting blackouts in smart grids.\nThis work provides innovative ideas which help us to build up a robuster and\nmore economic smart power system.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01509v1"
    },
    {
        "title": "Statistical Uncertainty in Quantitative Neutron Radiography",
        "authors": [
            "Florian M. Piegsa",
            "Anders P. Kaestner",
            "Aldo Antognini",
            "Andreas Eggenberger",
            "Klaus Kirch",
            "Gunther Wichmann"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We demonstrate a novel procedure to calibrate neutron detection systems\ncommonly used in standard neutron radiography. This calibration allows\ndetermining the uncertainties due to Poisson-like neutron counting statistics\nfor each individual pixel of a radiographic image. The obtained statistical\nerrors are necessary in order to perform a correct quantitative analysis. This\nfast and convenient method is applied to data measured at the cold neutron\nradiography facility ICON at the Paul Scherrer Institute. Moreover, from the\nresults the effective neutron flux at the beam line is determined.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.06849v2"
    },
    {
        "title": "Percolation thresholds of two-dimensional continuum systems of\n  rectangles [with erratum]",
        "authors": [
            "Jiantong Li",
            "Mikael Östling"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The present work introduces an efficient Monte Carlo algorithm for continuum\npercolation composed of randomly-oriented rectangles. By conducting extensive\nsimulations, we report high precision percolation thresholds for a variety of\nhomogeneous systems with different rectangle aspect ratios. This work verifies\nand extends the excluded area theory. It is confirmed that percolation\nthresholds are dominated by the average excluded areas for both homogeneous and\nheterogeneous rectangle systems (except for some special heterogeneous systems\nwhere the rectangle lengths differ too much from one another). In terms of the\nexcluded areas, generalized formulae are proposed to effectively predict\nprecise percolation thresholds for all these rectangle systems. This work is\ntherefore helpful for both practical applications and theoretical studies\nconcerning relevant systems.\n  The Erratum addresses errors in our earlier paper \"Percolation thresholds of\ntwo-dimensional continuum systems of rectangles\" published in [Phys. Rev. E 88,\n012101 (2013)].\n",
        "pdf_link": "http://arxiv.org/pdf/1604.01315v1"
    },
    {
        "title": "First-passage phenomena in hierarchical networks",
        "authors": [
            "Flavia Tavani",
            "Elena Agliari"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In this paper we study Markov processes and related first passage problems on\na class of weighted, modular graphs which generalize the Dyson hierarchical\nmodel. In these networks, the coupling strength between two nodes depends on\ntheir distance and is modulated by a parameter $\\sigma$. We find that, in the\nthermodynamic limit, ergodicity is lost and the \"distant\" nodes can not be\nreached. Moreover, for finite-sized systems, there exists a threshold value for\n$\\sigma$ such that, when $\\sigma$ is relatively large, the inhomogeneity of the\ncoupling pattern prevails and \"distant\" nodes are hardly reached. The same\nanalysis is carried on also for generic hierarchical graphs, where interactions\nare meant to involve $p$-plets ($p>2$) of nodes, finding that ergodicity is\nstill broken in the thermodynamic limit, but no threshold value for $\\sigma$ is\nevidenced, ultimately due to a slow growth of the network diameter with the\nsize.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.06266v1"
    },
    {
        "title": "Ubiquity of Benfords law and emergence of the reciprocal distribution",
        "authors": [
            "J. L. Friar",
            "T. Goldman",
            "J. Perez-Mercader"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We apply the Law of Total Probability to the construction of scale-invariant\nprobability distribution functions (pdfs), and require that probability\nmeasures be dimensionless and unitless under a continuous change of scales. If\nthe scale-change distribution function is scale invariant then the constructed\ndistribution will also be scale invariant. Repeated application of this\nconstruction on an arbitrary set of (normalizable) pdfs results again in\nscale-invariant distributions. The invariant function of this procedure is\ngiven uniquely by the reciprocal distribution, suggesting a kind of\nuniversality. We separately demonstrate that the reciprocal distribution\nresults uniquely from requiring maximum entropy for size-class distributions\nwith uniform bin sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.07391v1"
    },
    {
        "title": "Fractional Brownian motion time-changed by gamma and inverse gamma\n  process",
        "authors": [
            "A. Kumar",
            "A. Wyłomańska",
            "R. Połoczański",
            "S. Sundar"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Many real time-series exhibit behavior adequate to long range dependent data.\nAdditionally very often these time-series have constant time periods and also\nhave characteristics similar to Gaussian processes although they are not\nGaussian. Therefore there is need to consider new classes of systems to model\nthese kind of empirical behavior. Motivated by this fact in this paper we\nanalyze two processes which exhibit long range dependence property and have\nadditional interesting characteristics which may be observed in real phenomena.\nBoth of them are constructed as the superposition of fractional Brownian motion\n(FBM) and other process. In the first case the internal process, which plays\nrole of the time, is the gamma process while in the second case the internal\nprocess is its inverse. We present in detail their main properties paying main\nattention to the long range dependence property. Moreover, we show how to\nsimulate these processes and estimate their parameters. We propose to use a\nnovel method based on rescaled modified cumulative distribution function for\nestimation of parameters of the second considered process. This method is very\nuseful in description of rounded data, like waiting times of subordinated\nprocesses delayed by inverse subordinators. By using the Monte Carlo method we\nshow the effectiveness of proposed estimation procedures.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.00086v1"
    },
    {
        "title": "Canonical Horizontal Visibility Graphs are uniquely determined by their\n  degree sequence",
        "authors": [
            "Bartolo Luque",
            "Lucas Lacasa"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Horizontal visibility graphs (HVGs) are graphs constructed in correspondence\nwith number sequences that have been introduced and explored recently in the\ncontext of graph-theoretical time series analysis. In most of the cases simple\nmeasures based on the degree sequence (or functionals of these such as\nentropies over degree and joint degree distributions) appear to be highly\ninformative features for automatic classification and provide nontrivial\ninformation on the associated dynam- ical process, working even better than\nmore sophisticated topological metrics. It is thus an open question why these\nseemingly simple measures capture so much information. Here we prove that,\nunder suitable conditions, there exist a bijection between the adjacency matrix\nof an HVG and its degree sequence, and we give an explicit construction of such\nbijection. As a consequence, under these conditions HVGs are unigraphs and the\ndegree sequence fully encapsulates all the information of these graphs, thereby\ngiving a plausible reason for its apparently unreasonable effectiveness.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.05222v1"
    },
    {
        "title": "A deep convolutional neural network approach to single-particle\n  recognition in cryo-electron microscopy",
        "authors": [
            "Yanan Zhu",
            "Qi Ouyang",
            "Youdong Mao"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Background: Single-particle cryo-electron microscopy (cryo-EM) has become a\npopular tool for structural determination of biological macromolecular\ncomplexes. High-resolution cryo-EM reconstruction often requires hundreds of\nthousands of single-particle images. Particle extraction from experimental\nmicrographs thus can be laborious and presents a major practical bottleneck in\ncryo-EM structural determination. Existing computational methods of particle\npicking often use low-resolution templates as inputs for particle matching,\nmaking it possible to cause reference-dependent bias. It is critical to develop\na highly efficient template-free method to automatically recognize particle\nimages from cryo-EM micrographs. Results: We developed a deep learning-based\nalgorithmic framework, DeepEM, for single-particle recognition from noisy\ncryo-EM micrographs, enabling automated particle picking, selection and\nverification in an integrated fashion. The kernel of DeepEM is built upon a\nconvolutional neural network (CNN) of eight layers, which can be recursively\ntrained to be highly \"knowledgeable\". Our approach exhibits improved\nperformance and high precision when tested on the standard KLH dataset.\nApplication of DeepEM to several challenging experimental cryo-EM datasets\ndemonstrates its capability in avoiding selection of un-wanted particles and\nnon-particles even when true particles contain fewer features. Conclusions: The\nDeepEM method derived from a deep CNN allows automated particle extraction from\nraw cryo-EM micrographs in the absence of templates, which demonstrated\nimproved performance, objectivity and accuracy. Application of this novel\napproach is expected to free the labor involved in single-particle\nverification, thus promoting the efficiency of cryo-EM data processing.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.05543v2"
    },
    {
        "title": "Equation-free analysis of a dynamically evolving multigraph",
        "authors": [
            "Alexander Holiday",
            "Ioannis G. Kevrekidis"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In order to illustrate the adaptation of traditional continuum numerical\ntechniques to the study of complex network systems, we use the equation-free\nframework to analyze a dynamically evolving multigraph. This approach is based\non coupling short intervals of direct dynamic network simulation with\nappropriately-defined lifting and restriction operators, mapping the detailed\nnetwork description to suitable macroscopic (coarse-grained) variables and\nback. This enables the acceleration of direct simulations through Coarse\nProjective Integration (CPI), as well as the identification of coarse\nstationary states via a Newton-GMRES method. We also demonstrate the use of\ndata-mining, both linear (principal component analysis, PCA) and nonlinear\n(diffusion maps, DMAPS) to determine good macroscopic variables (observables)\nthrough which one can coarse-grain the model. These results suggest methods for\ndecreasing simulation times of dynamic real-world systems such as\nepidemiological network models. Additionally, the data-mining techniques could\nbe applied to a diverse class of problems to search for a succint,\nlow-dimensional description of the system in a small number of variables.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02818v1"
    },
    {
        "title": "Should unfolded histograms be used to test hypotheses?",
        "authors": [
            "Robert D. Cousins",
            "Samuel J. May",
            "Yipeng Sun"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In many analyses in high energy physics, attempts are made to remove the\neffects of detector smearing in data by techniques referred to as \"unfolding\"\nhistograms, thus obtaining estimates of the true values of histogram bin\ncontents. Such unfolded histograms are then compared to theoretical\npredictions, either to judge the goodness of fit of a theory, or to compare the\nabilities of two or more theories to describe the data. When doing this, even\ninformally, one is testing hypotheses. However, a more fundamentally sound way\nto test hypotheses is to smear the theoretical predictions by simulating\ndetector response and then comparing to the data without unfolding; this is\nalso frequently done in high energy physics, particularly in searches for new\nphysics. One can thus ask: to what extent does hypothesis testing after\nunfolding data materially reproduce the results obtained from testing by\nsmearing theoretical predictions? We argue that this \"bottom-line-test\" of\nunfolding methods should be studied more commonly, in addition to common\npractices of examining variance and bias of estimates of the true contents of\nhistogram bins. We illustrate bottom-line-tests in a simple toy problem with\ntwo hypotheses.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.07038v1"
    },
    {
        "title": "Seeking Maximum Linearity of Transfer Functions",
        "authors": [
            "Filipi N. Silva",
            "Cesar H. Comin",
            "Luciano da F. Costa"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Linearity is an important and frequently sought property in electronics and\ninstrumentation. Here, we report a method capable of, given a transfer\nfunction, identifying the respective most linear region of operation with a\nfixed width. This methodology, which is based on least squares regression and\nsystematic consideration of all possible regions, has been illustrated with\nrespect to both an analytical (sigmoid transfer function) and real-world\n(low-power, one-stage class A transistor amplifier) situations. In the former\ncase, the method was found to identity the theoretically optimal region of\noperation even in presence of noise. In the latter case, it was possible to\nidentify an amplifier circuit configuration providing a good compromise between\nlinearity, amplification and output resistance. The transistor amplifier\napplication, which was addressed in terms of transfer functions derived from\nits experimentally obtained characteristic surface, also yielded contributions\nsuch as the estimation of local constants of the device, as opposed to\ntypically considered average values. Moreover, the obtained characteristic\nsurfaces of the considered transistor (a generic, low signal device) revealed a\nsurprisingly complex structure. The reported method and results paves the way\nto several other applications in other types of devices and systems,\nintelligent control operation, and other areas such as identifying regions of\npower law behavior.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04008v1"
    },
    {
        "title": "Inferring collective dynamical states from widely unobserved systems",
        "authors": [
            "Jens Wilting",
            "Viola Priesemann"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  When assessing spatially-extended complex systems, one can rarely sample the\nstates of all components. We show that this spatial subsampling typically leads\nto severe underestimation of the risk of instability in systems with\npropagating events. We derive a subsampling-invariant estimator, and\ndemonstrate that it correctly infers the infectiousness of various diseases\nunder subsampling, making it particularly useful in countries with unreliable\ncase reports. In neuroscience, recordings are strongly limited by subsampling.\nHere, the subsampling-invariant estimator allows to revisit two prominent\nhypotheses about the brain's collective spiking dynamics:\nasynchronous-irregular or critical. We identify consistently for rat, cat and\nmonkey a state that combines features of both and allows input to reverberate\nin the network for hundreds of milliseconds. Overall, owing to its ready\napplicability, the novel estimator paves the way to novel insight for the study\nof spatially-extended dynamical systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.07035v3"
    },
    {
        "title": "Statistical properties of a filtered Poisson process with additive\n  random noise: Distributions, correlations and moment estimation",
        "authors": [
            "Audun Theodorsen",
            "Odd Erik Garcia",
            "Martin Rypdal"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Filtered Poisson processes are often used as reference models for\nintermittent fluc- tuations in physical systems. Such a process is here\nextended by adding a noise term, either as a purely additive term to the\nprocess or as a dynamical term in a stochastic differential equation. The\nlowest order moments, probability density function, auto-correlation function\nand power spectral density are derived and used to identify and compare the\neffects of the two different noise terms. Monte-Carlo studies of synthetic time\nseries are used to investigate the accuracy of model pa- rameter estimation and\nto identify methods for distinguishing the noise types. It is shown that the\nprobability density function and the three lowest order moments provide\naccurate estimations of the parameters, but are unable to separate the noise\ntypes. The auto-correlation function and the power spectral density also\nprovide methods for estimating the model parameters, as well as being capable\nof identifying the noise type. The number of times the signal crosses a\nprescribed threshold level in the positive direction also promises to be able\nto differentiate the noise type.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.01607v2"
    },
    {
        "title": "Estimation of presampling modulation transfer function in synchrotron\n  radiation microtomography",
        "authors": [
            "Ryuta Mizutani",
            "Keisuke Taguchi",
            "Akihisa Takeuchi",
            "Kentaro Uesugi",
            "Yoshio Suzuki"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The spatial resolution achieved by recent synchrotron radiation\nmicrotomographs should be estimated from the modulation transfer function (MTF)\non the micrometer scale. Step response functions of a synchrotron radiation\nmicrotomograph were determined by the slanted edge method by using\nhigh-precision surfaces of diamond crystal and ion-milled aluminum wire. Tilted\nreconstruction was introduced to enable any edge to be used as the slanted edge\nby defining the reconstruction pixel matrix in an arbitrary orientation. MTFs\nwere estimated from the step response functions of the slanted edges. The\nobtained MTFs coincided with MTF values estimated from square-wave patterns\nmilled on the aluminum surface. Although x-ray refraction influences should be\ntaken into account to evaluate MTFs, any flat surfaces with nanometer roughness\ncan be used to determine the spatial resolutions of microtomographs.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.02269v1"
    },
    {
        "title": "Practical Statistics for Particle Physicists",
        "authors": [
            "Luca Lista"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  These three lectures provide an introduction to the main concepts of\nstatistical data analysis useful for precision measurements and searches for\nnew signals in High Energy Physics. The frequentist and Bayesian approaches to\nprobability theory are introduced and, for both approaches, inference methods\nare presented. Hypothesis tests will be discussed, then significance and upper\nlimit evaluation will be presented with an overview of the modern and most\nadvanced techniques adopted for data analysis at the Large Hadron Collider.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.04150v2"
    },
    {
        "title": "Fitting Power-laws in empirical data with estimators that work for all\n  exponents",
        "authors": [
            "Rudolf Hanel",
            "Bernat Corominas-Murtra",
            "Bo Liu",
            "Stefan Thurner"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  It has been repeatedly stated that maximum likelihood (ML) estimates of\nexponents of power-law distributions can only be reliably obtained for\nexponents smaller than minus one. The main argument that power laws are\notherwise not normalizable, depends on the underlying sample space the data is\ndrawn from, and is true only for sample spaces that are unbounded from above.\nHere we show that power-laws obtained from bounded sample spaces (as is the\ncase for practically all data related problems) are always free of such\nlimitations and maximum likelihood estimates can be obtained for arbitrary\npowers without restrictions. Here we first derive the appropriate ML estimator\nfor arbitrary exponents of power-law distributions on bounded discrete sample\nspaces. We then show that an almost identical estimator also works perfectly\nfor continuous data. We implemented this ML estimator and discuss its\nperformance with previous attempts. We present a general recipe of how to use\nthese estimators and present the associated computer codes.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.05357v1"
    },
    {
        "title": "Tomographic X-ray data of a lotus root filled with attenuating objects",
        "authors": [
            "Tatiana A. Bubba",
            "Andreas Hauptmann",
            "Simo Huotari",
            "Juho Rimpeläinen",
            "Samuli Siltanen"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  This is the documentation of the tomographic X-ray data of a lotus root,\nfilled with four different attenuating objects, of different sizes. Data are\navailable at www.fips.fi/dataset.php, and can be freely used for scientific\npurposes with appropriate references to them, and to this document in\nhttp://arxiv.org/arXiv. The data set consists of (1) the X-ray sinogram of a\nsingle 2D slice of the lotus root with two different resolutions and (2) the\ncorresponding measurement matrices modeling the linear operation of the X-ray\ntransform. Each of these sinograms was obtained from a measured 360-projection\nfan-beam sinogram by down-sampling and taking logarithms. The original\n(measured) sinogram is also provided in its original form and resolution.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.07299v2"
    },
    {
        "title": "A Hierarchical Bayesian Model Accounting for Endmember Variability and\n  Abrupt Spectral Changes to Unmix Multitemporal Hyperspectral Images",
        "authors": [
            "Pierre-Antoine Thouvenin",
            "Nicolas Dobigeon",
            "Jean-Yves Tourneret"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Hyperspectral unmixing is a blind source separation problem which consists in\nestimating the reference spectral signatures contained in a hyperspectral\nimage, as well as their relative contribution to each pixel according to a\ngiven mixture model. In practice, the process is further complexified by the\ninherent spectral variability of the observed scene and the possible presence\nof outliers. More specifically, multi-temporal hyperspectral images, i.e.,\nsequences of hyperspectral images acquired over the same area at different time\ninstants, are likely to simultaneously exhibit moderate endmember variability\nand abrupt spectral changes either due to outliers or to significant time\nintervals between consecutive acquisitions. Unless properly accounted for,\nthese two perturbations can significantly affect the unmixing process. In this\ncontext, we propose a new unmixing model for multitemporal hyperspectral images\naccounting for smooth temporal variations, construed as spectral variability,\nand abrupt spectral changes interpreted as outliers. The proposed hierarchical\nBayesian model is inferred using a Markov chain Monte-Carlo (MCMC) method\nallowing the posterior of interest to be sampled and Bayesian estimators to be\napproximated. A comparison with unmixing techniques from the literature on\nsynthetic and real data allows the interest of the proposed approach to be\nappreciated.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.07792v4"
    },
    {
        "title": "Fast Bayesian inference of optical trap stiffness and particle diffusion",
        "authors": [
            "Sudipta Bera",
            "Shuvojit Paul",
            "Rajesh Singh",
            "Dipanjan Ghosh",
            "Avijit Kundu",
            "Ayan Banerjee",
            "R. Adhikari"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Bayesian inference provides a principled way of estimating the parameters of\na stochastic process that is observed discretely in time. The overdamped\nBrownian motion of a particle confined in an optical trap is generally modelled\nby the Ornstein-Uhlenbeck process and can be observed directly in experiment.\nHere we present Bayesian methods for inferring the parameters of this process,\nthe trap stiffness and the particle diffusion coefficient, that use exact\nlikelihoods and sufficient statistics to arrive at simple expressions for the\nmaximum a posteriori estimates. This obviates the need for Monte Carlo sampling\nand yields methods that are both fast and accurate. We apply these to\nexperimental data and demonstrate their advantage over commonly used\nnon-Bayesian fitting methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.00315v2"
    },
    {
        "title": "Statistics of predictions with missing higher order corrections",
        "authors": [
            "Laure Berthier",
            "Jeppe Trøst Nielsen"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Effective operators have been used extensively to understand small deviations\nfrom the Standard Model in the search for new physics. So far there has been no\ngeneral method to fit for small parameters when higher order corrections in\nthese parameters are present but unknown. We present a new technique that\nsolves this problem, allowing for an exact p-value calculation under the\nassumption that higher order theoretical contributions can be treated as\ngaussian distributed random variables. The method we propose is general, and\nmay be used in the analysis of any perturbative theoretical prediction,\nie.~truncated power series. We illustrate this new method by performing a fit\nof the Standard Model Effective Field Theory parameters, which include\neg.~anomalous gauge and four-fermion couplings.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.01783v2"
    },
    {
        "title": "Go With the Flow, on Jupiter and Snow. Coherence From Model-Free Video\n  Data without Trajectories",
        "authors": [
            "Abd AlRahman AlMomani",
            "Erik M. Bollt"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Viewing a data set such as the clouds of Jupiter, coherence is readily\napparent to human observers, especially the Great Red Spot, but also other\ngreat storms and persistent structures. There are now many different\ndefinitions and perspectives mathematically describing coherent structures, but\nwe will take an image processing perspective here. We describe an image\nprocessing perspective inference of coherent sets from a fluidic system\ndirectly from image data, without attempting to first model underlying flow\nfields, related to a concept in image processing called motion tracking. In\ncontrast to standard spectral methods for image processing which are generally\nrelated to a symmetric affinity matrix, leading to standard spectral graph\ntheory, we need a not symmetric affinity which arises naturally from the\nunderlying arrow of time. We develop an anisotropic, directed diffusion\noperator corresponding to flow on a directed graph, from a directed affinity\nmatrix developed with coherence in mind, and corresponding spectral graph\ntheory from the graph Laplacian. Our methodology is not offered as more\naccurate than other traditional methods of finding coherent sets, but rather\nour approach works with alternative kinds of data sets, in the absence of\nvector field. Our examples will include partitioning the weather and cloud\nstructures of Jupiter, and a local to Potsdam, N.Y. lake-effect snow event on\nEarth, as well as the benchmark test double-gyre system.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.01857v3"
    },
    {
        "title": "Improved Method to extract Nucleon Helicity Distributions using Event\n  Weighting",
        "authors": [
            "Jörg Pretz"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  An improved analysis method to extract quark helicity distributions in\nleading order (LO) QCD from semi-inclusive double spin asymmetries in deep\ninelastic scattering is presented. The method relies on the fact that\nfragmentation functions, describing the fragmentation of a quark into a hadron,\nhave a strong dependence on the energy fraction $z$ of the observed hadron.\nHadrons with large $z$ contain more information about the struck quark. This\ncan be used in a weighting procedure to improve the figure of merit (= inverse\nof variance). In numerical examples it is shown that one could gain 15-39\\%\ndepending on the quark flavor and cut on $z$.\n  Mathematically the problem can be described as finding an optimal solution in\nterms of the figure of merit for parameters $\\boldsymbol \\Theta$ determined\nfrom a system of linear equations ${\\bf B}(x) {\\boldsymbol \\Theta} ={\\bf\nY}(x)$, where the measured input vector ${\\bf Y}(x)$ is given as event\ndistributions depending on a random variable $x$, the coefficients of the\nmatrix ${\\bf B}(x)$ depend as well on $x$, whereas the parameter vector\n$\\boldsymbol\\Theta$ to be determined does not.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.06952v4"
    },
    {
        "title": "Leaders and followers: Quantifying consistency in spatio-temporal\n  propagation patterns",
        "authors": [
            "Thomas Kreuz",
            "Eero Satuvuori",
            "Martin Pofahl",
            "Mario Mulansky"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Repetitive spatio-temporal propagation patterns are encountered in fields as\nwide-ranging as climatology, social communication and network science. In\nneuroscience, perfectly consistent repetitions of the same global propagation\npattern are called a synfire pattern. For any recording of sequences of\ndiscrete events (in neuroscience terminology: sets of spike trains) the\nquestions arise how closely it resembles such a synfire pattern and which are\nthe spike trains that lead/follow. Here we address these questions and\nintroduce an algorithm built on two new indicators, termed SPIKE-Order and\nSpike Train Order, that define the Synfire Indicator value, which allows to\nsort multiple spike trains from leader to follower and to quantify the\nconsistency of the temporal leader-follower relationships for both the original\nand the optimized sorting. We demonstrate our new approach using artificially\ngenerated datasets before we apply it to analyze the consistency of propagation\npatterns in two real datasets from neuroscience (Giant Depolarized Potentials\nin mice slices) and climatology (El Ni~no sea surface temperature recordings).\nThe new algorithm is distinguished by conceptual and practical simplicity, low\ncomputational cost, as well as flexibility and universality.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.07986v4"
    },
    {
        "title": "Three Perspectives on Complexity $-$ Entropy, Compression, Subsymmetry",
        "authors": [
            "Nithin Nagaraj",
            "Karthi Balasubramanian"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  There is no single universally accepted definition of \"Complexity\". There are\nseveral perspectives on complexity and what constitutes complex behaviour or\ncomplex systems, as opposed to regular, predictable behaviour and simple\nsystems. In this paper, we explore the following perspectives on complexity:\n\"effort-to-describe\" (Shannon entropy $H$, Lempel-Ziv complexity $LZ$),\n\"effort-to-compress\" ($ETC$ complexity) and \"degree-of-order\" (Subsymmetry or\n$SubSym$). While Shannon entropy and $LZ$ are very popular and widely used,\n$ETC$ is a recently proposed measure for time series. In this paper, we also\npropose a novel normalized measure $SubSym$ based on the existing idea of\ncounting the number of subsymmetries or palindromes within a sequence. We\ncompare the performance of these complexity measures on the following tasks: a)\ncharacterizing complexity of short binary sequences of lengths 4 to 16, b)\ndistinguishing periodic and chaotic time series from 1D logistic map and 2D\nH\\'{e}non map, and c) distinguishing between tonic and irregular spiking\npatterns generated from the \"Adaptive exponential integrate-and-fire\" neuron\nmodel. Our study reveals that each perspective has its own advantages and\nuniqueness while also having an overlap with each other.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.00607v1"
    },
    {
        "title": "A universal rank-size law",
        "authors": [
            "Marcel Ausloos",
            "Roy Cerqueti"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  A mere hyperbolic law, like the Zipf's law power function, is often\ninadequate to describe rank-size relationships. An alternative theoretical\ndistribution is proposed based on theoretical physics arguments starting from\nthe Yule-Simon distribution. A modeling is proposed leading to a universal\nform. A theoretical suggestion for the \"best (or optimal) distribution\", is\nprovided through an entropy argument. The ranking of areas through the number\nof cities in various countries and some sport competition ranking serves for\nthe present illustrations.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.01659v1"
    },
    {
        "title": "Kalman-Takens filtering in the presence of dynamical noise",
        "authors": [
            "Franz Hamilton",
            "Tyrus Berry",
            "Timothy Sauer"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The use of data assimilation for the merging of observed data with dynamical\nmodels is becoming standard in modern physics. If a parametric model is known,\nmethods such as Kalman filtering have been developed for this purpose. If no\nmodel is known, a hybrid Kalman-Takens method has been recently introduced, in\norder to exploit the advantages of optimal filtering in a nonparametric\nsetting. This procedure replaces the parametric model with dynamics\nreconstructed from delay coordinates, while using the Kalman update formulation\nto assimilate new observations. We find that this hybrid approach results in\ncomparable efficiency to parametric methods in identifying underlying dynamics,\neven in the presence of dynamical noise. By combining the Kalman-Takens method\nwith an adaptive filtering procedure we are able to estimate the statistics of\nthe observational and dynamical noise. This solves a long standing problem of\nseparating dynamical and observational noise in time series data, which is\nespecially challenging when no dynamical model is specified.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.05414v1"
    },
    {
        "title": "Analyzing X-Ray tomographies of granular packings",
        "authors": [
            "Simon Weis",
            "Matthias Schröter"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Starting from three-dimensional volume data of a granular packing, as e.g.\nobtained by X-ray Computed Tomography, we discuss methods to first detect the\nindividual particles in the sample and then analyze their properties. This\nanalysis includes the pair correlation function, the volume and shape of the\nVoronoi cells and the number and type of contacts formed between individual\nparticles. We mainly focus on packings of monodisperse spheres, but we will\nalso comment on other monoschematic particles such as ellipsoids and\ntetrahedra. This paper is accompanied by a package of free software containing\nall programs (including source code) and an example three-dimensional dataset\nwhich allows the reader to reproduce and modify all examples given.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.06639v2"
    },
    {
        "title": "Covariance and correlation estimators in bipartite complex systems with\n  a double heterogeneity",
        "authors": [
            "Elena Puccio",
            "Jyrki Piilo",
            "Michele Tumminello"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We present a weighted estimator of the covariance and correlation in\nbipartite complex systems with a double layer of heterogeneity. The advantage\nprovided by the weighted estimators lies in the fact that the unweighted sample\ncovariance and correlation can be shown to possess a bias. Indeed, such a bias\naffects real bipartite systems, and, for example, we report its effects on two\nempirical systems, one social and the other biological. On the contrary, our\nnewly proposed weighted estimators remove the bias and are better suited to\ndescribe such systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07109v1"
    },
    {
        "title": "Redundancy and synergy in dual decompositions of mutual information gain\n  and information loss",
        "authors": [
            "Daniel Chicharro",
            "Stefano Panzeri"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Williams and Beer (2010) proposed a nonnegative mutual information\ndecomposition, based on the construction of information gain lattices, which\nallows separating the information that a set of variables contains about\nanother into components interpretable as the unique information of one\nvariable, or redundant and synergy components. In this work we extend the\nframework of Williams and Beer (2010) focusing on the lattices that underpin\nthe decomposition. We generalize the type of constructible lattices and examine\nthe relations between the terms in different lattices, for example relating\nbivariate and trivariate decompositions. We point out that, in information gain\nlattices, redundancy components are invariant across decompositions, but unique\nand synergy components are decomposition-dependent. Exploiting the connection\nbetween different lattices we propose a procedure to construct, in the general\nmultivariate case, information decompositions from measures of synergy or\nunique information. We introduce an alternative type of mutual information\ndecompositions based on information loss lattices, with the role and invariance\nproperties of redundancy and synergy components exchanged with respect to gain\nlattices. We study the correspondence between information gain and information\nloss lattices and we define dual decompositions that allow overcoming the\nintrinsic asymmetry between invariant and decomposition-dependent components,\nwhich hinders the consistent joint characterization of synergy and redundancy.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.09522v1"
    },
    {
        "title": "Analyzing a stochastic process driven by Ornstein-Uhlenbeck noise",
        "authors": [
            "B. Lehle",
            "J. Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  A scalar Langevin-type process $X(t)$ that is driven by Ornstein-Uhlenbeck\nnoise $\\eta(t)$ is non-Markovian. However, the joint dynamics of $X$ and $\\eta$\nis described by a Markov process in two dimensions. But even though there\nexists a variety of techniques for the analysis of Markov processes, it is\nstill a challenge to estimate the process parameters solely based on a given\ntime series of $X$. Such a partially observed 2D-process could, e.g., be\nanalyzed in a Bayesian framework using Markov chain Monte Carlo methods.\nAlternatively, an embedding strategy can be applied, where first the joint\ndynamic of $X$ and its temporal derivative $\\dot X$ is analyzed. Subsequently\nthe results can be used to determine the process parameters of $X$ and $\\eta$.\nIn this paper, we propose a more direct approach that is purely based on the\nmoments of the increments of $X$, which can be estimated for different\ntime-increments $\\tau$ from a given time series. From a stochastic\nTaylor-expansion of $X$, analytic expressions for these moments can be derived,\nwhich can be used to estimate the process parameters by a regression strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.00032v1"
    },
    {
        "title": "Sparse Representation of Gravitational Sound",
        "authors": [
            "Laura Rebollo-Neira",
            "A. Plastino"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Gravitational Sound clips produced by the Laser Interferometer\nGravitational-Wave Observatory (LIGO) and the Massachusetts Institute of\nTechnology (MIT) are considered within the particular context of data\nreduction. It is shown that these types of signals can be approximated at high\nquality using much less elementary components than those required within the\nstandard orthogonal basis framework. Furthermore, a measure a local sparsity is\nshown to render meaningful information about the variation of a signal along\ntime, by generating a set of local sparsity values which is much smaller than\nthe dimension of the signal. This point is stressed further by recourse to a\nmore complex signal, generated by Milde Science Communication to divulge\nGravitational Sound in the form a ring tone.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.01413v1"
    },
    {
        "title": "Nanoparticle Size Distribution Quantification: Results of a SAXS\n  Inter-Laboratory Comparison",
        "authors": [
            "Brian R. Pauw",
            "Claudia Kästner",
            "Andreas F. Thünemann"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We present the first world-wide inter-laboratory comparison of small-angle\nX-ray scattering (SAXS) for nanoparticle sizing. The measurands in this\ncomparison are the mean particle radius, the width of the size distribution and\nthe particle concentration. The investigated sample consists of dispersed\nsilver nanoparticles, surrounded by a stabilizing polymeric shell of\npoly(acrylic acid). The silver cores dominate the X-ray scattering pattern,\nleading to the determination of their radii size distribution using: i)\nGlatter's Indirect Fourier Transformation method, ii) classical model fitting\nusing SASfit and iii) a Monte Carlo fitting approach using McSAS. The\napplication of these three methods to the collected datasets produces\nconsistent mean number- and volume-weighted core radii of R$_n$ = 2.76 nm and\nR$_v$ = 3.20 nm, respectively. The corresponding widths of the log-normal radii\ndistribution of the particles were $\\sigma_n$ = 0.65 nm and $\\sigma_v$ = 0.71\nnm. The particle concentration determined using this method was 3.00 $\\pm$ 0.38\ng/L (4.20 $\\pm$ 0.73 $\\times$ 10$^{-6}$ mol/L). We show that the results are\nslightly biased by the choice of data evaluation procedure, but that no\nsubstantial differences were found between the results from data measured on a\nvery wide range of instruments: the participating laboratories at synchrotron\nSAXS beamlines, commercial and home-made instruments were all able to provide\ndata of high quality. Our results demonstrate that SAXS is a qualified method\nfor revealing particle size distributions in the sub-20 nm region (at least),\nout of reach for most other analytical methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.03902v1"
    },
    {
        "title": "Approximate Bayes learning of stochastic differential equations",
        "authors": [
            "Philipp Batz",
            "Andreas Ruttor",
            "Manfred Opper"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We introduce a nonparametric approach for estimating drift and diffusion\nfunctions in systems of stochastic differential equations from observations of\nthe state vector. Gaussian processes are used as flexible models for these\nfunctions and estimates are calculated directly from dense data sets using\nGaussian process regression. We also develop an approximate expectation\nmaximization algorithm to deal with the unobserved, latent dynamics between\nsparse observations. The posterior over states is approximated by a piecewise\nlinearized process of the Ornstein-Uhlenbeck type and the maximum a posteriori\nestimation of the drift is facilitated by a sparse Gaussian process\napproximation.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.05390v1"
    },
    {
        "title": "Measures of spike train synchrony for data with multiple time-scales",
        "authors": [
            "Eero Satuvuori",
            "Mario Mulansky",
            "Nebojsa Bozanic",
            "Irene Malvestio",
            "Fleur Zeldenrust",
            "Kerstin Lenk",
            "Thomas Kreuz"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Background: Measures of spike train synchrony are widely used in both\nexperimental and computational neuroscience. Time-scale independent and\nparameter-free measures, such as the ISI-distance, the SPIKE-distance and\nSPIKE-synchronization, are preferable to time-scale parametric measures, since\nby adapting to the local firing rate they take into account all the time-scales\nof a given dataset.\n  New Method: In data containing multiple time-scales (e.g. regular spiking and\nbursts) one is typically less interested in the smallest time-scales and a more\nadaptive approach is needed. Here we propose the A-ISI-distance, the\nA-SPIKE-distance and A-SPIKE-synchronization, which generalize the original\nmeasures by considering the local relative to the global time-scales. For the\nA-SPIKE-distance we also introduce a rate-independent extension called the\nRIA-SPIKE-distance, which focuses specifically on spike timing.\n  Results: The adaptive generalizations A-ISI-distance and A-SPIKE-distance\nallow to disregard spike time differences that are not relevant on a more\nglobal scale. A-SPIKE-synchronization does not any longer demand an\nunreasonably high accuracy for spike doublets and coinciding bursts. Finally,\nthe RIA-SPIKE-distance proves to be independent of rate ratios between spike\ntrains.\n  Comparison with Existing Methods: We find that compared to the original\nversions the A-ISI-distance and the A-SPIKE-distance yield improvements for\nspike trains containing different time-scales without exhibiting any unwanted\nside effects in other examples. A-SPIKE-synchronization matches spikes more\nefficiently than SPIKE-Synchronization.\n  Conclusions: With these proposals we have completed the picture, since we now\nprovide adaptive generalized measures that are sensitive to rate only\n(A-ISI-distance), to timing only (ARI-SPIKE-distance), and to both at the same\ntime (A-SPIKE-distance).\n",
        "pdf_link": "http://arxiv.org/pdf/1702.05394v2"
    },
    {
        "title": "A General Probabilistic Approach for Quantitative Assessment of LES\n  Combustion Models",
        "authors": [
            "Ross Johnson",
            "Hao Wu",
            "Matthias Ihme"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The Wasserstein metric is introduced as a probabilistic method to enable\nquantitative evaluations of LES combustion models. The Wasserstein metric can\ndirectly be evaluated from scatter data or statistical results using\nprobabilistic reconstruction against experimental data. The method is derived\nand generalized for turbulent reacting flows, and applied to validation tests\ninvolving the Sydney piloted jet flame. It is shown that the Wasserstein metric\nis an effective validation tool that extends to multiple scalar quantities,\nproviding an objective and quantitative evaluation of model deficiencies and\nboundary conditions on the simulation accuracy. Several test cases are\nconsidered, beginning with a comparison of mixture-fraction results, and the\nsubsequent extension to reactive scalars, including temperature and species\nmass fractions of \\ce{CO} and \\ce{CO2}. To demonstrate the versatility of the\nproposed method in application to multiple datasets, the Wasserstein metric is\napplied to a series of different simulations that were contributed to the\nTNF-workshop. Analysis of the results allowed to identify competing\ncontributions to model deviations, arising from uncertainties in the boundary\nconditions and model deficiencies. These applications demonstrate that the\nWasserstein metric constitutes an easily applicable mathematical tool that\nreduce multiscalar combustion data and large datasets into a scalar-valued\nquantitative measure.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.05539v2"
    },
    {
        "title": "STAR Data Reconstruction at NERSC/Cori, an adaptable Docker container\n  approach for HPC",
        "authors": [
            "Mustafa Mustafa",
            "Jan Balewski",
            "Jérôme Lauret",
            "Jefferson Porter",
            "Shane Canon",
            "Lisa Gerhardt",
            "Levente Hajdu",
            "Mark Lukascsyk"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  As HPC facilities grow their resources, adaptation of classic HEP/NP\nworkflows becomes a need. Linux containers may very well offer a way to lower\nthe bar to exploiting such resources and at the time, help collaboration to\nreach vast elastic resources on such facilities and address their massive\ncurrent and future data processing challenges. In this proceeding, we showcase\nSTAR data reconstruction workflow at Cori HPC system at NERSC. STAR software is\npackaged in a Docker image and runs at Cori in Shifter containers. We highlight\ntwo of the typical end-to-end optimization challenges for such pipelines: 1)\ndata transfer rate which was carried over ESnet after optimizing end points and\n2) scalable deployment of conditions database in an HPC environment. Our tests\ndemonstrate equally efficient data processing workflows on Cori/HPC, comparable\nto standard Linux clusters.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.06593v1"
    },
    {
        "title": "Combining parameter values or $p$-values",
        "authors": [
            "Louis Lyons",
            "Emilien Chapon"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We review the methods to combine several measurements, in the form of\nparameter values or $p$-values.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.05540v1"
    },
    {
        "title": "Mapping and discrimination of networks in the complexity-entropy plane",
        "authors": [
            "Marc Wiedermann",
            "Jonathan F. Donges",
            "Jürgen Kurths",
            "Reik V. Donner"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Complex networks are usually characterized in terms of their topological,\nspatial, or information-theoretic properties and combinations of the associated\nmetrics are used to discriminate networks into different classes or categories.\nHowever, even with the present variety of characteristics at hand it still\nremains a subject of current research to appropriately quantify a network's\ncomplexity and correspondingly discriminate between different types of complex\nnetworks, like infrastructure or social networks, on such a basis. Here, we\nexplore the possibility to classify complex networks by means of a statistical\ncomplexity measure that has formerly been successfully applied to distinguish\ndifferent types of chaotic and stochastic time series. It is composed of a\nnetwork's averaged per-node entropic measure characterizing the network's\ninformation content and the associated Jenson-Shannon divergence as a measure\nof disequilibrium. We study 29 real world networks and show that networks of\nthe same category tend to cluster in distinct areas of the resulting\ncomplexity-entropy plane. We demonstrate that within our framework, connectome\nnetworks exhibit among the highest complexity while, e.g, transportation and\ninfrastructure networks display significantly lower values. Furthermore, we\ndemonstrate the utility of our framework by applying it to families of random\nscale-free and Watts-Strogatz model networks. We then show in a second\napplication that the proposed framework is useful to objectively construct\nthreshold-based networks, such as functional climate networks or recurrence\nnetworks, by choosing the threshold such that the statistical network\ncomplexity is maximized.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.07599v2"
    },
    {
        "title": "Surrogate-based artifact removal from single-channel EEG",
        "authors": [
            "Mario Chavez",
            "Fanny Grosselin",
            "Aurore Bussalb",
            "Fabrizio De Vico Fallani",
            "Xavier Navarro-Sune"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The recent emergence and success of electroencephalography (EEG) in low-cost\nportable devices, has opened the door to a new generation of applications\nprocessing a small number of EEG channels for health monitoring and\nbrain-computer interfacing. These recordings are, however, contaminated by many\nsources of noise degrading the signals of interest, thus compromising the\ninterpretation of the underlying brain state. In this work, we propose a new\ndata-driven algorithm to effectively remove ocular and muscular artifacts from\nsingle-channel EEG: the surrogate-based artifact removal (SuBAR). Methods: By\nmeans of the time-frequency analysis of surrogate data, our approach is able to\nidentify and filter automatically ocular and muscular artifacts embedded in\nsingle-channel EEG. Results: In a comparative study using artificially contami-\nnated EEG signals, the efficacy of the algorithm in terms of noise removal and\nsignal distortion was superior to other traditionally-employed single-channel\nEEG denoising techniques: wavelet thresholding and the canonical correlation\nanalysis combined with an advanced version of the empirical mode decomposition.\nEven in the presence of mild and severe artifacts, our artifact removal method\nprovides a relative error 4 to 5 times lower than traditional techniques.\nSignificance: In view of these results, the SuBAR method is a promising\nsolution for mobile environments, such as ambulatory healthcare systems, sleep\nstage scoring or anesthesia monitoring, where very few EEG channels or even a\nsingle channel is available.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.07603v2"
    },
    {
        "title": "When intuition fails in assessing conditional risks: the example of the\n  frog riddle",
        "authors": [
            "Daniel Hetterich",
            "Florian Geissler"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Recently, the educational initiative TED-Ed has published a popular brain\nteaser coined the 'frog riddle', which illustrates non-intuitive implications\nof conditional probabilities. In its intended form, the frog riddle is a\nreformulation of the classic boy-girl paradox. However, the authors alter the\nnarrative of the riddle in a form, that subtly changes the way information is\nconveyed. The presented solution, unfortunately, does not take this point into\nfull account, and as a consequence, lacks consistency in the sense that\ndifferent parts of the problem are treated on unequal footing. We here review,\nhow the mechanism of receiving information matters, and why this is exactly the\nreason that such kind of problems challenge intuitive thinking. Subsequently,\nwe present a generalized solution, that accounts for the above difficulties,\nand preserves full logical consistency. Eventually, the relation to the\nboy-girl paradox is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.00902v1"
    },
    {
        "title": "Characterizing Time Series via Complexity-Entropy Curves",
        "authors": [
            "Haroldo V. Ribeiro",
            "Max Jauregui",
            "Luciano Zunino",
            "Ervin K. Lenzi"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The search for patterns in time series is a very common task when dealing\nwith complex systems. This is usually accomplished by employing a complexity\nmeasure such as entropies and fractal dimensions. However, such measures\nusually only capture a single aspect of the system dynamics. Here, we propose a\nfamily of complexity measures for time series based on a generalization of the\ncomplexity-entropy causality plane. By replacing the Shannon entropy by a\nmono-parametric entropy (Tsallis $q$-entropy) and after considering the proper\ngeneralization of the statistical complexity ($q$-complexity), we build up a\nparametric curve (the $q$-complexity-entropy curve) that is used for\ncharacterizing/classifying time series. Based on simple exact results and\nnumerical simulations of stochastic processes, we show that these curves can\ndistinguish among different long-range, short-range and oscillating correlated\nbehaviors. Also, we verify that simulated chaotic and stochastic time series\ncan be distinguished based on whether these curves are open or closed. We\nfurther test this technique in experimental scenarios related to chaotic laser\nintensity, stock price, sunspot, and geomagnetic dynamics, confirming its\nusefulness. Finally, we prove that these curves enhance the automatic\nclassification of time series with long-range correlations and interbeat\nintervals of healthy subjects and patients with heart disease.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.04779v1"
    },
    {
        "title": "Causality inference in stochastic systems from neurons to currencies:\n  Profiting from small sample size",
        "authors": [
            "Danh-Tai Hoang",
            "Juyong Song",
            "Vipul Periwal",
            "Junghyo Jo"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Success in modeling complex phenomena such as human perception hinges\ncritically on the availability of data and computational power. Significant\nprogress has been made in modeling such phenomena using probabilistic methods,\nparticularly in image analysis and speech recognition. Maximum Likelihood\nEstimation (MLE) combined with Bayesian model selection is the basis of much of\nthis progress, as MLE converges to the true model with copious data. In the\nsciences, large enough datasets are rarae aves, so alternatives to MLE must be\ndeveloped for small sample size. We introduce a data-driven statistical physics\napproach to model inference based on minimizing a free energy of data and show\nsuperior model recovery for small sample sizes. We demonstrate coupling\nstrength inference in non-equilibrium kinetic Ising models, including in the\ndifficult large coupling variability regime, and show scaling to systems of\narbitrary size. As applications, we infer a functional connectivity network in\nthe salamander retina and a currency exchange rate network from time-series\ndata of neuronal spiking and currency exchange rates, respectively. Accurate\nsmall sample size inference is critical for devising a profitable currency\nhedging strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.06384v2"
    },
    {
        "title": "Right-side-stretched multifractal spectra indicate small-worldness in\n  networks",
        "authors": [
            "Paweł Oświȩcimka",
            "Lorenzo Livi",
            "Stanisław Drożdż"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Complex network formalism allows to explain the behavior of systems composed\nby interacting units. Several prototypical network models have been proposed\nthus far. The small-world model has been introduced to mimic two important\nfeatures observed in real-world systems: i) local clustering and ii) the\npossibility to move across a network by means of long-range links that\nsignificantly reduce the characteristic path length. A natural question would\nbe whether there exist several \"types\" of small-world architectures, giving\nrise to a continuum of models with properties (partially) shared with other\nmodels belonging to different network families. Here, we take advantage of the\ninterplay between network theory and time series analysis and propose to\ninvestigate small-world signatures in complex networks by analyzing\nmultifractal characteristics of time series generated from such networks. In\nparticular, we suggest that the degree of right-sided asymmetry of multifractal\nspectra is linked with the degree of small-worldness present in networks. This\nclaim is supported by numerical simulations performed on several parametric\nmodels, including prototypical small-world networks, scale-free, fractal and\nalso real-world networks describing protein molecules. Our results also\nindicate that right-sided asymmetry emerges with the presence of the following\ntopological properties: low edge density, low average shortest path, and high\nclustering coefficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.07801v4"
    },
    {
        "title": "Design and Execution of make-like, distributed Analyses based on\n  Spotify's Pipelining Package Luigi",
        "authors": [
            "Marcel Rieger",
            "Martin Erdmann",
            "Benjamin Fischer",
            "Robert Fischer"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  In high-energy particle physics, workflow management systems are primarily\nused as tailored solutions in dedicated areas such as Monte Carlo production.\nHowever, physicists performing data analyses are usually required to steer\ntheir individual workflows manually which is time-consuming and often leads to\nundocumented relations between particular workloads. We present a generic\nanalysis design pattern that copes with the sophisticated demands of end-to-end\nHEP analyses and provides a make-like execution system. It is based on the\nopen-source pipelining package Luigi which was developed at Spotify and enables\nthe definition of arbitrary workloads, so-called Tasks, and the dependencies\nbetween them in a lightweight and scalable structure. Further features are\nmulti-user support, automated dependency resolution and error handling, central\nscheduling, and status visualization in the web. In addition to already\nbuilt-in features for remote jobs and file systems like Hadoop and HDFS, we\nadded support for WLCG infrastructure such as LSF and CREAM job submission, as\nwell as remote file access through the Grid File Access Library. Furthermore,\nwe implemented automated resubmission functionality, software sandboxing, and a\ncommand line interface with auto-completion for a convenient working\nenvironment. For the implementation of a $t\\bar{t}H$ cross section measurement,\nwe created a generic Python interface that provides programmatic access to all\nexternal information such as datasets, physics processes, statistical models,\nand additional files and values. In summary, the setup enables the execution of\nthe entire analysis in a parallelized and distributed fashion with a single\ncommand.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.00955v1"
    },
    {
        "title": "Identity Method Revisited",
        "authors": [
            "Claude A. Pruneau"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We discuss the impact of finite particle losses associated with instrumental\neffects in measurements of moments of produced multiplicities with the Identity\nMethod towards the evaluation of fluctuation measures such as $\\nu_{dyn}$. We\nshow that the identity method remains applicable provided it is modified to\ndetermine factorial moments $\\langle N(N-1)\\rangle$, rather than moments\n$\\langle N^2\\rangle$. We further show that $\\nu_{dyn}$ remains robust if\ndetection efficiencies are uniform across the measurement's acceptance. The\nrobustness is lost, however, if detection efficiencies are momentum dependent,\nalthough the identity methods remains applicable provided detection\nefficiencies can be determined with sufficient accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01333v1"
    },
    {
        "title": "Yadage and Packtivity - analysis preservation using parametrized\n  workflows",
        "authors": [
            "Kyle Cranmer",
            "Lukas Heinrich"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Preserving data analyses produced by the collaborations at LHC in a\nparametrized fashion is crucial in order to maintain reproducibility and\nre-usability. We argue for a declarative description in terms of individual\nprocessing steps - packtivities - linked through a dynamic directed acyclic\ngraph (DAG) and present an initial set of JSON schemas for such a description\nand an implementation - yadage - capable of executing workflows of analysis\npreserved via Linux containers.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01878v1"
    },
    {
        "title": "Predictability of Extreme Intensity Pulses in Optically Injected\n  Semiconductor Lasers",
        "authors": [
            "Nuria Martinez Alvarez",
            "Saurabh Borkar",
            "Cristina Masoller"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The predictability of extreme intensity pulses emitted by an optically\ninjected semiconductor laser is studied numerically, by using a well-known rate\nequation model. We show that symbolic ordinal time-series analysis allows to\nidentify the patterns of intensity oscillations that are likely to occur before\nan extreme pulse. The method also gives information about patterns which are\nunlikely to occur before an extreme pulse. The specific patterns identified\ncapture the topology of the underlying chaotic attractor and depend on the\nmodel parameters. The methodology proposed here can be useful for analyzing\ndata recorded from other complex systems that generate extreme fluctuations in\ntheir output signals.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.10142v1"
    },
    {
        "title": "Transition to Reconstructibility in Weakly Coupled Networks",
        "authors": [
            "Benedict J. Lünsmann",
            "Christoph Kirst",
            "Marc Timme"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Across scientific disciplines, thresholded pairwise measures of statistical\ndependence between time series are taken as proxies for the interactions\nbetween the dynamical units of a network. Yet such correlation measures often\nfail to reflect the underlying physical interactions accurately. Here we\nsystematically study the problem of reconstructing direct physical interaction\nnetworks from thresholding correlations. We explicate how local common cause\nand relay structures, heterogeneous in-degrees and non-local structural\nproperties of the network generally hinder reconstructibility. However, in the\nlimit of weak coupling strengths we prove that stationary systems with dynamics\nclose to a given operating point transition to universal reconstructiblity\nacross all network topologies.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.02726v1"
    },
    {
        "title": "Bayesian Block Histogramming for High Energy Physics",
        "authors": [
            "Brian Pollack",
            "Saptaparna Bhattacharya",
            "Michael Schmitt"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The Bayesian Block algorithm, originally developed for applications in\nastronomy, can be used to improve the binning of histograms in high energy\nphysics. The visual improvement can be dramatic, as shown here with two simple\nexamples. More importantly, this algorithm and the histogram is produces is a\nnon-parametric density estimate, providing a description of background\ndistributions that does not suffer from the arbitrariness of ad hoc analytical\nfunctions. The statistical power of an hypothesis test based on Bayesian Blocks\nis nearly as good as that obtained by fitting analytical functions. Two\nexamples are provided: a narrow peak on a smoothly-falling background, and an\nexcess in the tail of a background that falls rapidly over several orders of\nmagnitude. These examples show the usefulness of the binning provided by the\nBayesian Blocks algorithm both for presentation of data and when searching for\nnew physics.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.00810v2"
    },
    {
        "title": "A Deep Convolutional Neural Network to Analyze Position Averaged\n  Convergent Beam Electron Diffraction Patterns",
        "authors": [
            "Weizong Xu",
            "James M. LeBeau"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We establish a series of deep convolutional neural networks to automatically\nanalyze position averaged convergent beam electron diffraction patterns. The\nnetworks first calibrate the zero-order disk size, center position, and\nrotation without the need for pretreating the data. With the aligned data,\nadditional networks then measure the sample thickness and tilt. The performance\nof the network is explored as a function of a variety of variables including\nthickness, tilt, and dose. A methodology to explore the response of the neural\nnetwork to various pattern features is also presented. Processing patterns at a\nrate of $\\sim$0.1 s/pattern, the network is shown to be orders of magnitude\nfaster than a brute force method while maintaining accuracy. The approach is\nthus suitable for automatically processing big, 4D STEM data. We also discuss\nthe generality of the method to other materials/orientations as well as a\nhybrid approach that combines the features of the neural network with least\nsquares fitting for even more robust analysis. The source code is available at\nhttps://github.com/subangstrom/DeepDiffraction.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.00855v1"
    },
    {
        "title": "Phase-error estimation and image reconstruction from digital-holography\n  data using a Bayesian framework",
        "authors": [
            "Casey J. Pellizzari",
            "Mark F. Spencer",
            "Charles A. Bouman"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The estimation of phase errors from digital-holography data is critical for\napplications such as imaging or wave-front sensing. Conventional techniques\nrequire multiple i.i.d. data and perform poorly in the presence of high noise\nor large phase errors. In this paper we propose a method to estimate\nisoplanatic phase errors from a single data realization. We develop a\nmodel-based iterative reconstruction algorithm which computes the maximum a\nposteriori estimate of the phase and the speckle-free object reflectance. Using\nsimulated data, we show that the algorithm is robust against high noise and\nstrong phase errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.01142v1"
    },
    {
        "title": "Some comments on computational mechanics, complexity measures, and all\n  that",
        "authors": [
            "Peter Grassberger"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We comment on some conceptual and and technical problems related to\ncomputational mechanics, point out some errors in several papers, and\nstraighten out some wrong priority claims. We present explicitly the correct\nalgorithm for constructing a minimal unifilar hidden Markov model\n(\"$\\epsilon$-machine\") from a list of forbidden words and (exact) word\nprobabilities in a stationary stochastic process, and we comment on inference\nwhen these probabilities are only approximately known. In particular we propose\nminimization of forecasting complexity as an alternative basis for statistical\ninference of time series, in contrast to the traditional maximum entropy\nprinciple. We present a simple and precise way of estimating excess entropy\n(aka \"effective measure complexity\". Most importantly, however, we clarify some\nbasic conceptual problems. In particular, we show that there exist simple\nmodels (called \"totally recurrent graphs\") where none of the nodes of the\n\"$\\epsilon$-machine\" (the \"causal states\") corresponds to an element of a state\n(or history) space partition.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.03197v3"
    },
    {
        "title": "Long-range fluctuations and multifractality in connectivity density time\n  series of a wind speed monitoring network",
        "authors": [
            "Mohamed Laib",
            "Luciano Telesca",
            "Mikhail Kanevski"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  This paper studies the daily connectivity time series of a wind\nspeed-monitoring network using multifractal detrended fluctuation analysis. It\ninvestigates the long-range fluctuation and multifractality in the residuals of\nthe connectivity time series. Our findings reveal that the daily connectivity\nof the correlation-based network is persistent for any correlation threshold.\nFurther, the multifractality degree is higher for larger absolute values of the\ncorrelation threshold\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04216v2"
    },
    {
        "title": "A Conditional Model of Wind Power Forecast Errors and Its Application in\n  Scenario Generation",
        "authors": [
            "Zhiwen Wang",
            "Chen Shen",
            "Feng Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  In power system operation, characterizing the stochastic nature of wind power\nis an important albeit challenging issue. It is well known that distributions\nof wind power forecast errors often exhibit significant variability with\nrespect to different forecast values. Therefore, appropriate probabilistic\nmodels that can provide accurate information for conditional forecast error\ndistributions are of great need. On the basis of Gaussian mixture model, this\npaper constructs analytical conditional distributions of forecast errors for\nmultiple wind farms with respect to forecast values. The accuracy of the\nproposed probabilistic models is verified by using historical data. Thereafter,\na fast sampling method is proposed to generate scenarios from the conditional\ndistributions which are non-Gaussian and interdependent. The efficiency of the\nproposed sampling method is verified.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.06759v2"
    },
    {
        "title": "The evaluation of the systematic uncertainties for the finite MC samples\n  in the presence of negative weights",
        "authors": [
            "Petr Mandrik"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The analysis of results from HEP experiments often involves the estimates of\nthe composition of the binned data samples, based on Monte Carlo simulations of\nvarious sources. Due to a finite statistic of MC samples they have statistical\nfluctuation. This work proposes the method of incorporating the systematic\nuncertainties due to finite statistics of MC samples with negative weights. The\npossible approximations are discussed and the comparison of different methods\nare presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07708v1"
    },
    {
        "title": "Statistical Physics and Representations in Real and Artificial Neural\n  Networks",
        "authors": [
            "Simona Cocco",
            "Rémi Monasson",
            "Lorenzo Posani",
            "Sophie Rosay",
            "Jérôme Tubiana"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  This document presents the material of two lectures on statistical physics\nand neural representations, delivered by one of us (R.M.) at the Fundamental\nProblems in Statistical Physics XIV summer school in July 2017. In a first\npart, we consider the neural representations of space (maps) in the\nhippocampus. We introduce an extension of the Hopfield model, able to store\nmultiple spatial maps as continuous, finite-dimensional attractors. The phase\ndiagram and dynamical properties of the model are analyzed. We then show how\nspatial representations can be dynamically decoded using an effective Ising\nmodel capturing the correlation structure in the neural data, and compare\napplications to data obtained from hippocampal multi-electrode recordings and\nby (sub)sampling our attractor model. In a second part, we focus on the problem\nof learning data representations in machine learning, in particular with\nartificial neural networks. We start by introducing data representations\nthrough some illustrations. We then analyze two important algorithms, Principal\nComponent Analysis and Restricted Boltzmann Machines, with tools from\nstatistical physics.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.02470v1"
    },
    {
        "title": "Modeling correlated bursts by the bursty-get-burstier mechanism",
        "authors": [
            "Hang-Hyun Jo"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Temporal correlations of time series or event sequences in natural and social\nphenomena have been characterized by power-law decaying autocorrelation\nfunctions with decaying exponent $\\gamma$. Such temporal correlations can be\nunderstood in terms of power-law distributed interevent times with exponent\n$\\alpha$, and/or correlations between interevent times. The latter, often\ncalled correlated bursts, has recently been studied by measuring power-law\ndistributed bursty trains with exponent $\\beta$. A scaling relation between\n$\\alpha$ and $\\gamma$ has been established for the uncorrelated interevent\ntimes, while little is known about the effects of correlated interevent times\non temporal correlations. In order to study these effects, we devise the\nbursty-get-burstier model for correlated bursts, by which one can tune the\ndegree of correlations between interevent times, while keeping the same\ninterevent time distribution. We numerically find that sufficiently strong\ncorrelations between interevent times could violate the scaling relation\nbetween $\\alpha$ and $\\gamma$ for the uncorrelated case. A non-trivial\ndependence of $\\gamma$ on $\\beta$ is also found for some range of $\\alpha$. The\nimplication of our results is discussed in terms of the hierarchical\norganization of bursty trains at various timescales.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.05150v1"
    },
    {
        "title": "Direct Measurement of Fast Transients by Using Boot-strapped Waveform\n  Averaging",
        "authors": [
            "Mattias Olsson",
            "Fredrik Edman",
            "Khadga Jung Karki"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  An approximation to coherent sampling, also known as boot-strapped waveform\naveraging, is presented. The method uses digital cavities to determine the\ncondition for coherent sampling. It can be used to increase the effective\nsampling rate of a repetitive signal and the signal to noise ratio\nsimultaneously. The method is demonstrated by using it to directly measure the\nfluorescence lifetime from rhodamine 6G by digitizing the signal from a fast\navalanche photodiode. The obtained lifetime of 4.4+-0.1 ns is in agreement with\nthe known values.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08389v2"
    },
    {
        "title": "Deep Learning the Effects of Photon Sensors on the Event Reconstruction\n  Performance in an Antineutrino Detector",
        "authors": [
            "Chang-Wei Loh",
            "Zhi-Qiang Qian",
            "You-Hang Liu",
            "De-Wen Cao",
            "Rui Zhang",
            "Wei Wang",
            "Hai-Bo Yang",
            "Ming Qi"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We provide a fast approach incorporating the usage of deep learning for\nevaluating the effects of photon sensors in an antineutrino detector on the\nevent reconstruction performance therein. This work is an attempt to harness\nthe power of deep learning for detector designing and upgrade planning. Using\nthe Daya Bay detector as a benchmark case and the vertex reconstruction\nperformance as the objective for the deep neural network, we find that the\nphotomultiplier tubes (PMTs) have different relative importance to the vertex\nreconstruction. More importantly, the vertex position resolutions for the Daya\nBay detector follow approximately a multi-exponential relationship with respect\nto the number of PMTs and hence, the coverage. This could also assist in\ndeciding on the merits of installing additional PMTs for future detector plans.\nThe approach could easily be used with other objectives in place of vertex\nreconstruction.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.00607v3"
    },
    {
        "title": "A Paradox about Likelihood Ratios?",
        "authors": [
            "Louis Lyons"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We consider whether the asymptotic distributions for the log-likelihood ratio\ntest statistic are expected to be Gaussian or chi-squared. Two straightforward\nexamples provide insight on the difference.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.00775v2"
    },
    {
        "title": "Modeling long correlation times using additive binary Markov chains:\n  applications to wind generation time series",
        "authors": [
            "Juliane Weber",
            "Christopher Zachow",
            "Dirk Witthaut"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Wind power generation exhibits a strong temporal variability, which is\ncrucial for system integration in highly renewable power systems. Different\nmethods exist to simulate wind power generation but they often cannot represent\nthe crucial temporal fluctuations properly. We apply the concept of additive\nbinary Markov chains to model a wind generation time series consisting of two\nstates: periods of high and low wind generation. The only input parameter for\nthis model is the empirical autocorrelation function. The two state model is\nreadily extended to stochastically reproduce the actual generation per period.\nTo evaluate the additive binary Markov chain method, we introduce a coarse\nmodel of the electric power system to derive backup and storage needs. We find\nthat the temporal correlations of wind power generation, the backup need as a\nfunction of the storage capacity and the resting time distribution of high and\nlow wind events for different shares of wind generation can be reconstructed.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.03294v2"
    },
    {
        "title": "On the Practical Applications of Information Field Dynamics",
        "authors": [
            "Martin Dupont"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  In this study we explore a new simulation scheme for partial differential\nequations known as Information Field Dynamics (IFD). Information field dynamics\nattempts to improve on existing simulation schemes by incorporating Bayesian\nfield inference into the simulation scheme. The field inference is truly\nBayesian and thus depends on a notion of prior belief. A number of results are\npresented, both theoretical and practical. Many small fixes and results on the\ngeneral theory are presented, before exploring two general classes of\nsimulation schemes that are possible in the IFD framework. For both, we present\na set of theoretical results alongside the development of a prototype scheme.\nThe first class of models corresponds roughly to traditional fixed-grid\nnumerical PDE solvers. The prior Bayesian assumption in these models is that\nthe fields are smooth, and their correlation structure does not vary between\nlocations. For these reasons we call them translation-invariant schemes. We\nshow the requirements for stability of these schemes, but most importantly we\nprove that these schemes indeed converge to the true behaviour of the field in\nthe limit of high resolutions. Convergence had never been shown for any\nprevious IFD scheme. We also find the error scaling of these codes and show\nthat they implement something very analogous to a high-order finite-difference\nderivative approximation, which are the most elementary and well-studied\nnumerical schemes. This is an important result, which proves the validity of\nthe IFD approach. The second class of schemes, called the SPH-like schemes are\nsimilar to existing Smooth Particle Hydrodynamics codes, in which the\nsimulation grid moves with the flow of the field being modelled.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.06000v1"
    },
    {
        "title": "Three-dimensional convolutional neural networks for neutrinoless\n  double-beta decay signal/background discrimination in high-pressure gaseous\n  Time Projection Chamber",
        "authors": [
            "Pengcheng Ai",
            "Dong Wang",
            "Guangming Huang",
            "Xiangming Sun"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  In the search for neutrinoless double-beta decay, the high-pressure gaseous\nTime Projection Chamber has a distinct advantage, because the ionization charge\ntracks produced by particle interactions are extended and the detector captures\nthe full three-dimensional charge distribution with appropriate charge readout\nsystems. Such information of tracks provides a crucial extra-handle for\ndiscriminating signal events against backgrounds. In this paper, we constructed\na toy model to demonstrate where the discrimination power comes from and how\nmuch of it the neural network models have already harnessed. Then we adapted\n3-dimensional convolutional and residual neural networks on the simulated\ndouble-beta and background charge tracks and tested their capabilities in\nclassifying these two types of events. We show that both the 3D structure and\nthe overall depth of the neural networks significantly improve the accuracy of\nthe classifier and lead to results better than previous works. We also studied\ntheir performance under various spatial granularities as well as different\ndiffusion and noise conditions. The results indicate that the methods are\nstable and generalize well despite varying experimental conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.01482v5"
    },
    {
        "title": "Bayesian optimization for computationally extensive probability\n  distributions",
        "authors": [
            "Ryo Tamura",
            "Koji Hukushima"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  An efficient method for finding a better maximizer of computationally\nextensive probability distributions is proposed on the basis of a Bayesian\noptimization technique. A key idea of the proposed method is to use extreme\nvalues of acquisition functions by Gaussian processes for the next training\nphase, which should be located near a local maximum or a global maximum of the\nprobability distribution. Our Bayesian optimization technique is applied to the\nposterior distribution in the effective physical model estimation, which is a\ncomputationally extensive probability distribution. Even when the number of\nsampling points on the posterior distributions is fixed to be small, the\nBayesian optimization provides a better maximizer of the posterior\ndistributions in comparison to those by the random search method, the steepest\ndescent method, or the Monte Carlo method. Furthermore, the Bayesian\noptimization improves the results efficiently by combining the steepest descent\nmethod and thus it is a powerful tool to search for a better maximizer of\ncomputationally extensive probability distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04577v1"
    },
    {
        "title": "Topological structures are consistently overestimated in functional\n  complex networks",
        "authors": [
            "Massimiliano Zanin",
            "Seddik Belkoura",
            "Javier Gomez",
            "Cesar Alfaro",
            "Javier Cano"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Functional complex networks have meant a pivotal change in the way we\nunderstand complex systems, being the most outstanding one the human brain.\nThese networks have classically been reconstructed using a frequentist approach\nthat, while simple, completely disregards the uncertainty that derives from\ndata finiteness. We here provide an alternative solution based on Bayesian\ninference, with link weights treated as random variables described by\nprobability distributions, from which ensembles of networks are sampled. By\nusing both statistical and topological considerations, we prove that the role\nplayed by links' uncertainty is equivalent to the introduction of a random\nrewiring, whose omission leads to a consistent overestimation of topological\nstructures. We further show that this bias is enhanced in short time series,\nsuggesting the existence of a theoretical time resolution limit for obtaining\nreliable structures. We also propose a simple sampling process for correcting\ntopological values obtained in frequentist networks. We finally validate these\nconcepts through synthetic and real network examples, the latter representing\nthe brain electrical activity of a group of people during a cognitive task.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.04797v1"
    },
    {
        "title": "A fluctuation theorem for time-series of signal-response models with the\n  backward transfer entropy",
        "authors": [
            "Andrea Auconi",
            "Andrea Giansanti",
            "Edda Klipp"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The irreversibility of trajectories in stochastic dynamical systems is linked\nto the structure of their causal representation in terms of Bayesian networks.\nWe consider stochastic maps resulting from a time discretization with interval\n\\tau of signal-response models, and we find an integral fluctuation theorem\nthat sets the backward transfer entropy as a lower bound to the conditional\nentropy production. We apply this to a linear signal-response model providing\nanalytical solutions, and to a nonlinear model of receptor-ligand systems. We\nshow that the observational time \\tau has to be fine-tuned for an efficient\ndetection of the irreversibility in time-series.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.05294v1"
    },
    {
        "title": "Bayesian model selection with fractional Brownian motion",
        "authors": [
            "Jens Krog",
            "Lars H. Jacobsen",
            "Frederik W. Lund",
            "Daniel Wüstner",
            "Michael A. Lomholt"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We implement Bayesian model selection and parameter estimation for the case\nof fractional Brownian motion with measurement noise and a constant drift. The\napproach is tested on artificial trajectories and shown to make estimates that\nmatch well with the underlying true parameters, while for model selection the\napproach has a preference for simple models when the trajectories are finite.\nThe approach is applied to observed trajectories of vesicles diffusing in\nChinese hamster ovary cells. Here it is supplemented with a goodness-of-fit\ntest, which is able to reveal statistical discrepancies between the observed\ntrajectories and model predictions.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.01365v1"
    },
    {
        "title": "Towards an Information Geometric characterization/classification of\n  Complex Systems. II. Critical Parameter values from the (c,d)-manifold",
        "authors": [
            "Demetris P. K. Ghikas",
            "Fotios D. Oikonomou"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  In our previous paper (I) we derived information geometric objects from the\ntwo parameter generalized entropy of Hanel and Thurner (2011), using the c,d\nparameters as labels of the corresponding manifolds. Here we follow a\ncompletely different approach by considering these parameters as coordinates of\nour information manifolds. This gives a manageable two-dimensional manifold\namenable to easy manipulations but most importantly it offers a direct\ncharacterization of complex systems in terms of the pair of the c,d values. As\na result we obtain certain characteristic values from the scalar curvature\nwhich we could conjecture that they represent complex systems with specific\nbehavior. It is further observed that the boundary values of the c,d parameters\nwhich characterize the Hanel-Thurner classification are in some sense singular.\nThis asks for a regularization scheme which we try to establish.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.02384v1"
    },
    {
        "title": "Dependence of exponents on text length versus finite-size scaling for\n  word-frequency distributions",
        "authors": [
            "Alvaro Corral",
            "Francesc Font-Clos"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Some authors have recently argued that a finite-size scaling law for the\ntext-length dependence of word-frequency distributions cannot be conceptually\nvalid. Here we give solid quantitative evidence for the validity of such\nscaling law, both using careful statistical tests and analytical arguments\nbased on the generalized central-limit theorem applied to the moments of the\ndistribution (and obtaining a novel derivation of Heaps' law as a by-product).\nWe also find that the picture of word-frequency distributions with power-law\nexponents that decrease with text length [Yan and Minnhagen, Physica A 444, 828\n(2016)] does not stand with rigorous statistical analysis. Instead, we show\nthat the distributions are perfectly described by power-law tails with stable\nexponents, whose values are close to 2, in agreement with the classical Zipf's\nlaw. Some misconceptions about scaling are also clarified.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03718v1"
    },
    {
        "title": "Topological data analysis and diagnostics of compressible MHD turbulence",
        "authors": [
            "Irina Makarenko",
            "Paul Bushby",
            "Andrew Fletcher",
            "Robin Henderson",
            "Nikolay Makarenko",
            "Anvar Shukurov"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The predictions of mean-field electrodynamics can now be probed using direct\nnumerical simulations of random flows and magnetic fields. When modelling\nastrophysical MHD, it is important to verify that such simulations are in\nagreement with observations. One of the main challenges in this area is to\nidentify robust \\it{quantitative} measures to compare structures found in\nsimulations with those inferred from astrophysical observations. A similar\nchallenge is to compare quantitatively results from different simulations.\nTopological data analysis offers a range of techniques, including the Betti\nnumbers and persistence diagrams, that can be used to facilitate such a\ncomparison. After describing these tools, we first apply them to synthetic\nrandom fields and demonstrate that, when the data are standardized in a\nstraightforward manner, some topological measures are insensitive to either\nlarge-scale trends or the resolution of the data. Focusing upon one particular\nastrophysical example, we apply topological data analysis to HI observations of\nthe turbulent interstellar medium (ISM) in the Milky Way and to recent MHD\nsimulations of the random, strongly compressible ISM. We stress that these\ntopological techniques are generic and could be applied to any complex,\nmulti-dimensional random field.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.04688v1"
    },
    {
        "title": "Statistical algorithms for particle trajectography",
        "authors": [
            "Frédéric Magniette"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The various algorithms used to extrapolate particle trajectories from\nmeasurements are often very time-consuming with computational complexities\nwhich are typically quadratic. In this article, we propose a new algorithm\ncalled GEM with a lower complexity and reasonable performance on linear tracks.\nIt is an extension of the EM algorithm used to fit Gaussian mixtures. It works\nin arbitrary dimension and with an arbitrary number of simultaneous particles.\nIn a second part, we extend it to circular tracks (for charged particles) and\neven a mix of linear and circular tracks. This algorithm is implemented in an\nopen-source library called libgem and two applications are proposed, based on\ndata-sets from two kind of particle trackers.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06892v2"
    },
    {
        "title": "Visibility graphs for image processing",
        "authors": [
            "Jacopo Iacovacci",
            "Lucas Lacasa"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The family of image visibility graphs (IVGs) have been recently introduced as\nsimple algorithms by which scalar fields can be mapped into graphs. Here we\nexplore the usefulness of such operator in the scenario of image processing and\nimage classification. We demonstrate that the link architecture of the image\nvisibility graphs encapsulates relevant information on the structure of the\nimages and we explore their potential as image filters and compressors. We\nintroduce several graph features, including the novel concept of Visibility\nPatches, and show through several examples that these features are highly\ninformative, computationally efficient and universally applicable for general\npattern recognition and image classification tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.07125v1"
    },
    {
        "title": "Using Quantum Mechanics to Cluster Time Series",
        "authors": [
            "Clark Alexander",
            "Luke Shi",
            "Sofya Akhmametyeva"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  In this article we present a method by which we can reduce a time series into\na single point in $\\mathbb{R}^{13}$. We have chosen 13 dimensions so as to\nprevent too many points from being labeled as \"noise.\" When using a Euclidean\n(or Mahalanobis) metric, a simple clustering algorithm will with near certainty\nlabel the majority of points as \"noise.\" On pure physical considerations, this\nis not possible. Included in our 13 dimensions are four parameters which\ndescribe the coefficients of a cubic polynomial attached to a Gaussian picking\nup a general trend, four parameters picking up periodicity in a time series,\ntwo each for amplitude of a wave and period of a wave, and the final five\nreport the \"leftover\" noise of the detrended and aperiodic time series. Of the\nfinal five parameters, four are the centralized probabilistic moments, and the\nfinal for the relative size of the series. The first main contribution of this\nwork is to apply a theorem of quantum mechanics about the completeness of the\nsolutions to the quantum harmonic oscillator on $L^2(\\mathbb{R})$ to estimating\ntrends in time series. The second main contribution is the method of fitting\nparameters. After many numerical trials, we realized that methods such a\nNewton-Rhaphson and Levenberg-Marquardt converge extremely fast if the initial\nguess is good. Thus we guessed many initial points in our parameter space and\ncomputed only a few iterations, a technique common in Keogh's work on time\nseries clustering. Finally, we have produced a model which gives incredibly\naccurate results quickly. We ackowledge that there are faster methods as well\nof more accurate methods, but this work shows that we can still increase\ncomputation speed with little, if any, cost to accuracy in the sense of data\nclustering.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.01711v1"
    },
    {
        "title": "Differentiating resting brain states using ordinal symbolic analysis",
        "authors": [
            "C. Quintero-Quiroz",
            "Luis Montesano",
            "A. J. Pons",
            "M. C. Torrent",
            "J. García-Ojalvo",
            "C. Masoller"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Symbolic methods of analysis are valuable tools for investigating complex\ntime-dependent signals. In particular, the ordinal method defines sequences of\nsymbols according to the ordering in which values appear in a time series. This\nmethod has been shown to yield useful information, even when applied to signals\nwith large noise contamination. Here we use ordinal analysis to investigate the\ntransition between eyes closed (EC) and eyes open (EO) resting states. We\nanalyze two {EEG} datasets (with 71 and 109 healthy subjects) with different\nrecording conditions (sampling rates and the number of electrodes in the\nscalp). Using as diagnostic tools the permutation entropy, the entropy computed\nfrom symbolic transition probabilities, and an asymmetry coefficient (that\nmeasures the asymmetry of the likelihood of the transitions between symbols) we\nshow that ordinal analysis applied to the raw data distinguishes the two brain\nstates. In both datasets, we find that the EO state is characterized by higher\nentropies and lower asymmetry coefficient, as compared to the EC state. Our\nresults thus show that these diagnostic tools have the potential for detecting\nand characterizing changes in time-evolving brain states.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.03933v1"
    },
    {
        "title": "Study of constraint and impact of a nuisance parameter in maximum\n  likelihood method",
        "authors": [
            "Li-Gang Xia"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Maximum likelihood method is widely used for parameter estimation in high\nenergy physics. To consider various systematic uncertainties, tens of or even\nhundreds of nuisance parameters (NP) are introduced in a likelihood fit. The\nconstraint of a nuisance parameter and its impact on the parameter of interest\n(POI) will be the main concerns for a precise measurement. A fit involving many\nparameters is usually slow and it is even more time-consuming to investigate\nwhy a parameter is over-constrained or has a large impact. In this paper, we\nare trying to understand the reasons behind and provide simple formulae to\nestimate the constraint and impact directly.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.03961v4"
    },
    {
        "title": "Functional Decomposition: A new method for search and limit setting",
        "authors": [
            "Ryan Edgar",
            "Dante Amidei",
            "Christopher Grud",
            "Karishma Sekhon"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  In the analysis of High-Energy Physics data, it is frequently desired to\nseparate resonant signals from a smooth, non-resonant background. This paper\nintroduces a new technique - functional decomposition (FD) - to accomplish this\ntask. It is universal and readily able to describe often-problematic effects\nsuch as sculpting and trigger turn-ons.\n  Functional decomposition models a dataset as a truncated series expansion in\na complete set of orthonormal basis functions, using a process analogous to\nFourier analysis. A new family of orthonormal functions is presented, which has\nbeen expressly designed to accomplish this in a succinct way. A consistent\nsignal extraction methodology based on linear signal estimators is also\ndetailed, as is an automated method for selecting the method's (few)\nhyperparameters and preventing over-fitting.\n  The full collection of algorithms described in this paper have been\nimplemented in an easy-to-use software package, which will also be briefly\ndescribed.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.04536v1"
    },
    {
        "title": "Probabilistic enhancement of the Failure Forecast Method using a\n  stochastic differential equation and application to volcanic eruption\n  forecasts",
        "authors": [
            "Andrea Bevilacqua",
            "E. Bruce Pitman",
            "Abani Patra",
            "Augusto Neri",
            "Marcus Bursik",
            "Barry Voight"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We introduce a doubly stochastic method for performing material failure\ntheory based forecasts of volcanic eruptions. The method enhances the well\nknown Failure Forecast Method equation, introducing a new formulation similar\nto the Hull-White model in financial mathematics. In particular, we incorporate\na stochastic noise term in the original equation, and systematically\ncharacterize the uncertainty. The model is a stochastic differential equation\nwith mean reverting paths, where the traditional ordinary differential equation\ndefines the mean solution. Our implementation allows the model to make\nexcursions from the classical solutions, by including uncertainty in the\nestimation. The doubly stochastic formulation is particularly powerful, in that\nit provides a complete posterior probability distribution, allowing users to\ndetermine a worst case scenario with a specified level of confidence. We apply\nthe new method on historical datasets of precursory signals, across a wide\nrange of possible values of convexity in the solutions and amounts of\nscattering in the observations. The results show the increased forecasting\nskill of the doubly stochastic formulation of the equations if compared to\nstatistical regression.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.11654v4"
    },
    {
        "title": "Differential Correlation Measurements with the Identity Method",
        "authors": [
            "Claude Pruneau",
            "Alice Ohlson"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We present an extension of the identity method initially introduced for\nparticle yield fluctuation studies towards measurements of differential\ncorrelations. The extension is developed and illustrated in the context of\nmeasurements of the normalized two-particle cumulant $R_2$ but is adaptable to\nany correlation measurements, including differential flow measurements. The\nidentity method is also extended to account for an arbitrary number of particle\nidentification devices and signals.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.02264v1"
    },
    {
        "title": "A proposed solution for analysis management in high energy physics",
        "authors": [
            "Mingrui Zhao"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  This paper presents an architecture for the analysis management in high\nenergy physics experiments. Some new concepts on data analysis are introduced.\nA protocol for organizing and operating an analysis is raised. A toolkit\nfollowing this architecture is developed, which provides a solution of analysis\nmanagement with both flexibility and reproducibility. A foreseen development of\nthis toolkit is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.08787v1"
    },
    {
        "title": "High-Performance Reconstruction of Microscopic Force Fields from\n  Brownian Trajectories",
        "authors": [
            "Laura Pérez García",
            "Jaime Donlucas Pérez",
            "Giorgio Volpe",
            "Alejandro V. Arzola",
            "Giovanni Volpe"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The accurate measurement of microscopic force fields is crucial in many\nbranches of science and technology, from biophotonics and mechanobiology to\nmicroscopy and optomechanics. These forces are often probed by analysing their\ninfluence on the motion of Brownian particles. Here, we introduce a powerful\nalgorithm for microscopic Force Reconstruction via Maximum-likelihood-estimator\n(MLE) Analysis (FORMA) to retrieve the force field acting on a Brownian\nparticle from the analysis of its displacements. FORMA yields accurate\nsimultaneous estimations of both the conservative and non-conservative\ncomponents of the force field with important advantages over established\ntechniques, being parameter-free, requiring ten-fold less data and executing\norders-of-magnitude faster. We first demonstrate FORMA performance using\noptical tweezers. We then show how, outperforming any other available\ntechnique, FORMA can identify and characterise stable and unstable equilibrium\npoints in generic extended force fields. Thanks to its high performance, this\nnew algorithm can accelerate the development of microscopic and nanoscopic\nforce transducers capable of operating with high reliability, speed, accuracy\nand precision for applications in physics, biology and engineering.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.05468v1"
    },
    {
        "title": "Field Formulation of Parzen Data Analysis",
        "authors": [
            "D. Horn"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The Parzen window density is a well-known technique, associating Gaussian\nkernels with data points. It is a very useful tool in data exploration, with\nparticular importance for clustering schemes and image analysis. This method is\npresented here within a formalism containing scalar fields, such as the density\nfunction and its potential, and their corresponding gradients. The potential is\nderived from the density through the dependence of the latter on the common\nscale parameter of all Gaussian kernels. The loci of extrema of the density and\npotential scalar fields are points of interest which obey a variation condition\non a novel indicator function. They serve as focal points of clustering methods\ndepending on maximization of the density, or minimization of the potential,\naccordingly. The mixed inter-dependencies of the different fields in d-dim\ndata-space and 1-d scale-space, are discussed. They lead to a Schr\\H{o}dinger\nequation in d-dim, and to a diffusion equation in (d+1)-dim\n",
        "pdf_link": "http://arxiv.org/pdf/1808.08776v1"
    },
    {
        "title": "Extracting distribution parameters from multiple uncertain observations\n  with selection biases",
        "authors": [
            "Ilya Mandel",
            "Will M. Farr",
            "Jonathan R. Gair"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We derive a Bayesian framework for incorporating selection effects into\npopulation analyses. We allow for both measurement uncertainty in individual\nmeasurements and, crucially, for selection biases on the population of\nmeasurements, and show how to extract the parameters of the underlying\ndistribution based on a set of observations sampled from this distribution. We\nillustrate the performance of this framework with an example from\ngravitational-wave astrophysics, demonstrating that the mass ratio distribution\nof merging compact-object binaries can be extracted from Malmquist-biased\nobservations with substantial measurement uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.02063v2"
    },
    {
        "title": "General Resolution Enhancement Method in Atomic Force Microscopy (AFM)\n  Using Deep Learning",
        "authors": [
            "Y. Liu",
            "Q. M. Sun",
            "Dr. W. H. Lu",
            "Dr. H. L. Wang",
            "Y. Sun",
            "Z. T. Wang",
            "X. Lu",
            "Prof. K. Y. Zeng"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  This paper develops a resolution enhancement method for post-processing the\nimages from Atomic Force Microscopy (AFM). This method is based on deep\nlearning neural networks in the AFM topography measurements. In this study, a\nvery deep convolution neural network is developed to derive the high-resolution\ntopography image from the low-resolution topography image. The AFM measured\nimages from various materials are tested in this study. The derived\nhigh-resolution AFM images are comparable with the experimental measured\nhigh-resolution images measured at the same locations. The results suggest that\nthis method can be developed as a general post-processing method for AFM image\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.03704v1"
    },
    {
        "title": "Biased bootstrap sampling for efficient two-sample testing",
        "authors": [
            "Thomas P. S. Gillam",
            "Christopher G. Lester"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The so-called 'energy test' is a frequentist technique used in experimental\nparticle physics to decide whether two samples are drawn from the same\ndistribution. Its usage requires a good understanding of the distribution of\nthe test statistic, T, under the null hypothesis. We propose a technique which\nallows the extreme tails of the T-distribution to be determined more\nefficiently than possible with present methods. This allows quick evaluation of\n(for example) 5-sigma confidence intervals that otherwise would have required\nprohibitively costly computation times or approximations to have been made.\nFurthermore, we comment on other ways that T computations could be sped up\nusing established results from the statistics community. Beyond two-sample\ntesting, the proposed biased bootstrap method may provide benefit anywhere\nextreme values are currently obtained with bootstrap sampling.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.00335v6"
    },
    {
        "title": "On Micromechanical Parameter Identification With Integrated DIC and the\n  Role of Accuracy in Kinematic Boundary Conditions",
        "authors": [
            "O. Rokoš",
            "J. P. M. Hoefnagels",
            "R. H. J. Peerlings",
            "M. G. D. Geers"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Integrated Digital Image Correlation (IDIC) is nowadays a well established\nfull-field experimental procedure for reliable and accurate identification of\nmaterial parameters. It is based on the correlation of a series of images\ncaptured during a mechanical experiment, that are matched by displacement\nfields derived from an underlying mechanical model. In recent studies, it has\nbeen shown that when the applied boundary conditions lie outside the employed\nfield of view, IDIC suffers from inaccuracies. A typical example is a\nmicromechanical parameter identification inside a Microstructural Volume\nElement (MVE), whereby images are usually obtained by electron microscopy or\nother microscopy techniques but the loads are applied at a much larger scale.\nFor any IDIC model, MVE boundary conditions still need to be specified, and any\ndeviation or fluctuation in these boundary conditions may significantly\ninfluence the quality of identification. Prescribing proper boundary conditions\nis generally a challenging task, because the MVE has no free boundary, and the\nboundary displacements are typically highly heterogeneous due to the underlying\nmicrostructure. The aim of this paper is therefore first to quantify the\neffects of errors in the prescribed boundary conditions on the accuracy of the\nidentification in a systematic way. To this end, three kinds of mechanical\ntests, each for various levels of material contrast ratios and levels of image\nnoise, are carried out by means of virtual experiments. For simplicity, an\nelastic compressible Neo-Hookean constitutive model under plane strain\nassumption is adopted. It is shown that a high level of detail is required in\nthe applied boundary conditions. This motivates an improved boundary condition\napplication approach, which considers constitutive material parameters as well\nas kinematic variables at the boundary of the entire MVE as degrees of freedom\nin...\n",
        "pdf_link": "http://arxiv.org/pdf/1810.08245v1"
    },
    {
        "title": "QBDT, a new boosting decision tree method with systematic uncertainties\n  into training for High Energy Physics",
        "authors": [
            "Li-Gang Xia"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  A new boosting decision tree (BDT) method, QBDT, is proposed for the\nclassification problem in the field of high energy physics (HEP). In many HEP\nresearches, great efforts are made to increase the signal significance with the\npresence of huge background and various systematical uncertainties. Why not\ndevelop a BDT method targeting the significance directly? Indeed, the\nsignificance plays a central role in this new method. It is used to split a\nnode in building a tree and to be also the weight contributing to the BDT\nscore. As the systematical uncertainties can be easily included in the\nsignificance calculation, this method is able to learn about reducing the\neffect of the systematical uncertainties via training. Taking the search of the\nrare radiative Higgs decay in proton-proton collisions $pp \\to h + X \\to\n\\gamma\\tau^+\\tau^-+X$ as example, QBDT and the popular Gradient BDT (GradBDT)\nmethod are compared. QBDT is found to reduce the correlation between the signal\nstrength and systematical uncertainty sources and thus to give a better\nsignificance. The contribution to the signal strength uncertainty from the\nsystematical uncertainty sources using the new method is 50-85~\\% of that using\nthe GradBDT method.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.08387v4"
    },
    {
        "title": "Managing Many Simultaneous Systematic Uncertainties",
        "authors": [
            "Luca Lista",
            "Agostino De Iorio",
            "Alberto Orso Maria Iorio"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Recent statistical evaluations for High-Energy Physics measurements, in\nparticular those at the Large Hadron Collider, require careful evaluation of\nmany sources of systematic uncertainties at the same time. While the\nfundamental aspects of the statistical treatment are now consolidated, both\nusing a frequentist or a Bayesian approach, the management of many sources of\nuncertainties and their corresponding nuisance parameters in analyses that\ncombine multiple control regions and decay channels, in practice, may pose\nchallenging implementation issues, that make the analysis infrastructure\ncomplex and hard to manage, eventually resulting in simplifications in the\ntreatment of systematics, and in limitations to the result interpretation.\nTypical cases will be discussed, having in mind the most popular implementation\ntool, RooStats, with possible ideas about improving the management of such\ncases in future software implementations.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.11206v1"
    },
    {
        "title": "Data-driven inference of hidden nodes in networks",
        "authors": [
            "Danh-Tai Hoang",
            "Junghyo Jo",
            "Vipul Periwal"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  The explosion of activity in finding interactions in complex systems is\ndriven by availability of copious observations of complex natural systems.\nHowever, such systems, e.g. the human brain, are rarely completely observable.\nInteraction network inference must then contend with hidden variables affecting\nthe behavior of the observed parts of the system. We present a novel\ndata-driven approach for model inference with hidden variables. From\nconfigurations of observed variables, we identify the observed-to-observed,\nhidden-to-observed, observed-to-hidden, and hidden-to-hidden interactions, the\nconfigurations of hidden variables, and the number of hidden variables. We\ndemonstrate the performance of our method by simulating a kinetic Ising model,\nand show that our method outperforms existing methods. Turning to real data, we\ninfer the hidden nodes in a neuronal network in the salamander retina and a\nstock market network. We show that predictive modeling with hidden variables is\nsignificantly more accurate than that without hidden variables. Finally, an\nimportant hidden variable problem is to find the number of clusters in a\ndataset. We apply our method to classify MNIST handwritten digits. We find that\nthere are about 60 clusters which are roughly equally distributed amongst the\ndigits.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04122v1"
    },
    {
        "title": "Efficient description of experimental effects in amplitude analyses",
        "authors": [
            "Abhijit Mathad",
            "Daniel O'Hanlon",
            "Anton Poluektov",
            "Raul Rabadan"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Amplitude analysis is a powerful technique to study hadron decays. A\nsignificant complication in these analyses is the treatment of instrumental\neffects, such as background and selection efficiency variations, in the\nmultidimensional kinematic phase space. This paper reviews conventional methods\nto estimate efficiency and background distributions and outlines the methods of\ndensity estimation using Gaussian processes and artificial neural networks.\nSuch techniques see widespread use elsewhere, but have not gained popularity in\nuse for amplitude analyses. Finally, novel applications of these models are\nproposed, to estimate background density in the signal region from the\nsidebands in multiple dimensions, and a more general method for model-assisted\ndensity estimation using artificial neural networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.01452v2"
    },
    {
        "title": "Latent Representations of Dynamical Systems: When Two is Better Than One",
        "authors": [
            "Max Tegmark"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  A popular approach for predicting the future of dynamical systems involves\nmapping them into a lower-dimensional \"latent space\" where prediction is\neasier. We show that the information-theoretically optimal approach uses\ndifferent mappings for present and future, in contrast to state-of-the-art\nmachine-learning approaches where both mappings are the same. We illustrate\nthis dichotomy by predicting the time-evolution of coupled harmonic oscillators\nwith dissipation and thermal noise, showing how the optimal 2-mapping method\nsignificantly outperforms principal component analysis and all other approaches\nthat use a single latent representation, and discuss the intuitive reason why\ntwo representations are better than one. We conjecture that a single latent\nrepresentation is optimal only for time-reversible processes, not for e.g.\ntext, speech, music or out-of-equilibrium physical systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.03364v2"
    },
    {
        "title": "Implementation of GENFIT2 as an experiment independent track-fitting\n  framework",
        "authors": [
            "Tadeas Bilka",
            "Nils Braun",
            "Thomas Hauth",
            "Thomas Kuhr",
            "Lia Lavezzi",
            "Felix Metzner",
            "Stephan Paul",
            "Elisabetta Prencipe",
            "Markus Prim",
            "Johannes Rauch",
            "James Ritman",
            "Tobias Schlueter",
            "Stefano Spataro"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  The GENFIT toolkit, initially developed at the Technische Universitaet\nMuenchen, has been extended and modified to be more general and user-friendly.\nThe new GENFIT, called GENFIT2, provides track representation, track-fitting\nalgorithms and graphic visualization of tracks and detectors, and it can be\nused for any experiment that determines parameters of charged particle\ntrajectories from spacial coordinate measurements. Based on general Kalman\nfilter routines, it can perform extrapolations of track parameters and\ncovariance matrices. It also provides interfaces to Millepede II for alignment\npurposes, and RAVE for the vertex finder. Results of an implementation of\nGENFIT2 in basf2 and PandaRoot software frameworks are presented here.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.04405v2"
    },
    {
        "title": "An improved method for the estimation of the Gumbel distribution\n  parameters",
        "authors": [
            "Rubén Gómez González",
            "M. Isabel Parra",
            "Francisco Javier Acero",
            "Jacinto Martín"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Usual estimation methods for the parameters of extreme values distribution\nemploy only a few values, wasting a lot of information. More precisely, in the\ncase of the Gumbel distribution, only the block maxima values are used. In this\nwork, we propose a method to seize all the available information in order to\nincrease the accuracy of the estimations. This intent can be achieved by taking\nadvantage of the existing relationship between the parameters of the baseline\ndistribution, which generates data from the full sample space, and the ones for\nthe limit Gumbel distribution. In this way, an informative prior distribution\ncan be obtained. Different statistical tests are used to compare the behaviour\nof our method with the standard one, showing that the proposed method performs\nwell when dealing with very shortened available data. The empirical\neffectiveness of the approach is demonstrated through a simulation study and a\ncase study. Reduction in the credible interval width and enhancement in\nparameter location show that the results with improved prior adapt to very\nshortened data better than standard method does.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.07963v1"
    },
    {
        "title": "Representing ill-known parts of a numerical model using a machine\n  learning approach",
        "authors": [
            "Julien Brajard",
            "Anastase Charantonis",
            "Jérôme Sirven"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  In numerical modeling of the Earth System, many processes remain unknown or\nill represented (let us quote sub-grid processes, the dependence to unknown\nlatent variables or the non-inclusion of complex dynamics in numerical models)\nbut sometimes can be observed. This paper proposes a methodology to produce a\nhybrid model combining a physical-based model (forecasting the well-known\nprocesses) with a neural-net model trained from observations (forecasting the\nremaining processes). The approach is applied to a shallow-water model in which\nthe forcing, dissipative and diffusive terms are assumed to be unknown. We show\nthat the hybrid model is able to reproduce with great accuracy the unknown\nterms (correlation close to 1). For long term simulations it reproduces with no\nsignificant difference the mean state, the kinetic energy, the potential energy\nand the potential vorticity of the system. Lastly it is able to function with\nnew forcings that were not encountered during the training phase of the neural\nnetwork.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.07358v1"
    },
    {
        "title": "A tail-regression estimator for heavy-tailed distributions of known tail\n  indices and its application to continuum quantum Monte Carlo data",
        "authors": [
            "Pablo Lopez Rios",
            "Gareth J. Conduit"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Standard statistical analysis is unable to provide reliable confidence\nintervals on expectation values of probability distributions that do not\nsatisfy the conditions of the central limit theorem. We present a\nregression-based estimator of an arbitrary moment of a probability distribution\nwith power-law heavy tails that exploits knowledge of the exponents of its\nasymptotic decay to bypass this issue entirely. Our method is applied to\nsynthetic data and to energy and atomic force data from variational and\ndiffusion quantum Monte Carlo calculations, whose distributions have known\nasymptotic forms [J. R. Trail, Phys. Rev. E 77, 016703 (2008); A. Badinski et\nal., J. Phys.: Condens. Matter 22 074202 (2010)]. We obtain convergent,\naccurate confidence intervals on the variance of the local energy of an\nelectron gas and on the Hellmann-Feynman force on an atom in the all-electron\ncarbon dimer. In each of these cases the uncertainty on our estimator is 45%\nand 60 times smaller, respectively, than the nominal (ill-defined) standard\nerror.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.07684v2"
    },
    {
        "title": "A new method to determine multi-angular reflectance factor from\n  lightweight multispectral cameras with sky sensor in a target-less workflow\n  applicable to UAV",
        "authors": [
            "Klaus Schneider-Zapp",
            "Manuel Cubero-Castan",
            "Dai Shi",
            "Christoph Strecha"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  A new physically based method to estimate hemispheric-directional reflectance\nfactor (HDRF) from lightweight multispectral cameras that have a downwelling\nirradiance sensor is presented. It combines radiometry with photogrammetric\ncomputer vision to derive geometrically and radiometrically accurate data\npurely from the images, without requiring reflectance targets or any other\nadditional information apart from the imagery. The sky sensor orientation is\ninitially computed using photogrammetric computer vision and revised with a\nnon-linear regression comprising radiometric and photogrammetry-derived\ninformation. It works for both clear sky and overcast conditions. A\nground-based test acquisition of a Spectralon target observed from different\nviewing directions and with different sun positions using a typical\nmultispectral sensor configuration for clear sky and overcast showed that both\nthe overall value and the directionality of the reflectance factor as reported\nin the literature were well retrieved. An RMSE of 3% for clear sky and up to 5%\nfor overcast sky was observed.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.03301v1"
    },
    {
        "title": "Practical Statistics for Particle Physics",
        "authors": [
            "Roger John Barlow"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  This is the write-up of a set of lectures given at the Asia Europe Pacific\nSchool of High Energy Physics in Quy Nhon, Vietnam in September 2018, to an\naudience of PhD students in all branches of particle physics They cover the\ndifferent meanings of 'probability', particularly frequentist and Bayesian, the\nbinomial, Poisson and Gaussian distributions, hypothesis testing, estimation,\nerrors (including asymmetric and systematic errors) and goodness of fit.\nSeveral different methods used in setting upper limits are explained, followed\nby a discussion on why 5 sigma are conventionally required for a 'discovery'.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.12362v2"
    },
    {
        "title": "Changes in long-term properties of the Danube river level and flow\n  induced by damming",
        "authors": [
            "Djordje Stratimirovic",
            "Ilija Batas-Bjelic",
            "Vladimir Djurdjevic",
            "Suzana Blesic"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  In this paper we assessed changes in scaling properties of the river Danube\nlevel and flow data, associated with building of Djerdap/Iron Gates\nhydrological power plants positioned on the border of Romania and Serbia. We\nused detrended fluctuation analysis (DFA), wavelet transform spectral analysis\n(WTS) and wavelet-based modulus maxima method (WTMM) to investigate time series\nof river levels and river flows recorded at hydrological stations in the\nvicinity of dams and in the area of up to 480 km upstream from dams, and time\nseries of simulated NOAA-CIRES 20th Century Global Reanalysis precipitation\nrecords for the Djerdap/Iron Gates region. By comparing river dynamics during\nthe periods before and after construction of dams, we were able to register\nchanges in scaling that are different for recordings from upstream and from\ndownstream (from dams) areas. We found that damming caused appearance of\nhuman-made or enhancement of natural cycles in the small time scales region,\nwhich largely influenced the change in temporal scaling in downstream recording\nstations. We additionally found disappearance or decline in the amplitude of\nlarge-time-scale cycles as a result of damming, that changed the dynamics of\nupstream data. The most prominent finding of our paper is a demonstration of a\ncomplete or partial loss of annual cycles in the upstream stations' data that\nwe found to extend as far as 220 km from dams. We discussed probable sources of\nsuch found changes in scaling, aiming to provide explanations that could be of\nuse in future environmental assessments.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.13144v2"
    },
    {
        "title": "Revision of ISO 19229 to support the certification of calibration gases\n  for purity",
        "authors": [
            "Adriaan M. H. van der Veen",
            "Gerard Nieuwenkamp"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  The second edition of ISO 19229 expands the guidance in its predecessor in\ntwo ways. Firstly, it provides more support and examples describing possible\nexperimental approaches for purity analysis. A novelty is that it describes how\nthe beta distribution, or some other suitable probability distribution can be\nused to approximate the distribution of the output quantity, i.e., the fraction\nof a component. It also provides guidance on how to report coverage intervals\nin those cases, where the usual approximation from the GUM (Guide to the\nexpression of Uncertainty in Measurement) to use the normal or t distribution\nis inappropriate because of vicinity of zero. Coverage intervals play an\nimportant role in conformity assessment, and it is also customary to report\nmeasurement uncertainty in the form of a coverage interval, notwithstanding\nthat ISO/IEC 17025 does not explicitly require it. ISO 6141, which sets\nrequirements for certificates of calibration gas mixtures, does require the\nstatement of an expanded uncertainty, which has been interpreted that in the\ncase of a non-symmetric output probability distribution, a coverage interval\nshould be stated, along with the value and the standard uncertainty. This paper\ngives a brief background to the choices made and examples in ISO 19229.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.01216v1"
    },
    {
        "title": "Dashboard Task Monitor for Managing ATLAS User Analysis on the Grid",
        "authors": [
            "L Sargsyan",
            "J Andreeva",
            "M Jha",
            "E Karavakis",
            "L Kokoszkiewicz",
            "P Saiz",
            "J Schovancova",
            "D Tuckett"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  The organization of the distributed user analysis on the Worldwide LHC\nComputing Grid (WLCG) infrastructure is one of the most challenging tasks among\nthe computing activities at the Large Hadron Collider. The Experiment Dashboard\noffers a solution that not only monitors but also manages (kill, resubmit) user\ntasks and jobs via a web interface. The ATLAS Dashboard Task Monitor provides\nanalysis users with a tool that is independent of the operating system and Grid\nenvironment. This contribution describes the functionality of the application\nand its implementation details, in particular authentication, authorization and\naudit of the management operations.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05646v1"
    },
    {
        "title": "The 8-parameter Fisher-Bingham distribution on the sphere",
        "authors": [
            "Tianlu Yuan"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  The Fisher-Bingham distribution ($\\mathrm{FB}_8$) is an eight-parameter\nfamily of probability density functions (PDF) on $S^2$ that, under certain\nconditions, reduce to spherical analogues of bivariate normal PDFs. Due to\ndifficulties in computing its overall normalization constant, applications have\nbeen mainly restricted to subclasses of $\\mathrm{FB}_8$, such as the Kent\n($\\mathrm{FB}_5$) or von Mises-Fisher (vMF) distributions. However, these\nsubclasses often do not adequately describe directional data that are not\nsymmetric along great circles. The normalizing constant of $\\mathrm{FB}_8$ can\nbe numerically integrated, and recently Kume and Sei showed that it can be\ncomputed using an adjusted holonomic gradient method. Both approaches, however,\ncan be computationally expensive. In this paper, I show that the normalization\nof $\\mathrm{FB}_8$ can be expressed as an infinite sum consisting of\nhypergeometric functions, similar to that of the $\\mathrm{FB}_5$. This allows\nthe normalization to be computed under summation with adequate stopping\nconditions. I then fit the $\\mathrm{FB}_8$ to a synthetic dataset using a\nmaximum-likelihood approach and show its improvements over a fit with the more\nrestrictive $\\mathrm{FB}_5$ distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.08247v1"
    },
    {
        "title": "Second derivative analysis and alternative data filters for\n  multi-dimensional spectroscopies: a Fourier-space perspective",
        "authors": [
            "Rongjie Li",
            "Xiaoni Zhang",
            "Lin Miao",
            "Luca Stewart",
            "Erica Kotta",
            "Dong Qian",
            "Konstantine Kaznatcheev",
            "Jerzy T. Sadowski",
            "Elio Vescovo",
            "Abdullah Alharbi",
            "Ting Wu",
            "Takashi Taniguchi",
            "Kenji Watanabe",
            "Davood Shahrjerdi",
            "L. Andrew Wray"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  The second derivative image (SDI) method is widely applied to sharpen\ndispersive data features in multi-dimensional spectroscopies such as angle\nresolved photoemission spectroscopy (ARPES). Here, the SDI function is\nrepresented in Fourier space, where it has the form of a multi-band pass\nfilter. The interplay of the SDI procedure with undesirable noise and\nbackground features in ARPES data sets is reviewed, and it is shown that final\nimage quality can be improved by eliminating higher Fourier harmonics of the\nSDI filter. We then discuss extensions of SDI-like band pass filters to higher\ndimensional data sets, and how one can create even more effective filters with\nsome a priori knowledge of the spectral features.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.09608v1"
    },
    {
        "title": "Chi-squared Test for Binned, Gaussian Samples",
        "authors": [
            "Nicholas R. Hutzler"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We examine the $\\chi^2$ test for binned, Gaussian samples, including effects\ndue to the fact that the experimentally available sample standard deviation and\nthe unavailable true standard deviation have different statistical properties.\nFor data formed by binning Gaussian samples with bin size $n$, we find that the\nexpected value and standard deviation of the reduced $\\chi^2$ statistic is\n\\begin{equation} \\frac{n-1}{n-3}\\pm\n\\frac{n-1}{n-3}\\sqrt{\\frac{n-2}{n-5}}\\sqrt{\\frac{2}{N-1}}, \\end{equation} where\n$N$ is the total number of binned values. This is strictly larger in both mean\nand standard deviation than the value of $1\\pm (2/(N-1))^{1/2}$ reported in\nstandard treatments, which ignore the distinction between true and sample\nstandard deviation.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.11748v1"
    },
    {
        "title": "Precision annealing Monte Carlo methods for statistical data\n  assimilation and machine learning",
        "authors": [
            "Zheng Fang",
            "Adrian S. Wong",
            "Kangbo Hao",
            "Alexander J. A. Ty",
            "Henry D. I. Abarbanel"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  In statistical data assimilation (SDA) and supervised machine learning (ML),\nwe wish to transfer information from observations to a model of the processes\nunderlying those observations. For SDA, the model consists of a set of\ndifferential equations that describe the dynamics of a physical system. For ML,\nthe model is usually constructed using other strategies. In this paper, we\ndevelop a systematic formulation based on Monte Carlo sampling to achieve such\ninformation transfer. Following the derivation of an appropriate target\ndistribution, we present the formulation based on the standard\nMetropolis-Hasting (MH) procedure and the Hamiltonian Monte Carlo (HMC) method\nfor performing the high dimensional integrals that appear. To the extensive\nliterature on MH and HMC, we add (1) an annealing method using a hyperparameter\nthat governs the precision of the model to identify and explore the highest\nprobability regions of phase space dominating those integrals, and (2) a\nstrategy for initializing the state space search. The efficacy of the proposed\nformulation is demonstrated using a nonlinear dynamical model with chaotic\nsolutions widely used in geophysics.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.03137v2"
    },
    {
        "title": "Tackling limited simulation and small signals",
        "authors": [
            "Carlos A. Argüelles",
            "Austin Schneider",
            "Tianlu Yuan"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We present a new, analytic, Poisson likelihood derived, technique to account\nfor the statistical uncertainties inherent in simulation samples of limited\nsize. This method has better coverage properties than other techniques, is\nvalid for small data samples, and maintains good computational performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.10636v1"
    },
    {
        "title": "Universality of power-law exponents by means of maximum likelihood\n  estimation",
        "authors": [
            "Víctor Navas-Portella",
            "Álvaro González",
            "Isabel Serra",
            "Eduard Vives",
            "Álvaro Corral"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Power-law type distributions are extensively found when studying the\nbehaviour of many complex systems. However, due to limitations in data\nacquisition, empirical datasets often only cover a narrow range of observation,\nmaking it difficult to establish power-law behaviour unambiguously. In this\nwork we present a statistical procedure to merge different datasets with the\naim of obtaining a broader fitting range for the statistics of different\nexperiments or observations of the same system or the same universality class.\nThis procedure is applied to the Gutenberg-Richter law for earthquakes and for\nsynthetic earthquakes (acoustic emission events) generated in the laboratory:\nlabquakes. Different earthquake catalogs have been merged finding a\nGutenberg-Ricther law holding for more than eight orders of magnitude in\nseismic moment. The value of the exponent of the energy distribution of\nlabquakes depends on the material used in the compression experiments. By means\nof the procedure exposed in this manuscript, it has been found that the\nGutenberg-Richter law for earthquakes and charcoal labquakes can be\ncharacterized by the same power-law exponent.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12833v1"
    },
    {
        "title": "Time window to constrain the corner value of the global seismic-moment\n  distribution",
        "authors": [
            "Alvaro Corral",
            "Isabel Serra"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  It is well accepted that, at the global scale, the Gutenberg-Richter (GR) law\ndescribing the distribution of earthquake magnitude or seismic moment has to be\nmodified at the tail to properly account for the most extreme events. It is\ndebated, though, how much additional time of earthquake recording will be\nnecessary to properly constrain this tail. Using the global CMT catalog, we\nstudy how three modifications of the GR law that incorporate a corner-value\nparameter are compatible with the size of the largest observed earthquake in a\ngiven time window. Current data lead to a rather large range of parameter\nvalues (e.g., corner magnitude from 8.6 to 10.2 for the so-called tapered GR\ndistribution). Updating this estimation in the future will strongly depend on\nthe maximum magnitude observed, but, under reasonable assumptions, the range\nwill be substantially reduced by the end of this century, contrary to claims in\nprevious literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02516v1"
    },
    {
        "title": "Rare-Event Properties of the Nagel-Schreckenberg Model",
        "authors": [
            "Wiebke Staffeldt",
            "Alexander K. Hartmann"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We have studied the distribution of traffic flow $q$ for the\nNagel-Schreckenberg model by computer simulations. We applied a large-deviation\napproach, which allowed us to obtain the distribution $P(q)$ over more than one\nhundred decades in probability, down to probabilities like $10^{-140}$. This\nallowed us to characterize the flow distribution over a large range of the\nsupport and identify the characteristics of rare and even very rare traffic\nsituations. We observe a change of the distribution shape when increasing the\ndensity of cars from the free flow to the congestion phase. Furthermore, we\ncharacterize typical and rare traffic situations by measuring correlations of\n$q$ to other quantities like density of standing cars or number and size of\ntraffic jams.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.04681v1"
    },
    {
        "title": "Deconvolution of 3-D Gaussian kernels",
        "authors": [
            "Z. K. Silagadze"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Ulmer and Kaissl formulas for the deconvolution of one-dimensional Gaussian\nkernels are generalized to the three-dimensional case. The generalization is\nbased on the use of the scalar version of the Grad's multivariate Hermite\npolynomials which can be expressed through ordinary Hermite polynomials.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.07259v1"
    },
    {
        "title": "An open-source, end-to-end workflow for multidimensional photoemission\n  spectroscopy",
        "authors": [
            "Rui Patrick Xian",
            "Yves Acremann",
            "Steinn Ymir Agustsson",
            "Maciej Dendzik",
            "Kevin Bühlmann",
            "Davide Curcio",
            "Dmytro Kutnyakhov",
            "Frederico Pressacco",
            "Michael Heber",
            "Shuo Dong",
            "Tommaso Pincelli",
            "Jure Demsar",
            "Wilfried Wurth",
            "Philip Hofmann",
            "Martin Wolf",
            "Markus Scheidgen",
            "Laurenz Rettig",
            "Ralph Ernstorfer"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Characterization of the electronic band structure of solid state materials is\nroutinely performed using photoemission spectroscopy. Recent advancements in\nshort-wavelength light sources and electron detectors give rise to\nmultidimensional photoemission spectroscopy, allowing parallel measurements of\nthe electron spectral function simultaneously in energy, two momentum\ncomponents and additional physical parameters with single-event detection\ncapability. Efficient processing of the photoelectron event streams at a rate\nof up to tens of megabytes per second will enable rapid band mapping for\nmaterials characterization. We describe an open-source workflow that allows\nuser interaction with billion-count single-electron events in photoemission\nband mapping experiments, compatible with beamlines at $3^{\\text{rd}}$ and\n$4^{\\text{th}}$ generation light sources and table-top laser-based setups. The\nworkflow offers an end-to-end recipe from distributed operations on\nsingle-event data to structured formats for downstream scientific tasks and\nstorage to materials science database integration. Both the workflow and\nprocessed data can be archived for reuse, providing the infrastructure for\ndocumenting the provenance and lineage of photoemission data for future\nhigh-throughput experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.07714v3"
    },
    {
        "title": "Generic predictions of output probability based on complexities of\n  inputs and outputs",
        "authors": [
            "Kamaludin Dingle",
            "Guillermo Valle Pérez",
            "Ard A. Louis"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  For a broad class of input-output maps, arguments based on the coding theorem\nfrom algorithmic information theory (AIT) predict that simple (low Kolmogorov\ncomplexity) outputs are exponentially more likely to occur upon uniform random\nsampling of inputs than complex outputs are. Here, we derive probability bounds\nthat are based on the complexities of the inputs as well as the outputs, rather\nthan just on the complexities of the outputs. The more that outputs deviate\nfrom the coding theorem bound, the lower the complexity of their inputs. Our\nnew bounds are tested for an RNA sequence to structure map, a finite state\ntransducer and a perceptron. These results open avenues for AIT to be more\nwidely used in physics.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.00971v1"
    },
    {
        "title": "Identifying extra high frequency gravitational waves generated from\n  oscillons with cuspy potentials using deep neural networks",
        "authors": [
            "Li Li Wang",
            "Jin Li",
            "Nan Yang",
            "Xin Li"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  During oscillations of cosmology inflation around the minimum of a cuspy\npotential after inflation, the existence of extra high frequency gravitational\nwaves (HFGWs) (GHz) has been proven effectively recently. Based on the\nelectromagnetic resonance system for detecting such extra HFGWs, we adopt a new\ndata processing scheme to identify the corresponding GW signal, which is the\ntransverse perturbative photon fluxes (PPF). In order to overcome the problems\nof low efficiency and high interference in traditional data processing methods,\nwe adopt deep learning to extract PPF and make some source parameters\nestimation. Deep learning is able to provide an effective method to realize\nclassification and prediction tasks. Meanwhile, we also adopt anti-overfitting\ntechnique and make adjustment of some hyperparameters in the course of study,\nwhich improve the performance of classifier and predictor to a certain extent.\nHere the convolutional neural network (CNN) is used to implement deep learning\nprocess concretely. In this case, we investigate the classification accuracy\nvarying with the ratio between the number of positive and negative samples.\nWhen such ratio exceeds to 0.11, the accuracy could reach up to 100%.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.07862v1"
    },
    {
        "title": "Sampling strategy and statistical analysis for radioactive waste\n  characterization",
        "authors": [
            "Nadia Perot",
            "Alexandre Le Cocguen",
            "Dominique Carré",
            "Hervé Lamotte",
            "Anne Duhart-Barone",
            "Ingmar Pointeau"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  This paper describes the methodology we have developed to define a sampling\nstrategy adapted to operational constraints in order to characterize the\ndihydrogen flow rate of 2714 nuclear waste drums produced by radiolysis\nreaction of organic mixed with \\&alpha;-emitters. The objective was to perform\nfew but relevant measurements. Thus, a sample of only 38 drums has been\nselected to be measured. Statistical analysis of drum measurement data of\ndihydrogen rate provided an estimation of the mean and the upper bound of the\nphysical quantity of interest which gave a good convergence with global\nmeasurements from the ventilation system of the facility. Thereafter,\nperforming a factorial data analysis has demonstrated the representativeness of\nthe measurement data set and the sampling strategy assumption validity.\nMoreover, it provided information that has been used for a regression analysis\nto develop a linear prediction model of dihydrogen flow rate production for the\nwaste drum characterization.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08468v1"
    },
    {
        "title": "zfit: scalable pythonic fitting",
        "authors": [
            "Jonas Eschle",
            "Albert Puig Navarro",
            "Rafael Silva Coutinho",
            "Nicola Serra"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Statistical modeling is a key element in many scientific fields and\nespecially in High-Energy Physics (HEP) analysis. The standard framework to\nperform this task in HEP is the C++ ROOT/RooFit toolkit; with Python bindings\nthat are only loosely integrated into the scientific Python ecosystem. In this\npaper, zfit, a new alternative to RooFit written in pure Python, is presented.\nMost of all, zfit provides a well defined high-level API and workflow for\nadvanced model building and fitting, together with an implementation on top of\nTensorFlow, allowing a transparent usage of CPUs and GPUs. It is designed to be\nextendable in a very simple fashion, allowing the usage of cutting-edge\ndevelopments from the scientific Python ecosystem in a transparent way. The\nmain features of zfit are introduced, and its extension to data analysis,\nespecially in the context of HEP experiments, is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.13429v2"
    },
    {
        "title": "Template-free Pulse Height Estimation of Microcalorimeter Responses with\n  PCA",
        "authors": [
            "To Chin Yu"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We present a template-free method of estimating pulse height of\nmicro-calorimeter signals based on principal component analysis (PCA). The\nmethod is shown to improve the resolution on a simulated dataset by 25\\%\ncompared to the standard filtering technique that uses the averaged pulse as an\nestimation for the signal template. This technique is useful for reconstruction\nof pulses in micro-calorimeters with a high degree of pulse-shape variation\nwhere finding the correct signal templates is difficult.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.14261v2"
    },
    {
        "title": "Comparison of unfolding methods using RooFitUnfold",
        "authors": [
            "Lydia Brenner",
            "Pim Verschuuren",
            "Rahul Balasubramanian",
            "Carsten Burgard",
            "Vincent Croft",
            "Glen Cowan",
            "Wouter Verkerke"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  In this paper we describe RooFitUnfold, an extension of the RooFit\nstatistical software package to treat unfolding problems, and which includes\nmost of the unfolding methods that commonly used in particle physics. The\npackage provides a common interface to these algorithms as well as common\nuniform methods to evaluate their performance in terms of bias, variance and\ncoverage. In this paper we exploit this common interface of RooFitUnfold to\ncompare the performance of unfolding with the Richardson-Lucy, Iterative\nDynamically Stabilized, Tikhonov, Gaussian Process, Bin-by-bin and inversion\nmethods on several example problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.14654v2"
    },
    {
        "title": "A robust principal component analysis for outlier identification in\n  messy microcalorimeter data",
        "authors": [
            "J. W. Fowler",
            "B. K. Alpert",
            "Y. -I. Joe",
            "G. C. O'Neil",
            "D. S. Swetz",
            "J. N. Ullom"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  A principal component analysis (PCA) of clean microcalorimeter pulse records\ncan be a first step beyond statistically optimal linear filtering of pulses\ntowards a fully non-linear analysis. For PCA to be practical on spectrometers\nwith hundreds of sensors, an automated identification of clean pulses is\nrequired. Robust forms of PCA are the subject of active research in machine\nlearning. We examine a version known as coherence pursuit that is simple, fast,\nand well matched to the automatic identification of outlier records, as needed\nfor microcalorimeter pulse analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00423v1"
    },
    {
        "title": "Estimating quantities conserved by virtue of scale invariance in\n  timeseries",
        "authors": [
            "Erik D. Fagerholm",
            "W. M. C. Foulkes",
            "Yasir Gallero-Salas",
            "Fritjof Helmchen",
            "Karl J. Friston",
            "Rosalyn J. Moran",
            "Robert Leech"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  In contrast to the symmetries of translation in space, rotation in space, and\ntranslation in time, the known laws of physics are not universally invariant\nunder transformation of scale. However, the action can be invariant under\nchange of scale in the special case of a scale free dynamical system that can\nbe described in terms of a Lagrangian, that itself scales inversely with time.\nCrucially, this means symmetries under change of scale can exist in dynamical\nsystems under certain constraints. Our contribution lies in the derivation of a\ngeneralised scale invariant Lagrangian - in the form of a power series\nexpansion - that satisfies these constraints. This generalised Lagrangian\nfurnishes a normal form for dynamic causal models (i.e., state space models\nbased upon differential equations) that can be used to distinguish scale\ninvariance (scale symmetry) from scale freeness in empirical data. We establish\nface validity with an analysis of simulated data and then show how scale\ninvariance can be identified - and how the associated conserved quantities can\nbe estimated - in neuronal timeseries.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00775v1"
    },
    {
        "title": "Parameter uncertainties in weighted unbinned maximum likelihood fits",
        "authors": [
            "Christoph Langenbruch"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Parameter estimation via unbinned maximum likelihood fits is central for many\nanalyses performed in high energy physics. Unbinned maximum likelihood fits\nusing event weights, for example to statistically subtract background\ncontributions via the sPlot formalism, or to correct for acceptance effects,\nhave recently seen increasing use in the community. However, it is well known\nthat the naive approach to the estimation of parameter uncertainties via the\nsecond derivative of the logarithmic likelihood does not yield confidence\nintervals with the correct coverage in the presence of event weights. This\npaper derives the asymptotically correct expressions and compares them with\nseveral commonly used approaches for the determination of parameter\nuncertainties, some of which are shown to not generally be asymptotically\ncorrect. In addition, the effect of uncertainties on event weights is\ndiscussed, including uncertainties that can arise from the presence of nuisance\nparameters in the determination of sWeights.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.01303v4"
    },
    {
        "title": "Randomized Computer Vision Approaches for Pattern Recognition in Timepix\n  and Timepix3 Detectors",
        "authors": [
            "Petr Mánek",
            "Benedikt Bergmann",
            "Petr Burian",
            "Lukáš Meduna",
            "Stanislav Pospíšil",
            "Michal Suk"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Timepix and Timepix3 are hybrid pixel detectors ($256\\times 256$ pixels),\ncapable of tracking ionizing particles as isolated clusters of pixels. To\nefficiently analyze such clusters at potentially high rates, we introduce\nmultiple randomized pattern recognition algorithms inspired by computer vision.\nOffering desirable probabilistic bounds on accuracy and complexity, the\npresented methods are well-suited for use in real-time applications, and some\nmay even be modified to tackle trans-dimensional problems. In Timepix\ndetectors, which do not support data-driven acquisition, they have been shown\nto correctly separate clusters of overlapping tracks. In Timepix3 detectors,\nsimultaneous acquisition of Time-of-Arrival (ToA) and Time-over-Threshold (ToT)\npixel data enables reconstruction of the depth, transitioning from 2D to 3D\npoint clouds. The presented algorithms have been tested on simulated inputs,\ntest beam data from the Heidelberg Ion therapy Center and the Super Proton\nSynchrotron and were applied to data acquired in the MoEDAL and ATLAS\nexperiments at CERN.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.02367v1"
    },
    {
        "title": "Classification of tokamak plasma confinement states with convolutional\n  recurrent neural networks",
        "authors": [
            "F. Matos",
            "V. Menkovski",
            "F. Felici",
            "A. Pau",
            "F. Jenko"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  During a tokamak discharge, the plasma can vary between different confinement\nregimes: Low (L), High (H) and, in some cases, a temporary (intermediate\nstate), called Dithering (D). In addition, while the plasma is in H mode, Edge\nLocalized Modes (ELMs) can occur. The automatic detection of changes between\nthese states, and of ELMs, is important for tokamak operation. Motivated by\nthis, and by recent developments in Deep Learning (DL), we developed and\ncompared two methods for automatic detection of the occurrence of L-D-H\ntransitions and ELMs, applied on data from the TCV tokamak. These methods\nconsist in a Convolutional Neural Network (CNN) and a Convolutional Long Short\nTerm Memory Neural Network (Conv-LSTM). We measured our results with regards to\nELMs using ROC curves and Youden's score index, and regarding state detection\nusing Cohen's Kappa Index.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.04234v1"
    },
    {
        "title": "A new approach on estimating the fluid temperature in a multiphase flow\n  system using particle filter method",
        "authors": [
            "Zhuoran Dang"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Fluid temperature is important for the analysis of the heat transfers in\nthermal hydraulics. An accurate measurement or estimation of the fluid\ntemperature in multiphase flows is challenging. This is due to that the\nthermocouple signal that mixes with temperature signals for each phase and\nnon-negligible noises. This study provides a new approach to estimate the local\nfluid temperature in multiphase flows using experimental time-series\ntemperature signal. The thermocouple signal is considered to be a sequence with\nMarkov property and the particle filter method is utilized in the new method to\nextract the fluid temperature. A complete description of the new method is\npresented in this article.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.01803v1"
    },
    {
        "title": "Skeptical combination of experimental results using JAGS/rjags with\n  application to the K$^{\\pm}$ mass determination",
        "authors": [
            "Giulio D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The question of how to combine experimental results that `appear' to be in\nmutual disagreement, treated in detail years ago in a previous paper, is\nrevisited. The first novelty of the present note is the explicit use of\ngraphical models, in order to make the deterministic and probabilistic links\nbetween the variables of interest more evident. Then, instead of aiming for\nresults in closed formulae, the integrals of interest are evaluated by {\\em\nMarkov Chain Monte Carlo} (MCMC) sampling, with the algorithms (typically Gibbs\nSampler) implemented in the package JAGS (\"Just Another Gibbs Sampler\"). For\nconvenience, the JAGS functions are called from R scripts, thus gaining the\nadvantage given by the rich collection of mathematical, statistical and\ngraphical functions included in the R installation. The results of the previous\npaper are thus easily re-obtained and the method is applied to the\ndetermination of the charged kaon mass. This note, based on lectures to PhD\nstudents and young researchers has been written with a didactic touch, and the\nrelevant JAGS/rjags code is provided. (A curious bias arising from the\nsequential application of the $\\sqrt{\\chi^2/\\nu}$ scaling prescription to\n'apparently' discrepant results, found here, will be discussed in more detail\nin a separate paper.)\n",
        "pdf_link": "http://arxiv.org/pdf/2001.03466v1"
    },
    {
        "title": "Spatial Deconvolution of Aerial Radiometric Survey and its application\n  to the Fallout from a Radiological Dispersal Device",
        "authors": [
            "Laurel E. Sinclair",
            "Richard Fortin"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Mapping radioactive contamination using aerial survey measurements is an area\nunder active investigation today. The radiometric aerial survey technique has\nbeen extensively applied following reactor accidents and also would provide a\nkey tool for response to a malicious radiological or nuclear incident. Methods\nexist to calibrate the aerial survey system for quantification of the\nconcentration of natural radionuclides, which can provide guidance. However,\nthese methods have anticipated a spatial distribution of the source which is\nlarge in comparison to the survey altitude. In rapid emergency-response aerial\nsurveys of areas of safety concern, deposits of relatively small spatial extent\nmay be expected. The activity of such spatially restricted hot spots is\nunderestimated using the traditional methods. We present here a spatial\ndeconvolution method which can recover some of the variation smoothed out by\nthe averaging due to survey at altitude. We show that the method can recover\nthe true spatial distribution of concentration of a synthetic source. We then\napply the method to real aerial survey data collected following detonation of a\nradiological dispersal device. The findings and implications of the\ndeconvolution are then discussed by reference to a groundbased truckborne\nsurvey over the same contamination.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.03582v1"
    },
    {
        "title": "ABCNet: An attention-based method for particle tagging",
        "authors": [
            "Vinicius Mikuni",
            "Florencia Canelli"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  In high energy physics, graph-based implementations have the advantage of\ntreating the input data sets in a similar way as they are collected by collider\nexperiments. To expand on this concept, we propose a graph neural network\nenhanced by attention mechanisms called ABCNet. To exemplify the advantages and\nflexibility of treating collider data as a point cloud, two physically\nmotivated problems are investigated: quark-gluon discrimination and pileup\nreduction. The former is an event-by-event classification while the latter\nrequires each reconstructed particle to receive a classification score. For\nboth tasks ABCNet shows an improved performance compared to other algorithms\navailable.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.05311v2"
    },
    {
        "title": "Variational Dropout Sparsification for Particle Identification speed-up",
        "authors": [
            "Artem Ryzhikov",
            "Denis Derkach",
            "Mikhail Hushchyn"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Accurate particle identification (PID) is one of the most important aspects\nof the LHCb experiment. Modern machine learning techniques such as neural\nnetworks (NNs) are efficiently applied to this problem and are integrated into\nthe LHCb software. In this research, we discuss novel applications of neural\nnetwork speed-up techniques to achieve faster PID in LHC upgrade conditions. We\nshow that the best results are obtained using variational dropout\nsparsification, which provides a prediction (feedforward pass) speed increase\nof up to a factor of sixteen even when compared to a model with shallow\nnetworks.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.07493v1"
    },
    {
        "title": "WISDoM: characterizing neurological timeseries with the Wishart\n  distribution",
        "authors": [
            "Carlo Mengucci",
            "Daniel Remondini",
            "Gastone Castellani",
            "Enrico Giampieri"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  WISDoM (Wishart Distributed Matrices) is a new framework for the\nquantification of deviation of symmetric positive-definite matrices associated\nto experimental samples, like covariance or correlation matrices, from expected\nones governed by the Wishart distribution WISDoM can be applied to tasks of\nsupervised learning, like classification, in particular when such matrices are\ngenerated by data of different dimensionality (e.g. time series with same\nnumber of variables but different time sampling). We show the application of\nthe method in two different scenarios. The first is the ranking of features\nassociated to electro encephalogram (EEG) data with a time series design,\nproviding a theoretically sound approach for this type of studies. The second\nis the classification of autistic subjects of the ABIDE study, using brain\nconnectivity measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.10342v2"
    },
    {
        "title": "Deconvolution of the High Energy Particle Physics Data with Machine\n  Learning",
        "authors": [
            "Bora Işıldak",
            "Alper Hayreter",
            "Aidan R. Wiederhold"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  A method for correcting smearing effects using machine learning technique is\npresented. Compared to the standard deconvolution approaches in high energy\nparticle physics, the method can use more than one reconstructed variable to\npredict the value of unsmeared quantity on an event-by-event basis. In this\nparticular study, deconvolution is interpreted as a classification problem, and\nneural networks (NN) are trained to deconvolute the Z boson invariant mass\nspectrum generated with MadGraph and pythia8 Monte Carlo event generators in\norder to prove the principle. Results obtained from the machine learning method\nis presented and compared with the results obtained with traditional methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.10814v1"
    },
    {
        "title": "Fast Time Series Detrending with Applications to Heart Rate Variability\n  Analysis",
        "authors": [
            "M. Andrecut"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Here we discuss a new fast detrending method for the non-stationary RR time\nseries used in Heart Rate Variability analysis. The described method is based\non the diffusion equation, and we show numerically that it is equivalent to the\nwidely used Smoothing Priors Approach (SPA) and Wavelet Smoothing Approach\n(WSA) methods. The speed of the proposed method is comparable to the WSA method\nand it is several orders of magnitude faster than the SPA method, which makes\nit suitable for very long time series analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.06509v1"
    },
    {
        "title": "Using machine learning to separate hadronic and electromagnetic\n  interactions in the GlueX forward calorimeter",
        "authors": [
            "Rebecca Barsotti",
            "Matthew R. Shepherd"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The GlueX forward calorimeter is an array of 2800 lead glass modules that was\nconstructed to detect photons produced in the decays of hadrons. A background\nto this process originates from hadronic interactions in the calorimeter,\nwhich, in some instances, can be difficult to distinguish from low energy\nphoton interactions. Machine learning techniques were applied to the\nclassification of particle interactions in the GlueX forward calorimeter. The\nalgorithms were trained on data using decays of the $\\omega$ meson, which\ncontain both true photons and charged particles that interact with the\ncalorimeter. Algorithms were evaluated on efficiency, rate of false positives,\nrun time, and implementation complexity. An algorithm that utilizes a\nmulti-layer perceptron neural net was deployed in the GlueX software stack and\nprovides a signal efficiency of 85% with a background rejection of 60% for an\ninclusive $\\pi^0$ data sample for an intermediate quality constraint.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.09530v1"
    },
    {
        "title": "Automated detector simulation and reconstruction parametrization using\n  machine learning",
        "authors": [
            "D. Benjamin",
            "S. V. Chekanov",
            "W. Hopkins",
            "Y. Li",
            "J. R. Love"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Rapidly applying the effects of detector response to physics objects (e.g.\nelectrons, muons, showers of particles) is essential in high energy physics.\nCurrently available tools for the transformation from truth-level physics\nobjects to reconstructed detector-level physics objects involve manually\ndefining resolution functions. These resolution functions are typically derived\nin bins of variables that are correlated with the resolution (e.g.\npseudorapidity and transverse momentum). This process is time consuming,\nrequires manual updates when detector conditions change, and can miss important\ncorrelations. Machine learning offers a way to automate the process of building\nthese truth-to-reconstructed object transformations and can capture complex\ncorrelation for any given set of input variables. Such machine learning\nalgorithms, with sufficient optimization, could have a wide range of\napplications: improving phenomenological studies by using a better detector\nrepresentation, allowing for more efficient production of Geant4 simulation by\nonly simulating events within an interesting part of phase space, and studies\non future experimental sensitivity to new physics.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.11516v1"
    },
    {
        "title": "The statistical physics of discovering exogenous and endogenous factors\n  in a chain of events",
        "authors": [
            "Shinsuke Koyama",
            "Shigeru Shinomoto"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Event occurrence is not only subject to the environmental changes, but is\nalso facilitated by the events that have occurred in a system. Here, we develop\na method for estimating such extrinsic and intrinsic factors from a single\nseries of event-occurrence times. The analysis is performed using a model that\ncombines the inhomogeneous Poisson process and the Hawkes process, which\nrepresent exogenous fluctuations and endogenous chain-reaction mechanisms,\nrespectively. The model is fit to a given dataset by minimizing the free\nenergy, for which statistical physics and a path-integral method are utilized.\nBecause the process of event occurrence is stochastic, parameter estimation is\ninevitably accompanied by errors, and it can ultimately occur that exogenous\nand endogenous factors cannot be captured even with the best estimator. We\nobtained four regimes categorized according to whether respective factors are\ndetected. By applying the analytical method to real time series of debate in a\nsocial-networking service, we have observed that the estimated exogenous and\nendogenous factors are close to the first comments and the follow-up comments,\nrespectively. This method is general and applicable to a variety of data, and\nwe have provided an application program, by which anyone can analyze any series\nof event times.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.00659v1"
    },
    {
        "title": "HEPLike: an open source framework for experimental likelihood evaluation",
        "authors": [
            "Jihyun Bhom",
            "Marcin Chrzaszcz"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We present a computer framework to store and evaluate likelihoods coming from\nHigh Energy Physics experiments. Due to its flexibility it can be interfaced\nwith existing fitting codes and allows to uniform the interpretation of the\nexperimental results among users. The code is provided with large open\ndatabase, which contains the experimental measurements. The code is of use for\nusers who perform phenomenological studies, global fits or experimental\naverages.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.03956v1"
    },
    {
        "title": "Ghost Imaging with the Optimal Binary Sampling",
        "authors": [
            "Dongyue Yang",
            "Guohua Wu",
            "Bin Luo",
            "Longfei Yin"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  To extract the maximum information about the object from a series of binary\nsamples in ghost imaging applications, we propose and demonstrate a framework\nfor optimizing the performance of ghost imaging with binary sampling to\napproach the results without binarization. The method is based on maximizing\nthe information content of the signal arm detection, by formulating and solving\nthe appropriate parameter estimation problem - finding the binarization\nthreshold that would yield the reconstructed image with optimal Fisher\ninformation properties. Applying the 1-bit quantized Poisson statistics to a\nghost-imaging model with pseudo-thermal light, we derive the fundamental limit,\ni.e., the Cramer-Rao lower bound, as the benchmark for the evaluation of the\naccuracy of the estimator. Our theoertical model and experimental results\nsuggest that, with the optimal binarization threshold, coincident with the\nstatistical mean of all bucket samples, and large number of measurements, the\nperformance of binary sampling GI can approach that of the ordinary one without\nbinarization.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05313v1"
    },
    {
        "title": "Optimal statistical inference in the presence of systematic\n  uncertainties using neural network optimization based on binned Poisson\n  likelihoods with nuisance parameters",
        "authors": [
            "Stefan Wunsch",
            "Simon Jörger",
            "Roger Wolf",
            "Günter Quast"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Data analysis in science, e.g., high-energy particle physics, is often\nsubject to an intractable likelihood if the observables and observations span a\nhigh-dimensional input space. Typically the problem is solved by reducing the\ndimensionality using feature engineering and histograms, whereby the latter\ntechnique allows to build the likelihood using Poisson statistics. However, in\nthe presence of systematic uncertainties represented by nuisance parameters in\nthe likelihood, the optimal dimensionality reduction with a minimal loss of\ninformation about the parameters of interest is not known. This work presents a\nnovel strategy to construct the dimensionality reduction with neural networks\nfor feature engineering and a differential formulation of histograms so that\nthe full workflow can be optimized with the result of the statistical\ninference, e.g., the variance of a parameter of interest, as objective. We\ndiscuss how this approach results in an estimate of the parameters of interest\nthat is close to optimal and the applicability of the technique is demonstrated\nwith a simple example based on pseudo-experiments and a more complex example\nfrom high-energy particle physics.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.07186v3"
    },
    {
        "title": "Transfer Learning in Automated Gamma Spectral Identification",
        "authors": [
            "Eric T. Moore",
            "Johanna L. Turk",
            "William P. Ford",
            "Nathan J. Hoteling",
            "Lance S. McLean"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The models and weights of prior trained Convolutional Neural Networks (CNN)\ncreated to perform automated isotopic classification of time-sequenced\ngamma-ray spectra, were utilized to provide source domain knowledge as training\non new domains of potential interest. The previous results were achieved solely\nusing modeled spectral data. In this work we attempt to transfer the knowledge\ngained to the new, if similar, domain of solely measured data. The ability to\ntrain on modeled data and predict on measured data will be crucial in any\nsuccessful data-driven approach to this problem space.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.10524v1"
    },
    {
        "title": "Iterative Bayesian Monte Carlo for nuclear data evaluation",
        "authors": [
            "E. Alhassan",
            "D. Rochman",
            "A. Vasiliev",
            "M. Wohlmuther",
            "M. Hursin",
            "A. J. Koning",
            "H. Ferroukhi"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  In this work, we explore the use of an iterative Bayesian Monte Carlo (IBM)\nprocedure for nuclear data evaluation within a Talys Evaluated Nuclear data\nLibrary (TENDL) framework. In order to identify the model and parameter\ncombinations that reproduce selected experimental data, different physical\nmodels implemented within the TALYS code, were sampled and varied\nsimultaneously to produce random input files with unique model combinations.\nAll the models considered were assumed to be equal a priori. Parameters to\nthese models were then varied simultaneously using the TALYS code system to\nproduce a set of random ENDF files which were processed into x-y tables for\ncomparison with selected experimental data from the EXFOR database within a\nBayesian framework. To improve our fit to experimental data, we iteratively\nupdate our 'best' file - the file that maximises the likelihood function - by\nre-sampling model parameters around this file. The method proposed has been\napplied for the evaluation of p+Cd-111 and Co-59 between 1 - 100 MeV incident\nenergy region. Finally, the adjusted files were compared with experimental data\nfrom the EXFOR database as well as with evaluations from the TENDL-2017 and\nJENDL-4.0/HE nuclear data libraries.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.10827v1"
    },
    {
        "title": "Stochastic interpolation of sparsely sampled time series via multi-point\n  fractional Brownian bridges",
        "authors": [
            "J. Friedrich",
            "S. Gallon",
            "A. Pumir",
            "R. Grauer"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We propose and test a method to interpolate sparsely sampled signals by a\nstochastic process with a broad range of spatial and/or temporal scales. To\nthis end, we extend the notion of a fractional Brownian bridge, defined as\nfractional Brownian motion with a given scaling (Hurst) exponent $H$ and with\nprescribed start and end points, to a bridge process with an arbitrary number\nof intermediate and non-equidistant points. Determining the optimal value of\nthe Hurst exponent, $H_{opt}$, appropriate to interpolate the sparse signal, is\na very important step of our method. We demonstrate the validity of our method\non a signal from fluid turbulence in a high Reynolds number flow and discuss\nthe implications of the non-self-similar character of the signal. The method\nintroduced here could be instrumental in several physical problems, including\nastrophysics, particle tracking, specific tailoring of surrogate data, as well\nas in domains of natural and social sciences.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.07591v2"
    },
    {
        "title": "Comparison of different ML methods applied to the classification of\n  events with ttbar in the final state at the ATLAS experiment",
        "authors": [
            "Samuel Campo Martínez",
            "José Salt",
            "Santiago González de la Hoz",
            "Miguel Villaplana"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  This contribution describes the experience with the application of different\nMachine Learning (ML) techniques to a physics analysis case. The use case\nchosen is the classification of top-antitop events coming from BSM or from SM\nusing data from a repository of simulated events. The features of these events\nare represented by their kinematic observables. The initial objective was to\ncompare different ML methods in order to see whether they can lead to an\nimprovement in the classification, but the work has also helped us to test many\nvariations in the methods by changing hyper-parameters, using different\noptimisers, ensembles, etc. With this information we have been able to conduct\na comparative study that is useful for ensuring as complete control as possible\nof the methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.02135v1"
    },
    {
        "title": "Methods to Quantify Dislocation Behavior with Dark-field X-ray\n  Microscopy Timescans of Single-Crystal Aluminum",
        "authors": [
            "Arnulfo Gonzalez",
            "Marylesa Howard",
            "Sean Breckling",
            "Leora E. Dresselhaus-Marais"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Crystal defects play a large role in how materials respond to their\nsurroundings, yet there are many uncertainties in how extended defects form,\nmove, and interact deep beneath a material's surface. A newly developed imaging\ndiagnostic, dark-field X-ray microscopy (DFXM) can now visualize the behavior\nof line defects, known as dislocations, in materials under varying conditions.\nDFXM images visualize dislocations by imaging the very subtle long-range\ndistortions in the material's crystal lattice, which produce a characteristic\nadjoined pair of bright and dark regions. Full analysis of how these\ndislocations evolve can be used to refine material models, however, it requires\nquantitative characterization of the statistics of their shape, position and\nmotion. In this paper, we present a semi-automated approach to effectively\nisolate, track, and quantify the behavior of dislocations as composite objects.\nThis analysis drives the statistical characterization of the defects, to\ninclude dislocation velocity and orientation in the crystal, for example, and\nis demonstrated on DFXM images measuring the evolution of defects at 98$\\%$ of\nthe melting temperature for single-crystal aluminum, collected at the European\nSynchrotron Radiation Facility.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04972v1"
    },
    {
        "title": "On Explaining the Surprising Success of Reservoir Computing Forecaster\n  of Chaos? The Universal Machine Learning Dynamical System with Contrasts to\n  VAR and DMD",
        "authors": [
            "Erik Bollt"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Machine learning has become a widely popular and successful paradigm,\nincluding in data-driven science and engineering. A major application problem\nis data-driven forecasting of future states from a complex dynamical.\nArtificial neural networks (ANN) have evolved as a clear leader amongst many\nmachine learning approaches, and recurrent neural networks (RNN) are considered\nto be especially well suited for forecasting dynamical systems. In this\nsetting, the echo state networks (ESN) or reservoir computer (RC) have emerged\nfor their simplicity and computational complexity advantages. Instead of a\nfully trained network, an RC trains only read-out weights by a simple,\nefficient least squares method. What is perhaps quite surprising is that\nnonetheless an RC succeeds to make high quality forecasts, competitively with\nmore intensively trained methods, even if not the leader. There remains an\nunanswered question as to why and how an RC works at all, despite randomly\nselected weights. We explicitly connect the RC with linear activation and\nlinear read-out to well developed time-series literature on vector\nautoregressive averages (VAR) that includes theorems on representability\nthrough the WOLD theorem, which already perform reasonably for short term\nforecasts. In the case of a linear activation and now popular quadratic\nread-out RC, we explicitly connect to a nonlinear VAR (NVAR), which performs\nquite well. Further, we associate this paradigm to the now widely popular\ndynamic mode decomposition (DMD), and thus these three are in a sense different\nfaces of the same thing. We illustrate our observations in terms of popular\nbenchmark examples including Mackey-Glass differential delay equations and the\nLorenz63 system.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.06530v6"
    },
    {
        "title": "Navigating differential structures in complex networks",
        "authors": [
            "Leonardo L. Portes",
            "Michael Small"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Structural changes in a network representation of a system (e.g.,different\nexperimental conditions, time evolution), can provide insight on its\norganization, function and on how it responds to external perturbations. The\ndeeper understanding of how gene networks cope with diseases and treatments is\nmaybe the most incisive demonstration of the gains obtained through this\ndifferential network analysis point-of-view, which lead to an explosion of new\nnumeric techniques in the last decade. However, {\\it where} to focus ones\nattention, or how to navigate through the differential structures can be\noverwhelming even for few experimental conditions. In this paper, we propose a\ntheory and a methodological implementation for the characterization of shared\n\"structural roles\" of nodes simultaneously within and between networks, whose\noutcome is a highly {\\em interpretable} map. The main features and accuracy are\ninvestigated with numerical benchmarks generated by a stochastic block model.\nResults show that it can provide nuanced and interpretable information in\nscenarios with very different (i) community sizes and (ii) total number of\ncommunities, and (iii) even for a large number of 100 networks been compared\n(e.g., for 100 different experimental conditions). Then, we show evidence that\nthe strength of the method is its \"story-telling\"-like characterization of the\ninformation encoded in a set of networks, which can be used to pinpoint\nunexpected differential structures, leading to further investigations and\nproviding new insights. We provide an illustrative, exploratory analysis of\nfour gene co-expression networks from two cell types $\\times$ two treatments\n(interferon-$\\beta$ stimulated or control). The method proposed here allowed us\nto elaborate and test a set of very specific hypotheses related to {\\em unique}\nand {\\em subtle} nuances of the structural differences between these networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.09382v1"
    },
    {
        "title": "What is the likelihood function, and how is it used in particle physics?",
        "authors": [
            "Robert D. Cousins"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Likelihood functions are ubiquitous in data analyses at the LHC and elsewhere\nin particle physics. Partly because \"probability\" and \"likelihood\" are virtual\nsynonyms in everyday English, but crucially distinct in data analysis, there is\ngreat potential for confusion. Furthermore, each of various approaches to\nstatistical inference (likelihoodist, Neyman-Pearson, Bayesian) uses the\nlikelihood function in different ways. This note is intended to provide a brief\nintroduction at the advanced undergraduate or beginning graduate student level,\nciting a few papers giving examples and containing numerous pointers to the\nvast literature on likelihood. The Likelihood Principle (routinely violated in\nparticle physics analyses) is mentioned as an unresolved issue in the\nphilosophical foundations of statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.00356v1"
    },
    {
        "title": "Predictive Capability Maturity Quantification using Bayesian Network",
        "authors": [
            "Linyu Lin",
            "Nam Dinh"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  In nuclear engineering, modeling and simulations (M&Ss) are widely applied to\nsupport risk-informed safety analysis. Since nuclear safety analysis has\nimportant implications, a convincing validation process is needed to assess\nsimulation adequacy, i.e., the degree to which M&S tools can adequately\nrepresent the system quantities of interest. However, due to data gaps,\nvalidation becomes a decision-making process under uncertainties. Expert\nknowledge and judgments are required to collect, choose, characterize, and\nintegrate evidence toward the final adequacy decision. However, in validation\nframeworks CSAU: Code Scaling, Applicability, and Uncertainty (NUREG/CR-5249)\nand EMDAP: Evaluation Model Development and Assessment Process (RG 1.203), such\na decision-making process is largely implicit and obscure. When scenarios are\ncomplex, knowledge biases and unreliable judgments can be overlooked, which\ncould increase uncertainty in the simulation adequacy result and the\ncorresponding risks. Therefore, a framework is required to formalize the\ndecision-making process for simulation adequacy in a practical, transparent,\nand consistent manner. This paper suggests a framework \"Predictive Capability\nMaturity Quantification using Bayesian network (PCMQBN)\" as a quantified\nframework for assessing simulation adequacy based on information collected from\nvalidation activities. A case study is prepared for evaluating the adequacy of\na Smoothed Particle Hydrodynamic simulation in predicting the hydrodynamic\nforces onto static structures during an external flooding scenario. Comparing\nto the qualitative and implicit adequacy assessment, PCMQBN is able to improve\nconfidence in the simulation adequacy result and to reduce expected loss in the\nrisk-informed safety analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.03373v1"
    },
    {
        "title": "Heteroscedasticity and angle resolution in high-energy particle\n  tracking: revisiting \"Beyond the $\\sqrt{\\mathrm{N}}$ limit of the least\n  squares resolution and the lucky model\", by G. Landi and G. E. Landi",
        "authors": [
            "Denis Bernard"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  I re-examine a recent work by G. Landi and G. E. Landi. [arXiv:1808.06708\n[physics.ins-det]], in which the authors claim that the resolution of a tracker\nca vary linearly with the number of detection layers, $N$, that is, faster than\nthe commonly known $\\sqrt{N}$ variation, for a tracker of fixed length, in case\nthe precision of the position measurement is allowed to vary from layer to\nlayer, i.e. heteroscedasticity, and an appropriate analysis method, a weighted\nleast squares fit, is used.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.03451v1"
    },
    {
        "title": "Breaking Symmetries of the Reservoir Equations in Echo State Networks",
        "authors": [
            "Joschka Herteux",
            "Christoph Räth"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Reservoir computing has repeatedly been shown to be extremely successful in\nthe prediction of nonlinear time-series. However, there is no complete\nunderstanding of the proper design of a reservoir yet. We find that the\nsimplest popular setup has a harmful symmetry, which leads to the prediction of\nwhat we call mirror-attractor. We prove this analytically. Similar problems can\narise in a general context, and we use them to explain the success or failure\nof some designs. The symmetry is a direct consequence of the hyperbolic tangent\nactivation function. Further, four ways to break the symmetry are compared\nnumerically: A bias in the output, a shift in the input, a quadratic term in\nthe readout, and a mixture of even and odd activation functions. Firstly, we\ntest their susceptibility to the mirror-attractor. Secondly, we evaluate their\nperformance on the task of predicting Lorenz data with the mean shifted to\nzero. The short-time prediction is measured with the forecast horizon while the\nlargest Lyapunov exponent and the correlation dimension are used to represent\nthe climate. Finally, the same analysis is repeated on a combined dataset of\nthe Lorenz attractor and the Halvorsen attractor, which we designed to reveal\npotential problems with symmetry. We find that all methods except the output\nbias are able to fully break the symmetry with input shift and quadratic\nreadout performing the best overall.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.07103v2"
    },
    {
        "title": "Unsupervised clustering for collider physics",
        "authors": [
            "Vinicius Mikuni",
            "Florencia Canelli"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We propose a new method for Unsupervised clustering in particle physics named\nUCluster, where information in the embedding space created by a neural network\nis used to categorise collision events into different clusters that share\nsimilar properties. We show how this method can be applied to an unsupervised\nmulticlass classification as well as for anomaly detection, which can be used\nfor new physics searches.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.07106v3"
    },
    {
        "title": "Asymptotics for the fastest among n stochastics particles: role of an\n  extended initial distribution and an additional drift component",
        "authors": [
            "Suney Toste",
            "David Holcman"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We derive asymptotic formulas for the mean exit time $\\bar{\\tau}^{N}$ of the\nfastest among $N$ identical independently distributed Brownian particles to an\nabsorbing boundary for various initial distributions (partially uniformly and\nexponentially distributed). Depending on the tail of the initial distribution,\nwe report here a continuous algebraic decay law for $\\bar{\\tau}^{N}$, which\ndiffers from the classical Weibull or Gumbell results. We derive asymptotic\nformulas in dimension 1 and 2, for half-line and an interval that we compare\nwith stochastic simulations. We also obtain formulas for an additive constant\ndrift on the Brownian motion. Finally, we discuss some applications in cell\nbiology where a molecular transduction pathway involves multiple steps and a\nlong-tail initial distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08413v2"
    },
    {
        "title": "Towards Bayesian Data Compression",
        "authors": [
            "Johannes Harth-Kitzerow",
            "Reimar Leike",
            "Philipp Arras",
            "Torsten A. Enßlin"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  In order to handle large data sets omnipresent in modern science, efficient\ncompression algorithms are necessary. Here, a Bayesian data compression (BDC)\nalgorithm that adapts to the specific measurement situation is derived in the\ncontext of signal reconstruction. BDC compresses a data set under conservation\nof its posterior structure with minimal information loss given the prior\nknowledge on the signal, the quantity of interest. Its basic form is valid for\nGaussian priors and likelihoods. For constant noise standard deviation, basic\nBDC becomes equivalent to a Bayesian analog of principal component analysis.\nUsing Metric Gaussian Variational Inference, BDC generalizes to non-linear\nsettings. In its current form, BDC requires the storage of effective instrument\nresponse functions for the compressed data and corresponding noise encoding the\nposterior covariance structure. Their memory demand counteract the compression\ngain. In order to improve this, sparsity of the compressed responses can be\nobtained by separating the data into patches and compressing them separately.\nThe applicability of BDC is demonstrated by applying it to synthetic data and\nradio astronomical data. Still the algorithm needs further improvement as the\ncomputation time of the compression and subsequent inference exceeds the time\nof the inference with the original data.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.10375v2"
    },
    {
        "title": "A Brief Note of Analyzing and Plotting $ν_μ$ Disappearance in SBN\n  Detector under ROOT Framework",
        "authors": [
            "Castaly Fan"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  This is a brief technical note of analyzing the $\\nu_\\mu$ disappearance in\nSBN detector. We here provide a kind of method of plotting the histograms and\nthe heat map plot. We will explore the properties via ROOT framework supported\nby CERN. The main language we used is C++.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.13911v1"
    },
    {
        "title": "Using Deep Learning Techniques to Search for the MiniBooNE Low Energy\n  Excess in MicroBooNE with > 3$σ$ Sensitivity",
        "authors": [
            "Jarrett Moon"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  This thesis describes an analysis developed for the MicroBooNE experiment to\ninvestigate an anomalous excess of electron-like events observed in the\nMiniBooNE detector. The hypothesis investigated here is that the MiniBooNE\nanomaly represents appearance of electron neutrinos. Using an amalgam of novel\nDeep Learning and standard algorithmic techniques this analysis reconstructs\nand identifies a highly pure sample of charged current quasi-elastic muon\nneutrino and electron neutrino interactions. This thesis describes the steps in\nthe analysis chain and provides data-to-simulation comparisons for each step\nthat establish confidence in the final prediction. When interpreted in the\ncontext of a $\\nu e$ appearance like model, this analysis predicts a\n3.2$\\sigma$ sensitivity to exclude a standard model fluctuation which would\nappear as a MiniBooNE like anomaly using $7\\times10^{20}$ protons on target of\nMicroBooNE Data.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.14505v1"
    },
    {
        "title": "Applying clock comparison methods to pulsar timing observations",
        "authors": [
            "Siyuan Chen",
            "Francois Vernotte",
            "Enrico Rubiola"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Frequency metrology outperforms any other branch of metrology in accuracy\n(parts in $10^{-16}$) and small fluctuations ($<10^{-17}$). In turn, among\ncelestial bodies, the rotation speed of millisecond pulsars (MSP) is by far the\nmost stable ($<10^{-18}$). Therefore, the precise measurement of the time of\narrival (TOA) of pulsar signals is expected to disclose information about\ncosmological phenomena, and to enlarge our astrophysical knowledge. Related to\nthis topic, Pulsar Timing Array (PTA) projects have been developed and operated\nfor the last decades. The TOAs from a pulsar can be affected by local emission\nand environmental effects, in the direction of the propagation through the\ninterstellar medium or universally by gravitational waves from super massive\nblack hole binaries. These effects (signals) can manifest as a low-frequency\nfluctuation over time, phenomenologically similar to a red noise. While the\nremaining pulsar intrinsic and instrumental background (noise) are white. This\narticle focuses on the frequency metrology of pulsars. From our standpoint, the\npulsar is an accurate clock, to be measured simultaneously with several\ntelescopes in order to reject the uncorrelated white noise. We apply the modern\nstatistical methods of time-and-frequency metrology to simulated pulsar data,\nand we show the detection limit of the correlated red noise signal between\ntelescopes.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.01912v3"
    },
    {
        "title": "Serial Electron Diffraction Data Processing with diffractem and CrystFEL",
        "authors": [
            "Robert Bücker",
            "Pascal Hogan-Lamarre",
            "R. J. Dwayne Miller"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Serial electron diffraction (SerialED) is an emerging technique, which\napplies the snapshot data-collection mode of serial X-ray crystallography to\nthree-dimensional electron diffraction (3D ED), forgoing the conventional\nrotation method. Similarly to serial X-ray crystallography, this approach leads\nto almost complete absence of radiation damage effects even for the most\nsensitive samples, and allows for a high level of automation. However, SerialED\nalso necessitates new techniques of data processing, which combine existing\npipelines for rotation electron diffraction and serial X-ray crystallography\nwith some more particular solutions for challenges arising in SerialED\nspecifically. Here, we introduce our analysis pipeline for SerialED data, and\nits implementation using the CrystFEL and diffractem program packages. Detailed\nexamples are provided in extensive supplementary code.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.02977v1"
    },
    {
        "title": "Maximum-likelihood parameter estimation in terahertz time-domain\n  spectroscopy",
        "authors": [
            "Laleh Mohtashemi",
            "Paul Westlund",
            "Derek G. Sahota",
            "Graham B. Lea",
            "Ian Bushfield",
            "Payam Mousavi",
            "J. Steven Dodge"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We present a maximum-likelihood method for parameter estimation in terahertz\ntime-domain spectroscopy. We derive the likelihood function for a parameterized\nfrequency response function, given a pair of time-domain waveforms with known\ntime-dependent noise amplitudes. The method provides parameter estimates that\nare superior to other commonly-used methods, and provides a reliable measure of\nthe goodness of fit. We also develop a simple noise model that is parameterized\nby three dominant sources, and derive the likelihood function for their\namplitudes in terms of a set of repeated waveform measurements. We demonstrate\nthe method with applications to material characterization.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08509v3"
    },
    {
        "title": "Particle Track Reconstruction using Geometric Deep Learning",
        "authors": [
            "Yogesh Verma",
            "Satyajit Jena"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Muons are the most abundant charged particles arriving at sea level\noriginating from the decay of secondary charged pions and kaons. These\nsecondary particles are created when high-energy cosmic rays hit the atmosphere\ninteracting with air nuclei initiating cascades of secondary particles which\nled to the formation of extensive air showers (EAS). They carry essential\ninformation about the extra-terrestrial events and are characterized by large\nflux and varying angular distribution. To account for open questions and the\norigin of cosmic rays, one needs to study various components of cosmic rays\nwith energy and arriving direction. Because of the close relation between muon\nand neutrino production, it is the most important particle to keep track of. We\npropose a novel tracking algorithm based on the Geometric Deep Learning\napproach using graphical structure to incorporate domain knowledge to track\ncosmic ray muons in our 3-D scintillator detector. The detector is modeled\nusing the GEANT4 simulation package and EAS is simulated using CORSIKA (COsmic\nRay SImulations for KAscade) with a focus on muons originating from EAS. We\nshed some light on the performance, robustness towards noise and double hits,\nlimitations, and application of the proposed algorithm in tracking applications\nwith the possibility to generalize to other detectors for astrophysical and\ncollider experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08515v2"
    },
    {
        "title": "Machine Learning scientific competitions and datasets",
        "authors": [
            "David Rousseau",
            "Andrey Ustyuzhanin"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  A number of scientific competitions have been organised in the last few years\nwith the objective of discovering innovative techniques to perform typical High\nEnergy Physics tasks, like event reconstruction, classification and new physics\ndiscovery. Four of these competitions are summarised in this chapter, from\nwhich guidelines on organising such events are derived. In addition, a choice\nof competition platforms and available datasets are described\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08520v2"
    },
    {
        "title": "Sparse Modeling analysis of Extended X-ray Absorption Fine Structure\n  data using two-body expansion",
        "authors": [
            "Fabio Iesari",
            "Hiroyuki Setoyama",
            "Yasuhiko Igarashi",
            "Masato Okada",
            "Hiroyuki Kumazoe",
            "Kazunori Iwamitsu",
            "Ichiro Akai",
            "Yoshiki Seno",
            "Toshihiro Okajima"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Analysis of extended X-ray absorption fine structure (EXAFS) data by the use\nof sparse modeling is presented. We consider the two-body term in the n-body\nexpansion of the EXAFS signal to implement the method, together with\ncalculations of amplitudes and phase shifts to distinguish between different\nback-scattering elements. Within this approach no a priori assumption about the\nstructure is used, other than the elements present inside the material. We\napply the method to the experimental EXAFS signal of metals and oxides, for\nwhich we were able to extract the radial distribution function peak positions,\nand the Debye-Waller factor for first neighbors.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.01805v1"
    },
    {
        "title": "Efficiency corrections for mutually inclusive variables and particle\n  identification effect for mixed-cumulants in heavy-ion collisions",
        "authors": [
            "Arghya Chatterjee",
            "Toshihiro Nonaka",
            "ShinIchi Esumi",
            "Xiaofeng Luo"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Mix-cumulants of conserved charge distributions are sensitive observables for\nprobing properties of the QCD medium and phase transition in heavy-ion\ncollisions. To perform precise measurements, efficiency correction is one of\nthe most important step. In this study, using the binomial efficiency model, we\nderive efficiency correction formulas for mutually exclusive and inclusive\nvariables. The UrQMD model is applied to verify the validity of these formulas\nfor different types of correlations. Furthermore, we investigate the effect of\nthe multiplicity loss and contamination emerging from the particle\nidentifications. This study provides an important step toward future\nmeasurements of mixed-cumulants in relativistic heavy-ion collisions.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.08077v2"
    },
    {
        "title": "Detrended fluctuation analysis of earthquake data",
        "authors": [
            "Takumi Kataoka",
            "Tomoshige Miyaguchi",
            "Takuma Akimoto"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The detrended fluctuation analysis (DFA) is extensively useful in stochastic\nprocesses to unveil the long-term correlation. Here, we apply the DFA to point\nprocesses that mimick earthquake data. The point processes are synthesized by a\nmodel similar to the Epidemic-Type Aftershock Sequence model, and we apply the\nDFA to time series $N(t)$ of the point processes, where $N(t)$ is the\ncumulative number of events up to time $t$. Crossover phenomena are found in\nthe DFA for these time series, and extensive numerical simulations suggest that\nthe crossover phenomena are signatures of non-stationarity in the time series.\nWe also find that the crossover time represents a characteristic time scale of\nthe non-stationary process embedded in the time series. Therefore, the DFA for\npoint processes is especially useful in extracting information of\nnon-stationary processes when time series are superpositions of stationary and\nnon-stationary signals. Furthermore, we apply the DFA to the cumulative number\n$N(t)$ of real earthquakes in Japan, and we find a crossover phenomenon similar\nto that found for the synthesized data.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.09222v1"
    },
    {
        "title": "Robust Feature Disentanglement in Imaging Data via Joint Invariant\n  Variational Autoencoders: from Cards to Atoms",
        "authors": [
            "Maxim Ziatdinov",
            "Sergei Kalinin"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Recent advances in imaging from celestial objects in astronomy visualized via\noptical and radio telescopes to atoms and molecules resolved via electron and\nprobe microscopes are generating immense volumes of imaging data, containing\ninformation about the structure of the universe from atomic to astronomic\nlevels. The classical deep convolutional neural network architectures\ntraditionally perform poorly on the data sets having a significant\norientational disorder, that is, having multiple copies of the same or similar\nobject in arbitrary orientation in the image plane. Similarly, while clustering\nmethods are well suited for classification into discrete classes and manifold\nlearning and variational autoencoders methods can disentangle representations\nof the data, the combined problem is ill-suited to a classical non-supervised\nlearning paradigm. Here we introduce a joint rotationally (and translationally)\ninvariant variational autoencoder (j-trVAE) that is ideally suited to the\nsolution of such a problem. The performance of this method is validated on\nseveral synthetic data sets and extended to high-resolution imaging data of\nelectron and scanning probe microscopy. We show that latent space behaviors\ndirectly comport to the known physics of ferroelectric materials and quantum\nsystems. We further note that the engineering of the latent space structure via\nimposed topological structure or directed graph relationship allows for\napplications in topological discovery and causal physical learning.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.10180v1"
    },
    {
        "title": "On assuring the accurate alignment of laser sheets for planar and\n  stereoscopic PIV",
        "authors": [
            "Muhammad Shehzad",
            "Sean Lawrence",
            "Callum Atkinson",
            "Julio Soria"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Several calibration techniques have been proposed in the literature for the\ncalibration of two-component two-dimensional (2C-2D) particle image velocimetry\n(PIV) and three-component two-dimensional (3C-2D) stereoscopic PIV (SPIV)\nsystems. These techniques generally involve the use of a calibration target\nthat is assumed to be at the exact centre of the laser sheet within the field\nof view (FOV), which in practice is very difficult to achieve. In 3C-2D SPIV,\nseveral methods offer different correction schemes based on the computation of\na disparity map, which are aimed at correcting errors produced due to this\nmisalignment. These techniques adjust the calibration of individual cameras to\nreduce the disparity error, but in doing so can create unintended errors in the\nmeasurement position and/or the velocity measurements, such as introducing a\nbias in the measured three-component (3-C) displacements. This paper introduces\na novel method to ensure accurate alignment of the laser sheet with the\ncalibration target so that the uncertainty in displacement measurements is less\nthan or equal to the uncertainty inherent to the PIV and hence, no correction\nscheme is required. The proposed method has been validated with a simple\nexperiment in which true displacements are given to a particle container\n(illuminated by an aligned laser sheet) and the measured 3C displacements are\ncompared with the given true displacements. An uncertainty of less than 7.6\nmicrometres (equivalent to 0.114 pixels) in the measured 3C displacements\ndemonstrates the effectiveness of the new alignment method and eliminates the\nneed for any ad hoc post-correction scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.10323v1"
    },
    {
        "title": "GGS: a Generic Geant4 Simulation package for small- and medium-sized\n  particle detection experiments",
        "authors": [
            "Nicola Mori"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The Generic Geant4 Simulation (GGS) is a package designed to speed-up the\nrealization and deployment of Monte Carlo simulation software based on Geant4,\nfor small- and medium-sized high-energy experiments. For many common use cases,\nthe task of setting up a full-featured simulation of the detector is reduced to\nthe definition of the detector geometry, by providing generic and reusable\nimplementations of the mandatory Geant4 user classes (particle generation,\nscoring, output etc.). Extensibility is provided by a simple plugin system that\nallows replacing of the generic implementations distributed with GGS with\ncustom ones. These features make it especially suitable for cases where limited\nmanpower, like during preliminary detector design studies, can severely limit\nthe scope of an R\\&D program.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.10395v1"
    },
    {
        "title": "Identification of the nature of dynamical systems with recurrence plots\n  and convolution neural networks: A preliminary test",
        "authors": [
            "Daniel Han",
            "Giuseppe Orlando",
            "Sergei Fedotov"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  In this study, we present a method for classifying dynamical systems using a\nhybrid approach involving recurrence plots and a convolution neural network\n(CNN). This is performed by obtaining the recurrence matrix of a time series\ngenerated from a given dynamical system and then using a CNN to classify the\nrelated dynamics observed from the recurrence matrix. We consider three broad\nclasses of dynamics: chaotic, periodic, and stochastic. Using a relatively\nsimple CNN structure, we are able to obtain $\\sim 90\\%$ accuracy in\nclassification. The confusion matrix and receiver operating characteristic\ncurve of classification demonstrate the strength and viability of this hybrid\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.00866v1"
    },
    {
        "title": "Detecting frequency modulation in stochastic time series data",
        "authors": [
            "Adrian L. Hauber",
            "Christian Sigloch",
            "Jens Timmer"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We propose a new statistical test to identify non-stationary\nfrequency-modulated stochastic processes from time series data. Our method uses\nthe instantaneous phase as a discriminatory statistics with reliable critical\nvalues derived from surrogate data. We simulated an oscillatory second-order\nautoregressive process to evaluate the size and power of the test. We found\nthat the test we propose is able to correctly identify more than 99% of\nnon-stationary data when the frequency of simulated data is doubled after the\nfirst half of the time series. Our method is easily interpretable,\ncomputationally cheap and does not require choosing hyperparameters that are\ndependent on the data.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.14561v2"
    },
    {
        "title": "Quantifying Non-Stationarity with Information Theory",
        "authors": [
            "Carlos Granero-Belinchon",
            "Stéphane G. Roux",
            "Nicolas B. Garnier"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We introduce an index based on information theory to quantify the\nstationarity of a stochastic process.The index compares on the one hand the\ninformation contained in the increment at the time scale $\\tau$ of the process\nat time $t$ with, on the other hand, the extra information in the variable at\ntime $t$ that is not present at time $t-\\tau$. By varying the scale $\\tau$, the\nindex can explore a full range of scales. We thus obtain a multi-scale quantity\nthat is not restricted to the first two moments of the density distribution,\nnor to the covariance, but that probes the complete dependences in the process.\nThis index indeed provides a measure of the regularity of the process at a\ngiven scale.Not only is this index able to indicate whether a realization of\nthe process is stationary, but its evolution across scales also indicates how\nrough and non-stationary it is.We show how the index behaves for various\nsynthetic processes proposed to model fluid turbulence, as well as on\nexperimental fluid turbulence measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.00325v1"
    },
    {
        "title": "Wavescan: multiresolution regression of gravitational-wave data",
        "authors": [
            "Sergey Klimenko"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Identification of a transient gravitational-wave signal embedded into\nnon-stationary noise requires the analysis of time-dependent spectral\ncomponents in the resulting time series. The time-frequency distribution of the\nsignal power can be estimated with Gabor atoms, or wavelets, localized in time\nand frequency by a window function. Such analysis is limited by the\nHeisenberg-Gabor uncertainty, which does not allow a high-resolution\nlocalization of power with individual wavelets simultaneously in time and\nfrequency. As a result, the temporal and spectral leakage affects the\ntime-frequency distribution, limiting the identification of sharp features in\nthe power spectrum. This paper presents a time-frequency regression method\nwhere instead of a single window, a stack of wavelets with different windows\nspanning a wide range of resolutions is used to scan power at each\ntime-frequency location. Such a wavelet scan (dubbed in the paper as wavescan)\nextends the conventional multiresolution analysis to capture transient signals\nand remove the local power variations due to the temporal and spectral leakage.\nA wavelet, least affected by the leakage, is selected from the stack at each\ntime-frequency location to obtain the high-resolution localization of power.\nThe paper presents all stages of the multiresolution wavescan regression,\nincluding the estimation of the time-varying spectrum, identification of\ntransient signals in the time-frequency domain, and reconstruction of the\ncorresponding time-domain waveforms. To demonstrate the performance of the\nmethod, the wavescan regression is applied to the gravitational wave data from\nthe LIGO detectors.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.01096v1"
    },
    {
        "title": "Rapid and facile reconstruction of time-resolved fluorescence data with\n  exponentially modified Gaussians",
        "authors": [
            "Darien J. Morrow",
            "Xuedan Ma"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Analyte response is convoluted with instrument response in time resolved\nfluorescence data. Decoding the desired analyte information from the\nmeasurement usually requires iterative numerical convolutions. Here in, we show\nthat time resolved data can be completely, analytically reconstructed without\nnumerical convolutions. Our strategy relies on a summation of exponentially\nmodified Gaussians which encode all convolutions within easily evaluated\ncomplementary error functions. Compared to a numerical convolution strategy\nimplemented with Python, this new method is computationally cheaper and scales\nless steeply with the number of temporal points in the experimental dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.03561v2"
    },
    {
        "title": "The matrix optimum filter for Low Temperature Detectors dead-time\n  reduction",
        "authors": [
            "Matteo Borghesi",
            "Marco Faverzani",
            "Cecilia Ferrari",
            "Elena Ferri",
            "Andrea Giachero",
            "Angelo Nucciotti",
            "Luca Origo"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Experiments aiming at high sensitivities usually demand for a very high\nstatistics in order to reach more precise measurements. However, for those\nexploiting Low Temperature Detectors (LTDs), a high source activity may\nrepresent a drawback, if the events rate becomes comparable with the detector\ncharacteristic temporal response. Indeed, since commonly used optimum filtering\napproaches can only process LTDs signals well isolated in time, a\nnon-negligible part of the recorded experimental data-set is discarded and\nhence constitute the dead-time. In the presented study we demonstrate that,\nthanks to the matrix optimum filtering approach, the dead-time of an experiment\nexploiting LTDs can be strongly reduced.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.05549v2"
    },
    {
        "title": "Determining liquid crystal properties with ordinal networks and machine\n  learning",
        "authors": [
            "Arthur A. B. Pessa",
            "Rafael S. Zola",
            "Matjaz Perc",
            "Haroldo V. Ribeiro"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Machine learning methods are becoming increasingly important for the\ndevelopment of materials science. In spite of this, the use of image analysis\nin the development of these systems is still recent and underexplored,\nespecially in materials often studied via optical imaging techniques such as\nliquid crystals. Here we apply the recently proposed method of ordinal networks\nto map optical textures obtained from experimental samples of liquid crystals\ninto complex networks and use this representation jointly with a simple\nstatistical learning algorithm to investigate different physical properties of\nthese materials. Our research demonstrates that ordinal networks formed by only\n24 nodes encode crucial information about liquid crystal properties, thus\nallowing us to train simple machine learning models capable of identifying and\nclassifying mesophase transitions, distinguishing among different doping\nconcentrations used to induce chiral mesophases, and predicting sample\ntemperatures with outstanding accuracy. The precision and scalability of our\napproach indicate it can be used to probe properties of different materials in\nsituations involving large-scale datasets or real-time monitoring systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.05597v1"
    },
    {
        "title": "Comparison of Bayesian and particle swarm algorithms for hyperparameter\n  optimisation in machine learning applications in high energy physics",
        "authors": [
            "Laurits Tani",
            "Christian Veelken"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  When using machine learning (ML) techniques, users typically need to choose a\nplethora of algorithm-specific parameters, referred to as hyperparameters. In\nthis paper, we compare the performance of two algorithms, particle swarm\noptimisation (PSO) and Bayesian optimisation (BO), for the autonomous\ndetermination of these hyperparameters in applications to different ML tasks\ntypical for the field of high energy physics (HEP). Our evaluation of the\nperformance includes a comparison of the capability of the PSO and BO\nalgorithms to make efficient use of the highly parallel computing resources\nthat are characteristic of contemporary HEP experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.06809v2"
    },
    {
        "title": "Unifying Pairwise Interactions in Complex Dynamics",
        "authors": [
            "Oliver M. Cliff",
            "Annie G. Bryant",
            "Joseph T. Lizier",
            "Naotsugu Tsuchiya",
            "Ben D. Fulcher"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Scientists have developed hundreds of techniques to measure the interactions\nbetween pairs of processes in complex systems. But these computational methods,\nfrom correlation coefficients to causal inference, rely on distinct\nquantitative theories that remain largely disconnected. Here we introduce a\nlibrary of 237 statistics of pairwise interactions and assess their behavior on\n1053 multivariate time series from a wide range of real-world and\nmodel-generated systems. Our analysis highlights new commonalities between\ndifferent mathematical formulations, providing a unified picture of a rich\ninterdisciplinary literature. Using three real-world case studies, we then show\nthat simultaneously leveraging diverse methods from across science can uncover\nthose most suitable for addressing a given problem, yielding interpretable\nunderstanding of the conceptual formulations of pairwise dependence that drive\nsuccessful performance. Our framework is provided in extendable open software,\nenabling comprehensive data-driven analysis by integrating decades of\nmethodological advances.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.11941v2"
    },
    {
        "title": "Power laws in physics",
        "authors": [
            "James P. Sethna"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Getting the most from power-law-type data can be challenging. James Sethna\npoints out some of the pitfalls in studying power laws arising from emergent\nscale invariance, as well as important opportunities.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03365v1"
    },
    {
        "title": "Sensitivity of principal components to system changes in the presence of\n  non-stationarity",
        "authors": [
            "Henrik M. Bette",
            "Michael Schreckenberg",
            "Thomas Guhr"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Non-stationarity affects the sensitivity of change detection in correlated\nsystems described by sets of measurable variables. We study this by projecting\nonto different principal components. Non-stationarity is modeled as multiple\nnormal states that exist in the system even before a change occurs. The studied\nchanges occur in mean values, standard deviations or correlations of the\nvariables. Monte Carlo simulations are performed to test the sensitivity for\nchange detection with and without knowledge about the non-stationarity for\ndifferent system dimensions and numbers of normal states. A comparison clearly\nshows that the knowledge about the non-stationarity of the system greatly\nimproves change detection sensitivity for all principal components. This\nimprovement is largest for those components that already provide the greatest\npossibility for change detection in the stationary case. We illustrate our\nresults with an example using real traffic flow data, in which we detect a\nweekend and a bank holiday start as anomalies.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.04651v3"
    },
    {
        "title": "Academic mentees succeed in big groups, but thrive in small groups",
        "authors": [
            "Yanmeng Xing",
            "Ying Fan",
            "Roberta Sinatra",
            "An Zeng"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Mentoring is a key component of scientific achievements, contributing to\noverall measures of career success for mentees and mentors. A common success\nmetric in the scientific enterprise is acquiring a large research group, which\nis believed to indicate excellent mentorship and high-quality research.\nHowever, large, competitive groups might also amplify dropout rates, which are\nhigh especially among early career researchers. Here, we collect longitudinal\ngenealogical data on mentor-mentee relations and their publication, and study\nthe effects of a mentor's group on future academic survival and performance of\ntheir mentees. We find that mentees trained in large groups generally have\nbetter academic performance than mentees from small groups, if they continue\nworking in academia after graduation. However, we also find two surprising\nresults: Academic survival rate is significantly lower for (1) mentees from\nlarger groups, and for (2) mentees with more productive mentors. These findings\nreveal that success of mentors has a negative effect on the academic survival\nrate of mentees, raising important questions about the definition of successful\nmentorship and providing actionable suggestions concerning career development.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.05304v1"
    },
    {
        "title": "Learning governing physics from output only measurements",
        "authors": [
            "Tapas Tripura",
            "Souvik Chakraborty"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Extracting governing physics from data is a key challenge in many areas of\nscience and technology. The existing techniques for equations discovery are\ndependent on both input and state measurements; however, in practice, we only\nhave access to the output measurements only. We here propose a novel framework\nfor learning governing physics of dynamical system from output only\nmeasurements; this essentially transfers the physics discovery problem from the\ndeterministic to the stochastic domain. The proposed approach models the input\nas a stochastic process and blends concepts of stochastic calculus, sparse\nlearning algorithms, and Bayesian statistics. In particular, we combine\nsparsity promoting spike and slab prior, Bayes law, and Euler Maruyama scheme\nto identify the governing physics from data. The resulting model is highly\nefficient and works with sparse, noisy, and incomplete output measurements. The\nefficacy and robustness of the proposed approach is illustrated on several\nnumerical examples involving both complete and partial state measurements. The\nresults obtained indicate the potential of the proposed approach in identifying\ngoverning physics from output only measurement.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.05609v1"
    },
    {
        "title": "The velocity dependence of dry sliding friction at the nano-scale",
        "authors": [
            "Rasoul Kheiri",
            "Alexey A Tsukanov",
            "Nikolai V Brilliantov"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We performed molecular dynamics (MD) experiments to explore dry sliding\nfriction at the nanoscale. We used the setup comprised of a spherical particle\nbuilt up of 32,000 aluminium atoms, resting on a semi-space with a free\nsurface, modelled by a stack of merged graphene layers. We utilized LAMMPS with\nthe COMB3 many-body potentials for the inter-atomic interactions and Langevin\nthermostat which kept the system at $300 K$. We varied the normal load on the\nparticle and applied different tangential force, which caused the particle\nsliding. Based on the simulation data, we demonstrate that the friction force\n$F_{\\rm fr}$ linearly depends on the sliding velocity $v$, that is, $F_{\\rm\nfr}=-\\gamma v$, where $\\gamma$ is the friction coefficient. The observed\ndependence is in a sharp contrast with the macroscopic Amontons-Coulomb laws,\nwhich predict the velocity independence of sliding friction. We explain such a\ndependence by surface fluctuations of the thermal origin, which give rise to\nsurface corrugation hindering sliding motion. This mechanism is similar to that\nof the viscous friction force exerted on a body moving in viscous fluid.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.09564v1"
    },
    {
        "title": "Detecting groups of similar components in complex networks",
        "authors": [
            "J. Wang",
            "C. -H. Lai"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We study how to detect groups in a complex network each of which consists of\ncomponent nodes sharing a similar connection pattern. Based on the mixture\nmodels and the exploratory analysis set up by Newman and Leicht (Newman and\nLeicht 2007 {\\it Proc. Natl. Acad. Sci. USA} {\\bf 104} 9564), we develop an\nalgorithm that is applicable to a network with any degree distribution. The\npartition of a network suggested by this algorithm also applies to its\ncomplementary network. In general, groups of similar components are not\nnecessarily identical with the communities in a community network; thus\npartitioning a network into groups of similar components provides additional\ninformation of the network structure. The proposed algorithm can also be used\nfor community detection when the groups and the communities overlap. By\nintroducing a tunable parameter that controls the involved effects of the\nheterogeneity, we can also investigate conveniently how the group structure can\nbe coupled with the heterogeneity characteristics. In particular, an\ninteresting example shows a group partition can evolve into a community\npartition in some situations when the involved heterogeneity effects are tuned.\nThe extension of this algorithm to weighted networks is discussed as well.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.1612v2"
    },
    {
        "title": "Symmetries and modelling functions for diffusion processes",
        "authors": [
            "A. G. Nikitin",
            "S. V. Spichak",
            "Yu. S. Vedula",
            "A. G. Naumovets"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  A constructive approach to theory of diffusion processes is proposed, which\nis based on application of both the symmetry analysis and method of modelling\nfunctions. An algorithm for construction of the modelling functions is\nsuggested. This algorithm is based on the error functions expansion (ERFEX) of\nexperimental concentration profiles. The high-accuracy analytical description\nof the profiles provided by ERFEX approximation allows a convenient extraction\nof the concentration dependence of diffusivity from experimental data and\nprediction of the diffusion process. Our analysis is exemplified by its\nemployment to experimental results obtained for surface diffusion of lithium on\nthe molybdenum (112) surface pre-covered with dysprosium. The ERFEX\napproximation can be directly extended to many other diffusion systems.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.2177v2"
    },
    {
        "title": "Reducing the Variance of Likelihood Ratio Greeks with Monte Carlo",
        "authors": [
            "Luca Capriotti"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We investigate the use of Antithetic Variables, Control Variates and\nImportance Sampling to reduce the statistical errors of option sensitivities\ncalculated with the Likelihood Ratio Method in Monte Carlo. We show how\nAntithetic Variables solve the well-known problem of the divergence of the\nvariance of Delta for short maturities and small volatilities. With numerical\nexamples within a Gaussian Copula framework, we show how simple Control\nVariates and Importance Sampling strategies provide computational savings up to\nseveral orders of magnitude.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.2332v1"
    },
    {
        "title": "A random telegraph signal of Mittag-Leffler type",
        "authors": [
            "Simone Ferraro",
            "Michele Manzini",
            "Aldo Masoero",
            "Enrico Scalas"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  A general method is presented to explicitly compute autocovariance functions\nfor non-Poisson dichotomous noise based on renewal theory. The method is\nspecialized to a random telegraph signal of Mittag-Leffler type. Analytical\npredictions are compared to Monte Carlo simulations. Non-Poisson dichotomous\nnoise is non-stationary and standard spectral methods fail to describe it\nproperly as they assume stationarity.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.2577v1"
    },
    {
        "title": "Dynamic scaling approach to study time series fluctuations",
        "authors": [
            "Alexander S. Balankin"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We propose a new approach for properly analyzing stochastic time series by\nmapping the dynamics of time series fluctuations onto a suitable nonequilibrium\nsurface-growth problem. In this framework, the fluctuation sampling time\ninterval plays the role of time variable, whereas the physical time is treated\nas the analog of spatial variable. In this way we found that the fluctuations\nof many real-world time series satisfy the analog of the Family-Viscek dynamic\nscaling ansatz. This finding permits to use the powerful tools of kinetic\nroughening theory to classify, model, and forecast the fluctuations of\nreal-world time series.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.3269v1"
    },
    {
        "title": "Highly accurate recommendation algorithm based on high-order\n  similarities",
        "authors": [
            "Jian-Guo Liu",
            "Tao Zhou",
            "Bing-Hong Wang",
            "Yi-Cheng Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In this Letter, we introduce a modified collaborative filtering (MCF)\nalgorithm, which has remarkably higher accuracy than the standard collaborative\nfiltering. In the MCF, instead of the standard Pearson coefficient, the\nuser-user similarities are obtained by a diffusion process. Furthermore, by\nconsidering the second order similarities, we design an effective algorithm\nthat depresses the influence of mainstream preferences. The corresponding\nalgorithmic accuracy, measured by the ranking score, is further improved by\n24.9% in the optimal case. In addition, two significant criteria of algorithmic\nperformance, diversity and popularity, are also taken into account. Numerical\nresults show that the algorithm based on second order similarity can outperform\nthe MCF simultaneously in all three criteria.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.3726v2"
    },
    {
        "title": "Entropy: The Markov Ordering Approach",
        "authors": [
            "A. N. Gorban",
            "P. A. Gorban",
            "G. Judge"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The focus of this article is on entropy and Markov processes. We study the\nproperties of functionals which are invariant with respect to monotonic\ntransformations and analyze two invariant \"additivity\" properties: (i)\nexistence of a monotonic transformation which makes the functional additive\nwith respect to the joining of independent systems and (ii) existence of a\nmonotonic transformation which makes the functional additive with respect to\nthe partitioning of the space of states. All Lyapunov functionals for Markov\nchains which have properties (i) and (ii) are derived. We describe the most\ngeneral ordering of the distribution space, with respect to which all\ncontinuous-time Markov processes are monotonic (the {\\em Markov order}). The\nsolution differs significantly from the ordering given by the inequality of\nentropy growth. For inference, this approach results in a convex compact set of\nconditionally \"most random\" distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.1377v5"
    },
    {
        "title": "Exact feature probabilities in images with occlusion",
        "authors": [
            "Xaq Pitkow"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  To understand the computations of our visual system, it is important to\nunderstand also the natural environment it evolved to interpret. Unfortunately,\nexisting models of the visual environment are either unrealistic or too complex\nfor mathematical description. Here we describe a naturalistic image model and\npresent a mathematical solution for the statistical relationships between the\nimage features and model variables. The world described by this model is\ncomposed of independent, opaque, textured objects which occlude each other.\nThis simple structure allows us to calculate the joint probability distribution\nof image values sampled at multiple arbitrarily located points, without\napproximation. This result can be converted into probabilistic relationships\nbetween observable image features as well as between the unobservable\nproperties that caused these features, including object boundaries and relative\ndepth. Using these results we explain the causes of a wide range of natural\nscene properties, including highly non-gaussian distributions of image features\nand causal relations between pairs of edges. We discuss the implications of\nthis description of natural scenes for the study of vision.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.2950v1"
    },
    {
        "title": "Entropy-based parametric estimation of spike train statistics",
        "authors": [
            "J. C. Vasquez",
            "B. Cessac",
            "T. Viéville"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We consider the evolution of a network of neurons, focusing on the asymptotic\nbehavior of spikes dynamics instead of membrane potential dynamics. The spike\nresponse is not sought as a deterministic response in this context, but as a\nconditional probability : \"Reading out the code\" consists of inferring such a\nprobability. This probability is computed from empirical raster plots, by using\nthe framework of thermodynamic formalism in ergodic theory. This gives us a\nparametric statistical model where the probability has the form of a Gibbs\ndistribution. In this respect, this approach generalizes the seminal and\nprofound work of Schneidman and collaborators. A minimal presentation of the\nformalism is reviewed here, while a general algorithmic estimation method is\nproposed yielding fast convergent implementations. It is also made explicit how\nseveral spike observables (entropy, rate, synchronizations, correlations) are\ngiven in closed-form from the parametric estimation. This paradigm does not\nonly allow us to estimate the spike statistics, given a design choice, but also\nto compare different models, thus answering comparative questions about the\nneural code such as : \"are correlations (or time synchrony or a given set of\nspike patterns, ..) significant with respect to rate coding only ?\" A numerical\nvalidation of the method is proposed and the perspectives regarding spike-train\ncode analysis are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.3157v2"
    },
    {
        "title": "On the Expectation-Maximization Unfolding with Smoothing",
        "authors": [
            "Igor Volobouev"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Error propagation formulae are derived for the expectation-maximization\niterative unfolding algorithm regularized by a smoothing step. The effective\nnumber of parameters in the fit to the observed data is defined for unfolding\nprocedures. Based upon this definition, the Akaike information criterion is\nproposed as a principle for choosing the smoothing parameters in an automatic,\ndata-dependent manner. The performance and the frequentist coverage of the\nresulting method are investigated using simulated samples. A number of issues\nof general relevance to all unfolding techniques are discussed, including\nirreducible bias, uncertainty increase due to a data-dependent choice of\nregularization strength, and presentation of results.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.6500v2"
    },
    {
        "title": "Handling uncertainties in background shapes: the discrete profiling\n  method",
        "authors": [
            "P. D. Dauncey",
            "M. Kenzie",
            "N. Wardle",
            "G. J. Davies"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  A common problem in data analysis is that the functional form, as well as the\nparameter values, of the underlying model which should describe a dataset is\nnot known a priori. In these cases some extra uncertainty must be assigned to\nthe extracted parameters of interest due to lack of exact knowledge of the\nfunctional form of the model. A method for assigning an appropriate error is\npresented. The method is based on considering the choice of functional form as\na discrete nuisance parameter which is profiled in an analogous way to\ncontinuous nuisance parameters. The bias and coverage of this method are shown\nto be good when applied to a realistic example.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.6865v5"
    },
    {
        "title": "Fitting theory to data in the presence of background uncertainties",
        "authors": [
            "Byron Roe"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  When fitting theory to data in the presence of background uncertainties, the\nquestion of whether the spectral shape of the background happens to be similar\nto that of the theoretical model of physical interest has not generally been\nconsidered previously. These correlations in shape are considered in the\npresent note and found to make important corrections to the calculations. The\ndiscussion is phrased in terms of $\\chi^2$ fits, but the general considerations\napply to any fits. Including these new correlations provides a more powerful\ntest for confidence regions. Fake data studies, as used at present, may not be\noptimum.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.7075v2"
    },
    {
        "title": "Tempered Fractional Feynman-Kac Equation",
        "authors": [
            "Xiaochao Wu",
            "Weihua Deng",
            "Eli Barkai"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Functionals of Brownian/non-Brownian motions have diverse applications and\nattracted a lot of interest of scientists. This paper focuses on deriving the\nforward and backward fractional Feynman-Kac equations describing the\ndistribution of the functionals of the space and time tempered anomalous\ndiffusion, belonging to the continuous time random walk class. Several examples\nof the functionals are explicitly treated, including the occupation time in\nhalf-space, the first passage time, the maximal displacement, the fluctuations\nof the occupation fraction, and the fluctuations of the time-averaged position.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.00071v1"
    },
    {
        "title": "Development of an Ideal Observer that Incorporates Nuisance Parameters\n  and Processes List-Mode Data",
        "authors": [
            "Christopher J. MacGahan",
            "Matthew A. Kupinski",
            "Nathan R. Hilton",
            "Erik M. Brubaker",
            "William C. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Observer models were developed to process data in list-mode format in order\nto perform binary discrimination tasks for use in an arms-control-treaty\ncontext. Data used in this study was generated using GEANT4 Monte Carlo\nsimulations for photons using custom models of plutonium inspection objects and\na radiation imaging system. Observer model performance was evaluated and\npresented using the area under the receiver operating characteristic curve. The\nideal observer was studied under both signal-known-exactly conditions and in\nthe presence of unknowns such as object orientation and absolute count-rate\nvariability; when these additional sources of randomness were present, their\nincorporation into the observer yielded superior performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.01449v1"
    },
    {
        "title": "Modeling of critical experiments and its impact on integral covariance\n  matrices and correlation coefficients",
        "authors": [
            "Elisabeth Peters",
            "Fabian Sommer",
            "Maik Stuke"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In this manuscript we study the modeling of experimental data and its impact\non the resulting integral experimental covariance and correlation matrices. By\ninvestigating a set of three low enriched and water moderated UO2 fuel rod\narrays we found that modeling the same set of data with different, yet\nreasonable assumptions concerning the fuel rod composition and its geometric\nproperties leads to significantly different covariance matrices or correlation\ncoefficients. Following a Monte Carlo sampling approach, we show for nine\ndifferent modeling assumptions the corresponding correlation coefficients and\nsensitivity profiles for each pair of the effective neutron multiplication\nfactor keff. Within the 95% confidence interval the correlation coefficients\nvary from 0 to 1, depending on the modeling assumptions. Our findings show that\nthe choice of modeling can have a huge impact on integral experimental\ncovariance matrices. When the latter are used in a validation procedure to\nderive a bias, this procedure can be affected by the choice of modeling\nassumptions, too. The correct consideration of correlated data seems to be\ninevitable if the experimental data in a validation procedure is limited or one\ncannot rely on a sufficient number of uncorrelated data sets, e.g. from\ndifferent laboratories using different setups etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.04038v1"
    },
    {
        "title": "Non-spectral modes and how to find them in the Ornstein-Uhlenbeck\n  process with white μ-stable noise",
        "authors": [
            "F. Thiel",
            "I. M. Sokolov",
            "E. B. Postnikov"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We consider the Ornstein-Uhlenbeck process with a broad initial probability\ndistribution (Levy distribution), which exhibits so-called non-spectral modes.\nThe relaxation of such modes differs from those determined from the parameters\nof the corresponding Fokker-Planck equation. The first non-spectral mode is\nshown to govern the relaxation process and allows for estimation of the initial\ndistribution's Levy index. A method based on continuous wavelet transformation\nis proposed to extract both (spectral and non-spectral) relaxation rates from a\nstochastic data sample.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.04319v2"
    },
    {
        "title": "Simulated Annealing Approach to the Temperature-Emissivity Separation\n  Problem in Thermal Remote Sensing Part One: Mathematical Background",
        "authors": [
            "John A. Morgan"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The method of simulated annealing is adapted to the temperature-emissivity\nseparation (TES) problem. A patch of surface at the bottom of the atmosphere is\nassumed to be a greybody emitter with spectral emissivity $\\epsilon(k)$\ndescribable by a mixture of spectral endmembers. We prove that a simulated\nannealing search conducted according to a suitable schedule converges to a\nsolution maximizing the $\\textit{A-Posteriori}$ probability that spectral\nradiance detected at the top of the atmosphere originates from a patch with\nstipulated $T$ and $\\epsilon(k)$. Any such solution will be nonunique. The\naverage of a large number of simulated annealing solutions, however, converges\nalmost surely to a unique Maximum A-Posteriori solution for $T$ and\n$\\epsilon(k)$. The limitation to a stipulated set of endmember emissivities may\nbe relaxed by allowing the number of endmembers to grow without bound, and to\nbe generic continuous functions of wavenumber with bounded first derivatives\nwith respect to wavenumber.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05497v1"
    },
    {
        "title": "Predicting dataset popularity for the CMS experiment",
        "authors": [
            "Valentin Kuznetsov",
            "Ting Li",
            "Luca Giommi",
            "Daniele Bonacorsi",
            "Tony Wildish"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The CMS experiment at the LHC accelerator at CERN relies on its computing\ninfrastructure to stay at the frontier of High Energy Physics, searching for\nnew phenomena and making discoveries. Even though computing plays a significant\nrole in physics analysis we rarely use its data to predict the system behavior\nitself. A basic information about computing resources, user activities and site\nutilization can be really useful for improving the throughput of the system and\nits management. In this paper, we discuss a first CMS analysis of dataset\npopularity based on CMS meta-data which can be used as a model for dynamic data\nplacement and provide the foundation of data-driven approach for the CMS\ncomputing infrastructure.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.07226v1"
    },
    {
        "title": "Eigenanalysis of morphological diversity in silicon random\n  nanostructures formed via resist collapse",
        "authors": [
            "Makoto Naruse",
            "Morihisa Hoga",
            "Yasuyuki Ohyagi",
            "Shumpei Nishio",
            "Naoya Tate",
            "Naoki Yoshida",
            "Tsutomu Matsumoto"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  This paper demonstrates eigenanalysis to quantitatively reveal the diversity\nand capacity of identities offered by the morphological diversity in silicon\nnanostructures formed via random collapse of resist. The analysis suggests that\napproximately 10^115 possible identities are provided per 0.18-um^2 area of\nnanostructures, indicating that nanoscale morphological signatures will be\nextremely useful for future information security applications where securing\nidentities is critical. The eigenanalysis provides an intuitive physical\npicture and quantitative characterization of the diversity of structural\nfluctuations while unifying measurement stability concerns, which will be\nwidely applicable to other materials, devices, and system architectures.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.08205v2"
    },
    {
        "title": "Environment Identification in Flight using Sparse Approximation of Wing\n  Strain",
        "authors": [
            "Krithika Manohar",
            "Steven L. Brunton",
            "J. Nathan Kutz"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  This paper addresses the problem of identifying different flow environments\nfrom sparse data collected by wing strain sensors. Insects regularly perform\nthis feat using a sparse ensemble of noisy strain sensors on their wing. First,\nwe obtain strain data from numerical simulation of a Manduca sexta hawkmoth\nwing undergoing different flow environments. Our data-driven method learns\nlow-dimensional strain features originating from different aerodynamic\nenvironments using proper orthogonal decomposition (POD) modes in the frequency\ndomain, and leverages sparse approximation to classify a set of strain\nfrequency signatures using a dictionary of POD modes. This bio-inspired machine\nlearning architecture for dictionary learning and sparse classification permits\nfewer costly physical strain sensors while being simultaneously robust to\nsensor noise. A measurement selection algorithm identifies frequencies that\nbest discriminate the different aerodynamic environments in low-rank POD\nfeature space. In this manner, sparse and noisy wing strain data can be\nexploited to robustly identify different aerodynamic environments encountered\nin flight, providing insight into the stereotyped placement of neurons that act\nas strain sensors on a Manduca sexta hawkmoth wing.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00034v2"
    },
    {
        "title": "Beyond Zipf's Law: The Lavalette Rank Function and its Properties",
        "authors": [
            "Oscar Fontanelli",
            "Pedro Miramontes",
            "Yaning Yang",
            "Germinal Cocho",
            "Wentian Li"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Although Zipf's law is widespread in natural and social data, one often\nencounters situations where one or both ends of the ranked data deviate from\nthe power-law function. Previously we proposed the Beta rank function to\nimprove the fitting of data which does not follow a perfect Zipf's law. Here we\nshow that when the two parameters in the Beta rank function have the same\nvalue, the Lavalette rank function, the probability density function can be\nderived analytically. We also show both computationally and analytically that\nLavalette distribution is approximately equal, though not identical, to the\nlognormal distribution. We illustrate the utility of Lavalette rank function in\nseveral datasets. We also address three analysis issues on the statistical\ntesting of Lavalette fitting function, comparison between Zipf's law and\nlognormal distribution through Lavalette function, and comparison between\nlognormal distribution and Lavalette distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.01959v1"
    },
    {
        "title": "Short term fluctuations of wind and solar power systems",
        "authors": [
            "M. Anvari",
            "G. Lohmann",
            "M. Wächter",
            "P. Milan",
            "E. Lorenz",
            "D. Heinemann",
            "M. Reza Rahimi Tabar",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Wind and solar power are known to be highly influenced by weather events and\nmay ramp up or down abruptly. Such events in the power production influence not\nonly the availability of energy, but also the stability of the entire power\ngrid. By analysing significant amounts of data from several regions around the\nworld with resolutions of seconds to minutes, we provide strong evidence that\nrenewable wind and solar sources exhibit multiple types of variability and\nnonlinearity in the time scale of {\\it seconds} and characterise their\nstochastic properties. In contrast to previous findings, we show that only the\njumpy characteristic of renewable sources decreases when increasing the spatial\nsize over which the renewable energies are harvested. Otherwise, the strong\nnon-Gaussian, intermittent behaviour in the cumulative power of the total field\nsurvives even for a country-wide distribution of the systems. The strong\nfluctuating behaviour of renewable wind and solar sources can be well\ncharacterised by Kolmogorov-like power spectra and $q-$exponential probability\ndensity functions. Using the estimated potential shape of power time series, we\nquantify the jumpy or diffusive dynamic of the power. Finally we propose a time\ndelayed feedback technique as a control algorithm to suppress the observed\nshort term non-Gaussian statistics in spatially strong correlated and\nintermittent renewable sources.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.03426v1"
    },
    {
        "title": "Bypass rewiring and robustness of complex networks",
        "authors": [
            "Junsang Park",
            "Sang Geun Hahn"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  A concept of bypass rewiring is introduced and random bypass rewiring is\nanalytically and numerically investigated with simulations. Our results show\nthat bypass rewiring makes networks robust against removal of nodes including\nrandom failures and attacks. In particular, random bypass rewiring connects all\nnodes except the removed nodes on an even degree infinite network and makes the\npercolation threshold 0 for arbitrary occupation probabilities. In our example,\nthe even degree network is more robust than the original network with random\nbypass rewiring while the original network is more robust than the even degree\nnetworks without random bypass. We propose a greedy bypass rewiring algorithm\nwhich guarantees the maximum size of the largest component at each step,\nassuming which node will be removed next is unknown. The simulation result\nshows that the greedy bypass rewiring algorithm improves the robustness of the\nautonomous system of the Internet under attacks more than random bypass\nrewiring.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.09579v4"
    },
    {
        "title": "Sparsity enabled cluster reduced-order models for control",
        "authors": [
            "Eurika Kaiser",
            "Marek Morzynski",
            "Guillaume Daviller",
            "J Nathan Kutz",
            "Bingni W Brunton",
            "Steven L Brunton"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Characterizing and controlling nonlinear, multi-scale phenomena play\nimportant roles in science and engineering. Cluster-based reduced-order\nmodeling (CROM) was introduced to exploit the underlying low-dimensional\ndynamics of complex systems. CROM builds a data-driven discretization of the\nPerron-Frobenius operator, resulting in a probabilistic model for ensembles of\ntrajectories. A key advantage of CROM is that it embeds nonlinear dynamics in a\nlinear framework, and uncertainty can be managed with data assimilation. CROM\nis typically computed on high-dimensional data, however, access to and\ncomputations on this full-state data limit the online implementation of CROM\nfor prediction and control. Here, we address this key challenge by identifying\na small subset of critical measurements to learn an efficient CROM, referred to\nas sparsity-enabled CROM. In particular, we leverage compressive measurements\nto faithfully embed the cluster geometry and preserve the probabilistic\ndynamics. Further, we show how to identify fewer optimized sensor locations\ntailored to a specific problem that outperform random measurements. Both of\nthese sparsity-enabled sensing strategies significantly reduce the burden of\ndata acquisition and processing for low-latency in-time estimation and control.\nWe illustrate this unsupervised learning approach on three different\nhigh-dimensional nonlinear dynamical systems from fluids with increasing\ncomplexity, with one application in flow control. Sparsity-enabled CROM is a\ncritical facilitator for real-time implementation on high-dimensional systems\nwhere full-state information may be inaccessible.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.00038v1"
    },
    {
        "title": "A multifractal surrogate data generation algorithm that preserves\n  pointwise Holder regularity structure, with initial applications to\n  turbulence",
        "authors": [
            "Christopher J Keylock"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  An algorithm is described that can generate random variants of a time series\nor image while preserving the probability distribution of original values and\nthe pointwise Holder regularity. Thus, it preserves the multifractal properties\nof the data. Our algorithm is similar in principle to well-known algorithms\nbased on the preservation of the Fourier amplitude spectrum and original values\nof a time series. However, it is underpinned by a dual-tree complex wavelet\ntransform rather than a Fourier transform. Our method, which we term the\nIterated Amplitude Adjusted Wavelet Transform (IAAWT) method can be used to\ngenerate bootstrapped versions of multifractal data and, because it preserves\nthe pointwise Holder regularity but not the local Holder regularity, it can be\nused to test hypotheses concerning the presence of oscillating singularities in\na time series, an important feature of turbulence and econophysics data.\nBecause the locations of the data values are randomized with respect to the\nmultifractal structure, hypotheses about their mutual coupling can be tested,\nwhich is important for the velocity-intermittency structure of turbulence and\nself-regulating processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.00579v1"
    },
    {
        "title": "Combining Experiments with Systematic Errors",
        "authors": [
            "Roger John Barlow"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We consider fits to two or more datasets for which results from the sa me\nexperiment share a common systematic uncertainty in addition to their individ\nual statistical errors. This is important in extracting the maximum information\nfrom a set of similar bu t different experiments (or the same experiment under\ndifferent conditions) an alysing similar but different datasets, as happens at\nthe LHC and other particle colliders. There are two techniques in use: using\nthe full matrix and using extra paramneters, and we show, for a completely\ngeneral fit, that for an addit ive uncertainty they are in principle equivalent\neven though in practice the det ails differ. For a multiplicative error the\nmatrix fit is equivalent to the extra parameter fit if the factor is applied to\nthe data points but not if it is applied to the function: the former leads to\nbiassed estimates and the latter avoids them.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.03701v2"
    },
    {
        "title": "Subsampling scaling: a theory about inference from partly observed\n  systems",
        "authors": [
            "Anna Levina",
            "Viola Priesemann"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  In real-world applications, observations are often constrained to a small\nfraction of a system. Such spatial subsampling can be caused by the\ninaccessibility or the sheer size of the system, and cannot be overcome by\nlonger sampling. Spatial subsampling can strongly bias inferences about a\nsystem's aggregated properties. To overcome the bias, we derive analytically a\nsubsampling scaling framework that is applicable to different observables,\nincluding distributions of neuronal avalanches, of number of people infected\nduring an epidemic outbreak, and of node degrees. We demonstrate how to infer\nthe correct distributions of the underlying full system, how to apply it to\ndistinguish critical from subcritical systems, and how to disentangle\nsubsampling and finite size effects. Lastly, we apply subsampling scaling to\nneuronal avalanche models and to recordings from developing neural networks. We\nshow that only mature, but not young networks follow power-law scaling,\nindicating self-organization to criticality during development.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.04277v1"
    },
    {
        "title": "Single-View 3D Reconstruction of Correlated Gamma-Neutron Sources",
        "authors": [
            "Mateusz Monterial",
            "Peter Marleau",
            "Sara A. Pozzi"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We describe a new method of 3D image reconstruction of neutron sources that\nemit correlated gammas (e.g. Cf-252, Am-Be). This category includes a vast\nmajority of neutron sources important in nuclear threat search, safeguards and\nnon-proliferation. Rather than requiring multiple views of the source this\ntechnique relies on the source's intrinsic property of coincidence gamma and\nneutron emission. As a result only a single-view measurement of the source is\nrequired to perform the 3D reconstruction. In principle, any scatter camera\nsensitive to gammas and neutrons with adequate timing and interaction location\nresolution can perform this reconstruction. Using a neutron double scatter\ntechnique, we can calculate a conical surface of possible source locations. By\nincluding the time to a correlated gamma we further constrain the source\nlocation in three-dimensions by solving for the source-to-detector distance\nalong the surface of said cone. As a proof of concept we applied these\nreconstruction techniques on measurements taken with the the Mobile Imager of\nNeutrons for Emergency Responders (MINER). Two Cf-252 sources measured at 50\nand 60 cm from the center of the detector were resolved in their varying depth\nwith average radial distance relative resolution of 26%. To demonstrate the\ntechnique's potential with an optimized system we simulated the measurement in\nMCNPX-PoliMi assuming timing resolution of 200 ps (from 2 ns in the current\nsystem) and source interaction location resolution of 5 mm (from 3 cm). These\nsimulated improvements in scatter camera performance resulted in radial\ndistance relative resolution decreasing to an average of 11%.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.04435v1"
    },
    {
        "title": "Application of the Huang-Hilbert transform and natural time to the\n  analysis of Seismic Electric Signal activities",
        "authors": [
            "K. A. Papadopoulou",
            "E. S. Skordas"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The Huang-Hilbert transform is applied to Seismic Electric Signal (SES)\nactivities in order to decompose them into a number of Intrinsic Mode Functions\n(IMFs) and study which of these functions better represent the SES. The results\nare compared to those obtained from the analysis in a new time domain termed\nnatural time after having subtracted the magnetotelluric background from the\noriginal signal. It is shown that the instantaneous amplitudes of the IMFs can\nbe used for the distinction of SES from artificial noises when combined with\nthe natural time analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.08975v1"
    },
    {
        "title": "Application of Bayes' theorem for pulse shape discrimination",
        "authors": [
            "Mateusz Monterial",
            "Peter Marleau",
            "Shaun Clarke",
            "Sara Pozzi"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  A Bayesian approach is proposed for pulse shape discrimination of photons and\nneutrons in liquid organic scinitillators. Instead of drawing a decision\nboundary, each pulse is assigned a photon or neutron confidence probability.\nThis allows for photon and neutron classification on an event-by-event basis.\nThe sum of those confidence probabilities is used to estimate the number of\nphoton and neutron instances in the data. An iterative scheme, similar to an\nexpectation-maximization algorithm for Gaussian mixtures, is used to infer the\nratio of photons-to-neutrons in each measurement. Therefore, the probability\nspace adapts to data with varying photon-to-neutron ratios. A time-correlated\nmeasurement of Am-Be and separate measurements of $^{137}$Cs, $^{60}$Co and\n$^{232}$Th photon sources were used to construct libraries of neutrons and\nphotons. These libraries were then used to produce synthetic data sets with\nvarying ratios of photons-to-neutrons. Probability weighted method that we\nimplemented was found to maintain neutron acceptance rate of up to 90% up to\nphoton-to-neutron ratio of 2000, and performed 9% better than decision boundary\napproach. Furthermore, the iterative approach appropriately changed the\nprobability space with an increasing number of photons which kept the neutron\npopulation estimate from unrealistically increasing.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00973v1"
    },
    {
        "title": "Model-independent partial wave analysis using a massively-parallel\n  fitting framework",
        "authors": [
            "Liang Sun",
            "Rafael Aoude",
            "Alberto Correa Dos Reis",
            "Michael David Sokoloff"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The functionality of GooFit, a GPU-friendly framework for doing\nmaximum-likelihood fits, has been extended to extract model-independent S-wave\namplitudes in three-body decays such as $D^+ \\to h^+h^+h^-$. A full amplitude\nanalysis is done where the magnitudes and phases of the S-wave amplitudes are\nanchored at a finite number of $m^2(h^+h^-)$ control points, and a cubic spline\nis used to interpolate between these points. The amplitudes for P-wave and\nD-wave intermediate states are modeled as spin-dependent Breit-Wigner\nresonances. GooFit uses the Thrust library, with a CUDA backend for NVIDIA GPUs\nand an OpenMP backend for threads with conventional CPUs. Performance on a\nvariety of platforms is compared. Executing on systems with GPUs is typically a\nfew hundred times faster than executing the same algorithm on a single CPU.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.03284v1"
    },
    {
        "title": "Quantification of scaling exponent with crossover type phenomena for\n  different types of forcing in DC glow discharge plasma",
        "authors": [
            "Debajyoti Saha",
            "Pankaj Kumar Shaw",
            "Sabuj Ghosh",
            "M. S. Janaki",
            "A. N. Sekar Iyengar"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We have carried out a detailed study of scaling region using detrended\nfractal analysis test by applying different forcing likewise noise, sinusoidal,\nsquare on the floating potential fluctuations acquired under different\npressures in a DC glow discharge plasma. The transition in the dynamics is\nobserved through recurrence plot techniques which is an efficient method to\nobserve the critical regime transitions in dynamics. The complexity of the\nnonlinear fluctuation has been revealed with the help of recurrence\nquantification analysis which is a suitable tool for investigating recurrence,\nan ubiquitous feature providing a deep insight into the dynamics of real\ndynamical system. An informal test for stationarity which checks for the\ncompatibility of nonlinear approximations to the dynamics made in different\nsegments in a time series has been proposed. In case of sinusoidal, noise,\nsquare forcing applied on fluctuation acquired at P=0.12mbar only one dominant\nscaling region is observed whereas the forcing applied on fluctuation\n(P=0.004mbar) two prominent scaling regions have been explored reliably using\ndifferent forcing amplitudes indicating the signature of crossover phenomena.\nFurthermore a persistence long range behaviour has been observed in one of\nthese scaling regions. A comprehensive study of the quantifi- cation of scaling\nexponents has been carried out with the increase in amplitude and frequency of\nsinusoidal, square type of forcings. The scalings exponent is envisaged to be\nthe roughness of the time series. The method provides a single quantitative\nidea of the scaling exponent to quantify the correlation properties of a\nsignal.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.06017v1"
    },
    {
        "title": "Non-scaling displacement distributions as may be seen in fluorescence\n  correlation spectroscopy",
        "authors": [
            "S. M. J. Khadem",
            "I. M. Sokolov"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  A continuous time random walk (CTRW) model with waiting times following the\nLevy-stable distribution with exponential cut-off in equilibrium is a simple\ntheoretical model giving rise to normal, yet non-Gaussian diffusion. The\ndistribution of the particle displacements is explicitly time-dependent and\ndoes not scale. Since fluorescent correlation spectroscopy (FCS) is often used\nto investigate diffusion processes, we discuss the influence of this lack of\nscaling on the possible outcome of the FCS measurements and calculate the FCS\nautocorrelation curves for such equilibrated CTRWs. The results show that\nalthough the deviations from Gaussian behavior may be detected when analyzing\nthe short and long-time asymptotic behavior of the corresponding curves, their\nbodies are still perfectly fitted by the fit forms used for normal diffusion.\nThe diffusion coefficients obtaied from the fits may however differ\nconsiderably from the true tracer diffusion coefficients as describing the time\ndependence of the mean squared displacement\n",
        "pdf_link": "http://arxiv.org/pdf/1703.07117v1"
    },
    {
        "title": "Real-space analysis of scanning tunneling microscopy topography datasets\n  using sparse modeling approach",
        "authors": [
            "Masamichi J. Miyama",
            "Koji Hukushima"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  A sparse modeling approach is proposed for analyzing scanning tunneling\nmicroscopy topography data, which contains numerous peaks corresponding to\nsurface atoms. The method, based on the relevance vector machine with\n$\\mathrm{L}_1$ regularization and $k$-means clustering, enables separation of\nthe peaks and atomic center positioning with accuracy beyond the resolution of\nthe measurement grid. The validity and efficiency of the proposed method are\ndemonstrated using synthetic data in comparison to the conventional\nleast-square method. An application of the proposed method to experimental data\nof a metallic oxide thin film clearly indicates the existence of defects and\ncorresponding local lattice deformations.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.08643v1"
    },
    {
        "title": "Functions to map photoelectron distributions in a variety of setups in\n  angle-resolved photoemission spectroscopy",
        "authors": [
            "Y. Ishida",
            "S. Shin"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The distribution of photoelectrons acquired in angle-resolved photoemission\nspectroscopy can be mapped onto energy-momentum space of the Bloch electrons in\nthe crystal. The explicit forms of the mapping function $f$ depend on the\nconfiguration of the apparatus as well as on the type of the photoelectron\nanalyzer. We show that the existence of the analytic forms of $f^{\\text{-}1}$\nis guaranteed in a variety of setups. The variety includes the case when the\nanalyzer is equipped with a photoelectron deflector. Thereby, we provide a\ndemonstrative mapping program implemented by an algorithm that utilizes both\n$f$ and $f^{\\text{-}1}$. The mapping methodology is also usable in other\nspectroscopic methods such as momentum-resolved electron-energy loss\nspectroscopy.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.03450v2"
    },
    {
        "title": "High Dimensional Cluster Analysis Using Path Lengths",
        "authors": [
            "Kevin McIlhany",
            "Stephen Wiggins"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  A hierarchical scheme for clustering data is presented which applies to\nspaces with a high number of dimension ($N_{_{D}}>3$). The data set is first\nreduced to a smaller set of partitions (multi-dimensional bins). Multiple\nclustering techniques are used, including spectral clustering, however, new\ntechniques are also introduced based on the path length between partitions that\nare connected to one another. A Line-Of-Sight algorithm is also developed for\nclustering. A test bank of 12 data sets with varying properties is used to\nexpose the strengths and weaknesses of each technique. Finally, a robust\nclustering technique is discussed based on reaching a consensus among the\nmultiple approaches, overcoming the weaknesses found individually.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.04886v1"
    },
    {
        "title": "Azimuthal Anisotropy in High Energy Nuclear Collision - An Approach\n  based on Complex Network Analysis",
        "authors": [
            "Susmita Bhaduri",
            "Dr. Dipak Ghosh"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Recently, a complex network based method of Visibility Graph has been applied\nto confirm the scale-freeness and presence of fractal properties in the process\nof multiplicity fluctuation. Analysis of data obtained from experiments on\nhadron-nucleus and nucleus-nucleus interactions results in values of\nPower-of-Scale-freeness-of-Visibility-Graph-(PSVG) parameter extracted from the\nvisibility graphs. Here, the relativistic nucleus-nucleus interaction data have\nbeen analysed to detect azimuthal-anisotropy by extending the Visibility Graph\nmethod and extracting the average clustering coefficient, one of the important\ntopological parameters, from the graph. Azimuthal-distributions corresponding\nto different pseudorapidity-regions around the central-pseudorapidity value are\nanalysed utilising the parameter. Here we attempt to correlate the conventional\nphysical significance of this coefficient with respect to complex-network\nsystems, with some basic notions of particle production phenomenology, like\nclustering and correlation. Earlier methods for detecting anisotropy in\nazimuthal distribution, were mostly based on the analysis of statistical\nfluctuation. In this work, we have attempted to find deterministic information\non the anisotropy in azimuthal distribution by means of precise determination\nof topological parameter from a complex network perspective.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.10007v2"
    },
    {
        "title": "Filtration of the gravitational frequency shift in the radio links\n  communication with Earth's satellite",
        "authors": [
            "A. V. Gusev",
            "V. N. Rudenko"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  At present the Radioastron (RA) Earth's satellite having very elliptic orbit\nis used for probing of the gravitational red shift effect [1, 2]. Objective of\nthis test consists in the enhancing accuracy of measurement to check the\ncorrespondence of value of the effect to Einsten's theory at one order of value\nbetter then in was done in the GP-A experiment [3]. There are two H-masers in\ndisposal, one at the board of satellite and other at the Land Tracking Station\n(LTS). One can compare its mutual time rate using communication radio links\nbetween RA and LTS. In contrast with the GP-A experiment there is a possibility\nof measurement repetition and accumulation of data in the process of RA orbital\ncirculation. In principle it might be resulted in the increasing of the\nintegral accuracy. In this paper we investigate the achievable accuracy in the\nframe of particular method of the red shift extraction associated with the\ntechical specific of RA mission.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.00759v1"
    },
    {
        "title": "The case for preserving our knowledge and data in physics experiments",
        "authors": [
            "Frank Berghaus"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  This proceeding covers tools and technologies at our disposal for scientific\ndata preservation and shows that this extends the scientific reach of our\nexperiments. It is cost-efficient to warehouse data from completed experiments\non the tape archives of our national and international laboratories. These\nsubject-specific data stores also offer the technologies to capture and archive\nknowledge about experiments in the form of technical notes, electronic logs,\nwebsites, etc. Furthermore, it is possible to archive our source code and\ncomputing environments. The paper illustrates these challenges with experience\nfrom preserving the LEP data for the long term.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.01276v1"
    },
    {
        "title": "Combination of analysis techniques for efficient track reconstruction in\n  high multiplicity events",
        "authors": [
            "Ferenc Siklér"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  A novel combination of established data analysis techniques for\nreconstructing all charged-particle tracks in high energy collisions is\nproposed. It uses all information available in a collision event while keeping\ncompeting choices open as long as possible. Suitable track candidates are\nselected by transforming measured hits to a binned, three- or four-dimensional,\ntrack parameter space. It is accomplished by the use of templates taking\nadvantage of the translational and rotational symmetries of the detectors.\nTrack candidates and their corresponding hits, the nodes, form a usually highly\nconnected network, a bipartite graph, where we allow for multiple hit to track\nassignments, edges. The graph is cut into very many minigraphs by removing a\nfew of its vulnerable components, edged and nodes. Finally the hits are\ndistributed among the track candidates by exploring a deterministic decision\ntree. A depth-limited search is performed maximising the number of hits on\ntracks, and minimising the sum of track-fit $\\chi^2$. Simplified models of LHC\nsilicon trackers, as well as the relevant physics processes, are employed to\nstudy the performance (efficiency, purity, timing) of the proposed method in\nthe case of single or many simultaneous proton-proton collisions (high pileup),\nand for single heavy-ion collisions at the highest available energies.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.06146v1"
    },
    {
        "title": "Causal Decomposition in the Mutual Causation System",
        "authors": [
            "Albert C. Yang",
            "Norden E. Huang",
            "Chung-Kang Peng"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Inference of causality in time series has been principally based on the\nprediction paradigm. Nonetheless, the predictive causality approach may\noverlook the simultaneous and reciprocal nature of causal interactions observed\nin real world phenomena. Here, we present a causal decomposition approach that\nis not based on prediction, but based on the instantaneous phase dependency\nbetween the intrinsic components of a decomposed time series. The method\ninvolves two assumptions: (1) any cause effect relationship can be quantified\nwith instantaneous phase dependency between the source and target decomposed as\nintrinsic components at specific time scale, and (2) the phase dynamics in the\ntarget originating from the source are separable from the target itself. Using\nempirical mode decomposition, we show that the causal interaction is encoded in\ninstantaneous phase dependency at a specific time scale, and this phase\ndependency is diminished when the causal-related intrinsic component is removed\nfrom the effect. Furthermore, we demonstrate the generic applicability of our\nmethod to both stochastic and deterministic systems, and show the consistency\nof the causal decomposition method compared to existing methods, and finally\nuncover the key mode of causal interactions in both the modelled and actual\npredator prey system. We anticipate that this novel approach will assist with\nrevealing causal interactions in complex networks not accounted for by current\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.07292v1"
    },
    {
        "title": "The use of adversaries for optimal neural network training",
        "authors": [
            "Anton Hawthorne-Gonzalvez",
            "Martin Sevior"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  B-decay data from the Belle experiment at the KEKB collider have a\nsubstantial background from $e^{+}e^{-}\\to q \\bar{q}$ events. To suppress this\nwe employ deep neural network algorithms. These provide improved signal from\nbackground discrimination. However, the deep neural network develops a\nsubstantial correlation with the $\\Delta E$ kinematic variable used to\ndistinguish signal from background in the final fit due to its relationship\nwith input variables. The effect of this correlation is reduced by deploying an\nadversarial neural network. Overall the adversarial deep neural network\nperforms better than a Boosted Decision Tree algorithimn and a commercial\npackage, NeuroBayes, which employs a neural net with a single hidden layer.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.07790v2"
    },
    {
        "title": "Detecting dynamic spatial correlation patterns with generalized wavelet\n  coherence and non-stationary surrogate data",
        "authors": [
            "Mario Chavez",
            "Bernard Cazelles"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Time series measured from real-world systems are generally noisy, complex and\ndisplay statistical properties that evolve continuously over time. Here, we\npresent a method that combines wavelet analysis and non-stationary surrogates\nto detect short-lived spatial coherent patterns from multivari- ate\ntime-series. In contrast with standard methods, the surrogate data used here\nare realisations of a non-stationary stochastic process, preserving both the\namplitude and time-frequency distributions of original data. We evaluate this\nframework on synthetic and real-world time series, and we show that it can\nprovide useful insights into the time-resolved structure of spatially extended\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.04778v2"
    },
    {
        "title": "Calculating $p$-values and their significances with the Energy Test for\n  large datasets",
        "authors": [
            "W. Barter",
            "C. Burr",
            "C. Parkes"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The energy test method is a multi-dimensional test of whether two samples are\nconsistent with arising from the same underlying population, through the\ncalculation of a single test statistic (called the $T$-value). The method has\nrecently been used in particle physics to search for differences between\nsamples that arise from CP violation. The generalised extreme value function\nhas previously been used to describe the distribution of $T$-values under the\nnull hypothesis that the two samples are drawn from the same underlying\npopulation. We show that, in a simple test case, the distribution is not\nsufficiently well described by the generalised extreme value function. We\npresent a new method, where the distribution of $T$-values under the null\nhypothesis when comparing two large samples can be found by scaling the\ndistribution found when comparing small samples drawn from the same population.\nThis method can then be used to quickly calculate the $p$-values associated\nwith the results of the test.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.05222v2"
    },
    {
        "title": "Towards uncovering the structure of power fluctuations of wind farms",
        "authors": [
            "Huiwen Liu",
            "Yaqing Jin",
            "Nicloas Tobin",
            "Leonardo P. Chamorro"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The structure of the turbulence-driven power fluctuations in a wind farm is\nfundamentally described from basic concepts. A derived tuning-free model,\nsupported with experiments, reveals the underlying spectral content of the\npower fluctuations of a wind farm. It contains two power-law trends and\noscillations in the relatively low- and high-frequency ranges. The former is\nmostly due to the turbulent interaction between the flow and the turbine\nproperties; whereas the latter is due to the advection between turbine pairs.\nThe spectral wind-farm scale power fluctuations $\\Phi_P$ exhibits a power-law\ndecay proportional to $f^{-5/3-2}$ in the region corresponding to the\nturbulence inertial subrange and at relatively large scales, $\\Phi_P\\sim\nf^{-2}$. Due to the advection and turbulent diffusion of large-scale\nstructures, a spectral oscillation exists with the product of a sinusoidal\nbehavior and an exponential decay in the frequency domain.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06139v1"
    },
    {
        "title": "Gaining insight from large data volumes with ease",
        "authors": [
            "Valentin Kuznetsov"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Efficient handling of large data-volumes becomes a necessity in today's\nworld. It is driven by the desire to get more insight from the data and to gain\na better understanding of user trends which can be transformed into economic\nincentives (profits, cost-reduction, various optimization of data workflows,\nand pipelines). In this paper, we discuss how modern technologies are\ntransforming well established patterns in HEP communities. The new data insight\ncan be achieved by embracing Big Data tools for a variety of use-cases, from\nanalytics and monitoring to training Machine Learning models on a terabyte\nscale. We provide concrete examples within context of the CMS experiment where\nBig Data tools are already playing or would play a significant role in daily\noperations.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.04785v2"
    },
    {
        "title": "Understanding the boosted decision tree methods with the weak-learner\n  approximation",
        "authors": [
            "Li-Gang Xia"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Two popular boosted decsion tree (BDT) methods, Adaptive BDT (AdaBDT) and\nGradient BDT (GradBDT) are studied in the classification problem of separating\nsignal from background assuming all trees are weak learners. The following\nresults are obtained. a) The distribution of the BDT score is approximately\nGaussian for both methods. b) With more trees in training, the distance of the\nexpectaion of score distribution between signal and background is larger, but\nthe variance of both distributions becomes greater at the same time. c)\nExtenstion of the boosting mechanism in AdaBDT to any loss function is\npossible. d) AdaBDT is shown to be equivalent to the GradBDT with 2 terminal\nnodes for a decision tree. In the field high energy physics, many applications\npersue the best statistical significance. We also show that the maximization of\nthe statistical significance is closely related to the minimization of the loss\nfunction, which is the target of the BDT algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.04822v1"
    },
    {
        "title": "Gaussian Process Accelerated Feldman-Cousins Approach for Physical\n  Parameter Inference",
        "authors": [
            "Lingge Li",
            "Nitish Nayak",
            "Jianming Bian",
            "Pierre Baldi"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The unified approach of Feldman and Cousins allows for exact statistical\ninference of small signals that commonly arise in high energy physics. It has\ngained widespread use, for instance, in measurements of neutrino oscillation\nparameters in long-baseline experiments. However, the approach relies on the\nNeyman construction of the classical confidence interval and is computationally\nintensive as it is typically done in a grid-based fashion over the entire\nparameter space. In this letter, we propose an efficient algorithm for the\nFeldman-Cousins approach using Gaussian processes to construct confidence\nintervals iteratively. We show that in the neutrino oscillation context, one\ncan obtain confidence intervals 5 times faster in one dimension and 10 times\nfaster in two dimensions, while maintaining an accuracy above 99.5%.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.07050v3"
    },
    {
        "title": "Learning from power system data stream: phasor-detective approach",
        "authors": [
            "Mauro Escobar",
            "Daniel Bienstock",
            "Michael Chertkov"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Assuming access to synchronized stream of Phasor Measurement Unit (PMU) data\nover a significant portion of a power system interconnect, say controlled by an\nIndependent System Operator (ISO), what can you extract about past, current and\nfuture state of the system? We have focused on answering this practical\nquestions pragmatically - empowered with nothing but standard tools of data\nanalysis, such as PCA, filtering and cross-correlation analysis. Quite\nsurprisingly we have found that even during the quiet \"no significant events\"\nperiod this standard set of statistical tools allows the \"phasor-detective\" to\nextract from the data important hidden anomalies, such as problematic control\nloops at loads and wind farms, and mildly malfunctioning assets, such as\ntransformers and generators. We also discuss and sketch future challenges a\nmature phasor-detective can possibly tackle by adding machine learning and\nphysics modeling sophistication to the basic approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.07139v3"
    },
    {
        "title": "Generative Models for Fast Calorimeter Simulation.LHCb case",
        "authors": [
            "Viktoria Chekalina",
            "Elena Orlova",
            "Fedor Ratnikov",
            "Dmitry Ulyanov",
            "Andrey Ustyuzhanin",
            "Egor Zakharov"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Simulation is one of the key components in high energy physics. Historically\nit relies on the Monte Carlo methods which require a tremendous amount of\ncomputation resources. These methods may have difficulties with the expected\nHigh Luminosity Large Hadron Collider (HL LHC) need, so the experiment is in\nurgent need of new fast simulation techniques. We introduce a new Deep Learning\nframework based on Generative Adversarial Networks which can be faster than\ntraditional simulation methods by 5 order of magnitude with reasonable\nsimulation accuracy. This approach will allow physicists to produce a big\nenough amount of simulated data needed by the next HL LHC experiments using\nlimited computing resources.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.01319v2"
    },
    {
        "title": "Continuous cascades in the wavelet space as models for synthetic\n  turbulence",
        "authors": [
            "Jean-François Muzy"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We introduce a wide family of stochastic processes that are obtained as sums\nof self-similar localized \"waveforms\" with multiplicative intensity in the\nspirit of the Richardson cascade picture of turbulence. We establish the\nconvergence and the minimum regularity of our construction. We show that its\ncontinuous wavelet transform is characterized by stochastic self-similarity and\nmultifractal scaling properties. This model constitutes a stationary, \"grid\nfree\", extension of $\\cal W$-cascades introduced in the past by Arneodo, Bacry\nand Muzy using wavelet orthogonal basis. Moreover our approach generically\nprovides multifractal random functions that are not invariant by time reversal\nand therefore is able to account for skewed multifractal models and for the\nso-called \"leverage effect\". In that respect, it can be well suited to\nproviding synthetic turbulence models or to reproducing the main observed\nfeatures of asset price fluctuations in financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.02968v1"
    },
    {
        "title": "Finding the origin of noise transients in LIGO data with machine\n  learning",
        "authors": [
            "Marco Cavaglia",
            "Kai Staats",
            "Teerth Gill"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Quality improvement of interferometric data collected by gravitational-wave\ndetectors such as Advanced LIGO and Virgo is mission critical for the success\nof gravitational-wave astrophysics. Gravitational-wave detectors are sensitive\nto a variety of disturbances of non-astrophysical origin with characteristic\nfrequencies in the instrument band of sensitivity. Removing non-astrophysical\nartifacts that corrupt the data stream is crucial for increasing the number and\nstatistical significance of gravitational-wave detections and enabling refined\nastrophysical interpretations of the data. Machine learning has proved to be a\npowerful tool for analysis of massive quantities of complex data in astronomy\nand related fields of study. We present two machine learning methods, based on\nrandom forest and genetic programming algorithms, that can be used to determine\nthe origin of non-astrophysical transients in the LIGO detectors. We use two\nclasses of transients with known instrumental origin that were identified\nduring the first observing run of Advanced LIGO to show that the algorithms can\nsuccessfully identify the origin of non-astrophysical transients in real\ninterferometric data and thus assist in the mitigation of instrumental and\nenvironmental disturbances in gravitational-wave searches. While the data sets\ndescribed in this paper are specific to LIGO, and the exact procedures employed\nwere unique to the same, the random forest and genetic programming code bases\nand means by which they were applied as a dual machine learning approach are\ncompletely portable to any number of instruments in which noise is believed to\nbe generated through mechanical couplings, the source of which is not yet\ndiscovered.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.05225v1"
    },
    {
        "title": "\\texttt{GooStats}: A GPU-based framework for multi-variate analysis in\n  particle physics",
        "authors": [
            "Xuefeng Ding"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  \\texttt{GooStats} is a software framework that provides a flexible\nenvironment and common tools to implement multi-variate statistical analysis.\nThe framework is built upon the \\texttt{CERN ROOT}, \\texttt{MINUIT} and\n\\texttt{GooFit} packages. Running a multi-variate analysis in parallel on\ngraphics processing units yields a huge boost in performance and opens new\npossibilities. The design and benchmark of \\texttt{GooStats} are presented in\nthis article along with illustration of its application to statistical\nproblems.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.05686v1"
    },
    {
        "title": "Gradient Profile Estimation Using Exponential Cubic Spline Smoothing in\n  a Bayesian Framework",
        "authors": [
            "Kushani De Silva",
            "Carlo Cafaro",
            "Adom Giffin"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Attaining reliable profile gradients is of utmost relevance for many physical\nsystems. In most situations, the estimation of gradient can be inaccurate due\nto noise. It is common practice to first estimate the underlying system and\nthen compute the profile gradient by taking the subsequent analytic derivative.\nThe underlying system is often estimated by fitting or smoothing the data using\nother techniques. Taking the subsequent analytic derivative of an estimated\nfunction can be ill-posed. The ill-posedness gets worse as the noise in the\nsystem increases. As a result, the uncertainty generated in the gradient\nestimate increases. In this paper, a theoretical framework for a method to\nestimate the profile gradient of discrete noisy data is presented. The method\nis developed within a Bayesian framework. Comprehensive numerical experiments\nare conducted on synthetic data at different levels of random noise. The\naccuracy of the proposed method is quantified. Our findings suggest that the\nproposed gradient profile estimation method outperforms the state-of-the-art\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.04223v1"
    },
    {
        "title": "A deep neural network for simultaneous estimation of b jet energy and\n  resolution",
        "authors": [
            " CMS Collaboration"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We describe a method to obtain point and dispersion estimates for the\nenergies of jets arising from b quarks produced in proton-proton collisions at\nan energy of $\\sqrt{s} =$ 13 TeV at the CERN LHC. The algorithm is trained on a\nlarge simulated sample of b jets and validated on data recorded by the CMS\ndetector in 2017 corresponding to an integrated luminosity of 41 fb$^{-1}$. A\nmultivariate regression algorithm based on a deep feed-forward neural network\nemploys jet composition and shape information, and the properties of\nreconstructed secondary vertices associated with the jet. The results of the\nalgorithm are used to improve the sensitivity of analyses that make use of b\njets in the final state, such as the observation of Higgs boson decay to\n$\\mathrm{b\\bar{b}}$.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.06046v2"
    },
    {
        "title": "KLT Picker: Particle Picking Using Data-Driven Optimal Templates",
        "authors": [
            "Amitay Eldar",
            "Boris Landa",
            "Yoel Shkolnisky"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Particle picking is currently a critical step in the cryo-EM single particle\nreconstruction pipeline. Despite extensive work on this problem, for many data\nsets it is still challenging, especially for low SNR micrographs. We present\nthe KLT (Karhunen Loeve Transform) picker, which is fully automatic and\nrequires as an input only the approximated particle size. In particular, it\ndoes not require any manual picking. Our method is designed especially to\nhandle low SNR micrographs. It is based on learning a set of optimal templates\nthrough the use of multi-variate statistical analysis via the Karhunen Loeve\nTransform. We evaluate the KLT picker on publicly available data sets and\npresent high-quality results with minimal manual effort.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.06500v1"
    },
    {
        "title": "Rapid Identification of X-ray Diffraction Spectra Based on Very Limited\n  Data by Interpretable Convolutional Neural Networks",
        "authors": [
            "Hong Wang",
            "Yunchao Xie",
            "Dawei Li",
            "Heng Deng",
            "Yunxin Zhao",
            "Ming Xin",
            "Jian Lin"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Large volumes of data from material characterizations call for rapid and\nautomatic data analysis to accelerate materials discovery. Herein, we report a\nconvolutional neural network (CNN) that was trained based on theoretic data and\nvery limited experimental data for fast identification of experimental X-ray\ndiffraction (XRD) spectra of metal-organic frameworks (MOFs). To augment the\ndata for training the model, noise was extracted from experimental spectra and\nshuffled, then merged with the main peaks that were extracted from theoretical\nspectra to synthesize new spectra. For the first time, one-to-one material\nidentification was achieved. The optimized model showed the highest\nidentification accuracy of 96.7% for the Top 5 ranking among a dataset of 1012\nMOFs. Neighborhood components analysis (NCA) on the experimental XRD spectra\nshows that the spectra from the same material are clustered in groups in the\nNCA map. Analysis on the class activation maps of the last CNN layer further\ndiscloses the mechanism by which the CNN model successfully identifies\nindividual MOFs from the XRD spectra. This CNN model trained by the\ndata-augmentation technique would not only open numerous potential applications\nfor identifying XRD spectra for different materials, but also pave avenues to\nautonomously analyze data by other characterization tools such as FTIR, Raman,\nand NMR.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.07750v1"
    },
    {
        "title": "Human search in a fitness landscape: How to assess the difficulty of a\n  search problem",
        "authors": [
            "Oana Vuculescu",
            "Mads Kock Pedersen",
            "Jacob F. Sherson",
            "Carsten Bergenholtz"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Computational modeling is widely used to study how humans and organizations\nsearch and solve problems in fields such as economics, management, cultural\nevolution, and computer science. We argue that current computational modeling\nresearch on human problem-solving needs to address several fundamental issues\nin order to generate more meaningful and falsifiable contributions. Based on\ncomparative simulations and a new type of visualization of how to assess the\nnature of the fitness landscape, we address two key assumptions that approaches\nsuch as the NK framework rely on: that the NK captures the continuum of the\ncomplexity of empirical fitness landscapes, and that search behavior is a\ndistinct component, independent from the topology of the fitness landscape. We\nshow the limitations of the most common approach to conceptualize how complex,\nor rugged, a landscape is, as well as how the nature of the fitness landscape\nis fundamentally intertwined with search behavior. Finally, we outline broader\nimplications for how to simulate problem-solving.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.07954v2"
    },
    {
        "title": "Uncertainty Analysis of Stray Field Measurements by Quantitative\n  Magnetic Force Microscopy",
        "authors": [
            "Xiukun Hu",
            "Gaoliang Dai",
            "Sibylle Sievers",
            "Alexander Fernández Scarioni",
            "Volker Neu",
            "Mark Bieler",
            "Hans Werner Schumacher"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Magnetic force microscopy (MFM) measurements generally provide phase images\nwhich represent the signature of domain structures on the surface of\nnanomaterials. To quantitatively determine magnetic stray fields based on an\nMFM image requires calibrated properties of the magnetic tip. In this work, an\napproach is presented for calibrating a magnetic tip using a Co/Pt multilayered\nfilm as a reference sample which shows stable well-known magnetic properties\nand well-defined perpendicular band domains. The approach is based on a\nregularized deconvolution process in Fourier domain with a Wiener filter and\nthe L-curve method for determining a suitable regularization parameter to get a\nphysically reasonable result. The calibrated tip is applied for a traceable\nquantitative determination of the stray fields of a test sample which has a\npatial frequency spectrum covered by that of the reference sample. According to\nthe \"Guide to the expression of uncertainty in measurement\", uncertainties of\nthe processing algorithm are estimated considering the fact that the\nregularization influences significantly the quantitative analysis. We discuss\nrelevant uncertainty components and their propagations between real domain and\nFourier domain for both, the tip calibration procedure and the stray field\ncalculation, and propose an uncertainty evaluation procedure for quantitative\nmagnetic force microscopy.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.08437v2"
    },
    {
        "title": "Alternative to the application of PDG scale factors",
        "authors": [
            "Jens Erler",
            "Rodolfo Ferro-Hernandez"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The Particle Data Group recommends a set of procedures to be applied when\ndiscrepant data are to be combined. We introduce an alternative method based on\na more general and solid statistical framework, providing a robust way to\ninclude possible unknown systematic effects interfering with experimental\nmeasurements or their theoretical interpretation. The limit of large data sets\nand practical cases of interest are discussed in detail.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.01219v2"
    },
    {
        "title": "Deep learning for Gaussian process tomography model selection using the\n  ASDEX Upgrade SXR system",
        "authors": [
            "Francisco Matos",
            "Jakob Svensson",
            "Andrea Pavone",
            "Tomas Odstrcil",
            "Frank Jenko"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Gaussian process tomography (GPT) is a method used for obtaining real-time\ntomographic reconstructions of the plasma emissivity profile in a tokamak,\ngiven some model for the underlying physical processes involved. GPT can also\nbe used, thanks to Bayesian formalism, to perform model selection -- i.e.,\ncomparing different models and choosing the one with maximum evidence. However,\nthe computations involved in this particular step may become slow for data with\nhigh dimensionality, especially when comparing the evidence for many different\nmodels. Using measurements collected by the ASDEX Upgrade Soft X-ray (SXR)\ndiagnostic, we train a convolutional neural network (CNN) to map SXR\ntomographic projections to the corresponding GPT model whose evidence is\nhighest. We then compare the network's results, and the time required to\ncalculate them, with those obtained through analytical Bayesian formalism. In\naddition, we use the network's classifications to produce tomographic\nreconstructions of the plasma emissivity profile, whose quality we evaluate by\ncomparing their projection into measurement space with the existing\nmeasurements themselves.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.06429v1"
    },
    {
        "title": "Coping with dating errors in causality estimation",
        "authors": [
            "D. A. Smirnov",
            "N. Marwan",
            "S. F. M. Breitenbach",
            "F. Lechleitner",
            "J. Kurths"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  {We consider the problem of estimating causal influences between observed\nprocesses from time series possibly corrupted by errors in the time variable\n(dating errors) which are typical in palaeoclimatology, planetary science and\nastrophysics. \"Causality ratio\" based on the Wiener -- Granger causality is\nproposed and studied for a paradigmatic class of model systems to reveal\nconditions under which it correctly indicates directionality of unidirectional\ncoupling. It is argued that in case of {\\it a priori} known directionality, the\ncausality ratio allows a characterization of dating errors and observational\nnoise. Finally, we apply the developed approach to palaeoclimatic data and\nquantify the influence of solar activity on tropical Atlantic climate dynamics\nover the last two millennia. A stronger solar influence in the first millennium\nA.D. is inferred. The results also suggest a dating error of about 20 years in\nthe solar proxy time series over the same period.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.08322v1"
    },
    {
        "title": "MJOLNIR: A Software Package for Multiplexing Neutron Spectrometers",
        "authors": [
            "Jakob Lass",
            "Henrik Jacobsen",
            "Daniel G. Mazzone",
            "Kim Lefmann"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Novel multiplexing triple-axis neutron scattering spectrometers yield\nsignificant improvements of the common triple-axis instruments. While the\nplanar scattering geometry keeps ensuring compatibility with complex sample\nenvironments, a simultaneous detection of scattered neutrons at various angles\nand energies leads to tremendous improvements in the data acquisition rate.\nHere we report on the software package MJOLNIR that we have developed to handle\nthe resulting enhancement in data complexity. Using data from the new CAMEA\nspectrometer of the Swiss Spallation Neutron Source at the Paul Scherrer\nInstitut, we show how the software reduces, visualises and treats observables\nmeasured on multiplexing spectrometers. The software package has been\ngeneralised to a uniformed framework, allowing for collaborations across\nmultiplexing instruments at different facilities, further facilitating new\ndevelopments in data treatment, such as fitting routines and modelling of\nmulti-dimensional data.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.14816v1"
    },
    {
        "title": "Measuring topological descriptors of complex networks under uncertainty",
        "authors": [
            "Sebastian Raimondo",
            "Manlio De Domenico"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Revealing the structural features of a complex system from the observed\ncollective dynamics is a fundamental problem in network science. In order to\ncompute the various topological descriptors commonly used to characterize the\nstructure of a complex system (e.g. the degree, the clustering coefficient), it\nis usually necessary to completely reconstruct the network of relations between\nthe subsystems. Several methods are available to detect the existence of\ninteractions between the nodes of a network. By observing some physical\nquantities through time, the structural relationships are inferred using\nvarious discriminating statistics (e.g. correlations, mutual information,\netc.). In this setting, the uncertainty about the existence of the edges is\nreflected in the uncertainty about the topological descriptors. In this study,\nwe propose a novel methodological framework to evaluate this uncertainty,\nreplacing the topological descriptors, even at the level of a single node, with\nappropriate probability distributions, eluding the reconstruction phase. Our\ntheoretical framework agrees with the numerical experiments performed on a\nlarge set of synthetic and real-world networks. Our results provide a grounded\nframework for the analysis and the interpretation of widely used topological\ndescriptors, such as degree centrality, clustering and clusters, in scenarios\nwhere the existence of network connectivity is statistically inferred or when\nthe probabilities of existence $\\pi_{ij}$ of the edges are known. To this\npurpose we also provide a simple and mathematically grounded process to\ntransform the discriminating statistics into the probabilities $\\pi_{ij}$ .\n",
        "pdf_link": "http://arxiv.org/pdf/2009.06326v2"
    },
    {
        "title": "Reproducibility and Replication of Experimental Particle Physics Results",
        "authors": [
            "Thomas R. Junk",
            "Louis Lyons"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Recently, much attention has been focused on the replicability of scientific\nresults, causing scientists, statisticians, and journal editors to examine\nclosely their methodologies and publishing criteria. Experimental particle\nphysicists have been aware of the precursors of non-replicable research for\nmany decades and have many safeguards to ensure that the published results are\nas reliable as possible. The experiments require large investments of time and\neffort to design, construct, and operate. Large collaborations produce and\ncheck the results, and many papers are signed by more than three thousand\nauthors. This paper gives an introduction to what experimental particle physics\nis and to some of the tools that are used to analyze the data. It describes the\nprocedures used to ensure that results can be computationally reproduced, both\nby collaborators and by non-collaborators. It describes the status of publicly\navailable data sets and analysis tools that aid in reproduction and recasting\nof experimental results. It also describes methods particle physicists use to\nmaximize the reliability of the results, which increases the probability that\nthey can be replicated by other collaborations or even the same collaborations\nwith more data and new personnel. Examples of results that were later found to\nbe false are given, both with failed replication attempts and one with\nalarmingly successful replications. While some of the characteristics of\nparticle physics experiments are unique, many of the procedures and techniques\ncan be and are used in other fields.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.06864v4"
    },
    {
        "title": "Bayesian inference of 1D activity profiles from segmented gamma scanning\n  of a heterogeneous radioactive waste drum",
        "authors": [
            "Eric Laloy",
            "Bart Rogiers",
            "An Bielen",
            "Sven Boden"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We present a Bayesian approach to probabilistically infer vertical activity\nprofiles within a radioactive waste drum from segmented gamma scanning (SGS)\nmeasurements. Our approach resorts to Markov chain Monte Carlo (MCMC) sampling\nusing the state-of-the-art Hamiltonian Monte Carlo (HMC) technique and accounts\nfor two important sources of uncertainty: the measurement uncertainty and the\nuncertainty in the source distribution within the drum. In addition, our\nefficiency model simulates the contributions of all considered segments to each\ncount measurement. Our approach is first demonstrated with a synthetic example,\nafter which it is used to resolve the vertical activity distribution of 5\nnuclides in a real waste package.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.02112v2"
    },
    {
        "title": "Neural Network for 3D ICF Shell Reconstruction from Single Radiographs",
        "authors": [
            "Bradley T. Wolfe",
            "Zhizhong Han",
            "Jonathan S. Ben-Benjamin",
            "John L. Kline",
            "David S. Montgomery",
            "Elizabeth C. Merritt",
            "Paul A. Keiter",
            "Eric Loomis",
            "Brian M. Patterson",
            "Lindsey Kuettner",
            "Zhehui Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  In inertial confinement fusion (ICF), X-ray radiography is a critical\ndiagnostic for measuring implosion dynamics, which contains rich 3D\ninformation. Traditional methods for reconstructing 3D volumes from 2D\nradiographs, such as filtered backprojection, require radiographs from at least\ntwo different angles or lines of sight (LOS). In ICF experiments, space for\ndiagnostics is limited and cameras that can operate on the fast timescales are\nexpensive to implement, limiting the number of projections that can be\nacquired. To improve the imaging quality as a result of this limitation,\nconvolutional neural networks (CNN) have recently been shown to be capable of\nproducing 3D models from visible light images or medical X-ray images rendered\nby volumetric computed tomography LOS (SLOS). We propose a CNN to reconstruct\n3D ICF spherical shells from single radiographs. We also examine sensitivity of\nthe 3D reconstruction to different illumination models using preprocessing\ntechniques such as pseudo-flat fielding. To resolve the issue of the lack of 3D\nsupervision, we show that training the CNN utilizing synthetic radiographs\nproduced by known simulation methods allows for reconstruction of experimental\ndata as long as the experimental data is similar to the synthetic data. We also\nshow that the CNN allows for 3D reconstruction of shells that possess low mode\nasymmetries. Further comparisons of the 3D reconstructions with direct multiple\nLOS measurements are justified.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03646v2"
    },
    {
        "title": "A Digital Quantum Algorithm for Jet Clustering in High-Energy Physics",
        "authors": [
            "Diogo Pires",
            "Pedrame Bargassa",
            "João Seixas",
            "Yasser Omar"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Experimental High-Energy Physics (HEP), especially the Large Hadron Collider\n(LHC) programme at the European Organization for Nuclear Research (CERN), is\none of the most computationally intensive activities in the world. This demand\nis set to increase significantly with the upcoming High-Luminosity LHC\n(HL-LHC), and even more in future machines, such as the Future Circular\nCollider (FCC). As a consequence, event reconstruction, and in particular jet\nclustering, is bound to become an even more daunting problem, thus challenging\npresent day computing resources. In this work, we present the first digital\nquantum algorithm to tackle jet clustering, opening the way for digital quantum\nprocessors to address this challenging problem. Furthermore, we show that, at\npresent and future collider energies, our algorithm has comparable, yet\ngenerally lower complexity relative to the classical state-of-the-art $k_t$\nclustering algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.05618v1"
    },
    {
        "title": "Improved Asymptotic Formulae for Statistical Interpretation Based on\n  Likelihood Ratio Tests",
        "authors": [
            "Li-Gang Xia",
            "Yan Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  In this work, we try to improve the classic asymptotic formulae to describe\nthe probability distribution of likelihood-ratio statistical tests. The idea is\nto split the probability distribution function into two parts. One part is\nuniversal and described by the asymptotic formulae. The other part is\ncase-dependent and estimated explicitly using a 6-bin model proposed in this\nwork. The latter is similar to doing toy simulations and hence is able to\npredict the discrete structures in the probability distributions. The new\nasymptotic formulae provide a much better differential description of the test\nstatistics. The better performance is confirmed in two toy examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.06944v7"
    },
    {
        "title": "Convolutional neural network for self-mixing interferometric\n  displacement sensing",
        "authors": [
            "Stéphane Barland",
            "François Gustave"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Self mixing interferometry is a well established interferometric measurement\ntechnique. In spite of the robustness and simplicity of the concept,\ninterpreting the self-mixing signal is often complicated in practice, which is\ndetrimental to measurement availability. Here we discuss the use of a\nconvolutional neural network to reconstruct the displacement of a target from\nthe self mixing signal in a semiconductor laser. The network, once trained on\nperiodic displacement patterns, can reconstruct arbitrarily complex\ndisplacement in different alignment conditions and setups. The approach\nvalidated here is amenable to generalization to modulated schemes or even to\ntotally different self mixing sensing tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.07875v1"
    },
    {
        "title": "Ensemble learning and iterative training (ELIT) machine learning:\n  applications towards uncertainty quantification and automated experiment in\n  atom-resolved microscopy",
        "authors": [
            "Ayana Ghosh",
            "Bobby G. Sumpter",
            "Ondrej Dyck",
            "Sergei V. Kalinin",
            "Maxim Ziatdinov"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Deep learning has emerged as a technique of choice for rapid feature\nextraction across imaging disciplines, allowing rapid conversion of the data\nstreams to spatial or spatiotemporal arrays of features of interest. However,\napplications of deep learning in experimental domains are often limited by the\nout-of-distribution drift between the experiments, where the network trained\nfor one set of imaging conditions becomes sub-optimal for different ones. This\nlimitation is particularly stringent in the quest to have an automated\nexperiment setting, where retraining or transfer learning becomes impractical\ndue to the need for human intervention and associated latencies. Here we\nexplore the reproducibility of deep learning for feature extraction in\natom-resolved electron microscopy and introduce workflows based on ensemble\nlearning and iterative training to greatly improve feature detection. This\napproach both allows incorporating uncertainty quantification into the deep\nlearning analysis and also enables rapid automated experimental workflows where\nretraining of the network to compensate for out-of-distribution drift due to\nsubtle change in imaging conditions is substituted for a human operator or\nprogrammatic selection of networks from the ensemble. This methodology can be\nfurther applied to machine learning workflows in other imaging areas including\noptical and chemical imaging.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.08449v2"
    },
    {
        "title": "Reconstruction of nanoscale particles from single-shot wide-angle FEL\n  diffractions patterns with physics-informed neural networks",
        "authors": [
            "Thomas Stielow",
            "Stefan Scheel"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Single-shot wide-angle diffraction imaging is a widely used method to\ninvestigate the structure of non-crystallizing objects such as nanoclusters,\nlarge proteins or even viruses. Its main advantage is that information about\nthe three-dimensional structure of the object is already contained in a single\nimage. This makes it useful for the reconstruction of fragile and\nnon-reproducible particles without the need for tomographic measurements.\nHowever, currently there is no efficient numerical inversion algorithm\navailable that is capable of determining the object's structure in real time.\nNeural networks, on the other hand, excel in image processing tasks suited for\nsuch purpose. Here we show how a physics-informed deep neural network can be\nused to reconstruct complete three-dimensional object models of uniform, convex\nparticles on a voxel grid from single two-dimensional wide-angle scattering\npatterns. We demonstrate its universal reconstruction capabilities for silver\nnanoclusters, where the network uncovers novel geometric structures that\nreproduce the experimental scattering data with very high precision.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.09136v3"
    },
    {
        "title": "Inference of stochastic time series with missing data",
        "authors": [
            "Sangwon Lee",
            "Vipul Periwal",
            "Junghyo Jo"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Inferring dynamics from time series is an important objective in data\nanalysis. In particular, it is challenging to infer stochastic dynamics given\nincomplete data. We propose an expectation maximization (EM) algorithm that\niterates between alternating two steps: E-step restores missing data points,\nwhile M-step infers an underlying network model of restored data. Using\nsynthetic data generated by a kinetic Ising model, we confirm that the\nalgorithm works for restoring missing data points as well as inferring the\nunderlying model. At the initial iteration of the EM algorithm, the model\ninference shows better model-data consistency with observed data points than\nwith missing data points. As we keep iterating, however, missing data points\nshow better model-data consistency. We find that demanding equal consistency of\nobserved and missing data points provides an effective stopping criterion for\nthe iteration to prevent overshooting the most accurate model inference. Armed\nwith this EM algorithm with this stopping criterion, we infer missing data\npoints and an underlying network from a time-series data of real neuronal\nactivities. Our method recovers collective properties of neuronal activities,\nsuch as time correlations and firing statistics, which have previously never\nbeen optimized to fit.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.11816v2"
    },
    {
        "title": "Fast and scalable likelihood maximization for Exponential Random Graph\n  Models with local constraints",
        "authors": [
            "Nicolò Vallarano",
            "Matteo Bruno",
            "Emiliano Marchese",
            "Giuseppe Trapani",
            "Fabio Saracco",
            "Giulio Cimini",
            "Mario Zanon",
            "Tiziano Squartini"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Exponential Random Graph Models (ERGMs) have gained increasing popularity\nover the years. Rooted into statistical physics, the ERGMs framework has been\nsuccessfully employed for reconstructing networks, detecting statistically\nsignificant patterns in graphs, counting networked configurations with given\nproperties. From a technical point of view, the ERGMs workflow is defined by\ntwo subsequent optimization steps: the first one concerns the maximization of\nShannon entropy and leads to identify the functional form of the ensemble\nprobability distribution that is maximally non-committal with respect to the\nmissing information; the second one concerns the maximization of the likelihood\nfunction induced by this probability distribution and leads to its numerical\ndetermination. This second step translates into the resolution of a system of\n$O(N)$ non-linear, coupled equations (with $N$ being the total number of nodes\nof the network under analysis), a problem that is affected by three main\nissues, i.e. accuracy, speed and scalability. The present paper aims at\naddressing these problems by comparing the performance of three algorithms\n(i.e. Newton's method, a quasi-Newton method and a recently-proposed\nfixed-point recipe) in solving several ERGMs, defined by binary and weighted\nconstraints in both a directed and an undirected fashion. While Newton's method\nperforms best for relatively little networks, the fixed-point recipe is to be\npreferred when large configurations are considered, as it ensures convergence\nto the solution within seconds for networks with hundreds of thousands of nodes\n(e.g. the Internet, Bitcoin). We attach to the paper a Python code implementing\nthe three aforementioned algorithms on all the ERGMs considered in the present\nwork.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.12625v4"
    },
    {
        "title": "Comparative visualization of epidemiological data during various stages\n  of a pandemic",
        "authors": [
            "Thomas Kreuz"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  After COVID-19 was first reported in China at the end of 2019, it took only a\nfew months for this local crisis to turn into a global pandemic with\nunprecedented disruptions of everyday life. However, at any moment in time the\nsituation in different parts of the world is far from uniform and each country\nfollows its own epidemiological trajectory. In order to keep track of the\ncourse of the pandemic in many different places at the same time, it is vital\nto develop comparative visualizations that facilitate the recognition of common\ntrends and divergent behaviors. Similarly, it is important to always focus on\nthe information that is most relevant at any given point in time. In this study\nwe look at exactly one year of daily numbers of new cases and deaths and\npresent data visualizations that compare many different countries and are\nadapted to the overall stage of the pandemic. During the early stage when cases\nand deaths still rise we focus on the time lag relative to the current\nepicenter of the pandemic and the doubling times. Later we monitor the rise and\nfall of the daily numbers via wave detection plots. The transition between\nthese two stages takes place when the daily numbers stop rising for the first\ntime.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.11315v2"
    },
    {
        "title": "Progresses and Challenges in Link Prediction",
        "authors": [
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Link prediction is a paradigmatic problem in network science, which aims at\nestimating the existence likelihoods of nonobserved links, based on known\ntopology. After a brief introduction of the standard problem and metrics of\nlink prediction, this Perspective will summarize representative progresses\nabout local similarity indices, link predictability, network embedding, matrix\ncompletion, ensemble learning and others, mainly extracted from thousands of\nrelated publications in the last decade. Finally, this Perspective will outline\nsome long-standing challenges for future studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.11472v2"
    },
    {
        "title": "Readable and efficient HEP data analysis with bamboo",
        "authors": [
            "Pieter David"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  With the LHC continuing to collect more data and experimental analyses\nbecoming increasingly complex, tools to efficiently develop and execute these\nanalyses are essential. The bamboo framework defines a domain-specific\nlanguage, embedded in python, that allows to concisely express the analysis\nlogic in a functional style. The implementation based on ROOT's RDataFrame and\ncling C++ JIT compiler approaches the performance of dedicated native code.\nBamboo is currently being used for several CMS Run 2 analyses that rely on the\nNanoAOD data format, which will become more common in Run 3 and beyond, and for\nwhich many reusable components are included, but it provides many possibilities\nfor customisation, which allow for straightforward adaptation to other formats\nand workflows\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01889v2"
    },
    {
        "title": "Mapping causal patterns in crystalline solids",
        "authors": [
            "Chris Nelson",
            "Anna N. Morozovska",
            "Maxim A. Ziatdinov",
            "Eugene A. Eliseev",
            "Xiaohang Zhang",
            "Ichiro Takeuchi",
            "Sergei V. Kalinin"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The evolution of the atomic structures of the combinatorial library of\nSm-substituted thin film BiFeO3 along the phase transition boundary from the\nferroelectric rhombohedral phase to the non-ferroelectric orthorhombic phase is\nexplored using scanning transmission electron microscopy (STEM). Localized\nproperties including polarization, lattice parameter, and chemical composition\nare parameterized from atomic-scale imaging and their causal relationships are\nreconstructed using a linear non-Gaussian acyclic model (LiNGAM). This approach\nis further extended toward exploring the spatial variability of the causal\ncoupling using the sliding window transform method, which revealed that new\ncausal relationships emerged both at the expected locations, such as domain\nwalls and interfaces, but also at additional regions forming clusters in the\nvicinity of the walls or spatially distributed features. While the exact\nphysical origins of these relationships are unclear, they likely represent\nnanophase separated regions in the morphotropic phase boundaries. Overall, we\npose that an in-depth understanding of complex disordered materials away from\nthermodynamic equilibrium necessitates understanding not only of the generative\nprocesses that can lead to observed microscopic states, but also the causal\nlinks between multiple interacting subsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01951v1"
    },
    {
        "title": "FuncADL: Functional Analysis Description Language",
        "authors": [
            "Mason Proffitt",
            "Gordon Watts"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The traditional approach in HEP analysis software is to loop over every event\nand every object via the ROOT framework. This method follows an imperative\nparadigm, in which the code is tied to the storage format and steps of\nexecution. A more desirable strategy would be to implement a declarative\nlanguage, such that the storage medium and execution are not included in the\nabstraction model. This will become increasingly important to managing the\nlarge dataset collected by the LHC and the HL-LHC. A new analysis description\nlanguage (ADL) inspired by functional programming, FuncADL, was developed using\nPython as a host language. The expressiveness of this language was tested by\nimplementing example analysis tasks designed to benchmark the functionality of\nADLs. Many simple selections are expressible in a declarative way with FuncADL,\nwhich can be used as an interface to retrieve filtered data. Some limitations\nwere identified, but the design of the language allows for future extensions to\nadd missing features. FuncADL is part of a suite of analysis software tools\nbeing developed by the Institute for Research and Innovation in Software for\nHigh Energy Physics (IRIS-HEP). These tools will be available to develop highly\nscalable physics analyses for the LHC.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.02432v2"
    },
    {
        "title": "Analysis of reliability systems via Gini-type index",
        "authors": [
            "Motahareh Parsa",
            "Antonio Di Crescenzo",
            "Hadi Jabbari"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Different strategies of reliability theory for the analysis of coherent\nsystems have been studied by various researchers. Here, the Gini-type index is\nutilized as an applicable tool for the study and comparison of the ageing\nproperties of complex systems. A new stochastic order in terms of Gini-type\nindex is introduced to compare the speed of ageing of components and systems.\nThe parallel-series and series-parallel systems with shared components are\nstudied by their corresponding Gini-type indexes. Also, the generalization of\nGini-type index for the multidimensional case is discussed, and is used to\ncompare components lifetimes properties in the presence of other dependent\ncomponents. It is shown that the ageing properties of a component lifetime can\ndiffer when the other components are working or have already failed. Numerous\nillustrative examples are given for better intuition of Gini-type and\ngeneralized Gini-type indexes throughout the paper.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.11812v1"
    },
    {
        "title": "Jet characterization in Heavy Ion Collisions by QCD-Aware Graph Neural\n  Networks",
        "authors": [
            "Yogesh Verma",
            "Satyajit Jena"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The identification of jets and their constituents is one of the key problems\nand challenging task in heavy ion experiments such as experiments at RHIC and\nLHC. The presence of huge background of soft particles pose a curse for jet\nfinding techniques. The inabilities or lack of efficient techniques to filter\nout the background lead to a fake or combinatorial jet formation which may have\nan errorneous interpretation. In this article, we present Graph Reduction\ntechnique (GraphRed), a novel class of physics-aware and topology-based\nattention graph neural network built upon jet physics in heavy ion collisions.\nThis approach directly works with the physical observables of variable-length\nset of final state particles on an event-by-event basis to find most likely\njet-induced particles in an event. This technique demonstrate the robustness\nand applicability of this method for finding jet-induced particles and show\nthat graph architectures are more efficient than previous frameworks. This\ntechnique exhibit foremost time a classifier working on particle-level in each\nheavy ion event produced at the LHC. We present the applicability and\nintegration of the model with current jet finding algorithms such as FastJet.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.14906v2"
    },
    {
        "title": "Signal estimation in On/Off measurements including event-by-event\n  variables",
        "authors": [
            "Giacomo D'Amico",
            "Tomislav Terzić",
            "Jelena Strišković",
            "Michele Doro",
            "Marcel Strzys",
            "Juliane van Scherpenberg"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Signal estimation in the presence of background noise is a common problem in\nseveral scientific disciplines. An 'On/Off' measurement is performed when the\nbackground itself is not known, being estimated from a background control\nsample. The 'frequentist' and Bayesian approaches for signal estimation in\nOn/Off measurements are reviewed and compared, focusing on the weakness of the\nformer and on the advantages of the latter in correctly addressing the\nPoissonian nature of the problem. In this work, we devise a novel\nreconstruction method, dubbed BASiL (Bayesian Analysis including Single-event\nLikelihoods), for estimating the signal rate based on the Bayesian formalism.\nIt uses information on event-by-event individual parameters and their\ndistribution for the signal and background population. Events are thereby\nweighted according to their likelihood of being a signal or a background event\nand background suppression can be achieved without performing fixed fiducial\ncuts. Throughout the work, we maintain a general notation, that allows to apply\nthe method generically, and provide a performance test using real data and\nsimulations of observations with the MAGIC telescopes, as demonstration of the\nperformance for Cherenkov telescopes. BASiL allows to estimate the signal more\nprecisely, avoiding loss of exposure due to signal extraction cuts. We expect\nits applicability to be straightforward in similar cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.01019v1"
    },
    {
        "title": "Geant4Reweight: a framework for evaluating and propagating hadronic\n  interaction uncertainties in GEANT4",
        "authors": [
            "J. Calcutt",
            "C. Thorpe",
            "K. Mahn",
            "Laura Fields"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Geant4Reweight is an open-source C++ framework that allows users to 1) weight\ntracks produced by the GEANT4 particle transport Monte Carlo simulation\naccording to hadron interaction cross section variations and 2) estimate\nuncertainties in GEANT4 interaction models by comparing the simulation's hadron\ninteraction cross section predictions to data. The ability to weight hadron\ntransport as simulated by GEANT4 is crucial to the propagation of systematic\nuncertainties related to secondary hadronic interactions in current and\nupcoming neutrino oscillation experiments, including MicroBooNE, NOvA, and\nDUNE, as well as hadron test beam experiments such as ProtoDUNE. We provide\nmotivation for weighting hadron tracks in GEANT4 in the context of systematic\nuncertainty propagation, a description of GEANT4's transport simulation\ntechnique, and a description of our weighting technique and fitting framework\nin the momentum range 0--10 GeV/c, which is typical for the hadrons produced by\nneutrino interactions in these experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.01744v3"
    },
    {
        "title": "Autonomous Experiments for Neutron Three-Axis Spectrometers (TAS) with\n  Log-Gaussian Processes",
        "authors": [
            "Mario Teixeira Parente",
            "Georg Brandl",
            "Christian Franz",
            "Astrid Schneidewind",
            "Marina Ganeva"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Autonomous experiments are excellent tools to increase the efficiency of\nmaterial discovery. Indeed, AI and ML methods can help optimizing valuable\nexperimental resources as, for example, beam time in neutron scattering\nexperiments, in addition to scientists' knowledge and experience. Active\nlearning methods form a particular class of techniques that acquire knowledge\non a specific quantity of interest by autonomous decisions on what or where to\ninvestigate next based on previous measurements. For instance, Gaussian Process\nRegression (GPR) is a well-known technique that can be exploited to accomplish\nactive learning tasks for scattering experiments as was recently demonstrated.\nGaussian processes are not only capable to approximate functions by their\nposterior mean function, but can also quantify uncertainty about the\napproximation itself. Hence, if we perform function evaluations at locations of\nhighest uncertainty, the function can be \"optimally\" learned in an iterative\nmanner. We suggest the use of log-Gaussian processes, being a natural approach\nto successfully conduct autonomous neutron scattering experiments in general\nand TAS experiments with the instrument PANDA at MLZ in particular.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.07716v2"
    },
    {
        "title": "The mini-DST: a high-level LCIO format",
        "authors": [
            "Shin-ichi Kawada"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  A new LCIO-based data format called mini-DST has been developed, which\ncombines Particle Flow Object (PFO) and event-level information, including the\noutput of the most important high-level reconstruction algorithms. Originally\ntriggered by Snowmass 2021 studies, the mini-DST is useful for beginners as a\nstarting point of an analysis. In this note, we discuss the basics and contents\nof the mini-DST, how to create the mini-DST file from fully simulated MC\nsamples, and the limitations of mini-DST.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.08622v1"
    },
    {
        "title": "Extraction of physically meaningful endmembers from STEM spectrum-images\n  combining geometrical and statistical approaches",
        "authors": [
            "Pavel Potapov",
            "Axel Lubk"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  This article addresses extraction of physically meaningful information from\nSTEM EELS and EDX spectrum-images using methods of Multivariate Statistical\nAnalysis. The problem is interpreted in terms of data distribution in a\nmulti-dimensional factor space, which allows for a straightforward and\nintuitively clear comparison of various approaches. A new computationally\nefficient and robust method for finding physically meaningful endmembers in\nspectrum-image datasets is presented. The method combines the geometrical\napproach of Vertex Component Analysis with the statistical approach of Bayesian\ninference. The algorithm is described in detail at an example of EELS\nspectrum-imaging of a multi-compound CMOS transistor.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.10416v1"
    },
    {
        "title": "Reconstructing magnetic deflections from sets of proton images using\n  differential evolution",
        "authors": [
            "Joseph M. Levesque",
            "Lauren J. Beesley"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Proton imaging is a powerful technique for imaging electromagnetic fields\nwithin an experimental volume, in which spatial variations in proton fluence\nare a result of deflections to proton trajectories due to interaction with the\nfields. When deflections are large, proton trajectories can overlap, and this\nnonlinearity creates regions of greatly increased proton fluence on the image,\nknown as caustics. The formation of caustics has been a persistent barrier to\nreconstructing the underlying fields from proton images. We have developed a\nnew method for reconstructing the path-integrated magnetic fields which begins\nto address the problem posed by caustics. Our method uses multiple proton\nimages of the same object, each image at a different energy, to fill in the\ninformation gaps and provide some uniqueness when reconstructing caustic\nfeatures. We use a differential evolution algorithm to iteratively estimate the\nunderlying deflection function which accurately reproduces the observed proton\nfluence at multiple proton energies simultaneously. We test this reconstruction\nmethod using synthetic proton images generated for three different,\ncylindrically symmetric field geometries at various field amplitudes and levels\nof proton statistics, and present reconstruction results from a set of\nexperimental images. The method we propose requires no assumption of deflection\nlinearity and can reliably solve for fields underlying linear, nonlinear, and\ncaustic proton image features for the selected geometries, and is shown to be\nfairly robust to noise in the input proton intensity.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.02080v2"
    },
    {
        "title": "Stitching Monte Carlo samples",
        "authors": [
            "Karl Ehataht",
            "Christian Veelken"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Monte Carlo (MC) simulations are extensively used for various purposes in\nmodern high-energy physics (HEP) experiments. Precision measurements of\nestablished Standard Model processes or searches for new physics often require\nthe collection of vast amounts of data. It is often difficult to produce MC\nsamples containing an adequate number of events to allow for a meaningful\ncomparison with the data, as substantial computing resources are required to\nproduce and store such samples. One solution often employed when producing MC\nsamples for HEP experiments is to partition the phase space of particle\ninteractions into multiple regions and produce the MC samples separately for\neach region. This approach allows to adapt the size of the MC samples to the\nneeds of physics analyses that are performed in these regions. In this paper we\npresent a procedure for combining MC samples that overlap in phase space. The\nprocedure is based on applying suitably chosen weights to the simulated events.\nWe refer to the procedure as \"stitching\". The paper includes different examples\nfor applying the procedure to simulated proton-proton collisions at the CERN\nLarge Hadron Collider.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.04360v2"
    },
    {
        "title": "Threshold-free estimation of entropy from a Pearson matrix",
        "authors": [
            "H. Felippe",
            "A. Viol",
            "D. B. de Araujo",
            "M. G. E. da Luz",
            "F. Palhano-Fontes",
            "H. Onias",
            "E. P. Raposo",
            "G. M. Viswanathan"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  There is demand in diverse fields for a reliable method of estimating the\nentropy associated with correlations. The estimation of a unique entropy\ndirectly from the Pearson correlation matrix has remained an open problem for\nmore than half a century. All existing approaches lack generality insofar as\nthey require thresholding choices that arbitrarily remove possibly important\ninformation. Here we propose an objective procedure for directly estimating a\nunique entropy of a general Pearson matrix. We show that upon rescaling the\nPearson matrix satisfies all necessary conditions for an analog of the von\nNeumann entropy to be well defined. No thresholding is required. We demonstrate\nthe method by estimating the entropy from neuroimaging time series of the human\nbrain under the influence of a psychedelic.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.05379v2"
    },
    {
        "title": "RanBox: Anomaly Detection in the Copula Space",
        "authors": [
            "Tommaso Dorigo",
            "Martina Fumanelli",
            "Chiara Maccani",
            "Marija Mojsovska",
            "Giles C. Strong",
            "Bruno Scarpa"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The unsupervised search for overdense regions in high-dimensional feature\nspaces, where locally high population densities may be associated with\nanomalous contaminations to an otherwise more uniform population, is of\nrelevance to applications ranging from fundamental research to industrial use\ncases. Motivated by the specific needs of searches for new phenomena in\nparticle collisions, we propose a novel approach that targets signals of\ninterest populating compact regions of the feature space. The method consists\nin a systematic scan of subspaces of a standardized copula of the feature\nspace, where the minimum p-value of a hypothesis test of local uniformity is\nsought by gradient descent. We characterize the performance of the proposed\nalgorithm and show its effectiveness in several experimental situations.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.05747v1"
    },
    {
        "title": "Extensions to Multifidelity Monte Carlo Methods for Simulations of\n  Chaotic Systems",
        "authors": [
            "Todd A. Oliver",
            "Christopher S. Simmons",
            "Robert D. Moser"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Multifidelity Monte Carlo methods often rely on a preprocessing phase\nconsisting of standard Monte Carlo sampling to estimate correlation\ncoefficients between models of different fidelity to determine the weights and\nnumber of samples for each level. For computationally intensive models, as are\noften encountered in simulations of chaotic systems, this up-front cost can be\nprohibitive. In this work, a correlation estimation procedure is developed for\nthe case in which the highest and next highest fidelity models are generated\nvia discretizing the same mathematical model using different resolution. The\nprocedure uses discretization error estimates to estimate the required\ncorrelation coefficient without the need to sample the highest fidelity model,\nwhich can dramatically decrease the cost of the preprocessing phase. The method\nis extended to chaotic problems by using discretization error estimates that\naccount for the statistical nature of common quantities of interest and the\naccompanying finite sampling errors that pollute estimates of such quantities\nof interest. The methodology is then demonstrated on a model problem based on\nthe Kuramoto-Sivashinsky equation.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.13844v1"
    },
    {
        "title": "Use of Machine Learning Technique to maximize the signal over background\n  for $H \\rightarrow ττ$",
        "authors": [
            "Kanhaiya Gupta"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  In recent years, artificial neural networks (ANNs) have won numerous contests\nin pattern recognition and machine learning. ANNS have been applied to problems\nranging from speech recognition to prediction of protein secondary structure,\nclassification of cancers, and gene prediction. Here, we intend to maximize the\nchances of finding the Higgs boson decays to two $\\tau$ leptons in the pseudo\ndataset using a Machine Learning technique to classify the recorded events as\nsignal or background.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.14257v2"
    },
    {
        "title": "LHC physics dataset for unsupervised New Physics detection at 40 MHz",
        "authors": [
            "Ekaterina Govorkova",
            "Ema Puljak",
            "Thea Aarrestad",
            "Maurizio Pierini",
            "Kinga Anna Woźniak",
            "Jennifer Ngadiuba"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  In particle detectors at the Large Hadron Collider, tens of terabytes of data\nare produced every second from proton-proton collisions occurring at a rate of\n40 megahertz. This data rate is reduced to a sustainable level by a real-time\nevent filter processing system which decides whether each collision event\nshould be kept for further analysis or be discarded. We introduce a dataset of\nproton collision events which emulates a typical data stream collected by such\na real-time processing system, pre-filtered by requiring the presence of at\nleast one electron or muon. This dataset could be used to develop novel event\nselection strategies and assess their sensitivity to new phenomena. In\nparticular, by publishing this dataset we intend to stimulate a community-based\neffort towards the design of novel algorithms for performing unsupervised New\nPhysics detection, customized to fit the bandwidth, latency and computational\nresource constraints of the real-time event selection system of a typical\nparticle detector.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.02157v2"
    },
    {
        "title": "Space-Time Finite Element for Sensor Fusion",
        "authors": [
            "Markus Pagitz"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Drones estimate their position and orientation with the help of various\nsensors. Their data streams, that differ with respect to the sampling rate and\nstandard deviation, need to be fused to get an accurate position and\norientation estimate. It is subsequently shown that a nonlinear space-time\nfinite element and static condensation can be used to accomplish this task.\nThis is done, for the sake of clarity, in three stages. The first stage\nestimates the local magnetic north vector with the help of magnetometers and\ngyroscopes. The second stage projects the remaining sensor data onto the plane\nthat is orthogonal to the local magnetic north vector and the third stage\nsolves the corresponding two-dimensional problem.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.09021v1"
    },
    {
        "title": "Processing of non-constant baseline pulses: a matrix technique",
        "authors": [
            "C. Ferrari",
            "M. Borghesi",
            "M. Faverzani",
            "E. Ferri",
            "A. Giachero",
            "A. Nucciotti"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  For a high source activity experiment, such as HOLMES, non-constant baseline\npulses could constitute a great fraction of the data-set. We test the optimal\nfilter matrix technique, proposed to process these pulses, on simulated\nresponses of HOLMES microcalorimeters.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.12258v1"
    },
    {
        "title": "Quantifying high-order interdependencies on individual patterns via the\n  local O-information: theory and applications to music analysis",
        "authors": [
            "Tomas Scagliarini",
            "Daniele Marinazzo",
            "Yike Guo",
            "Sebastiano Stramaglia",
            "Fernando E. Rosas"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  High-order, beyond-pairwise interdependencies are at the core of biological,\neconomic, and social complex systems, and their adequate analysis is paramount\nto understand, engineer, and control such systems. This paper presents a\nframework to measure high-order interdependence that disentangles their effect\non each individual pattern exhibited by a multivariate system. The approach is\ncentred on the 'local O-information', a new measure that assesses the balance\nbetween synergistic and redundant interdependencies at each pattern. To\nillustrate the potential of this framework, we present a detailed analysis of\nmusic scores from J.S. Bach, which reveals how high-order interdependence is\ndeeply connected with highly non-trivial aspects of the musical discourse. Our\nresults place the local O-information as a promising tool of wide\napplicability, which opens new perspectives for analysing high-order\nrelationships in the patterns exhibited by complex systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.11625v1"
    },
    {
        "title": "Effect of finite Reynolds number on self-similar crossing statistics and\n  fractal measurements in turbulence",
        "authors": [
            "Michael Heisel"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Stochastic simulations are used to create synthetic one-dimensional telegraph\napproximation (TA) signals based on turbulent zero crossings, where the\ninterval between crossings is governed by a power law probability distribution\nwith exponent $\\alpha$. The power law exponent is determined for statistics of\nsimulated TA signals, namely the box-counting fractal dimension $D_1$, energy\nspectrum exponent $\\beta_{TA}$, and an intermittency exponent $\\mu_{TA}$. For\nthe binary TA signal with no variability in amplitude, the parameters are\nrelated linearly as $D_1 = 2 - \\beta_{TA} = 1 - \\mu_{TA}$. The relations are\nunchanged if the crossing interval distribution has a finite power law region\n(i.e. inertial subrange) representing a flow with finite Reynolds number.\nHowever, the finite distribution yields statistics that are not truly\nscale-invariant, and distorts the linear relation between the statistic\nexponents and $\\alpha$. The behavior is due to finite-size effects apparent\nfrom the survival function, or the complementary cumulative distribution, which\nfor finite Reynolds number is only approximately self-similar and has an\neffective exponent differing from $\\alpha$. An expression presented for the\neffective exponent recovers the expected relations between $\\alpha$ and the TA\nstatistics. The findings demonstrate how a finite Reynolds number can affect\nindicators of self-similarity, fractality, and intermittency observed from\nsingle-point measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.12059v2"
    },
    {
        "title": "A deep learned nanowire segmentation model using synthetic data\n  augmentation",
        "authors": [
            "Binbin Lin",
            "Nima Emami",
            "David A Santos",
            "Yuting Luo",
            "Sarbajit Banerjee",
            "Bai-Xiang Xu"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Automatized object identification and feature analysis of experimental image\ndata are indispensable for data-driven material science; deep-learning-based\nsegmentation algorithms have been shown to be a promising technique to achieve\nthis goal. However, acquiring high-resolution experimental images and assigning\nlabels in order to train such algorithms is challenging and costly in terms of\nboth time and labor. In the present work, we apply synthetic images, which\nresemble the experimental image data in terms of geometrical and visual\nfeatures, to train state-of-art deep learning-based Mask R-CNN algorithms to\nsegment vanadium pentoxide (V2O5) nanowires, a canonical cathode material,\nwithin optical intensity-based images from spectromicroscopy. The performance\nevaluation demonstrates that even though the deep learning model is trained on\npure synthetically generated structures, it can segment real optical\nintensity-based spectromicroscopy images of complex V2O5 nanowire structures in\noverlapped particle networks, thus providing reliable statistical information.\nThe model can further be used to segment nanowires in scanning electron\nmicroscopy (SEM) images, which are fundamentally different from the training\ndataset known to the model. The proposed methodology of using a purely\nsynthetic dataset to train the deep learning model can be extended to any\noptical intensity-based images of variable particle morphology, extent of\nagglomeration, material class, and beyond.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.04429v2"
    },
    {
        "title": "Ariadne: PyTorch Library for Particle Track Reconstruction Using Deep\n  Learning",
        "authors": [
            "Pavel Goncharov",
            "Egor Schavelev",
            "Anastasia Nikolskaya",
            "Gennady Ososkov"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Particle tracking is a fundamental part of the event analysis in high energy\nand nuclear physics. Events multiplicity increases each year along with the\ndrastic growth of the experimental data which modern HENP detectors produce, so\nthe classical tracking algorithms such as the well-known Kalman filter cannot\nsatisfy speed and scaling requirements. At the same time, breakthroughs in the\nstudy of deep learning open an opportunity for the application of\nhigh-performance deep neural networks for solving tracking problems in a dense\nenvironment of experiments with heavy ions. However, there are no\nwell-documented software libraries for deep learning track reconstruction yet.\nWe introduce Ariadne, the first open-source library for particle tracking based\non the PyTorch deep learning framework. The goal of our library is to provide a\nsimple interface that allows one to prepare train and test datasets and to\ntrain and evaluate one of the deep tracking models implemented in the library\non the data from your specific experiment. The user experience is greatly\nfacilitated because of the system of gin-configurations. The modular structure\nof the library and abstract classes let the user develop his data processing\npipeline and deep tracking model easily. The proposed library is open-source to\nfacilitate academic research in the field of particle tracking based on deep\nlearning.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.08982v1"
    },
    {
        "title": "Generalized Poincaré Orthogonality: A New Approach to POLSAR Data\n  Analysis",
        "authors": [
            "Shane R. Cloude",
            "Ashlin Richardson"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  In this paper we outline a new approach to the analysis of polarimetric\nsynthetic aperture (POLSAR) data. Here we exploit target orthogonality as a\nmulti-dimensional extension of wave orthogonality, familiar on the Poincar\\'e\nsphere. We first show how to formulate a general basis for a complex orthogonal\nscattering space using a generalization of the Poincar\\'e formulation, and then\nshow how to optimize the backscattered signal in this space for both monostatic\nand bistatic radar systems. We illustrate application of the new approach,\nfirst to ship detection, using data collected off the north-west of Scotland\nand then land-use applications in a mixed scene around Glasgow, Scotland, both\nusing L-band ALOS-2 POLSAR data.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.09093v1"
    },
    {
        "title": "Inferring the shape of data: A probabilistic framework for analyzing\n  experiments in the natural sciences",
        "authors": [
            "Korak Kumar Ray",
            "Anjali R. Verma",
            "Ruben L. Gonzalez Jr",
            "Colin D. Kinz-Thompson"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  A critical step in data analysis for many different types of experiments is\nthe identification of features with theoretically defined shapes in\nN-dimensional datasets; examples of this process include finding peaks in\nmulti-dimensional molecular spectra or emitters in fluorescence microscopy\nimages. Identifying such features involves determining if the overall shape of\nthe data is consistent with an expected shape, however, it is generally unclear\nhow to quantitatively make this determination. In practice, many analysis\nmethods employ subjective, heuristic approaches, which complicates the\nvalidation of any ensuing results - especially as the amount and dimensionality\nof the data increase. Here, we present a probabilistic solution to this problem\nby using Bayes' rule to calculate the probability that the data has any one of\nseveral potential shapes. This probabilistic approach may be used to\nobjectively compare how well different theories describe a dataset, identify\nchanges between datasets, and detect features within data using a corollary\nmethod called Bayesian Inference-based Template Search (BITS); several\nproof-of-principle examples are provided. Altogether, this mathematical\nframework serves as an automated 'engine' capable of computationally executing\nanalysis decisions currently made by visual inspection across the sciences.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.12462v3"
    },
    {
        "title": "Sparse Data Generation for Particle-Based Simulation of Hadronic Jets in\n  the LHC",
        "authors": [
            "Breno Orzari",
            "Thiago Tomei",
            "Maurizio Pierini",
            "Mary Touranakou",
            "Javier Duarte",
            "Raghav Kansal",
            "Jean-Roch Vlimant",
            "Dimitrios Gunopulos"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We develop a generative neural network for the generation of sparse data in\nparticle physics using a permutation-invariant and physics-informed loss\nfunction. The input dataset used in this study consists of the particle\nconstituents of hadronic jets due to its sparsity and the possibility of\nevaluating the network's ability to accurately describe the particles and jets\nproperties. A variational autoencoder composed of convolutional layers in the\nencoder and decoder is used as the generator. The loss function consists of a\nreconstruction error term and the Kullback-Leibler divergence between the\noutput of the encoder and the latent vector variables. The\npermutation-invariant loss on the particles' properties is combined with two\nmean-squared error terms that measure the difference between input and output\njets mass and transverse momentum, which improves the network's generation\ncapability as it imposes physics constraints, allowing the model to learn the\nkinematics of the jets.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.15197v1"
    },
    {
        "title": "Deep learning study of an electromagnetic calorimeter",
        "authors": [
            "Elihu Sela",
            "Shan Huang",
            "David Horn"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The accurate and precise extraction of information from a modern particle\nphysics detector, such as an electromagnetic calorimeter, may be complicated\nand challenging. In order to overcome the difficulties we propose processing\nthe detector output using the deep-learning methodology. Our algorithmic\napproach makes use of a known network architecture, which is being modified to\nfit the problems at hand. The results are of high quality (biases of order 2%)\nand, moreover, indicate that most of the information may be derived from only a\nfraction of the detector. We conclude that such an analysis helps us\nunderstanding the essential mechanism of the detector and should be performed\nas a part of its designing procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.01532v1"
    },
    {
        "title": "HL-LHC Computing Review Stage 2, Common Software Projects: Data Science\n  Tools for Analysis",
        "authors": [
            "Jim Pivarski",
            "Eduardo Rodrigues",
            "Kevin Pedro",
            "Oksana Shadura",
            "Benjamin Krikler",
            "Graeme A. Stewart"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  This paper was prepared by the HEP Software Foundation (HSF) PyHEP Working\nGroup as input to the second phase of the LHCC review of High-Luminosity LHC\n(HL-LHC) computing, which took place in November, 2021. It describes the\nadoption of Python and data science tools in HEP, discusses the likelihood of\nfuture scenarios, and recommendations for action by the HEP community.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02194v1"
    },
    {
        "title": "Adaptive Model Refinement Approach for Bayesian Uncertainty\n  Quantification in Turbulence Model",
        "authors": [
            "Fanzhi Zeng",
            "Wei Zhang",
            "Jinping Li",
            "Tianxin Zhang",
            "Chao Yan"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The Bayesian uncertainty quantification technique has become well established\nin turbulence modeling over the past few years. However, it is computationally\nexpensive to construct a globally accurate surrogate model for Bayesian\ninference in a high-dimensional design space, which limits uncertainty\nquantification for complex flow configurations. Borrowing ideas from stratified\nsampling and inherited sampling, an adaptive model refinement approach is\nproposed in this work, which concentrates on asymptotically improving the local\naccuracy of the surrogate model in the high-posterior-density region by\nadaptively appending model evaluation points. To achieve this goal, a\nmodification of inherited Latin hypercube sampling is proposed and then\nintegrated into the Bayesian framework. The effectiveness and efficiency of the\nproposed approach are demonstrated through a two-dimensional heat source\ninversion problem and its extension to a high-dimensional design space.\nCompared with the prior-based method, the adaptive model refinement approach\nhas the ability to obtain more reliable inference results using fewer\nevaluation points. Finally, the approach is applied to parametric uncertainty\nquantification of the Menter shear-stress transport turbulence model for an\naxisymmetric transonic bump flow and provides convincing numerical results.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.08488v1"
    },
    {
        "title": "Quantum Multi-Parameter Adaptive Bayesian Estimation and Application to\n  Super-Resolution Imaging",
        "authors": [
            "Kwan Kit Lee",
            "Christos Gagatsos",
            "Saikat Guha",
            "Amit Ashok"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  In Bayesian estimation theory, the estimator ${\\hat \\theta} = E[\\theta|l]$\nattains the minimum mean squared error (MMSE) for estimating a scalar parameter\nof interest $\\theta$ from the observation of $l$ through a noisy channel\n$P_{l|\\theta}$, given a prior $P_\\theta$ on $\\theta$. In quantum sensing tasks,\nthe user gets $\\rho_\\theta$, the quantum state that encodes $\\theta$. They\nchoose a measurement, a positive-operator valued measure (POVM) $\\Pi_l$, which\ninduces the channel $P_{l|\\theta} = {\\rm Tr}(\\rho_\\theta \\Pi_l)$ to the\nmeasurement outcome $l$, on which the aforesaid classical MMSE estimator is\nemployed. Personick found the optimum POVM $\\Pi_l$ that minimizes the MMSE over\nall possible measurements, and that MMSE. This result from 1971 is less-widely\nknown than the quantum Fisher information (QFI), which lower bounds the\nvariance of an unbiased estimator over all measurements, when $P_\\theta$ is\nunavailable. For multi-parameter estimation, i.e., when $\\theta$ is a vector,\nin Fisher quantum estimation theory, the inverse of the QFI matrix provides an\noperator lower bound to the covariance of an unbiased estimator. However, there\nhas been little work on quantifying quantum limits and measurement designs, for\nmulti-parameter quantum estimation in the {\\em Bayesian} setting. In this\npaper, we build upon Personick's result to construct a Bayesian adaptive\nmeasurement scheme for multi-parameter estimation when $N$ copies of\n$\\rho_\\theta$ are available. We illustrate an application to localizing a\ncluster of point emitters in a highly sub-Rayleigh angular field-of-view, an\nimportant problem in fluorescence microscopy and astronomy. Our algorithm\ntranslates to a multi-spatial-mode transformation prior to a photon-detection\narray, with electro-optic feedback to adapt the mode sorter. We show that this\nreceiver performs far superior to quantum-noise-limited focal-plane direct\nimaging.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.09980v2"
    },
    {
        "title": "Neural network analysis of neutron and X-ray reflectivity data:\n  automated analysis using mlreflect, experimental errors and feature\n  engineering",
        "authors": [
            "Alessandro Greco",
            "Vladimir Starostin",
            "Evelyn Edel",
            "Valentin Munteanu",
            "Nadine Russegger",
            "Ingrid Dax",
            "Chen Shen",
            "Florian Bertram",
            "Alexander Hinderhofer",
            "Alexander Gerlach",
            "Frank Schreiber"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  This work demonstrates the Python package mlreflect which implements an\noptimized pipeline for the automized analysis of reflectometry data using\nmachine learning. The package combines several training and data treatment\ntechniques discussed in previous publications. The predictions made by the\nneural network are accurate and robust enough to serve as good starting\nparameters for an optional subsequent least mean squares (LMS) fit of the data.\nIt is shown that for a large dataset of 242 reflectivity curves of various thin\nfilms on silicon substrates, the pipeline reliably finds an LMS minimum very\nclose to a fit produced by a human researcher with the application of physical\nknowledge and carefully chosen boundary conditions. Furthermore, the\ndifferences between simulated and experimental data and their implications for\nthe training and performance of neural networks are discussed. The experimental\ntest set is used to determine the optimal noise level during training.\nFurthermore, the extremely fast prediction times of the neural network are\nleveraged to compensate for systematic errors by sampling slight variations of\nthe data.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.11609v1"
    },
    {
        "title": "Revisiting N$_2$-N$_2$ collisional linewidth models for S-branch\n  rotational Raman scattering",
        "authors": [
            "Mark Linne",
            "Nils Torge Mecker",
            "Christopher J. Kliewer",
            "David Escofet-Martin",
            "Brian Peterson"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  This paper presents an evaluation of two commonly accepted Raman linewidth\nmodels typically used to fit CARS model parameters to data; the Modified\nExponential Gap (MEG) and the Energy Corrected Sudden (ECS) models. The\nadjustable parameters in each model have been fit to published experimental\nlinewidths, and comparisons are made to the various publications that have\nalready provided similar adjustable constants. The approaches presented in the\nliterature are discussed as are differences with the findings presented here.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.00509v1"
    },
    {
        "title": "Hyperparameter optimization of data-driven AI models on HPC systems",
        "authors": [
            "Eric Wulff",
            "Maria Girone",
            "Joosep Pata"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  In the European Center of Excellence in Exascale computing \"Research on AI-\nand Simulation-Based Engineering at Exascale\" (CoE RAISE), researchers develop\nnovel, scalable AI technologies towards Exascale. This work exercises High\nPerformance Computing resources to perform large-scale hyperparameter\noptimization using distributed training on multiple compute nodes. This is part\nof RAISE's work on data-driven use cases which leverages AI- and HPC\ncross-methods developed within the project. In response to the demand for\nparallelizable and resource efficient hyperparameter optimization methods,\nadvanced hyperparameter search algorithms are benchmarked and compared. The\nevaluated algorithms, including Random Search, Hyperband and ASHA, are tested\nand compared in terms of both accuracy and accuracy per compute resources\nspent. As an example use case, a graph neural network model known as MLPF,\ndeveloped for the task of Machine-Learned Particle-Flow reconstruction in High\nEnergy Physics, acts as the base model for optimization. Results show that\nhyperparameter optimization significantly increased the performance of MLPF and\nthat this would not have been possible without access to large-scale High\nPerformance Computing resources. It is also shown that, in the case of MLPF,\nthe ASHA algorithm in combination with Bayesian optimization gives the largest\nperformance increase per compute resources spent out of the investigated\nalgorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.01112v1"
    },
    {
        "title": "Event Weighting vs. Event Counting",
        "authors": [
            "Joerg Pretz"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Goal of these proceedings is to introduce a method based on event weighting\nin particle physics experiments. Weighting means that events are not just\ncounted as integer numbers but are assigned a weight factor according to their\nimportance in the analysis. This method has a close connection to the maximum\nlikelihood method known to reach the smallest statistical error. The purpose of\nthis document is to give a more educational overview on the subject. As an\nexample the extraction of a beam polarization from scattered particles is\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.05319v1"
    },
    {
        "title": "A Note on A Priori Forecasting and Simplicity Bias in Time Series",
        "authors": [
            "Kamaludin Dingle",
            "Rafiq Kamal",
            "Boumediene Hamzi"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  To what extent can we forecast a time series without fitting to historical\ndata? Can universal patterns of probability help in this task? Deep relations\nbetween pattern Kolmogorov complexity and pattern probability have recently\nbeen used to make \\emph{a priori} probability predictions in a variety of\nsystems in physics, biology and engineering. Here we study \\emph{simplicity\nbias} (SB) -- an exponential upper bound decay in pattern probability with\nincreasing complexity -- in discretised time series extracted from the World\nBank Open Data collection. We predict upper bounds on the probability of\ndiscretised series patterns, without fitting to trends in the data. Thus we\nperform a kind of `forecasting without training data', predicting time series\nshape patterns \\emph{a priori}, but not the actual numerical value of the\nseries. Additionally we make predictions about which of two discretised series\nis more likely with accuracy of $\\sim$80\\%, much higher than a 50\\% baseline\nrate, just by using the complexity of each series. These results point to a\npromising perspective on practical time series forecasting and integration with\nmachine learning methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.05391v2"
    },
    {
        "title": "Analysis and application of multiplicative stochastic process with a\n  sample-dependent lower bound",
        "authors": [
            "Ken Yamamoto",
            "Yoshihiro Yamazaki"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  A multiplicative stochastic process with the lower bound lognormally\ndistributed is investigated. For the process, the model is constructed, and its\ndistribution function (involving four parameters) and the related statistical\nproperties are derived. By adjusting the parameters, it is confirmed that the\ntheoretical distribution is consistent with empirical distributions of some\nreal data.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.06812v1"
    },
    {
        "title": "Analysis Cyberinfrastructure: Challenges and Opportunities",
        "authors": [
            "Kevin Lannon",
            "Paul Brenner",
            "Mike Hildreth",
            "Kenyi Hurtado Anampa",
            "Alan Malta Rodrigues",
            "Kelci Mohrman",
            "Doug Thain",
            "Benjamin Tovar"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Analysis cyberinfrastructure refers to the combination of software and\ncomputer hardware used to support late-stage data analysis in High Energy\nPhysics (HEP). For the purposes of this white paper, late-stage data analysis\nrefers specifically to the step of transforming the most reduced common data\nformat produced by a given experimental collaboration (for example, nanoAOD for\nthe CMS experiment) into histograms. In this white paper, we reflect on\nobservations gathered from a recent experience with data analysis using a\nrecent, python-based analysis framework, and extrapolate these experiences\nthough the High-Luminosity LHC era as way of highlighting potential R\\&D topics\nin analysis cyberinfrastructure.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08811v1"
    },
    {
        "title": "Celeritas: GPU-accelerated particle transport for detector simulation in\n  High Energy Physics experiments",
        "authors": [
            "S. C. Tognini",
            "P. Canal",
            "T. M. Evans",
            "G. Lima",
            "A. L. Lund",
            "S. R. Johnson",
            "S. Y. Jun",
            "V. R. Pascuzzi",
            "P. K. Romano"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Within the next decade, experimental High Energy Physics (HEP) will enter a\nnew era of scientific discovery through a set of targeted programs recommended\nby the Particle Physics Project Prioritization Panel (P5), including the\nupcoming High Luminosity Large Hadron Collider (LHC) HL-LHC upgrade and the\nDeep Underground Neutrino Experiment (DUNE). These efforts in the Energy and\nIntensity Frontiers will require an unprecedented amount of computational\ncapacity on many fronts including Monte Carlo (MC) detector simulation. In\norder to alleviate this impending computational bottleneck, the Celeritas MC\nparticle transport code is designed to leverage the new generation of\nheterogeneous computer architectures, including the exascale computing power of\nU.S. Department of Energy (DOE) Leadership Computing Facilities (LCFs), to\nmodel targeted HEP detector problems at the full fidelity of Geant4. This paper\npresents the planned roadmap for Celeritas, including its proposed code\narchitecture, physics capabilities, and strategies for integrating it with\nexisting and future experimental HEP computing workflows.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.09467v2"
    },
    {
        "title": "Regularised unfolding with a discrete-valued penalty function",
        "authors": [
            "Michael Schmelling"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Regularisation allows one to handle ill-posed inverse problems. Here we focus\non discrete unfolding problems. The properties of the results are characterised\nby the consistency between measurements and unfolding result and by the\nposterior response matrix. We introduce a novel regularisation scheme based on\na discrete-valued penalty function and compare its performance to that of a\nsimple cutoff-regularisation. The discrete-valued penalty function does not\nrequire a regularisation parameter that needs to be adjusted on a case-by-case\nbasis. In toy studies very satisfactory results are obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.09579v2"
    },
    {
        "title": "Fractal and multifractal descriptors restore ergodicity broken by\n  non-Gaussianity in time series",
        "authors": [
            "Damian G. Kelty-Stephen",
            "Madhur Mangalam"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Ergodicity breaking is a challenge for biological and psychological sciences.\nErgodicity is a necessary condition for linear causal modeling. Long-range\ncorrelations and non-Gaussianity characterizing various biological and\npsychological measurements break ergodicity routinely, threatening our capacity\nfor causal modeling. Long-range correlations (e.g., in fractional Gaussian\nnoise, a.k.a. \"pink noise\") break ergodicity--in raw Gaussian series, as well\nas in some but not all standard descriptors of variability, i.e., in\ncoefficient of variation (CV) and root mean square (RMS) but not standard\ndeviation (SD) for longer series. The present work demonstrates that\nprogressive increases in non-Gaussianity conspire with long-range correlations\nto break ergodicity in SD for all series lengths. Meanwhile, explicitly\nencoding the cascade dynamics that can generate temporally correlated\nnon-Gaussian noise offers a way to restore ergodicity to our causal models.\nSpecifically, fractal and multifractal properties encode both scale-invariant\npower-law correlations and their variety, respectively, both of which features\nindex the underlying cascade parameters. Fractal and multifractal descriptors\nof long-range correlated non-Gaussian processes show no ergodicity breaking and\nhence, provide a more stable explanation for the long-range correlated\nnon-Gaussian form of biological and psychological processes. Fractal and\nmultifractal descriptors offer a path to restoring ergodicity to causal\nmodeling in these fields.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.00572v1"
    },
    {
        "title": "Clustering free-falling paper motion with complexity and entropy",
        "authors": [
            "Arthur A. B. Pessa",
            "Matjaz Perc",
            "Haroldo V. Ribeiro"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Many simple natural phenomena are characterized by complex motion that\nappears random at first glance, but that often displays underlying patterns and\nbehavior that can be clustered in groups. The movement of small pieces of paper\nfalling through the air is one of these systems whose complete mathematical\ndescription seems unworkable. Understanding these types of motion thus demands\nautomated experimentation capable of producing large datasets covering\ndifferent behaviors -- a task that has become feasible only recently with\nadvances in computer vision and machine learning methods. Here we use one of\nthese datasets related to the motion of free-falling paper with different\nshapes to propose an information-theoretical approach that automatically\nclusters different types of behavior. We evaluate the permutation entropy and\nstatistical complexity from time series related to the observable area of\nfree-falling paper pieces captured by a video camera. We find that chaotic and\ntumbling motions have a distinct average degree of entropy and complexity,\nallowing us to accurately discriminate between these two types of behavior with\na simple unsupervised machine learning algorithm. Our method has a performance\ncomparable to other approaches based on physical quantities but does not depend\non reconstructing the three-dimensional falling trajectory.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.14097v1"
    },
    {
        "title": "Bayesian estimation of correlation functions",
        "authors": [
            "Angel Gutierrez-Rubio",
            "Juan S. Rojas-Arias",
            "Jun Yoneda",
            "Seigo Tarucha",
            "Daniel Loss",
            "Peter Stano"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We apply Bayesian statistics to the estimation of correlation functions. We\ngive the probability distributions of auto- and cross-correlations as functions\nof the data. Our procedure uses the measured data optimally and informs about\nthe certainty level of the estimation. Our results apply to general stationary\nprocesses and their essence is a non-parametric estimation of spectra. It\nallows one to better understand the statistical noise fluctuations, assess the\ncorrelations between two variables, and postulate parametric models of spectra\nthat can be further tested. We also propose a method to numerically generate\ncorrelated noise with a given spectrum.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.03611v2"
    },
    {
        "title": "Augmented Transition Path Theory for Sequences of Events",
        "authors": [
            "Chatipat Lorpaiboon",
            "Jonathan Weare",
            "Aaron R. Dinner"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Transition path theory provides a statistical description of the dynamics of\na reaction in terms of local spatial quantities. In its original formulation,\nit is limited to reactions that consist of trajectories flowing from a reactant\nset A to a product set B. We extend the basic concepts and principles of\ntransition path theory to reactions in which trajectories exhibit a specified\nsequence of events and illustrate the utility of this generalization on\nexamples.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.05067v2"
    },
    {
        "title": "Improving Bayesian radiological profiling of waste drums using Dirichlet\n  priors, Gaussian process priors, and hierarchical modeling",
        "authors": [
            "Eric Laloy",
            "Bart Rogiers",
            "An Bielen",
            "Alessandro Borella",
            "Sven Boden"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We present three methodological improvements of the \"SCK CEN approach\" for\nBayesian inference of the radionuclide inventory in radioactive waste drums,\nfrom radiological measurements. First we resort to the Dirichlet distribution\nfor the prior distribution of the isotopic vector. The Dirichlet distribution\npossesses the attractive property that the elements of its vector samples sum\nup to 1. Second, we demonstrate that such Dirichlet priors can be incorporated\nwithin an hierarchical modeling of the prior uncertainty in the isotopic\nvector, when prior information about isotopic composition is available. Our\nused Bayesian hierarchical modeling framework makes use of this available\ninformation but also acknowledges its uncertainty by letting to a controlled\nextent the information content of the indirect measurement data (i.e., gamma\nand neutron counts) shape the actual prior distribution of the isotopic vector.\nThird, we propose to regularize the Bayesian inversion by using Gaussian\nprocess (GP) prior modeling when inferring 1D spatially-distributed quantities.\nAs of uncertainty in the efficiencies, we keep using the same stylized drum\nmodeling approach as proposed in our previous work to account for the source\ndistribution uncertainty across the vertical direction of the drum. A series of\nsynthetic tests followed by application to a real waste drum show that\ncombining hierarchical modeling of the prior isotopic composition uncertainty\ntogether with GP prior modeling of the vertical Pu profile across the drum\nworks well. We also find that our GP prior can handles both cases with and\nwithout spatial correlation. The computational times involved by our proposed\napproach are on the order of a few hours, say about 2, to provide uncertainty\nestimates for all variables of interest in the considered inverse problem. This\nwarrants further investigations to speed up the inference.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.07786v4"
    },
    {
        "title": "A machine learning photon detection algorithm for coherent X-ray\n  ultrafast fluctuation analysis",
        "authors": [
            "Sathya R. Chitturi",
            "Nicolas G. Burdet",
            "Youssef Nashed",
            "Daniel Ratner",
            "Aashwin Mishra",
            "TJ Lane",
            "Matthew Seaberg",
            "Vincent Esposito",
            "Chun H. Yoon",
            "Mike Dunne",
            "Joshua J. Turner"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  X-ray free electron laser (XFEL) experiments have brought unique capabilities\nand opened new directions in research, such as creating new states of matter or\ndirectly measuring atomic motion. One such area is the ability to use finely\nspaced sets of coherent x-ray pulses to be compared after scattering from a\ndynamic system at different times. This enables the study of fluctuations in\nmany-body quantum systems at the level of the ultrafast pulse durations, but\nthis method has been limited to a select number of examples and required\ncomplex and advanced analytical tools. By applying a new methodology to this\nproblem, we have made qualitative advances in three separate areas that will\nlikely also find application to new fields. As compared to the `droplet-type'\nmodels which typically are used to estimate the photon distributions on\npixelated detectors to obtain the coherent X-ray speckle patterns, our\nalgorithm pipeline achieves an order of magnitude speedup on CPU hardware and\ntwo orders of magnitude improvement on GPU hardware. We also find that it\nretains accuracy in low-contrast conditions, which is the typical regime for\nmany experiments in structural dynamics. Finally, it can predict photon\ndistributions in high average-intensity applications, a regime which up until\nnow, has not been accessible. Our AI-assisted algorithm will enable a wider\nadoption of x-ray coherence spectroscopies, by both automating previously\nchallenging analyses and enabling new experiments that were not otherwise\nfeasible without the developments described in this work.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.09064v1"
    },
    {
        "title": "Boosted decision trees",
        "authors": [
            "Yann Coadou"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Boosted decision trees are a very powerful machine learning technique. After\nintroducing specific concepts of machine learning in the high-energy physics\ncontext and describing ways to quantify the performance and training quality of\nclassifiers, decision trees are described. Some of their shortcomings are then\nmitigated with ensemble learning, using boosting algorithms, in particular\nAdaBoost and gradient boosting. Examples from high-energy physics and software\nused are also presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.09645v1"
    },
    {
        "title": "Markov property of the Super-MAG Auroral Electrojet Indices",
        "authors": [
            "Simone Benella",
            "Giuseppe Consolini",
            "Mirko Stumpo",
            "Tommaso Alberti",
            "Jesper W. Gjerloev"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The dynamics of the Earth's magnetosphere exhibits strongly fluctuating\npatterns as well as non-stationary and non-linear interactions, more pronounced\nduring magnetospheric substorms and magnetic storms. This complex dynamics\ncomprises both stochastic and deterministic features occurring at different\ntime scales. Here we investigate the stochastic nature of the magnetospheric\nsubstorm dynamics by analysing the Markovian character of SuperMAG SME and SML\ngeomagnetic indices. By performing the Chapman-Kolmogorov test, the SME/SML\ndynamics appears to satisfy the Markov condition at scales below 60 minutes.\nThe Kramers-Moyal analysis instead highlights that a purely diffusive process\nis not representative of the magnetospheric dynamics, thus a model that\nincludes both diffusion and Poisson-jump processes is used to reproduce the SME\ndynamical features at small scales. A discussion of the similarities and\ndifferences between this model and the SME properties is provided with a\nspecial emphasis on the metastability of the Earth's magnetospheric dynamics.\nFinally, the relevance of our results in the framework of Space Weather is also\naddressed.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.10378v3"
    },
    {
        "title": "Entropy estimation in bidimensional sequences",
        "authors": [
            "F. N. M. de Sousa Filho",
            "V. G. Pereira de Sá",
            "E. Brigatti"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We investigate the performance of entropy estimation methods, based either on\nblock entropies or compression approaches, in the case of bidimensional\nsequences. We introduce a validation dataset made of images produced by a large\nnumber of different natural systems, in the vast majority characterized by\nlong-range correlations, which produce a large spectrum of entropies. Results\nshow that the framework based on lossless compressors applied to the\none-dimensional projection of the considered dataset leads to poor estimates.\nThis is because higher-dimensional correlations are lost in the projection\noperation. The adoption of compression methods which do not introduce\ndimensionality reduction improves the performance of this approach. By far, the\nbest estimation of the asymptotic entropy is generated by the faster\nconvergence of the traditional block-entropies method. As a by-product of our\nanalysis, we show how a specific compressor method can be used as a potentially\ninteresting technique for automatic detection of symmetries in textures and\nimages.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.02672v1"
    },
    {
        "title": "GAN with an Auxiliary Regressor for the Fast Simulation of the\n  Electromagnetic Calorimeter Response",
        "authors": [
            "Alexander Rogachev",
            "Fedor Ratnikov"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  High energy physics experiments essentially rely on simulated data for\nphysics analyses. However, running detailed simulation models requires a\ntremendous amount of computation resources. New approaches to speed up detector\nsimulation are therefore needed. The generation of calorimeter responses is\noften the most expensive component of the simulation chain for HEP experiments.\nIt was shown that deep learning techniques, especially Generative Adversarial\nNetworks, may be used to reproduce the calorimeter response. However, those\napplications are challenging, as the generated responses need evaluation not\nonly in terms of image consistency: different physics-based quality metrics\nshould be also taken into consideration. In our work, we develop a multitask\nGAN-based framework with the goal to speed up the response generation of the\nElectromagnetic Calorimeter (ECAL) of the LHCb detector at LHC. We introduce\nthe Auxiliary Regressor as a second task to evaluate a proxy metric of the\ngiven input that is used by the Discriminator of the GAN. We show that this\napproach improves the stability of GAN and the model produces samples with\nbetter physics distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.06329v1"
    },
    {
        "title": "Statistical significance testing for mixed priors: a combined Bayesian\n  and frequentist analysis",
        "authors": [
            "Jakob Robnik",
            "Uroš Seljak"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  In many hypothesis testing applications, we have mixed priors, with\nwell-motivated informative priors for some parameters but not for others. The\nBayesian methodology uses the Bayes factor and is helpful for the informative\npriors, as it incorporates Occam's razor via multiplicity or trials factor in\nthe Look Elsewhere Effect. However, if the prior is not known completely, the\nfrequentist hypothesis test via the false positive rate is a better approach,\nas it is less sensitive to the prior choice. We argue that when only partial\nprior information is available, it is best to combine the two methodologies by\nusing the Bayes factor as a test statistic in the frequentist analysis. We show\nthat the standard frequentist likelihood-ratio test statistic corresponds to\nthe Bayes factor with a non-informative Jeffrey's prior. We also show that\nmixed priors increase the statistical power in frequentist analyses over the\nlikelihood ratio test statistic. We develop an analytic formalism that does not\nrequire expensive simulations using a statistical mechanics approach to\nhypothesis testing in Bayesian and frequentist statistics. We introduce the\ncounting of states in a continuous parameter space using the uncertainty volume\nas the quantum of the state. We show that both the p-value and Bayes factor can\nbe expressed as energy versus entropy competition. We present analytic\nexpressions that generalize Wilks' theorem beyond its usual regime of validity\nand work in a non-asymptotic regime. In specific limits, the formalism\nreproduces existing expressions, such as the p-value of linear models and\nperiodograms. We apply the formalism to an example of exoplanet transits, where\nmultiplicity can be more than $10^7$. We show that our analytic expressions\nreproduce the p-values derived from numerical simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.06784v1"
    },
    {
        "title": "Fast Columnar Physics Analyses of Terabyte-Scale LHC Data on a\n  Cache-Aware Dask Cluster",
        "authors": [
            "Niclas Eich",
            "Martin Erdmann",
            "Peter Fackeldey",
            "Benjamin Fischer",
            "Dennis Noll",
            "Yannik Rath"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The development of an LHC physics analysis involves numerous investigations\nthat require the repeated processing of terabytes of data. Thus, a rapid\ncompletion of each of these analysis cycles is central to mastering the science\nproject. We present a solution to efficiently handle and accelerate physics\nanalyses on small-size institute clusters. Our solution is based on three key\nconcepts: Vectorized processing of collision events, the \"MapReduce\" paradigm\nfor scaling out on computing clusters, and efficiently utilized SSD caching to\nreduce latencies in IO operations. Using simulations from a Higgs pair\nproduction physics analysis as an example, we achieve an improvement factor of\n$6.3$ in runtime after one cycle and even an overall speedup of a factor of\n$14.9$ after $10$ cycles.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.08598v1"
    },
    {
        "title": "On the efficacy of the wisdom of crowds to forecast economic indicators",
        "authors": [
            "Nilton S. Siqueira Neto",
            "José F. Fontanari"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The interest in the wisdom of crowds stems mainly from the possibility of\ncombining independent forecasts from experts in the hope that many expert minds\nare better than a few. Hence the relevant subject of study nowadays is the Vox\nExpertorum rather than Galton's original Vox Populi. Here we use the Federal\nReserve Bank of Philadelphia's Survey of Professional Forecasters to analyze\n$15455$ forecasting contests to predict a variety of economic indicators. We\nfind that the median has advantages over the mean as a method to combine the\nexperts' estimates: the odds that the crowd beats all participants of a\nforecasting contest is $0.015$ when the aggregation is given by the mean and\n$0.026$ when it is given by the median. In addition, the median is always\nguaranteed to beat the majority of the participants, whereas the mean beats\nthat majority in 67 percent of the forecasts only. Both aggregation methods\nyield a $20$ percent error on the average, which must be contrasted with the\n$15$ percent error of the contests' winners. A standard time series forecasting\nalgorithm, the ARIMA model, yields a $31$ percent error on the average.\nHowever, since the expected error of a randomly selected forecaster is about\n$22$ percent, our conclusion is that selective attention is the most likely\nexplanation for the mysterious high accuracy of the crowd reported in the\nliterature.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.08924v2"
    },
    {
        "title": "Measuring the attenuation length of muon number in the air shower with\n  muon detectors of 3/4 LHAASO array",
        "authors": [
            "Xiaoting Feng",
            "Hengying Zhang",
            "Cunfeng Feng",
            "Lingling Ma"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  LHAASO KM2A consists of 5915 scintillation detectors and 1188 muon detectors,\nand the muon detectors cover 4% area of the whole array with 30 m spacing. The\nmuon number of air shower events, with very high energy, is investigated with\nthe data recorded by muon detector of the 3/4 LHAASO array in 2021. The\nattenuation length of muon number in the air shower is measured by fitting the\nmuon number with constant flux in various zenith angles, based on the constant\nintensity cut method. The variation of the attenuation length as shower energy\nfrom hundreds TeV to tens PeV is presented. The results of simulation also is\npresented for comparing.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.12117v4"
    },
    {
        "title": "A Simple Engineering Technique to Estimate the First Derivative of an\n  Empirical Function",
        "authors": [
            "Emmanuil Beygelzimer",
            "Yan Beygelzimer"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  A heuristic formula for 5-point approximation of the first derivative of an\nunknown function whose values are measured with an error at unequally spaced\npoints is proposed. The derivative at a given point is calculated using the\neffective increments of the function and the argument, taking into account the\ndifferent weight coefficients for near and far measurement points. Simulation\nmodeling on test functions with known derivatives is applied to determine\nrational values of weight coefficients. Simulation results are described in\ndetail on two test functions, one of which simulates the process of water\ncooling of a hot steel sheet, the second is a complex oscillatory process with\nvariable frequency and amplitude. It was found that the optimal values of\nweight coefficients remain approximately the same for essentially different\nfunctions, which allows us to recommend the same formula for all cases. In\ncontrast to the classical methods of numerical differentiation of functions\ntabulated at unequally spaced nodes, the proposed formula simultaneously takes\ninto account the smoothing of empirical data. It is shown that this\nsignificantly increases the accuracy of the numerical derivative estimate even\nin cases where the random error of the function measurement is a very small\nvalue, from 1%. The formula obtained is recommended for use in solving any\nproblems requiring the estimation of the derivative of an empirical function,\nincluding the calculation of the stress-strain state of metal, the description\nof thermal processes, and the determination of the properties of materials.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.05822v1"
    },
    {
        "title": "Carbon Monitor-Power: near-real-time monitoring of global power\n  generation on hourly to daily scales",
        "authors": [
            "Biqing Zhu",
            "Xuanren Song",
            "Zhu Deng",
            "Wenli Zhao",
            "Da Huo",
            "Taochun Sun",
            "Piyu Ke",
            "Duo Cui",
            "Chenxi Lu",
            "Haiwang Zhong",
            "Chaopeng Hong",
            "Jian Qiu",
            "Steven J. Davis",
            "Pierre Gentine",
            "Philippe Ciais",
            "Zhu Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We constructed a frequently updated, near-real-time global power generation\ndataset: Carbon Monitor-Power since January, 2016 at national levels with\nnear-global coverage and hourly-to-daily time resolution. The data presented\nhere are collected from 37 countries across all continents for eight source\ngroups, including three types of fossil sources (coal, gas, and oil), nuclear\nenergy and four groups of renewable energy sources (solar energy, wind energy,\nhydro energy and other renewables including biomass, geothermal, etc.). The\nglobal near-real-time power dataset shows the dynamics of the global power\nsystem, including its hourly, daily, weekly and seasonal patterns as influenced\nby daily periodical activities, weekends, seasonal cycles, regular and\nirregular events (i.e., holidays) and extreme events (i.e., the COVID-19\npandemic). The Carbon Monitor-Power dataset reveals that the COVID-19 pandemic\ncaused strong disruptions in some countries (i.e., China and India), leading to\na temporary or long-lasting shift to low carbon intensity, while it had only\nlittle impact in some other countries (i.e., Australia). This dataset offers a\nlarge range of opportunities for power-related scientific research and\npolicy-making.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.06086v1"
    },
    {
        "title": "Library transfer between distinct Laser-Induced Breakdown Spectroscopy\n  systems with shared standards",
        "authors": [
            "J. Vrábel",
            "E. Képeš",
            "P. Nedělník",
            "J. Buday",
            "J. Cempírek",
            "P. Pořízka",
            "J. Kaiser"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The mutual incompatibility of distinct spectroscopic systems is among the\nmost limiting factors in Laser-Induced Breakdown Spectroscopy (LIBS). The cost\nrelated to setting up a new LIBS system is increased, as its extensive\ncalibration is required. Solving the problem would enable inter-laboratory\nreference measurements and shared spectral libraries, which are fundamental for\nother spectroscopic techniques. In this work, we study a simplified version of\nthis challenge where LIBS systems differ only in used spectrometers and\ncollection optics but share all other parts of the apparatus, and collect\nspectra simultaneously from the same plasma plume. Extensive datasets measured\nas hyperspectral images of heterogeneous specimens are used to train machine\nlearning models that can transfer spectra between systems. The transfer is\nrealized by a pipeline that consists of a variational autoencoder (VAE) and a\nfully-connected artificial neural network (ANN). In the first step, we obtain a\nlatent representation of the spectra which were measured on the Primary system\n(by using the VAE). In the second step, we map spectra from the Secondary\nsystem to corresponding locations in the latent space (by the ANN). Finally,\nSecondary system spectra are reconstructed from the latent space to the space\nof the Primary system. The transfer is evaluated by several figures of merit\n(Euclidean and cosine distances, both spatially resolved; k-means clustering of\ntransferred spectra). The methodology is compared to several baseline\napproaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.07637v2"
    },
    {
        "title": "Beamline Simulation for the NNBAR Experiment at the European Spallation\n  Source",
        "authors": [
            "Matthias Holl",
            "Rodion Kolevatov",
            "Bernhard Meirose",
            "David Milstead",
            "Blahoslav Rataj",
            "Valentina Santoro",
            "Luca Zanini"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The HIBEAM and NNBAR experiments are a proposed fundamental science\nexperiments at the European Spallation Source, performing high precision\nsearches for neutron conversions in several baryon number violating (BNV)\nchannels. For simulations of the NNBAR beamline, a new sampling method has been\ndeveloped. The method is based on probability density evaluation and duct\nsource biasing and enables the simulations of the entire NNBAR beamline with\nhigh statistics while also preserving correlations of the neutron tracks.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.08997v1"
    },
    {
        "title": "Thermal Activation of Dry Sliding Friction at The Nano-scale",
        "authors": [
            "Rasoul Kheiri",
            "Alexey A Tsukanov"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Molecular dynamic (MD) simulations are applied to investigate the dependency\nof the kinetic friction coefficient on the temperature at the nano-scale. The\nsystem is comprised of an aluminum spherical particle consisting of 32000 atoms\nin an FCC lattice sliding on a stack of several layers of graphene, and the\nsimulations have done using LAMMPS. The interaction potential is\ncharge-optimized many-body (COMB3) potential and a Langevin thermostat keep the\nsystem at a nearly constant temperature. With an assumption of linear viscous\nfriction, $F_{fr}= - \\gamma v$, the kinetic friction coefficient $\\gamma$ is\nderived and plotted at different temperatures in the interval of $T \\in [1,\n600] K$. As a result, by increasing temperature, the kinetic friction\ncoefficient is decreased. Consequently, while the friction is assumed as a\nlinear viscous model, the results are similar to the thermal activation in\natomic-scale friction. That is, (1) by increasing sliding velocity friction\nforce will be increased and (2) by increasing temperature, kinetic friction\ncoefficient decreases.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.04846v1"
    },
    {
        "title": "A noise-robust Multivariate Multiscale Permutation Entropy for two-phase\n  flow characterisation",
        "authors": [
            "John Stewart Fabila-Carrasco",
            "Chao Tan",
            "Javier Escudero"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Using a graph-based approach, we propose a multiscale permutation entropy to\nexplore the complexity of multivariate time series over multiple time scales.\nThis multivariate multiscale permutation entropy (MPEG) incorporates the\ninteraction between channels by constructing an underlying graph for each\ncoarse-grained time series and then applying the recent permutation entropy for\ngraph signals. Given the challenge posed by noise in real-world data analysis,\nwe investigate the robustness to noise of MPEG using synthetic time series and\ndemonstrating better performance than similar multivariate entropy metrics.\n  Two-phase flow data is an important industrial process characterised by\ncomplex, dynamic behaviour. MPEG characterises the flow behaviour transition of\ntwo-phase flow by incorporating information from different scales. The\nexperimental results show that MPEG is sensitive to the dynamic of flow\npatterns, allowing us to distinguish between different flow patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.09030v1"
    },
    {
        "title": "The Hurricane Track Fit Consensus Model for Improving Hurricane\n  Forecasting",
        "authors": [
            "Nathan Ginis",
            "Timothy Marchok"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We present a new method for creating a model consensus to improve real-time\nhurricane track prediction. The method is based on the statistical fitting of\nhistoric numerical model track forecasts to the observed storm positions and\nlearning from their historical errors and biases. Our method is closest to the\nHFIP Corrected Consensus Approach (HCCA) methodology while using an alternative\nmodel formulation. Our method creates a separate consensus model for each\nforecast hour making it possible to independently correct the bias of each\ninput model for that specific hour. This approach, which we call the Hurricane\nTrack Fit (HFIT) model, is computationally efficient and scalable to additional\nnumerical models as input, and it produces interpretable coefficients weighing\nmodel contributions.\n  The new method is evaluated for the 2014-2021 hurricane seasons in the\nAtlantic basin using the input from the best-performing operational track\nforecast guidance at the National Hurricane Center (NHC): the U.S. National\nWeather Service Global Forecast System deterministic and ensemble mean models,\nEuropean Centre for Medium-Range Weather Forecasts deterministic model, the NWS\nHurricane Weather Research and Forecasting model and the NHC equally weighted\nnumerical model track consensus (TVCA). The results of the cross-validation for\nthe 2014-2021 hurricane track dataset show that the HFIT consensus model\nconsistently reduces the track forecast errors compared to those from the input\nmodels and the official NHC forecasts (OFCL). For example, at 24h the HFIT\ntrack forecast errors are smaller by 18.5% and 15.6% than those in AVNI and\nEMXI respectively, and 23% and 15% smaller at 72h. The HFIT forecasts show a\nreduction of errors compared to OFCL by 8.1% at 24h and 7.5% at 72h. We also\ndiscuss the successful real-time operational performance of HFIT during the\n2022 hurricane season.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.16382v1"
    },
    {
        "title": "Rational Tracer: a Tool for Faster Rational Function Reconstruction",
        "authors": [
            "Vitaly Magerya"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Rational Tracer (Ratracer) is a tool to simplify complicated arithmetic\nexpressions using modular arithmetics and rational function reconstruction,\nwith the main idea of separating the construction of expressions (via tracing,\ni.e. recording the list of operations) and their subsequent evaluation during\nrational reconstruction. Ratracer can simplify arithmetic expressions (provided\nas text files), solutions of linear equation systems (specifically targeting\nIntegration-by-Parts (IBP) relations between Feynman integrals), and even more\ngenerally: arbitrary sequences of rational operations, defined in C++ using the\nprovided library ratracer.h. Any of these can also be automatically expanded\ninto series prior to reconstruction. This paper describes the usage of Ratracer\nspecifically focusing on IBP reduction, and demonstrates its performance\nbenefits by comparing with Kira+FireFly and Fire6. Specifically, Ratracer\nachieves a typical ~10x probe time and ~5x overall time speedup over\nKira+FireFly, and even higher if only a few terms in $\\varepsilon$ need to be\nreconstructed.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.03572v1"
    },
    {
        "title": "Retention Time Prediction for Chromatographic Enantioseparation by\n  Quantile Geometry-enhanced Graph Neural Network",
        "authors": [
            "Hao Xu",
            "Jinglong Lin",
            "Dongxiao Zhang",
            "Fanyang Mo"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  A new research framework is proposed to incorporate machine learning\ntechniques into the field of experimental chemistry to facilitate\nchromatographic enantioseparation. A documentary dataset of chiral molecular\nretention times (CMRT dataset) in high-performance liquid chromatography is\nestablished to handle the challenge of data acquisition. Based on the CMRT\ndataset, a quantile geometry-enhanced graph neural network is proposed to learn\nthe molecular structure-retention time relationship, which shows a satisfactory\npredictive ability for enantiomers. The domain knowledge of chromatography is\nincorporated into the machine learning model to achieve multi-column\nprediction, which paves the way for chromatographic enantioseparation\nprediction by calculating the separation probability. Experiments confirm that\nthe proposed research framework works well in retention time prediction and\nchromatographic enantioseparation facilitation, which sheds light on the\napplication of machine learning techniques to the experimental scene and\nimproves the efficiency of experimenters to speed up scientific discovery.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.03602v2"
    },
    {
        "title": "Automated Learning: An Implementation of The A* Search Algorithm over\n  The Random Base Functions",
        "authors": [
            "Nima Tatari"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  This letter explains an algorithm for finding a set of base functions. The\nmethod aims to capture the leading behavior of the dataset in terms of a few\nbase functions. Implementation of the A-star search will help find these\nfunctions, while the gradient descent optimizes the parameters of the functions\nat each search step. We will show the resulting plots to compare the\nextrapolation with the unseen data.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.05085v1"
    },
    {
        "title": "ATLAS flavour-tagging algorithms for the LHC Run 2 $pp$ collision\n  dataset",
        "authors": [
            " ATLAS Collaboration"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The flavour-tagging algorithms developed by the ATLAS Collaboration and used\nto analyse its dataset of $\\sqrt s = 13$ TeV $pp$ collisions from Run 2 of the\nLarge Hadron Collider are presented. These new tagging algorithms are based on\nrecurrent and deep neural networks, and their performance is evaluated in\nsimulated collision events. These developments yield considerable improvements\nover previous jet-flavour identification strategies. At the 77% $b$-jet\nidentification efficiency operating point, light-jet (charm-jet) rejection\nfactors of 170 (5) are achieved in a sample of simulated Standard Model\n$t\\bar{t}$ events; similarly, at a $c$-jet identification efficiency of 30%, a\nlight-jet ($b$-jet) rejection factor of 70 (9) is obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.16345v2"
    },
    {
        "title": "A Workflow Management System Guide",
        "authors": [
            "Caspar Schmitt",
            "Boyang Yu",
            "Thomas Kuhr"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  A workflow describes the entirety of processing steps in an analysis, such as\nemployed in many fields of physics. Workflow management makes the dependencies\nbetween individual steps of a workflow and their computational requirements\nexplicit, such that entire workflows can be executed in a stand-alone manner.\nThough the use of workflow management is widely recommended in the interest of\ntransparency, reproducibility and data preservation, choosing among the large\nvariety of available workflow management tools can be overwhelming. We compare\nselected workflow management tools concerning all relevant criteria and make\nrecommendations for different use cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.01422v2"
    },
    {
        "title": "Machine Learning technique for isotopic determination of radioisotopes\n  using HPGe $\\mathrmγ$-ray spectra",
        "authors": [
            "Ajeeta Khatiwada",
            "Marc Klasky",
            "Marcie Lombardi",
            "Jason Matheny",
            "Arvind Mohan"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  $\\mathrm{\\gamma}$-ray spectroscopy is a quantitative, non-destructive\ntechnique that may be utilized for the identification and quantitative isotopic\nestimation of radionuclides. Traditional methods of isotopic determination have\nvarious challenges that contribute to statistical and systematic uncertainties\nin the estimated isotopics. Furthermore, these methods typically require\nnumerous pre-processing steps, and have only been rigorously tested in\nlaboratory settings with limited shielding. In this work, we examine the\napplication of a number of machine learning based regression algorithms as\nalternatives to conventional approaches for analyzing $\\mathrm{\\gamma}$-ray\nspectroscopy data in the Emergency Response arena. This approach not only\neliminates many steps in the analysis procedure, and therefore offers potential\nto reduce this source of systematic uncertainty, but is also shown to offer\ncomparable performance to conventional approaches in the Emergency Response\nApplication.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01415v1"
    },
    {
        "title": "Hyperparameter optimization, quantum-assisted model performance\n  prediction, and benchmarking of AI-based High Energy Physics workloads using\n  HPC",
        "authors": [
            "Eric Wulff",
            "Maria Girone",
            "David Southwick",
            "Juan Pablo García Amboage",
            "Eduard Cuba"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Training and Hyperparameter Optimization (HPO) of deep learning-based AI\nmodels are often compute resource intensive and calls for the use of\nlarge-scale distributed resources as well as scalable and resource efficient\nhyperparameter search algorithms. This work studies the potential of using\nmodel performance prediction to aid the HPO process carried out on High\nPerformance Computing systems. In addition, a quantum annealer is used to train\nthe performance predictor and a method is proposed to overcome some of the\nproblems derived from the current limitations in quantum systems as well as to\nincrease the stability of solutions. This allows for achieving results on a\nquantum machine comparable to those obtained on a classical machine, showing\nhow quantum computers could be integrated within classical machine learning\ntuning pipelines.\n  Furthermore, results are presented from the development of a containerized\nbenchmark based on an AI-model for collision event reconstruction that allows\nus to compare and assess the suitability of different hardware accelerators for\ntraining deep neural networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.15053v1"
    },
    {
        "title": "Module-based regularization improves Gaussian graphical models when\n  observing noisy data",
        "authors": [
            "Magnus Neuman",
            "Joaquín Calatayud",
            "Viktor Tasselius",
            "Martin Rosvall"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Inferring relations from correlational data allows researchers across the\nsciences to uncover complex connections between variables for insights into the\nunderlying mechanisms. The researchers often represent inferred relations using\nGaussian graphical models, requiring regularization to sparsify the models.\nAcknowledging that the modular structure of the inferred network is often\nstudied, we suggest module-based regularization to balance under- and\noverfitting. Compared with the graphical lasso, a standard approach using the\nGaussian log-likelihood for estimating the regularization strength, this\napproach better recovers and infers modular structure in noisy synthetic and\nreal data. The module-based regularization technique improves the usefulness of\nGaussian graphical models in the many applications where they are employed.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.16796v3"
    },
    {
        "title": "De-novo Identification of Small Molecules from Their GC-EI-MS Spectra",
        "authors": [
            "Adam Hájek",
            "Michal Starý",
            "Filip Jozefov",
            "Helge Hecht",
            "Elliott Price",
            "Aleš Křenek"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Identification of experimentally acquired mass spectra of unknown compounds\npresents a~particular challenge because reliable spectral databases do not\ncover the potential chemical space with sufficient density. Therefore machine\nlearning based \\emph{de-novo} methods, which derive molecular structure\ndirectly from its mass spectrum gained attention recently. We present a~novel\nmethod in this family, addressing a~specific usecase of GC-EI-MS spectra, which\nis particularly hard due to lack of additional information from the first stage\nof MS/MS experiments, on which the previously published methods rely. We\nanalyze strengths and drawbacks or our approach and discuss future directions.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.01634v1"
    },
    {
        "title": "Signal identification without signal formulation",
        "authors": [
            "Yoh-ichi Mototake",
            "Y-h. Taguchi"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  When there are signals and noises, physicists try to identify signals by\nmodeling them, whereas statisticians oppositely try to model noise to identify\nsignals. In this study, we applied the statisticians' concept of signal\ndetection of physics data with small-size samples and high dimensions without\nmodeling the signals. Most of the data in nature, whether noises or signals,\nare assumed to be generated by dynamical systems; thus, there is essentially no\ndistinction between these generating processes. We propose that the correlation\nlength of a dynamical system and the number of samples are crucial for the\npractical definition of noise variables among the signal variables generated by\nsuch a system. Since variables with short-term correlations reach normal\ndistributions faster as the number of samples decreases, they are regarded to\nbe ``noise-like'' variables, whereas variables with opposite properties are\n``signal-like'' variables. Normality tests are not effective for data of\nsmall-size samples with high dimensions. Therefore, we modeled noises on the\nbasis of the property of a noise variable, that is, the uniformity of the\nhistogram of the probability that a variable is a noise. We devised a method of\ndetecting signal variables from the structural change of the histogram\naccording to the decrease in the number of samples. We applied our method to\nthe data generated by globally coupled map, which can produce time series data\nwith different correlation lengths, and also applied to gene expression data,\nwhich are typical static data of small-size samples with high dimensions, and\nwe successfully detected signal variables from them. Moreover, we verified the\nassumption that the gene expression data also potentially have a dynamical\nsystem as their generation model, and found that the assumption is compatible\nwith the results of signal extraction.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.06522v1"
    },
    {
        "title": "Approach to Data Science with Multiscale Information Theory",
        "authors": [
            "Shahid Nawaz",
            "Muhammad Saleem",
            "F. V. Kusmartsev",
            "Dalaver H. Anjum"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Data Science is a multidisciplinary field that plays a crucial role in\nextracting valuable insights and knowledge from large and intricate datasets.\nWithin the realm of Data Science, two fundamental components are Information\nTheory (IT) and Statistical Mechanics (SM), which provide a theoretical\nframework for understanding dataset properties. IT enables efficient storage\nand transmission of information, while SM focuses on the behavior of systems\ncomprising numerous interacting components. In the context of data science, SM\nallows us to model complex interactions among variables within a dataset. By\nleveraging these tools, data scientists can gain a profound understanding of\ndata properties, leading to the development of advanced models and algorithms\nfor analysis and interpretation. Consequently, data science has the potential\nto drive accurate predictions and enhance decision-making across various\ndomains, including finance, marketing, healthcare, and scientific research.\n  In this paper, we apply this data science framework to a large and intricate\nquantum mechanical system composed of particles. Our research demonstrates that\nthe dynamic and probabilistic nature of such systems can be effectively\naddressed using a Multiscale Entropic Dynamics (MED) approach, derived from the\nBoltzmann methods of SM. Through the MED approach, we can describe the system's\ndynamics by formulating a general form of the Nonlinear Schr\\\"odinger equation\nand how it can be applied to various systems with particles and\nquasi-particles, such as electrons, plasmons, polarons, and solitons. By\nemploying this innovative approach, we pave the way for a deeper understanding\nof quantum mechanical systems and their behaviors within complex materials.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.13576v1"
    },
    {
        "title": "Singular Vectors of Sums of Rectangular Random Matrices and Optimal\n  Estimators of High-Rank Signals: The Extensive Spike Model",
        "authors": [
            "Itamar D. Landau",
            "Gabriel C. Mel",
            "Surya Ganguli"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Across many disciplines from neuroscience and genomics to machine learning,\natmospheric science and finance, the problems of denoising large data matrices\nto recover signals obscured by noise, and of estimating the structure of these\nsignals, are of fundamental importance. A key to solving these problems lies in\nunderstanding how the singular value structure of a signal is deformed by\nnoise. This question has been thoroughly studied in the well-known spiked\nmatrix model, in which data matrices originate from low-rank signals perturbed\nby additive noise, in an asymptotic limit where the size of these matrices\ntends to infinity but the signal rank remains finite. We first show,\nstrikingly, that the singular value structure of large finite matrices (of size\n$\\sim 1000$) with even moderate-rank signals, as low as $10$, is not accurately\npredicted by the finite-rank theory, thereby limiting the application of this\ntheory to real data. To address these deficiencies, we analytically compute how\nthe singular values and vectors of an arbitrary high-rank signal matrix are\ndeformed by additive noise. We next study an asymptotic limit corresponding to\nan $\\textit{extensive}$ spike model, in which the rank of the hidden signal is\nproportional to the size of the data matrix, while both tend to infinity. We\nmap out the phase diagram of the singular value structure of the extensive\nspike model as a joint function of signal strength and rank. We further exploit\nthese analytics to derive optimal rotationally invariant denoisers to recover\nhidden $\\textit{high}$-rank signals from data, as well as optimal invariant\nestimators of the signal covariance structure. Overall, our results provide\nfundamental theory governing how high-dimensional signals are deformed by\nadditive noise, together with practical formulas for optimal denoising and\ncovariance estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.00340v2"
    },
    {
        "title": "Fast estimation of the look-elsewhere effect using Gaussian random\n  fields",
        "authors": [
            "Juehang Qin",
            "Rafael F. Lang"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  We discuss the use of Gaussian random fields to estimate the look-elsewhere\neffect correction. We show that Gaussian random fields can be used to model the\nnull-hypothesis significance maps from a large set of statistical problems\ncommonly encountered in physics, such as template matching and likelihood ratio\ntests. Some specific examples are searches for dark matter using pixel arrays,\nsearches for astronomical transients, and searches for fast-radio bursts.\nGaussian random fields can be sampled efficiently in the frequency domain, and\nthe excursion probability can be fitted with these samples to extend any\nestimation of the look-elsewhere effect to lower $p$-values. We demonstrate\nthis using two example template matching problems. Finally, we apply this to\nestimate the trial factor of a $4^3$ accelerometer array for the detection of\ndark matter tracks in the Windchime project. When a global significance of\n$3\\sigma$ is required, the estimated trial factor for such an accelerometer\narray is $10^{14}$ for a one-second search, and $10^{22}$ for a one-year\nsearch.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.01713v2"
    },
    {
        "title": "An information field theory approach to Bayesian state and parameter\n  estimation in dynamical systems",
        "authors": [
            "Kairui Hao",
            "Ilias Bilionis"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Dynamical system state estimation and parameter calibration problems are\nubiquitous across science and engineering. Bayesian approaches to the problem\nare the gold standard as they allow for the quantification of uncertainties and\nenable the seamless fusion of different experimental modalities. When the\ndynamics are discrete and stochastic, one may employ powerful techniques such\nas Kalman, particle, or variational filters. Practitioners commonly apply these\nmethods to continuous-time, deterministic dynamical systems after discretizing\nthe dynamics and introducing fictitious transition probabilities. However,\napproaches based on time-discretization suffer from the curse of dimensionality\nsince the number of random variables grows linearly with the number of\ntime-steps. Furthermore, the introduction of fictitious transition\nprobabilities is an unsatisfactory solution because it increases the number of\nmodel parameters and may lead to inference bias. To address these drawbacks,\nthe objective of this paper is to develop a scalable Bayesian approach to state\nand parameter estimation suitable for continuous-time, deterministic dynamical\nsystems. Our methodology builds upon information field theory. Specifically, we\nconstruct a physics-informed prior probability measure on the function space of\nsystem responses so that functions that satisfy the physics are more likely.\nThis prior allows us to quantify model form errors. We connect the system's\nresponse to observations through a probabilistic model of the measurement\nprocess. The joint posterior over the system responses and all parameters is\ngiven by Bayes' rule. To approximate the intractable posterior, we develop a\nstochastic variational inference algorithm. In summary, the developed\nmethodology offers a powerful framework for Bayesian estimation in dynamical\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.02150v2"
    },
    {
        "title": "Learning thermodynamically constrained equations of state with\n  uncertainty",
        "authors": [
            "Himanshu Sharma",
            "Jim A. Gaffney",
            "Dimitrios Tsapetis",
            "Michael D. Shields"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Numerical simulations of high energy-density experiments require equation of\nstate (EOS) models that relate a material's thermodynamic state variables --\nspecifically pressure, volume/density, energy, and temperature. EOS models are\ntypically constructed using a semi-empirical parametric methodology, which\nassumes a physics-informed functional form with many tunable parameters\ncalibrated using experimental/simulation data. Since there are inherent\nuncertainties in the calibration data (parametric uncertainty) and the assumed\nfunctional EOS form (model uncertainty), it is essential to perform uncertainty\nquantification (UQ) to improve confidence in the EOS predictions. Model\nuncertainty is challenging for UQ studies since it requires exploring the space\nof all possible physically consistent functional forms. Thus, it is often\nneglected in favor of parametric uncertainty, which is easier to quantify\nwithout violating thermodynamic laws. This work presents a data-driven machine\nlearning approach to constructing EOS models that naturally captures model\nuncertainty while satisfying the necessary thermodynamic consistency and\nstability constraints. We propose a novel framework based on physics-informed\nGaussian process regression (GPR) that automatically captures total uncertainty\nin the EOS and can be jointly trained on both simulation and experimental data\nsources. A GPR model for the shock Hugoniot is derived and its uncertainties\nare quantified using the proposed framework. We apply the proposed model to\nlearn the EOS for the diamond solid state of carbon, using both density\nfunctional theory data and experimental shock Hugoniot data to train the model\nand show that the prediction uncertainty reduces by considering the\nthermodynamic constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.17004v2"
    },
    {
        "title": "Reconstruction of Stochastic Dynamics from Large Streamed Datasets",
        "authors": [
            "William Davis"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The complex dynamics of physical systems can often be modeled with stochastic\ndifferential equations. However, computational constraints inhibit the\nestimation of dynamics from large time-series datasets. I present a method for\nestimating drift and diffusion functions from inordinately large datasets\nthrough the use of incremental, online, updating statistics. I demonstrate the\nvalidity and utility of this method by analyzing three large, varied synthetic\ndatasets, as well as an empirical turbulence dataset. This method will\nhopefully facilitate the analysis of complex systems from exceedingly large,\n\"big data\" scientific datasets, as well as real-time streamed data.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.00445v3"
    },
    {
        "title": "Linear approximation to the statistical significance autocovariance\n  matrix in the asymptotic regime",
        "authors": [
            "V. Ananiev",
            "A. L. Read"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Approximating significance scans of searches for new particles in high-energy\nphysics experiments as Gaussian fields is a well-established way to estimate\nthe trials factors required to quantify global significances. We propose a\nnovel, highly efficient method to estimate the covariance matrix of such a\nGaussian field. The method is based on the linear approximation of statistical\nfluctuations of the signal amplitude. For one-dimensional searches the upper\nbound on the trials factor can then be calculated directly from the covariance\nmatrix. For higher dimensions, the Gaussian process described by this\ncovariance matrix may be sampled to calculate the trials factor directly. This\nmethod also serves as the theoretical basis for a recent study of the trials\nfactor with an empirically constructed set of Asmiov-like background datasets.\nWe illustrate the method with studies of a $H \\rightarrow \\gamma \\gamma$\ninspired model that was used in the empirical paper.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.03995v2"
    },
    {
        "title": "Uncertainty components in profile likelihood fits",
        "authors": [
            "Andrés Pinto",
            "Zhibo Wu",
            "Fabrice Balli",
            "Nicolas Berger",
            "Maarten Boonekamp",
            "Émilien Chapon",
            "Tatsuo Kawamoto",
            "Bogdan Malaescu"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  When a measurement of a physical quantity is reported, the total uncertainty\nis usually decomposed into statistical and systematic uncertainties. This\ndecomposition is not only useful to understand the contributions to the total\nuncertainty, but also required to propagate these contributions in subsequent\nanalyses, such as combinations or interpretation fits including results from\nother measurements or experiments. In profile-likelihood fits, widely applied\nin high-energy physics analyses, contributions of systematic uncertainties are\nroutinely quantified using \"impacts\", which are not adequate for such\napplications. We discuss the difference between impacts and actual uncertainty\ncomponents, and establish methods to determine the latter in a wide range of\nstatistical models.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.04007v4"
    },
    {
        "title": "Renormalization Group-Motivated Learning",
        "authors": [
            "Jonathan Landy",
            "Tsvi Tlusty",
            "YeongKyu Lee",
            "YongSeok Jho"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  We introduce an RG-inspired coarse-graining for extracting the collective\nfeatures of data. The key to successful coarse-graining lies in finding\nappropriate pairs of data sets. We coarse-grain the two closest data in a\nregular real-space RG in a lattice while considers the overall information loss\nin momentum-space RG. Here we compromise the two measures for the non-spatial\ndata set. For weakly correlated data close to Gaussian, we use the correlation\nof data as a metric for the proximity of data points, but minimize an overall\nprojection error for optimal coarse-graining steps. It compresses the data to\nmaximize the correlation between the two data points to be compressed while\nminimizing the correlation between the paired data and other data points. We\nshow that this approach can effectively reduce the dimensionality of the data\nwhile preserving the essential features. We extend our method to incorporate\nnon-linear features by replacing correlation measures with mutual information.\nThis results in an information-bottleneck-like trade-off: maximally compress\nthe data while preserving the information among the compressed data and the\nrest. Indeed, our approach can be interpreted as an exact form of\ninformation-bottleneck-like trade off near linear data. We examine our method\nwith random Gaussian data and the Ising model to demonstrate its validity and\napply glass systems. Our approach has potential applications in various fields,\nincluding machine learning and statistical physics.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.08936v1"
    },
    {
        "title": "Beam Detection Based on Machine Learning Algorithms",
        "authors": [
            "Haoyuan Li",
            "Qing Yin"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The positions of free electron laser beams on screens are precisely\ndetermined by a sequence of machine learning models. Transfer training is\nconducted in a self-constructed convolutional neural network based on VGG16\nmodel. Output of intermediate layers are passed as features to a support vector\nregression model. With this sequence, 85.8% correct prediction is achieved on\ntest data.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.00718v1"
    },
    {
        "title": "Unsupervised Learning of Part Similarity for Goal-Guided Accelerated\n  Experiment Design in Metal Additive Manufacturing",
        "authors": [
            "Rui Liu",
            "Sen Liu",
            "Xiaoli Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Metal additive manufacturing is gaining broad interest and increased use in\nthe industrial and academic fields. However, the quantification and\ncommercialization of standard parts usually require extensive experiments and\nexpensive post-characterization, which impedes the rapid development and\nadaptation of metal AM technologies. In this work, a similarity-based\nacceleration (S-acceleration) method for design of experiments is developed to\nreduce the time and costs associated with unveiling process-property (porosity\ndefects) relationships during manufacturing. With S-acceleration, part semantic\nfeatures from machine-setting parameters and physics-effects informed\ncharacteristics are explored for measuring mutual part similarities. A\nuser-defined simplification rate of experiments is proposed to purposely remove\nredundant parts before conducting experiments printing without sacrificing\ninformation gain as original full factorial experiment design. This\nS-acceleration design of experiments is demonstrated on a Concept Laser M2\nmachine for the experimental plan of modeling relationships between process\nparameters and part porosity defects. The printed part has 2 mm diameter by 4\nmm tall pin geometry considering variations in build location and orientation,\nlaser settings and powder feedstock are held constant. In total, 242 parts are\nmeasured to create a ground truth data set of porosity levels by using X-ray\ntomography microscopy. The S-acceleration method is assessed for performance\nconsidering 40%, 50%, and 60% of user-defined experiment simplification rates.\nThe repeated experiments are removed without ignoring the minority experiments\noutlier, assuring a similar process-property relation in the original\nexperiment plan. The experiment number is significantly reduced based on part\nsimilarity with minimal compromise of model accuracy and obtained knowledge.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.01524v2"
    },
    {
        "title": "Robust reconstruction of sparse network dynamics",
        "authors": [
            "Tiago Pereira",
            "Edmilson Roque dos Santos",
            "Sebastian van Strien"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Reconstruction of the network interaction structure from multivariate time\nseries is an important problem in multiple fields of science. This problem is\nill-posed for large networks leading to the reconstruction of false\ninteractions. We put forward the Ergodic Basis Pursuit (EBP) method that uses\nthe network dynamics' statistical properties to ensure the exact reconstruction\nof sparse networks when a minimum length of time series is attained. We show\nthat this minimum time series length scales quadratically with the node degree\nbeing probed and logarithmic with the network size. Our approach is robust\nagainst noise and allows us to treat the noise level as a parameter. We show\nthe reconstruction power of the EBP in experimental multivariate time series\nfrom optoelectronic networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.06433v1"
    },
    {
        "title": "A method of maximum likelihood fit to data with non-uniform efficiencies",
        "authors": [
            "Chenxu Yu",
            "Yanxi Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Estimations of physical parameters using data usually involve non-uniform\nexperimental efficiencies. In this article, a method of maximum likelihood fit\nis introduced using the efficiency as a weight, while the probability\ndistribution function is kept unaffected by the efficiency. A brief proof and\npseudo-experiment studies suggest that this method gives unbiased estimation of\nparameters. For cases where the probability distribution function can be\nnormalized analytically, this method significant reduces the usage of computing\nresources.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.15490v1"
    },
    {
        "title": "An adaptive Bayesian approach to gradient-free global optimization",
        "authors": [
            "Jianneng Yu",
            "Alexandre V. Morozov"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Many problems in science and technology require finding global minima or\nmaxima of various objective functions. The functions are typically\nhigh-dimensional; each function evaluation may entail a significant\ncomputational cost. The importance of global optimization has inspired\ndevelopment of numerous heuristic algorithms based on analogies with physical,\nchemical or biological systems. Here we present a novel algorithm, SmartRunner,\nwhich employs a Bayesian probabilistic model informed by the history of\naccepted and rejected moves to make a decision about the next random trial.\nThus, SmartRunner intelligently adapts its search strategy to a given objective\nfunction and moveset, with the goal of maximizing fitness gain (or energy loss)\nper function evaluation. Our approach can be viewed as adding a simple adaptive\npenalty to the original objective function, with SmartRunner performing hill\nascent or descent on the modified landscape. This penalty can be added to many\nother global optimization algorithms. We explored SmartRunner's performance on\na standard set of test functions, finding that it compares favorably against\nseveral widely-used alternatives: simulated annealing, stochastic hill\nclimbing, evolutionary algorithm, and taboo search. Interestingly, adding the\nadaptive penalty to the first three of these algorithms considerably enhances\ntheir performance. We have also employed SmartRunner to study the\nSherrington-Kirkpatrick (SK) spin glass model and Kauffman's NK fitness model -\ntwo NP-hard problems characterized by numerous local optima. In systems with\nquenched disorder, SmartRunner performs well compared to the other global\noptimizers. Moreover, in finite SK systems it finds close-to-optimal\nground-state energies averaged over disorder.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.04591v1"
    },
    {
        "title": "Information Flow as an Emergent Property of Divergence in Phase-Space",
        "authors": [
            "Praveen Kumar"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Recent developments have created the ability to quantify information flow\namong components that interact in a dynamical system, and have led to\nsignificant advances in characterizing the dependence between the variables\ninvolved. In particular, they have been used to characterize causal dependency\nand feedback using observations across diverse fields such as environment,\nclimate, finance, and human health. What causes information flow among coupled\ncomponents of a dynamical system? This fundamental question has remained\nunanswered so far. Here it is established that the information flow is an\nemergent response resulting from the divergence of trajectories in phase-space\nof a dynamical system. This finding shows that the dynamics encapsulated in the\ntraditional expression of Liouville equation, which neglects this divergence,\nmerely propagates the dependence encoded in the initial conditions. However,\nwhen this is not the case, the informational dependence between the components\nchange creating an information flow. This finding has significant implications\nin a variety of fields, both for the interpretation of observational data for\ncausal inference in natural dynamics, and design of systems with targeted\ninformational dependency.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.07191v2"
    },
    {
        "title": "Using Monte Carlo Tree Search to Calculate Mutual Information in High\n  Dimensions",
        "authors": [
            "Nick Carrara",
            "Jesse Ernst"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Mutual information is an important measure of the dependence among variables.\nIt has become widely used in statistics, machine learning, biology, etc.\nHowever, the standard techniques for estimating it often perform poorly in\nhigher dimensions or with noisy variables. Here we significantly improve one of\nthe standard methods for calculating mutual information by combining it with a\nmodified Monte Carlo Tree Search. We present results which show that our method\ngives accurate results where the standard methods fail. We also describe the\nsoftware implementation of our method and give details on the\npublicly-available code.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.08516v1"
    },
    {
        "title": "Thermodynamically rational decision making under uncertainty",
        "authors": [
            "Dorian Daimer",
            "Susanne Still"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  An analytical characterization of thermodynamically rational agent behaviour\nis obtained for a simple, yet non--trivial example of a ``Maxwell's demon\"\noperating with partial information. Our results provide the first fully\ntransparent physical understanding of a decision problem under uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.10476v3"
    },
    {
        "title": "End-to-end deep learning inference with CMSSW via ONNX using docker",
        "authors": [
            "Purva Chaudhari",
            "Shravan Chaudhari",
            "Ruchi Chudasama",
            "Sergei Gleyzer"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Deep learning techniques have been proven to provide excellent performance\nfor a variety of high-energy physics applications, such as particle\nidentification, event reconstruction and trigger operations. Recently, we\ndeveloped an end-to-end deep learning approach to identify various particles\nusing low-level detector information from high-energy collisions. These models\nwill be incorporated in the CMS software framework (CMSSW) to enable their use\nfor particle reconstruction or for trigger operation in real-time.\nIncorporating these computational tools in the experimental framework presents\nnew challenges. This paper reports an implementation of the end-to-end deep\nlearning inference with the CMS software framework. The inference has been\nimplemented on GPU for faster computation using ONNX. We have benchmarked the\nONNX inference with GPU and CPU using NERSCs Perlmutter cluster by building a\ndocker image of the CMS software framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.14254v1"
    },
    {
        "title": "An Object Condensation Pipeline for Charged Particle Tracking at the\n  High Luminosity LHC",
        "authors": [
            "Kilian Lieret",
            "Gage DeZoort"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Recent work has demonstrated that graph neural networks (GNNs) trained for\ncharged particle tracking can match the performance of traditional algorithms\nwhile improving scalability to prepare for the High Luminosity LHC experiment.\nMost approaches are based on the edge classification (EC) paradigm, wherein\ntracker hits are connected by edges, and a GNN is trained to prune edges,\nresulting in a collection of connected components representing tracks. These\nconnected components are usually collected by a clustering algorithm and the\nresulting hit clusters are passed to downstream modules that may assess track\nquality or fit track parameters. In this work, we consider an alternative\napproach based on object condensation (OC), a multi-objective learning\nframework designed to cluster points belonging to an arbitrary number of\nobjects, in this context tracks, and regress the properties of each object. We\ndemonstrate that OC shows very promising results when applied to the pixel\ndetector of the trackML dataset and can, in some cases, recover tracks that are\nnot reconstructable when relying on the output of an EC alone. The results have\nbeen obtained with a modular and extensible open-source implementation that\nallows us to efficiently train and evaluate the performance of various OC\narchitectures and related approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.16754v1"
    },
    {
        "title": "FunTuple: A new N-tuple component for offline data processing at the\n  LHCb experiment",
        "authors": [
            "Abhijit Mathad",
            "Martina Ferrillo",
            "Sacha Barré",
            "Patrick Koppenburg",
            "Patrick Owen",
            "Gerhard Raven",
            "Eduardo Rodrigues",
            "Nicola Serra"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The offline software framework of the LHCb experiment has undergone a\nsignificant overhaul to tackle the data processing challenges that will arise\nin the upcoming Run 3 and Run 4 of the Large Hadron Collider. This paper\nintroduces FunTuple, a novel component developed for offline data processing\nwithin the LHCb experiment. This component enables the computation and storage\nof a diverse range of observables for both reconstructed and simulated events\nby leveraging on the tools initially developed for the trigger system. This\nfeature is crucial for ensuring consistency between trigger-computed and\noffline-analysed observables. The component and its tool suite offer users\nflexibility to customise stored observables, and its reliability is validated\nthrough a full-coverage set of rigorous unit tests. This paper comprehensively\nexplores FunTuple's design, interface, interaction with other algorithms, and\nits role in facilitating offline data processing for the LHCb experiment for\nthe next decade and beyond.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.02433v3"
    },
    {
        "title": "Deep Learning for EELS hyperspectral images unmixing -- using\n  autoencoders",
        "authors": [
            "N. Brun",
            "G. Lambert",
            "L. Bocher"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Spatially resolved Electron Energy-Loss Spectroscopy (EELS) conducted in a\nScanning Transmission Electron Microscope (STEM) enables the acquisition of\nhyperspectral images (HSIs). Spectral unmixing (SU) is the process of\ndecomposing each spectrum of an HSI into a combination of representative\nspectra (endmembers) corresponding to compounds present in the sample along\nwith their local proportions (abundances). SU is a complex task, and various\nmethods have been developed in different communities using HSIs. However, none\nof these methods fully satisfy the STEM-EELS requirements. Recent advancements\nin remote sensing, which focus on Deep Learning techniques, have the potential\nto meet these requirements, particularly Autoencoders (AEs). In this study, the\nperformance of Deep Learning methods using AE for SU is evaluated, and their\nresults are compared with traditional methods. Synthetic HSIs have been created\nto quantitatively assess the outcomes of the unmixing process using specific\nmetrics. The methods are subsequently applied to a series of experimental data.\nThe findings demonstrate the promising potential of AE as a tool for STEM-EELS\nSU, marking a starting point for exploring more sophisticated Neural Networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.08302v1"
    },
    {
        "title": "A global significance evaluation method using simulated events",
        "authors": [
            "Kelly J Yi",
            "Leonard G Spiegel",
            "Zhen Hu"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  In High-Energy Physics experiments it is often necessary to evaluate the\nglobal statistical significance of apparent resonances observed in invariant\nmass spectra. One approach to determining significance is to use simulated\nevents to find the probability of a random fluctuation in the background\nmimicking a real signal. As a high school summer project, we demonstrate a\nmethod with Monte Carlo simulated events to evaluate the global significance of\na potential resonance with some assumptions. This method for determining\nsignificance is general and can be applied, with appropriate modification, to\nother resonances.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.14317v2"
    },
    {
        "title": "Data reduction strategy in the PandaX-4T experiment",
        "authors": [
            "Yubo Zhou",
            "Xun Chen"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The PandaX-4T experiment is designed for multiple purposes, including\nsearches for solar neutrinos, weakly interacting massive particles, and rare\ndouble beta decays of xenon isotopes. The experiment produces a huge amount of\nraw data that needs to be stored for related physical analyses in a wide energy\nrange. With the upgrading of the PandaX-4T experiment, the doubled sampling\nrate resulted in a larger data size, which challenges both the cost and the\ndata processing speed. To address this issue, we propose a data reduction\nstrategy by removing the noise tail of large signals and downsampling the\nremaining parts of them. This strategy reduces the requirement for storage by\n65% while increasing data processing speed. The influences on physical analyses\non different topics at different energy regions are negligible.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.12412v2"
    },
    {
        "title": "Blind analysis in Physics experiments: Is this trip necessary?",
        "authors": [
            "Robert Golub"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Based on the work of Klein and Roodman [\\cite{JoshK}] we present an alternate\nconclusion as to the charm of blind analysis in physics experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.13542v1"
    },
    {
        "title": "A Ceph S3 Object Data Store for HEP",
        "authors": [
            "Nick Smith",
            "Bo Jayatilaka",
            "David Mason",
            "Oliver Gutsche",
            "Alison Peisker",
            "Robert Illingworth",
            "Chris Jones"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  We present a novel data format design that obviates the need for data tiers\nby storing individual event data products in column objects. The objects are\nstored and retrieved through Ceph S3 technology, with a layout designed to\nminimize metadata volume and maximize data processing parallelism. Performance\nbenchmarks of data storage and retrieval are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.16321v1"
    },
    {
        "title": "Fast Posterior Probability Sampling with Normalizing Flows and Its\n  Applicability in Bayesian analysis in Particle Physics",
        "authors": [
            "Mathias El Baz",
            "Federico Sánchez"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  In this study, we use Rational-Quadratic Neural Spline Flows, a sophisticated\nparametrization of Normalizing Flows, for inferring posterior probability\ndistributions in scenarios where direct evaluation of the likelihood is\nchallenging at inference time. We exemplify this approach using the T2K near\ndetector as a working example, focusing on learning the posterior probability\ndistribution of neutrino flux binned in neutrino energy. The predictions of the\ntrained model are conditioned at inference time by the momentum and angle of\nthe outgoing muons released after neutrino-nuclei interaction. This\nconditioning allows for the generation of personalized posterior distributions,\ntailored to the muon observables, all without necessitating a full retraining\nof the model for each new dataset. The performances of the model are studied\nfor different shapes of the posterior distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.02045v2"
    },
    {
        "title": "Leveraging the Urysohn Lemma of Topology for an Enhanced Binary\n  Classifier",
        "authors": [
            "Ernesto Lopez Fune"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  In this article we offer a comprehensive analysis of the Urysohn's classifier\nin a binary classification context. It utilizes Urysohn's Lemma of Topology to\nconstruct separating functions, providing rigorous and adaptable solutions.\nNumerical experiments demonstrated exceptional performance, with scores ranging\nfrom 95% to 100%. Notably, the Urysohn's classifier outperformed CatBoost and\nKNN in various scenarios. Despite sensitivity to the p-metric parameter, it\nproved robust and adaptable. The Urysohn's classifier's mathematical rigor and\nadaptability make it promising for binary classification, with applications in\nmedical diagnosis, fraud detection and cyber security. Future research includes\nparameter optimization and combining the Urysohn's classifier with other\ntechniques. It offers an elegant and principled approach to classification,\nensuring integrity and valuable data insights.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.11948v1"
    },
    {
        "title": "Generative Models for Simulation of KamLAND-Zen",
        "authors": [
            "Z. Fu",
            "C. Grant",
            "D. M. Krawiec",
            "A. Li",
            "L. Winslow"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The next generation of searches for neutrinoless double beta decay\n(0{\\nu}\\b{eta}\\b{eta}) are poised to answer deep questions on the nature of\nneutrinos and the source of the Universe's matter-antimatter asymmetry. They\nwill be looking for event rates of less than one event per ton of instrumented\nisotope per year. To claim discovery, accurate and efficient simulations of\ndetector events that mimic 0{\\nu}\\b{eta}\\b{eta} is critical. Traditional Monte\nCarlo (MC) simulations can be supplemented by machine-learning-based generative\nmodels. In this work, we describe the performance of generative models designed\nfor monolithic liquid scintillator detectors like KamLAND to produce highly\naccurate simulation data without a predefined physics model. We demonstrate its\nability to recover low-level features and perform interpolation. In the future,\nthe results of these generative models can be used to improve event\nclassification and background rejection by providing high-quality abundant\ngenerated data.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.14372v1"
    },
    {
        "title": "Kinematic Characterization of Micro-Mobility Vehicles During Evasive\n  Maneuvers",
        "authors": [
            "Paolo Terranova",
            "Shu-Yuan Liu",
            "Sparsh Jain",
            "Johan Engstrom",
            "Miguel Perez"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  There is an increasing need to comprehensively characterize the kinematic\nperformances of different Micromobility Vehicles (MMVs). This study aims to: 1)\ncharacterize the kinematic behaviors of different MMVs during emergency\nmaneuvers; 2) explore the influence of different MMV power sources on the\ndevice performances; 3) investigate if piecewise linear models are suitable for\nmodeling MMV trajectories. A test track experiment where 40 frequent riders\nperformed emergency braking and swerving maneuvers riding a subset of electric\nMMVs, their traditional counterparts, and, in some cases, behaving as running\npedestrians. A second experiment was conducted to determine the MMVs swerving\nlower boundaries. Device power source resulted having a statistically\nsignificant influence on kinematic capabilities of the MMVs: while e-MMVs\ndisplayed superior braking capabilities compared to their traditional\ncounterparts, the opposite was observed in terms of swerving performance.\nFurthermore, performances varied significantly across the different MMV\ntypologies, with handlebar-based devices consistently outperforming the\nhandlebar-less devices across the metrics considered. The piecewise linear\nmodels used for braking profiles fit well for most MMVs, except for skateboards\nand pedestrians due to foot-ground engagement. These findings underscore that\nthe effectiveness of steering or braking in preventing collisions may vary\ndepending on the type and power source of the device. This study also\ndemonstrates the applicability of piecewise linear models for generating\nparameterized functions that accurately model braking trajectories, providing a\nvaluable resource for automated systems developers. The model, however, also\nreveals that the single brake ramp assumption does not apply for certain types\nof MMVs or for pedestrians, indicating the necessity for further improvements.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.14717v1"
    },
    {
        "title": "Velocity-based sparse photon clustering for space debris ranging by\n  single-photon Lidar",
        "authors": [
            "Xialin Liu",
            "Jia Qiang",
            "Genghua Huang",
            "Liang Zhang",
            "Zheng Zhao",
            "Rong Shu"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Single-photon Lidar (SPL) offers unprecedented sensitivity and time\nresolution, which enables Satellite Laser Ranging (SLR) systems to identify\nspace debris from distances spanning thousands of kilometers. However, existing\nSPL systems face limitations in distance-trajectory extraction due to the\nwidespread and undifferentiated noise photons. In this paper, we propose a\nnovel velocity-based sparse photon clustering algorithm, leveraging the\nvelocity correlation of the target's echo signal photons in the distance-time\ndimension, by computing and searching the velocity and acceleration of photon\ndistance points between adjacent pulses over a period of time and subsequently\nclustering photons with the same velocity and acceleration. Our algorithm can\nextract object trajectories from sparse photon data, even in low\nsignal-to-noise ratio (SNR) conditions. To verify our method, we establish a\nground simulation experimental setup for a single-photon ranging Lidar system.\nThe experimental results show that our algorithm can extract the quadratic\ntrack with over 99 percent accuracy in only tens of milliseconds, with a signal\nphoton counting rate of 5 percent at -20 dB SNR. Our method provides an\neffective approach for detecting and sensing extremely weak signals at the\nsub-photon level in space.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.04147v1"
    },
    {
        "title": "Learning effective good variables from physical data",
        "authors": [
            "Giulio Barletta",
            "Giovanni Trezza",
            "Eliodoro Chiavazzo"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  We assume that a sufficiently large database is available, where a physical\nproperty of interest and a number of associated ruling primitive variables or\nobservables are stored. We introduce and test two machine learning approaches\nto discover possible groups or combinations of primitive variables: The first\napproach is based on regression models whereas the second on classification\nmodels. The variable group (here referred to as the new effective good\nvariable) can be considered as successfully found, when the physical property\nof interest is characterized by the following effective invariant behaviour: In\nthe first method, invariance of the group implies invariance of the property up\nto a given accuracy; in the other method, upon partition of the physical\nproperty values into two or more classes, invariance of the group implies\ninvariance of the class. For the sake of illustration, the two methods are\nsuccessfully applied to two popular empirical correlations describing the\nconvective heat transfer phenomenon and to the Newton's law of universal\ngravitation.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.05226v1"
    },
    {
        "title": "Machine learning approach to detect dynamical states from recurrence\n  measures",
        "authors": [
            "Dheeraja Thakur",
            "Athul Mohan",
            "G. Ambika",
            "Chandrakala Meena"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  We integrate machine learning approaches with nonlinear time series analysis,\nspecifically utilizing recurrence measures to classify various dynamical states\nemerging from time series. We implement three machine learning algorithms\nLogistic Regression, Random Forest, and Support Vector Machine for this study.\nThe input features are derived from the recurrence quantification of nonlinear\ntime series and characteristic measures of the corresponding recurrence\nnetworks. For training and testing we generate synthetic data from standard\nnonlinear dynamical systems and evaluate the efficiency and performance of the\nmachine learning algorithms in classifying time series into periodic, chaotic,\nhyper-chaotic, or noisy categories. Additionally, we explore the significance\nof input features in the classification scheme and find that the features\nquantifying the density of recurrence points are the most relevant.\nFurthermore, we illustrate how the trained algorithms can successfully predict\nthe dynamical states of two variable stars, SX Her and AC Her from the data of\ntheir light curves.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.10298v2"
    },
    {
        "title": "Undercoverage in high-statistics counting experiments with finite MC\n  samples",
        "authors": [
            "Cristina-Andreea Alexe",
            "Joshua L. Bendavid",
            "Lorenzo Bianchini",
            "Davide Bruschini"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  We consider the problem of setting a confidence interval on a parameter of\ninterest from a high-statistics counting experiment in the presence of\nsystematic uncertainties modeled as unconstrained nuisance parameters. We use\nthe profile-likelihood test statistic in the asymptotic limit for confidence\ninterval setting and focus on the case where the likelihood function is derived\nfrom a finite sample of Monte Carlo simulated events. We prove as a general\nresult that statistical uncertainties in the Monte Carlo sample affect the\ncoverage of the confidence interval always in the same direction, namely they\nlead to a systematic undercoverage of the interval. We argue that such spurious\neffects might not be fully accounted for by statistical methods that are\nusually adopted in HEP measurements to counteract the effects of finite-size MC\nsamples, such as those based on the Barlow-Beeston likelihood.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.10542v1"
    },
    {
        "title": "Response Theory via Generative Score Modeling",
        "authors": [
            "Ludovico Theo Giorgini",
            "Katherine Deck",
            "Tobias Bischoff",
            "Andre Souza"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  We introduce an approach for analyzing the responses of dynamical systems to\nexternal perturbations that combines score-based generative modeling with the\nGeneralized Fluctuation-Dissipation Theorem (GFDT). The methodology enables\naccurate estimation of system responses, including those with non-Gaussian\nstatistics. We numerically validate our approach using time-series data from\nthree different stochastic partial differential equations of increasing\ncomplexity: an Ornstein-Uhlenbeck process with spatially correlated noise, a\nmodified stochastic Allen-Cahn equation, and the 2D Navier-Stokes equations. We\ndemonstrate the improved accuracy of the methodology over conventional methods\nand discuss its potential as a versatile tool for predicting the statistical\nbehavior of complex dynamical systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.01029v3"
    },
    {
        "title": "Disentangling high order effects in the transfer entropy",
        "authors": [
            "Sebastiano Stramaglia",
            "Luca Faes",
            "Jesus M. Cortes",
            "Daniele Marinazzo"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Transfer Entropy (TE), the primary method for determining directed\ninformation flow within a network system, can exhibit bias - either in\ndeficiency or excess - during both pairwise and conditioned calculations, owing\nto high-order dependencies among the dynamic processes under consideration and\nthe remaining processes in the system used for conditioning. Here, we propose a\nnovel approach. Instead of conditioning TE on all network processes except the\ndriver and target, as in its fully conditioned version, or not conditioning at\nall, as in the pairwise approach, our method searches for both the multiplets\nof variables that maximize information flow and those that minimize it. This\nprovides a decomposition of TE into unique, redundant, and synergistic atoms.\nOur approach enables the quantification of the relative importance of\nhigh-order effects compared to pure two-body effects in information transfer\nbetween two processes, while also highlighting the processes that contribute to\nbuilding these high-order effects alongside the driver. We demonstrate the\napplication of our approach in climatology by analyzing data from El Ni\\~{n}o\nand the Southern Oscillation.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.03229v4"
    },
    {
        "title": "Quantifying Systematic Uncertainties in Experimental Physics: An\n  Approximation Method",
        "authors": [
            "Lu Li"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  In the domain of physics experiments, data fitting is a pivotal technique for\nextracting insights from both experimental and simulated datasets. This article\npresents an approximation method designed to estimate the systematic errors\nprevalent in data analyses. By applying our method to the Nab experiment, we\ncompare our findings with simulation-derived results, thereby confirming the\nconcordance of our approach with established simulation outcomes. This\ncorroboration highlights the versatility of our method as a good tool for\nvalidating simulation results across various experimental contexts.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09763v2"
    },
    {
        "title": "Entropy corrected geometric Brownian motion",
        "authors": [
            "Rishabh Gupta",
            "Ewa A. Drzazga-Szczȩśniak",
            "Sabre Kais",
            "Dominik Szczȩśniak"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The geometric Brownian motion (GBM) is widely employed for modeling\nstochastic processes, yet its solutions are characterized by the log-normal\ndistribution. This comprises predictive capabilities of GBM mainly in terms of\nforecasting applications. Here, entropy corrections to GBM are proposed to go\nbeyond log-normality restrictions and better account for intricacies of real\nsystems. It is shown that GBM solutions can be effectively refined by arguing\nthat entropy is reduced when deterministic content of considered data\nincreases. Notable improvements over conventional GBM are observed for several\ncases of non-log-normal distributions, ranging from a dice roll experiment to\nreal world data.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.06253v2"
    },
    {
        "title": "Normalizing Flows for Domain Adaptation when Identifying $Λ$\n  Hyperon Events",
        "authors": [
            "Rowan Kelleher",
            "Anselm Vossen"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  This study focuses on the novel application of a normalizing flow as a method\nof domain adaptation. Normalizing flows offer a way to transform data points\nbetween two different distributions. The present study investigates a method of\ntransforming latent representations of physics data to a normal distribution\nand then to a physics distribution again. The final distribution models a\nsimulated distribution. Following the transformation process, the data can be\nclassified by a neural network trained on labeled simulation data. The present\nstudy succeeds in training two normalizing flows that can transform between\ndata (or simulation) and a Gaussian distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.14804v2"
    },
    {
        "title": "Deep Probabilistic Direction Prediction in 3D with Applications to\n  Directional Dark Matter Detectors",
        "authors": [
            "Majd Ghrear",
            "Peter Sadowski",
            "Sven Einar Vahsen"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  We present the first method to probabilistically predict 3D direction in a\ndeep neural network model. The probabilistic predictions are modeled as a\nheteroscedastic von Mises-Fisher distribution on the sphere $\\mathbb{S}^2$,\ngiving a simple way to quantify aleatoric uncertainty. This approach\ngeneralizes the cosine distance loss which is a special case of our loss\nfunction when the uncertainty is assumed to be uniform across samples. We\ndevelop approximations required to make the likelihood function and gradient\ncalculations stable. The method is applied to the task of predicting the 3D\ndirections of electrons, the most complex signal in a class of experimental\nparticle physics detectors designed to demonstrate the particle nature of dark\nmatter and study solar neutrinos. Using simulated Monte Carlo data, the initial\ndirection of recoiling electrons is inferred from their tortuous trajectories,\nas captured by the 3D detectors. For $40\\,$keV electrons in a $70\\%$\n$\\textrm{He}$ $30 \\%$ $\\textrm{CO}_2$ gas mixture at STP, the new approach\nachieves a mean cosine distance of $0.104$ ($26^\\circ$) compared to $0.556$\n($64^\\circ$) achieved by a non-machine learning algorithm. We show that the\nmodel is well-calibrated and accuracy can be increased further by removing\nsamples with high predicted uncertainty. This advancement in probabilistic 3D\ndirectional learning could increase the sensitivity of directional dark matter\ndetectors.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.15949v2"
    },
    {
        "title": "The CMS statistical analysis and combination tool: COMBINE",
        "authors": [
            " CMS Collaboration"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  This paper describes the COMBINE software package used for statistical\nanalyses by the CMS Collaboration. The package, originally designed to perform\nsearches for a Higgs boson and the combined analysis of those searches, has\nevolved to become the statistical analysis tool presently used in the majority\nof measurements and searches performed by the CMS Collaboration. It is not\nspecific to the CMS experiment, and this paper is intended to serve as a\nreference for users outside of the CMS Collaboration, providing an outline of\nthe most salient features and capabilities. Readers are provided with the\npossibility to run COMBINE and reproduce examples provided in this paper using\na publicly available container image. Since the package is constantly evolving\nto meet the demands of ever-increasing data sets and analysis sophistication,\nthis paper cannot cover all details of COMBINE. However, the online\ndocumentation referenced within this paper provides an up-to-date and complete\nuser guide.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.06614v2"
    },
    {
        "title": "Visualization for physics analysis improvement and applications in\n  BESIII",
        "authors": [
            "Zhi-Jun Li",
            "Ming-Kuan Yuan",
            "Yun-Xuan Song",
            "Yan-Gu Li",
            "Jing-Shu Li",
            "Sheng-Sen Sun",
            "Xiao-Long Wang",
            "Zheng-Yun You",
            "Ya-Jun Mao"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Modern particle physics experiments usually rely on highly complex and\nlarge-scale spectrometer devices. In high energy physics experiments,\nvisualization helps detector design, data quality monitoring, offline data\nprocessing, and has great potential for improving physics analysis. In addition\nto the traditional physics data analysis based on statistical methods,\nvisualization provides unique intuitive advantages in searching for rare signal\nevents and reducing background noises. By applying the event display tool to\nseveral physics analyses in the BESIII experiment, we demonstrate that\nvisualization can benefit potential physics discovery and improve the signal\nsignificance. With the development of modern visualization techniques, it is\nexpected to play a more important role in future data processing and physics\nanalysis of particle physics experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.07951v2"
    },
    {
        "title": "Reconstruction of network dynamics from partial observations",
        "authors": [
            "Tyrus Berry",
            "Timothy Sauer"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  We investigate the reconstruction of time series from dynamical networks that\nare partially observed. In particular, we address the extent to which the time\nseries at a node of the network can be successfully reconstructed when\nmeasuring from another node, or subset of nodes, corrupted by observational\nnoise. We will assume the dynamical equations of the network are known, and\nthat the dynamics are not necessarily low-dimensional. The case of linear\ndynamics is treated first, and leads to a definition of observation error\nmagnification factor (OEMF) that measures the magnification of noise in the\nreconstruction process. Subsequently, the definition is applied to nonlinear\nand chaotic dynamics. Comparison of OEMF for different target/observer\ncombinations can lead to better understanding of how to optimally observe a\nnetwork. As part of the study, a computational method for reconstructing time\nseries from partial observations is presented and analyzed.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.13088v2"
    },
    {
        "title": "A direct method of continuous unwrapping the phase from an interferogram\n  image",
        "authors": [
            "V. Berejnov",
            "B. Y. Rubinstein"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  A new method recovers phase difference of interfering wavefronts from a\npattern of interference fringes, avoiding discontinuity problem. The continuous\nphase is a solution of the first order differential equation of the\ninterferogram function computed from the fringe intensity profile selected\nalong the pathway over the interferogram.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.14437v1"
    },
    {
        "title": "PHYSTAT Informal Review: Marginalizing versus Profiling of Nuisance\n  Parameters",
        "authors": [
            "Robert D. Cousins",
            "Larry Wasserman"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  This is a writeup, with some elaboration, of the talks by the two authors (a\nphysicist and a statistician) at the first PHYSTAT Informal review on January\n24, 2024. We discuss Bayesian and frequentist approaches to dealing with\nnuisance parameters, in particular, integrated versus profiled likelihood\nmethods. In regular models, with finitely many parameters and large sample\nsizes, the two approaches are asymptotically equivalent. But, outside this\nsetting, the two methods can lead to different tests and confidence intervals.\nAssessing which approach is better generally requires comparing the power of\nthe tests or the length of the confidence intervals. This analysis has to be\nconducted on a case-by-case basis. In the extreme case where the number of\nnuisance parameters is very large, possibly infinite, neither approach may be\nuseful. Part I provides an informal history of usage in high energy particle\nphysics, including a simple illustrative example. Part II includes an overview\nof some more recently developed methods in the statistics literature, including\nmethods applicable when the use of the likelihood function is problematic.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.17180v1"
    },
    {
        "title": "A Statistical Method for Improving Momentum Measurement of Photon\n  Conversions Reconstructed from Single Electrons",
        "authors": [
            "Ahmet Bingül",
            "Zekeriya Uysal"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The reconstruction of photon conversions is importantin order to improve the\nreconstruction efficiency of the physics measurements involving photons.\nHowever, there are significant number of conversions in which only one of the\ntwo tracks emitted electrons is reconstructed in the detector due to very\nasymmetric energy sharing between the electron-positron pair. The momentum\ndetermination of the parent photon can be improved by estimating the missing\nenergy in such conversions. In this study, we propose a simple statistical\nmethod that can be used to determine the mean value of the missing energy. By\nusing simulated minimum bias events at LHC conditions and a toy detector\nsimulation, the performance of the method is tested for several decay channels\ncommonly used in particle physics analyses. A considerable improvement in the\nmass reconstruction precision is obtained when reconstructing particles\ndecaying to photons whose energies are less than 20 GeV.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.01454v1"
    },
    {
        "title": "Decomposing causality into its synergistic, unique, and redundant\n  components",
        "authors": [
            "Álvaro Martínez-Sánchez",
            "Gonzalo Arranz",
            "Adrián Lozano-Durán"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Causality lies at the heart of scientific inquiry, serving as the fundamental\nbasis for understanding interactions among variables in physical systems.\nDespite its central role, current methods for causal inference face significant\nchallenges due to nonlinear dependencies, stochastic interactions,\nself-causation, collider effects, and influences from exogenous factors, among\nothers. While existing methods can effectively address some of these\nchallenges, no single approach has successfully integrated all these aspects.\nHere, we address these challenges with SURD: Synergistic-Unique-Redundant\nDecomposition of causality. SURD quantifies causality as the increments of\nredundant, unique, and synergistic information gained about future events from\npast observations. The formulation is non-intrusive and applicable to both\ncomputational and experimental investigations, even when samples are scarce. We\nbenchmark SURD in scenarios that pose significant challenges for causal\ninference and demonstrate that it offers a more reliable quantification of\ncausality compared to previous methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12411v3"
    },
    {
        "title": "SwdFold:A Reweighting and Unfolding method based on Optimal Transport\n  Theory",
        "authors": [
            "Chu-Cheng Pan",
            "Xiang Dong",
            "Yu-Chang Sun",
            "Ao-Yan Cheng",
            "Ao-Bo Wang",
            "Yu-Xuan Hu",
            "Hao Cai"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  High-energy physics experiments rely heavily on precise measurements of\nenergy and momentum, yet face significant challenges due to detector\nlimitations, calibration errors, and the intrinsic nature of particle\ninteractions. Traditional unfolding techniques have been employed to correct\nfor these distortions, yet they often suffer from model dependency and\nstability issues. We present a novel method, SwdFold, which utilizes the\nprinciples of optimal transport to provide a robust, model-independent\nframework to estimate the probability density ratio for data unfolding. It not\nonly unfold the toy experimental event by reweighted simulated data\ndistributions closely with true distributions but also maintains the integrity\nof physical features across various observables. We can expect it can enable\nmore reliable predictions and comprehensive analyses as a high precision\nreweighting and unfolding tool in high-energy physics.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.01635v1"
    },
    {
        "title": "A minimalistic and general weighted averaging method for inconsistent\n  data",
        "authors": [
            "Martino Trassinelli",
            "Marleen Maxton"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The weighted average of inconsistent data is a common and tedious problem\nthat many scientists have encountered. The standard weighted average is not\nrecommended for these cases, and different alternative methods are proposed in\nthe literature. Here, we discuss a method first proposed by Sivia in 1996 that\nis based on Bayesian statistics and keeps the number of assumptions to a\nminimum. We propose this approach as a new standard for calculating weighted\naverages. The uncertainty associated with each input value is considered to be\njust a lower bound of the true unknown uncertainty.The resulting likelihood\nfunction is no longer Gaussian, but has smoothly decreasing wings, which allows\nfor a better treatment of scattered data and outliers. The proposed method is\ntested on a series of data sets: simulations, CODATA recommended value of the\nNewtonian gravitational constant, and some particle properties from the\nParticle Data Group, including the proton charge radius. A freely available\nPython library is also provided for a simple implementation of the proposed\naveraging method.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.08293v2"
    },
    {
        "title": "An insightful approach to bearings-only tracking in log-polar\n  coordinates",
        "authors": [
            "Athena Helena Xiourouppa",
            "Dmitry Mikhin",
            "Melissa Humphries",
            "John Maclean"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The choice of coordinate system in a bearings-only (BO) tracking problem\ninfluences the methods used to observe and predict the state of a moving\ntarget. Modified Polar Coordinates (MPC) and Log-Polar Coordinates (LPC) have\nsome advantages over Cartesian coordinates. In this paper, we derive\nclosed-form expressions for the target state prior distribution after ownship\nmanoeuvre: the mean, covariance, and higher-order moments in LPC. We explore\nthe use of these closed-form expressions in simulation by modifying an existing\nBO tracker that uses the UKF. Rather than propagating sigma points, we directly\nsubstitute current values of the mean and covariance into the time update\nequations at the ownship turn. This modified UKF, the CFE-UKF, performs\nsimilarly to the pure UKF, verifying the closed-form expressions. The\nclosed-form third and fourth central moments indicate non-Gaussianity of the\ntarget state when the ownship turns. By monitoring these metrics and\nappropriately initialising relative range error, we can achieve a desired\noutput mean estimated range error (MRE). The availability of these higher-order\nmoments facilitates other extensions of the tracker not possible with a\nstandard UKF.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19599v1"
    },
    {
        "title": "Approximate solutions of a general stochastic velocity-jump process\n  subject to discrete-time noisy observations",
        "authors": [
            "Arianna Ceccarelli",
            "Alexander P. Browning",
            "Ruth E. Baker"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Advances in experimental techniques allow the collection of\nhigh-space-and-time resolution data that track individual motile entities over\ntime. This poses the question of how to use these data to efficiently and\neffectively calibrate motion models. However, typical mathematical models often\noverlook the inherent aspects of data collection, such as the discreteness and\nthe experimental noise of the measured locations. In this paper, we focus on\nvelocity-jump models suitable to describe single-agent motion in one spatial\ndimension, characterised by successive Markovian transitions between a finite\nnetwork of $n$ states, each with a specified velocity and a fixed rate of\nswitching to every other state. Since the problem of finding the exact\ndistributions of discrete-time noisy data is generally intractable, we derive a\nseries of approximations for the data distributions and compare them to\nin-silico data generated by the models using four example network structures.\nThese comparisons suggest that the approximations are accurate given\nsufficiently infrequent state switching, or equivalently, a sufficiently high\ndata collection frequency. Moreover, for infrequent switching, the PDFs\ncomparisons highlight the importance of accounting for the correlation between\nsubsequent measured locations, due to the likely permanence in the state\nvisited in the previous measurement. The approximate distributions computed can\nbe used for fast parameter inference and model selection between a range of\nvelocity-jump models using single-agent tracking data.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19787v2"
    },
    {
        "title": "Unbalanced optimal transport for stochastic particle tracking",
        "authors": [
            "Kairui Hao",
            "Atharva Hans",
            "Pavlos Vlachos",
            "Ilias Bilionis"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Non-invasive flow measurement techniques, such as particle tracking\nvelocimetry, resolve 3D velocity fields by pairing tracer particle positions in\nsuccessive time steps. These trajectories are crucial for evaluating physical\nquantities like vorticity, shear stress, pressure, and coherent structures.\nTraditional approaches deterministically reconstruct particle positions and\nextract particle tracks using tracking algorithms. However, reliable track\nestimation is challenging due to measurement noise caused by high particle\ndensity, particle image overlap, and falsely reconstructed 3D particle\npositions. To overcome this challenge, probabilistic approaches quantify the\nepistemic uncertainty in particle positions, typically using a Gaussian\nprobability distribution. However, the standard deterministic tracking\nalgorithms relying on nearest-neighbor search do not directly extend to the\nprobabilistic setting. Moreover, such algorithms do not necessarily find\nglobally consistent solutions robust to reconstruction errors. This paper aims\nto develop a globally consistent nearest-neighborhood algorithm that robustly\nextracts stochastic particle tracks from the reconstructed Gaussian particle\ndistributions in all frames. Our tracking algorithm relies on the unbalanced\noptimal transport theory in the metric space of Gaussian measures.\nSpecifically, we optimize a binary transport plan for efficiently moving the\nGaussian distributions of reconstructed particle positions between time frames.\nWe achieve this by computing the partial Wasserstein distance in the metric\nspace of Gaussian measures. Our tracking algorithm is robust to position\nreconstruction errors since it automatically detects the number of particles\nthat should be matched through hyperparameter optimization. Finally, we\nvalidate our method using an in vitro flow experiment using a 3D-printed\ncerebral aneurysm.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.04583v1"
    },
    {
        "title": "Combination of operational modal analysis algorithms to identify modal\n  parameters of an actual centrifugal compressor",
        "authors": [
            "Leandro O. Zague",
            "Daniel A. Castello",
            "Carlos F. T. Matt"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The novelty of the current work is precisely to propose a statistical\nprocedure to combine estimates of the modal parameters provided by any set of\nOperational Modal Analysis (OMA) algorithms so as to avoid preference for a\nparticular one and also to derive an approximate joint probability distribution\nof the modal parameters, from which engineering statistics of interest such as\nmean value and variance are readily provided. The effectiveness of the proposed\nstrategy is assessed considering measured data from an actual centrifugal\ncompressor. The statistics obtained for both forward and backward modal\nparameters are finally compared against modal parameters identified during\nstandard stability verification testing (SVT) of centrifugal compressors prior\nto shipment, using classical Experimental Modal Analysis (EMA) algorithms. The\ncurrent work demonstrates that combination of OMA algorithms can provide quite\naccurate estimates for both the modal parameters and the associated\nuncertainties with low computational costs.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.07273v1"
    },
    {
        "title": "Extracting self-similarity from data",
        "authors": [
            "Nikos Bempedelis",
            "Luca Magri",
            "Konstantinos Steiros"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The identification of self-similarity is an indispensable tool for\nunderstanding and modelling physical phenomena. Unfortunately, this is not\nalways possible to perform formally in highly complex problems. We propose a\nmethodology to extract the similarity variables of a self-similar physical\nprocess directly from data, without prior knowledge of the governing equations\nor boundary conditions, based on an optimization problem and symbolic\nregression. We analyze the accuracy and robustness of our method in five\nproblems which have been influential in fluid mechanics research: a laminar\nboundary layer, Burger's equation, a turbulent wake, a collapsing cavity, and\ngrid-generated turbulence. Our analysis considers datasets acquired via both\nnumerical and wind-tunnel experiments. The algorithm recovers the known\nself-similarity expressions in the first four problems and generates new\ninsights in the final problem (grid turbulence).\n",
        "pdf_link": "http://arxiv.org/pdf/2407.10724v2"
    },
    {
        "title": "Wavelength calibration and spectral sensitivity correction of\n  luminescence measurements for dosimetry applications: method comparison\n  tested on the IR-RF of K-feldspar",
        "authors": [
            "Mariana Sontag-González",
            "Dirk Mittelstraß",
            "Sebastian Kreutzer",
            "Markus Fuchs"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Spectroscopic investigations provide important insights into the composition\nof luminescence emissions relevant to trapped-charge dating of sediments.\nAccurate wavelength calibration and a correction for the wavelength-dependent\ndetection efficiency of the spectrometer system are crucial to ensure the\ncorrect spectrum interpretation and allow for its comparison with those\nobtained from other systems. However, to achieve an accurate detection\nefficiency correction, it is necessary to obtain the device-specific spectral\nresponse function (SRF). Here, we compare two SRF approximation methods by\nusing either a calibration lamp of known irradiance or calculating the product\nof efficiency curves provided by the manufacturers of all known optical\nelements. We discuss the results using radiofluorescence (RF) measurements of\ntwo K-feldspar samples as an example. Feldspar infra-red (IR) RF spectra are\nknown to be composed of several overlapping emissions, whose variation with\nsample mineralogy is still poorly understood and requires more extensive\ninvestigations. We find that both methods of sensitivity correction yield\nbroadly similar results. However, the observed differences can alter a\nspectrum's interpretation. For example, we observe that after peak\ndeconvolution the maximum signal wavelength of the IR-RF peak used for dating\napplications differs by ~3-13 nm between the two methods, depending on sample\nand diffraction grating. We recommend using calibration lamps to determine a\ndevice's SRF but highlight the need to consider issues such as higher-order\nsignals in the choice of filters to establish the SRF's reliable wavelength\nrange. Additionally, we find that a simple and inexpensive fluorescent white\nlight yields an acceptable wavelength calibration comparable to that obtained\nfrom a specialized light source.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.07991v1"
    },
    {
        "title": "Analog-Based Forecasting of Turbulent Velocity: Relationship between\n  Unpredictability and Intermittency",
        "authors": [
            "Ewen Frogé",
            "Carlos Granero-Belinchon",
            "Stéphane G. Roux",
            "Nicolas B. Garnier",
            "Thierry Chonavel"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  This study evaluates the performance of analog-based methodologies to\npredict, in a statistical way, the longitudinal velocity in a turbulent flow.\nThe data used comes from hot wire experimental measurements from the Modane\nwind tunnel. We compared different methods and explored the impact of varying\nthe number of analogs and their sizes on prediction accuracy. We illustrate\nthat the innovation, defined as the difference between the true velocity value\nand the prediction value, highlights particularly unpredictable events that we\ndirectly link with extreme events of the velocity gradients and so to\nintermittency. A statistical study of the innovation indicates that while the\nestimator effectively seizes linear correlations, it fails to fully capture\nhigher-order dependencies. The innovation underscores the presence of\nintermittency, revealing the limitations of current predictive models and\nsuggesting directions for future improvements in turbulence forecasting.This\nstudy evaluates the performance of analog-based methodologies to predict the\nlongitudinal velocity in a turbulent flow. The data used comes from hot wire\nexperimental measurements from the Modane wind tunnel. We compared different\nmethods and explored the impact of varying the number of analogs and their\nsizes on prediction accuracy. We illustrate that the innovation, defined as the\ndifference between the true velocity value and the prediction value, highlights\nparticularly unpredictable events that we directly link with extreme events of\nthe velocity gradients and so to intermittency. This result indicates that\nwhile the estimator effectively seizes linear correlations, it fails to fully\ncapture higher-order dependencies. The innovation underscores the presence of\nintermittency, revealing the limitations of current predictive models and\nsuggesting directions for future improvements in turbulence forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.07792v2"
    },
    {
        "title": "Signal model parameter scan using Normalizing Flow",
        "authors": [
            "Masahiko Saito",
            "Masahiro Morinaga",
            "Tomoe Kishimoto",
            "Junichi Tanaka"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  This paper presents a parameter scan technique for BSM signal models based on\nnormalizing flow. Normalizing flow is a type of deep learning model that\ntransforms a simple probability distribution into a complex probability\ndistribution as an invertible function. By learning an invertible\ntransformation between a complex multidimensional distribution, such as\nexperimental data observed in collider experiments, and a multidimensional\nnormal distribution, the normalizing flow model gains the ability to sample (or\ngenerate) pseudo experimental data from random numbers and to evaluate a\nlog-likelihood value from multidimensional observed events. The normalizing\nflow model can also be extended to take multidimensional conditional variables\nas arguments. Thus, the normalizing flow model can be used as a generator and\nevaluator of pseudo experimental data conditioned by the BSM model parameters.\nThe log-likelihood value, the output of the normalizing flow model, is a\nfunction of the conditional variables. Therefore, the model can quickly\ncalculate gradients of the log-likelihood to the conditional variables.\nFollowing this property, it is expected that the most likely set of conditional\nvariables that reproduce the experimental data, i.e. the optimal set of\nparameters for the BSM model, can be efficiently searched. This paper\ndemonstrates this on a simple dataset and discusses its limitations and future\nextensions.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.13201v1"
    },
    {
        "title": "Electric Field Reconstruction with Information Field Theory",
        "authors": [
            "Simon Strähnz",
            "Tim Huege",
            "Philipp Frank",
            "Torsten Enßlin"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Reconstructing the electric field from the measured voltages in an antenna,\nunfolding the antenna response, comes with several problems. Due to the\nnoisiness of the signal it is often necessary to disregard part of the\nbandwidth of the antenna. It is also not guaranteed, that this system of\nequations can be inverted at all. In any case, the noise of the measurement\nwill be converted into noise on the electric field. This could be solved by\nBayesian inference, however, the electric field is continuous, which would lead\nto an infinite-dimensional latent space. Information field theory (IFT) has\nbeen developed to deal with this problem and allow for Bayesian reasoning on\nfields. It provides a theoretical backbone and effective tools to approach the\ninference as a discrete problem in the continuum limit, taking the continuous\nnature of fields into account. We will present a first working signal model\nthat can be used with IFT-based inference algorithms, which can successfully\nreconstruct the electric field. The model is based on the current understanding\nof air shower emission physics, modelling geomagnetic and charge-excess\nemission and their respective polarisation and spectra separately. Since\nBayesian inference provides the posterior distribution, this method also gives\nan estimate on the uncertainty of the measured field. The performance of this\nmethod will be demonstrated with Monte-Carlo simulations of air shower radio\nsignals.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.14970v2"
    },
    {
        "title": "High Level Reconstruction with Deep Learning using ILD Full Simulation",
        "authors": [
            "Taikan Suehara",
            "Risako Tagami",
            "Lai Gui",
            "Tatsuki Murata",
            "Tomohiko Tanabe",
            "Wataru Ootani",
            "Masaya Ishino"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Deep learning can give a significant impact on physics performance of\nelectron-positron Higgs factories such as ILC and FCCee. We are working on two\ntopics on event reconstruction to apply deep learning. The first is jet flavor\ntagging, in which we apply particle transformer to ILD full simulation to\nobtain jet flavor, including strange tagging. The second is particle flow,\nwhich clusters calorimeter hits and assigns tracks to them to improve jet\nenergy resolution. We modified the algorithm developed in context of CMS HGCAL\nbased on GravNet and Object Condensation techniques and add a track-cluster\nassignment function into the network. The overview and performance of these\nalgorithms are described.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.08772v1"
    },
    {
        "title": "Learning from the past: predicting critical transitions with machine\n  learning trained on surrogates of historical data",
        "authors": [
            "Zhiqin Ma",
            "Chunhua Zeng",
            "Yi-Cheng Zhang",
            "Thomas M. Bury"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Complex systems can undergo critical transitions, where slowly changing\nenvironmental conditions trigger a sudden shift to a new, potentially\ncatastrophic state. Early warning signals for these events are crucial for\ndecision-making in fields such as ecology, biology and climate science. Generic\nearly warning signals motivated by dynamical systems theory have had mixed\nsuccess on real noisy data. More recent studies found that deep learning\nclassifiers trained on synthetic data could improve performance. However,\nneither of these methods take advantage of historical, system-specific data.\nHere, we introduce an approach that trains machine learning classifiers\ndirectly on surrogate data of past transitions, namely surrogate data-based\nmachine learning (SDML). The approach provides early warning signals in\nempirical and experimental data from geology, climatology, sociology, and\ncardiology with higher sensitivity and specificity than two widely used generic\nearly warning signals -- variance and lag-1 autocorrelation. Since the approach\nis trained directly on surrogates of historical data, it is not bound by the\nrestricting assumption of a local bifurcation like previous methods. This\nsystem-specific approach can contribute to improved early warning signals to\nhelp humans better prepare for or avoid undesirable critical transitions.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.09707v1"
    },
    {
        "title": "Datatractor: Metadata, automation, and registries for extractor\n  interoperability in the chemical and materials sciences",
        "authors": [
            "Matthew L. Evans",
            "Gian-Marco Rignanese",
            "David Elbert",
            "Peter Kraus"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Two key issues hindering the transition towards FAIR data science are the\npoor discoverability and inconsistent instructions for the use of data\nextractor tools, i.e., how we go from raw data files created by instruments, to\naccessible metadata and scientific insight. If the existing format conversion\ntools are hard to find, install, and use, their reimplementation will lead to a\nduplication of effort, and an increase in the associated maintenance burden is\ninevitable. The Datatractor framework presented in this work addresses these\nissues. First, by providing a curated registry of such extractor tools their\ndiscoverability will increase. Second, by describing them using a standardised\nbut lightweight schema, their installation and use is machine-actionable.\nFinally, we provide a reference implementation for such data extraction. The\nDatatractor framework can be used to provide a public-facing data extraction\nservice, or be incorporated into other research data management tools providing\nadded value.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.18839v1"
    },
    {
        "title": "AI Meets Antimatter: Unveiling Antihydrogen Annihilations",
        "authors": [
            "Ashley Ferreira",
            "Mahip Singh",
            "Andrea Capra",
            "Ina Carli",
            "Daniel Duque Quiceno",
            "Wojciech T. Fedorko",
            "Makoto M. Fujiwara",
            "Muyan Li",
            "Lars Martin",
            "Yukiya Saito",
            "Gareth Smith",
            "Anqi Xu"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The ALPHA-g experiment at CERN aims to perform the first-ever direct\nmeasurement of the effect of gravity on antimatter, determining its weight to\nwithin 1% precision. This measurement requires an accurate prediction of the\nvertical position of annihilations within the detector. In this work, we\npresent a novel approach to annihilation position reconstruction using an\nensemble of models based on the PointNet deep learning architecture. The newly\ndeveloped model, PointNet Ensemble for Annihilation Reconstruction (PEAR)\noutperforms the standard approach to annihilation position reconstruction,\nproviding more than twice the resolution while maintaining a similarly low\nbias. This work may also offer insights for similar efforts applying deep\nlearning to experiments that require high resolution and low bias.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.00961v2"
    },
    {
        "title": "Numerical Estimation of Limiting Large-Deviation Rate Functions",
        "authors": [
            "Peter Werner",
            "Alexander K. Hartmann"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  For statistics of rare events in systems obeying a large-deviation principle,\nthe rate function is a key quantity. When numerically estimating the rate\nfunction one is always restricted to finite system sizes. Thus, if the interest\nis in the limiting rate function for infinite system sizes, first, several\nsystem sizes have to be studied numerically. Here, rare-event algorithms using\nbiased ensembles give access to the low-probability region. Second, some kind\nof system-size extrapolation has to be performed.\n  Here we demonstrate how rare-event importance sampling schemes can be\ncombined with multi-histogram reweighting, which allows for rather general\napplicability of the approach, independent of specific sampling algorithms. We\nstudy two ways of performing the system-size extrapolation, either directly\nacting on the empirical rate functions, or on the scaled cumulant generating\nfunctions, to obtain the infinite-size limit. The presented method is\ndemonstrated for a binomial distributed variable and the largest connected\ncomponent in Erd\\\"os-R\\'enyi random graphs. Analytical solutions are available\nin both cases for direct comparison. It is observed in particular that phase\ntransitions appearing in the biased ensembles can lead to systematic deviations\nfrom the true result.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.04206v1"
    },
    {
        "title": "Assessing high-order effects in feature importance via predictability\n  decomposition",
        "authors": [
            "Marlis Ontivero-Ortega",
            "Luca Faes",
            "Jesus M Cortes",
            "Daniele Marinazzo",
            "Sebastiano Stramaglia"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Leveraging the large body of work devoted in recent years to describe\nredundancy and synergy in multivariate interactions among random variables, we\npropose a novel approach to quantify cooperative effects in feature importance,\none of the most used techniques for explainable artificial intelligence. In\nparticular, we propose an adaptive version of a well-known metric of feature\nimportance, named Leave One Covariate Out (LOCO), to disentangle high-order\neffects involving a given input feature in regression problems. LOCO is the\nreduction of the prediction error when the feature under consideration is added\nto the set of all the features used for regression. Instead of calculating the\nLOCO using all the features at hand, as in its standard version, our method\nsearches for the multiplet of features that maximize LOCO and for the one that\nminimize it. This provides a decomposition of the LOCO as the sum of a two-body\ncomponent and higher-order components (redundant and synergistic), also\nhighlighting the features that contribute to building these high-order effects\nalongside the driving feature. We report the application to proton/pion\ndiscrimination from simulated detector measures by GEANT.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.09964v1"
    },
    {
        "title": "DMCpy: A powder and single crystal neutron diffraction software for DMC",
        "authors": [
            "Jakob Lass",
            "Samuel Harrison Moody",
            "Øystein Slagtern Fjellvåg"
        ],
        "category": "physics.data-an",
        "published_year": "2025",
        "summary": "  The recently upgraded DMC diffractometer at SINQ, equipped with a\nstate-of-the-art 2D He detector, enables high-resolution neutron diffraction\nexperiments optimized for both powder and single-crystal studies. To address\nthe increased complexity and volume of data produced by this instrument, we\ndeveloped DMCPy, a Python-based software package tailored specifically for DMC\ndata analysis. DMCPy facilitates seamless data reduction and visualization,\nsupporting conversion to reciprocal space, normalization, and masking of\ndetector artifacts. Its modular architecture integrates tools for analyzing\nboth powder diffraction patterns and single-crystal datasets, including\nadvanced visualization features like 3D reciprocal space mapping and\ninteractive scan inspection. By streamlining workflows and enhancing data\ninterpretation, DMCPy empowers researchers to unlock the full potential of the\nDMC instrument for probing nuclear and magnetic structures in condensed matter\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.08845v1"
    },
    {
        "title": "Calibration of Scanning Thermal Microscope using Optimal Estimation of\n  Function Parameters by Iterated Linearization",
        "authors": [
            "Anna Charvátová Campbell",
            "Petr Klapetek",
            "Radek Šlesinger",
            "Jan Martinek",
            "Václav Hortvík",
            "Viktor Witkovský",
            "Gejza Wimmer"
        ],
        "category": "physics.data-an",
        "published_year": "2025",
        "summary": "  Scanning thermal microscopy is a unique tool for the study of thermal\nproperties at the nanoscale. However, calibration of the method is a crucial\nproblem. When analyzing local thermal conductivity, direct calibration is not\npossible and reference samples are used instead. As the calibration dependence\nis non-linear and there are only a few calibration points, this represents a\nmetrological challenge that needs complex data processing. In this contribution\nwe present use of the OEFPIL algorithm for robust and single-step evaluation of\nlocal thermal conductivities and their uncertainties, simplifying this\nprocedure. Furthermore, we test the suitability of SThM calibration for\nautomated measurement.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.08961v1"
    },
    {
        "title": "Reconstructing Time-of-Flight Detector Values of Angular Streaking Using\n  Machine Learning",
        "authors": [
            "David Meier",
            "Wolfram Helml",
            "Thorsten Otto",
            "Bernhard Sick",
            "Jens Viefhaus",
            "Gregor Hartmann"
        ],
        "category": "physics.data-an",
        "published_year": "2025",
        "summary": "  Angular streaking experiments enable for experimentation in the attosecond\nregions. However, the deployed Time-of-flight detectors are susceptible to\nnoise and failure. These shortcomings make the outputs of the Time-of-flight\ndetectors hard to understand for humans and further processing, such as for\nexample the extraction of beam properties. In this article, we present an\napproach to remove high noise levels and reconstruct up to three failed\nTime-of-flight detectors from an arrangement of 16 Time-of-flight detectors.\nDue to its fast evaluation time, the presented method is applicable online\nduring a running experiment. It is trained with simulation data, and we show\nthe results of denoising and reconstruction of our method on real-world\nexperiment data.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.08966v1"
    },
    {
        "title": "Unreal Probabilities: Partial Truth with Clifford Numbers",
        "authors": [
            "Carlos C. Rodriguez"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  This paper introduces and studies the basic properties of Clifford algebra\nvalued conditional measures.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9808010v1"
    },
    {
        "title": "Millisecond and Binary Pulsars as Nature's Frequency Standards. II.\n  Effects of Low-Frequency Timing Noise on Residuals and Measured Parameters",
        "authors": [
            "Sergei M. Kopeikin"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  Pulsars are the most stable natural frequency standards. They can be applied\nto a number of principal problems of modern astronomy and time-keeping\nmetrology. The full exploration of pulsar properties requires obtaining\nunbiased estimates of the spin and orbital parameters. These estimates depend\nessentially on the random noise component being revealed in the residuals of\ntime of arrivals (TOA). In the present paper, the influence of low-frequency\n(\"red\") timing noise with spectral indices from 1 to 6 on TOA residuals,\nvariances, and covariances of estimates of measured parameters of single and\nbinary pulsars are studied. In order to determine their functional dependence\non time, an analytic technique of processing of observational data in time\ndomain is developed which takes into account both stationary and non-stationary\ncomponents of noise. Our analysis includes a simplified timing model of a\nbinary pulsar in a circular orbit and procedure of estimation of pulsar\nparameters and residuals under the influence of red noise. We reconfirm that\nuncorrelated white noise of errors of measurements of TOA brings on gradually\ndecreasing residuals, variances and covariances of all parameters. On the other\nhand, we show that any red noise causes the residuals, variances, and\ncovariances of certain parameters to increase with time. Hence, the low\nfrequency noise corrupts our observations and reduces experimental\npossibilities for better tests of General Relativity Theory. We also treat in\ndetail the influence of a polynomial drift of noise on the residuals and\nfitting parameters. Results of the analitic analysis are used for discussion of\na statistic describing stabilities of kinematic and dynamic pulsar time scales.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9811014v2"
    },
    {
        "title": "New Physics Discovery Potential in Future Experiments",
        "authors": [
            "S. I. Bityukov",
            "N. V. Krasnikov"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  We propose a method to estimate the probability of new physics discovery in\nfuture high energy physics experiments. Physics simulation gives both the\naverage numbers <Nb> of background and <Ns> of signal events. We find that the\nproper definition of the significance for <Nb>, <Ns> >> 1 is S_{12} =\nsqrt(<Ns>+<Nb>) - sqrt(<Nb>) in comparison with often used significances S_1 =\n<Ns>/sqrt(<Nb>) and S_2 = <N_s>/sqrt(<Ns> + <Nb>). We propose a method for\ntaking into account the systematical errors related to nonexact knowledge of\nbackground and signal cross sections. An account of such systematics is very\nessential in the search for supersymmetry at LHC. We also propose a method for\nestimation of exclusion limits on new physics in future experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9811025v3"
    },
    {
        "title": "Millisecond and Binary Pulsars as Nature's Frequency Standards. III.\n  Fourier Analysis and Spectral Sensitivity of Timing Observations to\n  Low-Frequency Noise",
        "authors": [
            "Sergei M. Kopeikin",
            "Vladimir A. Potapov"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  Millisecond and binary pulsars are the most stable natural frequency\nstandards which admits to introduce modified versions of universal and\nephemeris time scales based correspondingly on the intrinsic rotation of pulsar\nand on its orbital motion around barycenter of a binary system. Measured\nstability of these time scales depends on numerous physical phenomena which\naffect rotational and orbital motion of the pulsar and observer on the Earth,\nperturb propagation of electromagnetic pulses from pulsar to the observer and\nbring about random fluctuations in the rate of atomic clock used as a primary\ntime reference in timing observations. On the long time intervals the main\nreason for the instability of the pulsar time scales is the presence of\ncorrelated, low-frequency timing noise in residuals of times of arrivals (TOA)\nof pulses from the pulsar which has both astrophysical and geophysical origin.\nHence, the timing noise can carry out the important physical information about\ninterstellar medium, interior structure of the pulsar, stochastic gravitational\nwaves coming from the early universe, etc. Each specific type of the\nlow-frequency noise can be described in framework of power law spectrum model.\nAlthough the data processing of pulsar timing observations in time domain seems\nto be the most imformative it is significantly important to know to which\nspectral bands single and binary pulsars, considered as detectors of the\nlow-frequency noise signal, are the most sensitive. The given problem is\nexamined in the present article.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9812050v1"
    },
    {
        "title": "Universal geometric approach to uncertainty, entropy and information",
        "authors": [
            "Michael J. W. Hall"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  It is shown that for any ensemble, whether classical or quantum, continuous\nor discrete, there is only one measure of the \"volume\" of the ensemble that is\ncompatible with several basic geometric postulates. This volume measure is thus\na preferred and universal choice for characterising the inherent spread,\ndispersion, localisation, etc, of the ensemble. Remarkably, this unique\n\"ensemble volume\" is a simple function of the ensemble entropy, and hence\nprovides a new geometric characterisation of the latter quantity. Applications\ninclude unified, volume-based derivations of the Holevo and Shannon bounds in\nquantum and classical information theory; a precise geometric interpretation of\nthermodynamic entropy for equilibrium ensembles; a geometric derivation of\nsemi-classical uncertainty relations; a new means for defining classical and\nquantum localization for arbitrary evolution processes; a geometric\ninterpretation of relative entropy; and a new proposed definition for the\nspot-size of an optical beam. Advantages of the ensemble volume over other\nmeasures of localization (root-mean-square deviation, Renyi entropies, and\ninverse participation ratio) are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9903045v2"
    },
    {
        "title": "Teaching statistics in the physics curriculum: Unifying and clarifying\n  role of subjective probability",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "1999",
        "summary": "  Subjective probability is based on the intuitive idea that probability\nquantifies the degree of belief that an event will occur. A probability theory\nbased on this idea represents the most general framework for handling\nuncertainty. A brief introduction to subjective probability and Bayesian\ninference is given, with comments on typical misconceptions which tend to\ndiscredit it and comparisons to other approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9908014v2"
    },
    {
        "title": "Nonlinear denoising of transient signals with application to event\n  related potentials",
        "authors": [
            "A. Effern",
            "K. Lehnertz",
            "T. Schreiber",
            "T. Grunwald",
            "P. David",
            "C. E. Elger"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  We present a new wavelet based method for the denoising of {\\it event related\npotentials} ERPs), employing techniques recently developed for the paradigm of\ndeterministic chaotic systems. The denoising scheme has been constructed to be\nappropriate for short and transient time sequences using circular state space\nembedding. Its effectiveness was successfully tested on simulated signals as\nwell as on ERPs recorded from within a human brain. The method enables the\nstudy of individual ERPs against strong ongoing brain electrical activity.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0001069v1"
    },
    {
        "title": "Applying MDL to Learning Best Model Granularity",
        "authors": [
            "Qiong Gao",
            "Ming Li",
            "Paul Vitanyi"
        ],
        "category": "physics.data-an",
        "published_year": "2000",
        "summary": "  The Minimum Description Length (MDL) principle is solidly based on a provably\nideal method of inference using Kolmogorov complexity. We test how the theory\nbehaves in practice on a general problem in model selection: that of learning\nthe best model granularity. The performance of a model depends critically on\nthe granularity, for example the choice of precision of the parameters. Too\nhigh precision generally involves modeling of accidental noise and too low\nprecision may lead to confusion of models that should be distinguished. This\nprecision is often determined ad hoc. In MDL the best model is the one that\nmost compresses a two-part code of the data set: this embodies ``Occam's\nRazor.'' In two quite different experimental settings the theoretical value\ndetermined using MDL coincides with the best value found experimentally. In the\nfirst experiment the task is to recognize isolated handwritten characters in\none subject's handwriting, irrespective of size and orientation. Based on a new\nmodification of elastic matching, using multiple prototypes per character, the\noptimal prediction rate is predicted for the learned parameter (length of\nsampling interval) considered most likely by MDL, which is shown to coincide\nwith the best value found experimentally. In the second experiment the task is\nto model a robot arm with two degrees of freedom using a three layer\nfeed-forward neural network where we need to determine the number of nodes in\nthe hidden layer giving best modeling performance. The optimal model (the one\nthat extrapolizes best on unseen examples) is predicted for the number of nodes\nin the hidden layer considered most likely by MDL, which again is found to\ncoincide with the best value found experimentally.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0005062v1"
    },
    {
        "title": "Image reconstruction without prior information",
        "authors": [
            "Keith S Cover"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  A novel framework for designing image reconstruction algorithms for linear\nforward problems is proposed. The framework is based on the novel concept of\nconserving the information in the data during image reconstruction rather than\nsupplementing it with prior information. The framework offers an explanation as\nto why the popular reconstruction algorithms for MRI, CT and convolution are\ngenerally expressible as left invertible matrices. Also, the framework can be\nused to improve linear deconvolution and tackle such stubborn linear inverse\nproblems as the Laplace transform.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0101058v3"
    },
    {
        "title": "Information-theoretic approach to the study of control systems",
        "authors": [
            "Hugo Touchette",
            "Seth Lloyd"
        ],
        "category": "physics.data-an",
        "published_year": "2001",
        "summary": "  We propose an information-theoretic framework for analyzing control systems\nbased on the close relationship of controllers to communication channels. A\ncommunication channel takes an input state and transforms it into an output\nstate. A controller, similarly, takes the initial state of a system to be\ncontrolled and transforms it into a target state. In this sense, a controller\ncan be thought of as an actuation channel that acts on inputs to produce\ndesired outputs. In this transformation process, two different control\nstrategies can be adopted: (i) the controller applies an actuation dynamics\nthat is independent of the state of the system to be controlled (open-loop\ncontrol); or (ii) the controller enacts an actuation dynamics that is based on\nsome information about the state of the controlled system (closed-loop\ncontrol). Using this communication channel model of control, we provide\nnecessary and sufficient conditions for a system to be perfectly controllable\nand perfectly observable in terms of information and entropy. In addition, we\nderive a quantitative trade-off between the amount of information gathered by a\nclosed-loop controller and its relative performance advantage over an open-loop\ncontroller in stabilizing a system. This work supplements earlier results [H.\nTouchette, S. Lloyd, Phys. Rev. Lett. 84, 1156 (2000)] by providing new\nderivations of the advantage afforded by closed-loop control and by proposing\nan information-based optimality criterion for control systems. New applications\nof this approach pertaining to proportional controllers, and the control of\nchaotic maps are also presented.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0104007v2"
    },
    {
        "title": "Mark correlations: relating physical properties to spatial distributions",
        "authors": [
            "Claus Beisbart",
            "Martin Kerscher",
            "Klaus Mecke"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Mark correlations provide a systematic approach to look at objects both\ndistributed in space and bearing intrinsic information, for instance on\nphysical properties. The interplay of the objects' properties (marks) with the\nspatial clustering is of vivid interest for many applications; are, e.g.,\ngalaxies with high luminosities more strongly clustered than dim ones? Do\nneighbored pores in a sandstone have similar sizes? How does the shape of\nimpact craters on a planet depend on the geological surface properties? In this\narticle, we give an introduction into the appropriate mathematical framework to\ndeal with such questions, i.e. the theory of marked point processes. After\nhaving clarified the notion of segregation effects, we define universal test\nquantities applicable to realizations of a marked point processes. We show\ntheir power using concrete data sets in analyzing the luminosity-dependence of\nthe galaxy clustering, the alignment of dark matter halos in gravitational\n$N$-body simulations, the morphology- and diameter-dependence of the Martian\ncrater distribution and the size correlations of pores in sandstone. In order\nto understand our data in more detail, we discuss the Boolean depletion model,\nthe random field model and the Cox random field model. The first model\ndescribes depletion effects in the distribution of Martian craters and pores in\nsandstone, whereas the last one accounts at least qualitatively for the\nobserved luminosity-dependence of the galaxy clustering.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0201069v1"
    },
    {
        "title": "Vector- and tensor-valued descriptors for spatial patterns",
        "authors": [
            "Claus Beisbart",
            "Robert Dahlke",
            "Klaus Mecke",
            "Herbert Wagner"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Higher-rank Minkowski valuations are efficient means for describing the\ngeometry and connectivity of spatial patterns. We show how to extend the\nframework of the scalar Minkowski valuations to vector- and tensor-valued\nmeasures. The properties of these extensions are described in detail. We show\nthe versatility of these measures by using simple toy models as well as real\ndata. Our applications cover the morphology of galaxy clusters, the structure\nof spiral galaxies, and the geometry of molecules. Furthermore, we consider a\nphysical ansatz closely related to higher-rank Minkowski valuations, the\nRosenfeld functional known from density functional theory.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0203072v1"
    },
    {
        "title": "Source separation as an exercise in logical induction",
        "authors": [
            "Kevin H. Knuth"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  We examine the relationship between the Bayesian and information-theoretic\nformulations of source separation algorithms. This work makes use of the\nrelationship between the work of Claude E. Shannon and the \"Recent\nContributions\" by Warren Weaver (Shannon & Weaver 1949) as clarified by Richard\nT. Cox (1979) and expounded upon by Robert L. Fry (1996) as a duality between a\nlogic of assertions and a logic of questions. Working with the logic of\nassertions requires the use of probability as a measure of degree of\nimplication. This leads to a Bayesian formulation of the problem. Whereas,\nworking with the logic of questions requires the use of entropy as a measure of\nthe bearing of a question on an issue leading to an information-theoretic\nformulation of the problem.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0204075v1"
    },
    {
        "title": "Bayesian Source Separation and Localization",
        "authors": [
            "Kevin H. Knuth"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  The problem of mixed signals occurs in many different contexts; one of the\nmost familiar being acoustics. The forward problem in acoustics consists of\nfinding the sound pressure levels at various detectors resulting from sound\nsignals emanating from the active acoustic sources. The inverse problem\nconsists of using the sound recorded by the detectors to separate the signals\nand recover the original source waveforms. In general, the inverse problem is\nunsolvable without additional information. This general problem is called\nsource separation, and several techniques have been developed that utilize\nmaximum entropy, minimum mutual information, and maximum likelihood. In\nprevious work, it has been demonstrated that these techniques can be recast in\na Bayesian framework. This paper demonstrates the power of the Bayesian\napproach, which provides a natural means for incorporating prior information\ninto a source model. An algorithm is developed that utilizes information\nregarding both the statistics of the amplitudes of the signals emitted by the\nsources and the relative locations of the detectors. Using this prior\ninformation, the algorithm finds the most probable source behavior and\nconfiguration. Thus, the inverse problem can be solved by simultaneously\nperforming source separation and localization. It should be noted that this\nalgorithm is not designed to account for delay times that are often important\nin acoustic source separation. However, a possible application of this\nalgorithm is in the separation of electrophysiological signals obtained using\nelectroencephalography (EEG) and magnetoencephalography (MEG).\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0205069v1"
    },
    {
        "title": "Wireless communications with diffuse waves",
        "authors": [
            "S. E. Skipetrov"
        ],
        "category": "physics.data-an",
        "published_year": "2002",
        "summary": "  Diffuse, multiple-scattered waves can be very efficient for information\ntransfer through disordered media, provided that antenna arrays are used for\nboth transmission and reception of signals. Information capacity C of a\ncommunication channel between two identical linear arrays of n equally-spaced\nantennas, placed in a disordered medium with diffuse scattering, grows linearly\nwith n and can attain considerable values, if antenna spacing a > lambda/2,\nwhere lambda is the wavelength. Decrease of a below lambda/2 makes the signals\nreceived by different antennas partially correlated, thus introducing\nredundancy and reducing capacity of the communication system. When the size of\nantenna arrays is well below lambda/2, the scaling of C with n becomes\nlogarithmic and capacity is low.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0207071v1"
    },
    {
        "title": "Scaling in Non-stationary time series I",
        "authors": [
            "M. Ignaccolo",
            "P. Allegrini",
            "P. Grigolini",
            "P. Hamilton",
            "B. J. West"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  Most data processing techniques, applied to biomedical and sociological time\nseries, are only valid for random fluctuations that are stationary in time.\nUnfortunately, these data are often non stationary and the use of techniques of\nanalysis resting on the stationary assumption can produce a wrong information\non the scaling, and so on the complexity of the process under study. Herein, we\ntest and compare two techniques for removing the non-stationary influences from\ncomputer generated time series, consisting of the superposition of a slow\nsignal and a random fluctuation. The former is based on the method of wavelet\ndecomposition, and the latter is a proposal of this paper, denoted by us as\nstep detrending technique. We focus our attention on two cases, when the slow\nsignal is a periodic function mimicking the influence of seasons, and when it\nis an aperiodic signal mimicking the influence of a population change (increase\nor decrease). For the purpose of computational simplicity the random\nfluctuation is taken to be uncorrelated. However, the detrending techniques\nhere illustrated work also in the case when the random component is correlated.\nThis expectation is fully confirmed by the sociological applications made in\nthe companion paper. We also illustrate a new procedure to assess the existence\nof a genuine scaling, based on the adoption of diffusion entropy, multiscaling\nanalysis and the direct assessment of scaling. Using artificial sequences, we\nshow that the joint use of all these techniques yield the detection of the real\nscaling, and that this is independent of the technique used to detrend the\noriginal signal.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0301057v1"
    },
    {
        "title": "Scaling in Non-stationary Time Series II: Teen Birth Phenomenon",
        "authors": [
            "M. Ignaccolo",
            "P. Allegrini",
            "P. Grigolini",
            "P. Hamilton",
            "B. J. West"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  This paper is devoted to the problem of statistical mechanics raised by the\nanalysis of an issue of sociological interest: the teen birth phenomenon. It is\nexpected that these data are characterized by correlated fluctuations,\nreflecting the cooperative properties of the process. However, the assessment\nof the anomalous scaling generated by these correlations is made difficult, and\nambiguous as well, by the non-stationary nature of the data that show a clear\ndependence on seasonal periodicity (periodic component) and an average changing\nslowly in time (slow component), as well. We use the detrending techniques\ndescribed in the companion paper \\cite{paper1}, to safely remove all the biases\nand to derive the genuine scaling of the teen birth phenomenon.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0301058v1"
    },
    {
        "title": "Power and beauty of interval methods",
        "authors": [
            "Marek W. Gutowski"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  Interval calculus is a relatively new branch of mathematics. Initially\nunderstood as a set of tools to assess the quality of numerical calculations\n(rigorous control of rounding errors), it became a discipline in its own rights\ntoday. Interval methods are usefull whenever we have to deal with\nuncertainties, which can be rigorously bounded. Fuzzy sets, rough sets and\nprobability calculus can perform similar tasks, yet only the interval methods\nare able to (dis)prove, with mathematical rigor, the (non)existence of desired\nsolution(s). Known are several problems, not presented here, which cannot be\neffectively solved by any other means.\n  This paper presents basic notions and main ideas of interval calculus and two\nexamples of useful algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0302034v2"
    },
    {
        "title": "The uniformly most powerful test of statistical significance for\n  counting-type experiments with background",
        "authors": [
            "L. Fleysher",
            "R. Fleysher",
            "T. J. Haines",
            "A. I. Mincer",
            "P. Nemethy"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  In this paper, after a discussion of general properties of statistical tests,\nwe present the construction of the most powerful hypothesis test for\ndetermining the existence of a new phenomenon in counting-type experiments\nwhere the observed Poisson process is subject to a Poisson distributed\nbackground with unknown mean.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0306146v1"
    },
    {
        "title": "Spatial methods for event reconstruction in CLEAN",
        "authors": [
            "Kevin J. Coakley",
            "Daniel N. McKinsey"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  In CLEAN (Cryogenic Low Energy Astrophysics with Noble gases), a proposed\nneutrino and dark matter detector, background discrimination is possible if one\ncan determine the location of an ionizing radiation event with high accuracy.\nWe simulate ionizing radiation events that produce multiple scintillation\nphotons within a spherical detection volume filled with liquid neon. We\nestimate the radial location of a particular ionizing radiation event based on\nthe observed count data corresponding to that event. The count data are\ncollected by detectors mounted at the spherical boundary of the detection\nvolume. We neglect absorption, but account for Rayleigh scattering. To account\nfor wavelength-shifting of the scintillation light, we assume that photons are\nabsorbed and re-emitted at the detectors. Here, we develop spatial Maximum\nLikelihood methods for event reconstruction, and study their performance in\ncomputer simulation experiments. We also study a method based on the centroid\nof the observed count data. We calibrate our estimates based on training data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0309033v1"
    },
    {
        "title": "Pouzyry: a novel class of algorithms for restoring a function from a\n  random sample",
        "authors": [
            "F. V. Tkachov"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  A novel class of algorithms for restoring a function from a random sample is\nbased on the concept of weak convergence, borrows algorithmic solutions from\nthe Optimal Jet Finder (hep-ph/0301185), offers a considerable algorithmic\nflexibility, is applicable to non-positive functions, is insensitive to the\nchoice of coordinate axes. A first implementation demonstrates feasibility of\nthe approach.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0401012v3"
    },
    {
        "title": "Limits and Confidence Intervals in the Presence of Nuisance Parameters",
        "authors": [
            "Wolfgang A. Rolke",
            "Angel M. Lopez",
            "Jan Conrad"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We study the frequentist properties of confidence intervals computed by the\nmethod known to statisticians as the Profile Likelihood. It is seen that the\ncoverage of these intervals is surprisingly good over a wide range of possible\nparameter values for important classes of problems, in particular whenever\nthere are additional nuisance parameters with statistical or systematic errors.\nPrograms are available for calculating these intervals.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0403059v5"
    },
    {
        "title": "Asymmetric Uncertainties: Sources, Treatment and Potential Dangers",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  The issue of asymmetric uncertainties resulting from fits, nonlinear\npropagation and systematic effects is reviewed. It is shown that, in all cases,\nwhenever a published result is given with asymmetric uncertainties, the value\nof the physical quantity of interest is biased with respect to what would be\nobtained using at best all experimental and theoretical information that\ncontribute to evaluate the combined uncertainty. The probabilistic solution to\nthe problem is provided both in exact and in approximated forms.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0403086v2"
    },
    {
        "title": "Statistical properties of acoustic emission signals from metal cutting\n  processes",
        "authors": [
            "F. A. Farrelly",
            "A. Petri",
            "L. Pitolli",
            "G. Pontuale",
            "A. Tagliani",
            "P. L. Novi Inverardi"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Acoustic Emission (AE) data from single point turning machining are analysed\nin this paper in order to gain a greater insight of the signal statistical\nproperties for Tool Condition Monitoring (TCM) applications. A statistical\nanalysis of the time series data amplitude and root mean square (RMS) value at\nvarious tool wear levels are performed, &#65533;nding that ageing features can\nbe revealed in all cases from the observed experimental histograms. In\nparticular, AE data amplitudes are shown to be distributed with a power-law\nbehaviour above a cross-over value. An analytic model for the RMS values\nprobability density function (pdf) is obtained resorting to the Jaynes' maximum\nentropy principle (MEp); novel technique of constraining the modelling function\nunder few fractional moments, instead of a greater amount of ordinary moments,\nleads to well-tailored functions for experimental histograms.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0404128v1"
    },
    {
        "title": "Extending Granger causality to nonlinear systems",
        "authors": [
            "Nicola Ancona",
            "Daniele Marinazzo",
            "Sebastiano Stramaglia"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We consider extension of Granger causality to nonlinear bivariate time\nseries. In this frame, if the prediction error of the first time series is\nreduced by including measurements from the second time series, then the second\ntime series is said to have a causal influence on the first one. Not all the\nnonlinear prediction schemes are suitable to evaluate causality, indeed not all\nof them allow to quantify how much the knowledge of the other time series\ncounts to improve prediction error. We present a novel approach with bivariate\ntime series modelled by a generalization of radial basis functions and show its\napplication to a pair of unidirectionally coupled chaotic maps and to a\nphysiological example.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0405009v1"
    },
    {
        "title": "On Bayesian Treatment of Systematic Uncertainties in Confidence Interval\n  Calculation",
        "authors": [
            "Fredrik Tegenfeldt",
            "Jan Conrad"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  In high energy physics, a widely used method to treat systematic\nuncertainties in confidence interval calculations is based on combining a\nfrequentist construction of confidence belts with a Bayesian treatment of\nsystematic uncertainties. In this note we present a study of the coverage of\nthis method for the standard Likelihood Ratio (aka Feldman & Cousins)\nconstruction for a Poisson process with known background and Gaussian or\nlog-Normal distributed uncertainties in the background or signal efficiency.\nFor uncertainties in the signal efficiency of upto 40 % we find over-coverage\non the level of 2 to 4 % depending on the size of uncertainties and the region\nin signal space. Uncertainties in the background generally have smaller effect\non the coverage. A considerable smoothing of the coverage curves is observed. A\nsoftware package is presented which allows fast calculation of the confidence\nintervals for a variety of assumptions on shape and size of systematic\nuncertainties for different nuisance parameters. The calculation speed allows\nexperimenters to test the coverage for their specific conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0408039v2"
    },
    {
        "title": "Statistics of transition times, phase diffusion and synchronization in\n  periodically driven bistable systems",
        "authors": [
            "Peter Talkner",
            "Lukasz Machura",
            "Michael Schindler",
            "Peter Hanggi",
            "Jerzy Luczka"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  The statistics of transitions between the metastable states of a periodically\ndriven bistable Brownian oscillator are investigated on the basis of a\ntwo-state description by means of a master equation with time-dependent rates.\nThe results are compared with extensive numerical simulations of the Langevin\nequation for a sinusoidal driving force. Very good agreement is achieved both\nfor the counting statistics of the number of transitions and the residence time\ndistribution of the process in either state. The counting statistics\ncorroborate in a consistent way the interpretation of stochastic resonance as a\nsynchronisation phenomenon for a properly defined generalized Rice phase.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0409065v3"
    },
    {
        "title": "From Observations to Hypotheses: Probabilistic Reasoning Versus\n  Falsificationism and its Statistical Variations",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  Testing hypotheses is an issue of primary importance in the scientific\nresearch, as well as in many other human activities. Much clarification about\nit can be achieved if the process of learning from data is framed in a\nstochastic model of causes and effects. Formulated with Poincare's words, the\n\"essential problem of the experimental method\" becomes then solving a \"problem\nin the probability of causes\", i.e. ranking the several hypotheses, that might\nbe responsible for the observations, in credibility. This probabilistic\napproach to the problem (nowadays known as the Bayesian approach) differs from\nthe standard (i.e. frequentistic) statistical methods of hypothesis tests. The\nlatter methods might be seen as practical attempts of implementing the ideal of\nfalsificationism, that can itself be viewed as an extension of the proof by\ncontradiction of the classical logic to the experimental method. Some\ncriticisms concerning conceptual as well as practical aspects of na\\\"\\i ve\nfalsificationism and conventional, frequentistic hypothesis tests are\npresented, and the alternative, probabilistic approach is outlined.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0412148v2"
    },
    {
        "title": "Performance of an Operating High Energy Physics Data Grid: D0SAR-Grid",
        "authors": [
            "B. Abbott",
            "P. Baringer",
            "T. Bolton",
            "Z. Greenwood",
            "E. Gregores",
            "H. Kim",
            "C. Leangsuksun",
            "D. Meyer",
            "N. Mondal",
            "S. Novaes",
            "B. Quinn",
            "H. Severini",
            "P. Skubic",
            "J. Snow",
            "M. Sosebee",
            "J. Yu"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  The D0 experiment at Fermilab's Tevatron will record several petabytes of\ndata over the next five years in pursuing the goals of understanding nature and\nsearching for the origin of mass. Computing resources required to analyze these\ndata far exceed capabilities of any one institution. Moreover, the widely\nscattered geographical distribution of D0 collaborators poses further serious\ndifficulties for optimal use of human and computing resources. These\ndifficulties will exacerbate in future high energy physics experiments, like\nthe LHC. The computing grid has long been recognized as a solution to these\nproblems. This technology is being made a more immediate reality to end users\nin D0 by developing a grid in the D0 Southern Analysis Region (D0SAR),\nD0SAR-Grid, using all available resources within it and a home-grown local task\nmanager, McFarm. We will present the architecture in which the D0SAR-Grid is\nimplemented, the use of technology and the functionality of the grid, and the\nexperience from operating the grid in simulation, reprocessing and data\nanalyses for a currently running HEP experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0501164v1"
    },
    {
        "title": "Finding local community structure in networks",
        "authors": [
            "Aaron Clauset"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  Although the inference of global community structure in networks has recently\nbecome a topic of great interest in the physics community, all such algorithms\nrequire that the graph be completely known. Here, we define both a measure of\nlocal community structure and an algorithm that infers the hierarchy of\ncommunities that enclose a given vertex by exploring the graph one vertex at a\ntime. This algorithm runs in time O(d*k^2) for general graphs when $d$ is the\nmean degree and k is the number of vertices to be explored. For graphs where\nexploring a new vertex is time-consuming, the running time is linear, O(k). We\nshow that on computer-generated graphs this technique compares favorably to\nalgorithms that require global knowledge. We also use this algorithm to extract\nmeaningful local clustering information in the large recommender network of an\nonline retailer and show the existence of mesoscopic structure.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0503036v1"
    },
    {
        "title": "A New Technique for Finding Needles in Haystacks: A Geometric Approach\n  to Distinguishing Between a New Source and Random Fluctuations",
        "authors": [
            "Ramani S. Pilla",
            "Catherine Loader",
            "Cyrus Taylor"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We propose a new test statistic based on a score process for determining the\nstatistical significance of a putative signal that may be a small perturbation\nto a noisy experimental background. We derive the reference distribution for\nthis score test statistic; it has an elegant geometrical interpretation as well\nas broad applicability. We illustrate the technique in the context of a model\nproblem from high-energy particle physics. Monte Carlo experimental results\nconfirm that the score test results in a significantly improved rate of signal\ndetection.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0505200v1"
    },
    {
        "title": "Sifting data in the real world",
        "authors": [
            "Martin M. Block"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  In the real world, experimental data are rarely, if ever, distributed as a\nnormal (Gaussian) distribution. As an example, a large set of data--such as the\ncross sections for particle scattering as a function of energy contained in the\narchives of the Particle Data Group--is a compendium of all published data, and\nhence, unscreened. Inspection of similar data sets quickly shows that, for many\nreasons, these data sets have many outliers--points well beyond what is\nexpected from a normal distribution--thus ruling out the use of conventional\n$\\chi^2$ techniques. This note suggests an adaptive algorithm that allows a\nphenomenologist to apply to the data sample a sieve whose mesh is coarse enough\nto let the background fall through, but fine enough to retain the preponderance\nof the signal, thus sifting the data. A prescription is given for finding a\nrobust estimate of the best-fit model parameters in the presence of a noisy\nbackground, together with a robust estimate of the model parameter errors, as\nwell as a determination of the goodness-of-fit of the data to the theoretical\nhypothesis. Extensive computer simulations are carried out to test the\nalgorithm for both its accuracy and stability under varying background\nconditions.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0506010v2"
    },
    {
        "title": "Ab initio yield curve dynamics",
        "authors": [
            "Raymond J. Hawkins",
            "B. Roy Frieden",
            "Joseph L. D'Anna"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We derive an equation of motion for interest-rate yield curves by applying a\nminimum Fisher information variational approach to the implied probability\ndensity. By construction, solutions to the equation of motion recover observed\nbond prices. More significantly, the form of the resulting equation explains\nthe success of the Nelson Siegel approach to fitting static yield curves and\nthe empirically observed modal structure of yield curves. A practical numerical\nimplementation of this equation of motion is found by using the Karhunen-Loeve\nexpansion and Galerkin's method to formulate a reduced-order model of yield\ncurve dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0507098v1"
    },
    {
        "title": "Financial Applications of Random Matrix Theory: Old Laces and New Pieces",
        "authors": [
            "M. Potters",
            "J. P. Bouchaud",
            "L. Laloux"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  This contribution to the proceedings of the Cracow meeting on `Applications\nof Random Matrix Theory' summarizes a series of studies, some old and others\nmore recent on financial applications of Random Matrix Theory (RMT). We first\nreview some early results in that field, with particular emphasis on the\napplications of correlation cleaning to portfolio optimisation, and discuss the\nextension of the Marcenko-Pastur (MP) distribution to a non trivial `true'\nunderlying correlation matrix. We then present new results concerning different\nproblems that arise in a financial context: (a) the generalisation of the MP\nresult to the case of an empirical correlation matrix (ECM) constructed using\nexponential moving averages, for which we give a new elegant derivation (b) the\nspecific dynamics of the `market' eigenvalue and its associated eigenvector,\nwhich defines an interesting Ornstein-Uhlenbeck process on the unit sphere and\n(c) the problem of the dependence of ECM's on the observation frequency of the\nreturns and its interpretation in terms of lagged cross-influences.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0507111v1"
    },
    {
        "title": "Trend followers lose more often than they gain",
        "authors": [
            "Marc Potters",
            "Jean-Philippe Bouchaud"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We solve exactly a simple model of trend following strategy, and obtain the\nanalytical shape of the profit per trade distribution. This distribution is non\ntrivial and has an option like, asymmetric structure. The degree of asymmetry\ndepends continuously on the parameters of the strategy and on the volatility of\nthe traded asset. While the average gain per trade is always exactly zero, the\nfraction f of winning trades decreases from f=1/2 for small volatility to f=0\nfor high volatility, showing that this winning probability does not give any\ninformation on the reliability of the strategy but is indicative of the trading\nstyle.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0508104v1"
    },
    {
        "title": "Small scale behavior of financial data",
        "authors": [
            "Andreas P. Nawroth",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  A new approach is presented to describe the change in the statistics of the\nlog return distribution of financial data as a function of the timescale. To\nthis purpose a measure is introduced, which quantifies the distance of a\nconsidered distribution to a reference distribution. The existence of a small\ntimescale regime is demonstrated, which exhibits different properties compared\nto the normal timescale regime. This regime seems to be universal for\nindividual stocks. It is shown that the existence of this small timescale\nregime is not dependent on the special choice of the distance measure or the\nreference distribution. These findings have important implications for risk\nanalysis, in particular for the probability of extreme events.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0509257v2"
    },
    {
        "title": "Spatial Random Field Models Inspired from Statistical Physics with\n  Applications in the Geosciences",
        "authors": [
            "D. T. Hristopulos"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  The spatial structure of fluctuations in spatially inhomogeneous processes\ncan be modeled in terms of Gibbs random fields. A local low energy estimator\n(LLEE) is proposed for the interpolation (prediction) of such processes at\npoints where observations are not available. The LLEE approximates the spatial\ndependence of the data and the unknown values at the estimation points by\nlow-lying excitations of a suitable energy functional. It is shown that the\nLLEE is a linear, unbiased, non-exact estimator. In addition, an expression for\nthe uncertainty (standard deviation) of the estimate is derived.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0510035v4"
    },
    {
        "title": "Time series of stock price and of two fractal overlap: Anticipating\n  market crashes?",
        "authors": [
            "Bikas K. Chakrabarti",
            "Arnab Chatterjee",
            "Pratip Bhattacharyya"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We find prominent similarities in the features of the time series for the\noverlap of two Cantor sets when one set moves with uniform relative velocity\nover the other and time series of stock prices. An anticipation method for some\nof the crashes have been proposed here, based on these observations.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0510047v1"
    },
    {
        "title": "Large dimension forecasting models and random singular value spectra",
        "authors": [
            "Jean-Philippe Bouchaud",
            "Laurent Laloux",
            "M. Augusta Miceli",
            "Marc Potters"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We present a general method to detect and extract from a finite time sample\nstatistically meaningful correlations between input and output variables of\nlarge dimensionality. Our central result is derived from the theory of free\nrandom matrices, and gives an explicit expression for the interval where\nsingular values are expected in the absence of any true correlations between\nthe variables under study. Our result can be seen as the natural generalization\nof the Marcenko-Pastur distribution for the case of rectangular correlation\nmatrices. We illustrate the interest of our method on a set of macroeconomic\ntime series.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0512090v1"
    },
    {
        "title": "What is the most competitive sport?",
        "authors": [
            "E. Ben-Naim",
            "F. Vazquez",
            "S. Redner"
        ],
        "category": "physics.data-an",
        "published_year": "2005",
        "summary": "  We present an extensive statistical analysis of the results of all sports\ncompetitions in five major sports leagues in England and the United States. We\ncharacterize the parity among teams by the variance in the winning fraction\nfrom season-end standings data and quantify the predictability of games by the\nfrequency of upsets from game results data. We introduce a mathematical model\nin which the underdog team wins with a fixed upset probability. This model\nquantitatively relates the parity among teams with the predictability of the\ngames, and it can be used to estimate the upset frequency from standings data.\nWe propose the likelihood of upsets as a measure of competitiveness.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0512143v1"
    },
    {
        "title": "Non Poisson intermittent events in price formation",
        "authors": [
            "Antonella Greco",
            "Luca Sorriso-Valvo",
            "Vincenzo Carbone"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The formation of price in a financial market is modelled as a chain of Ising\nspin with three fundamental figures of trading. We investigate the time\nbehaviour of the model, and we compare the results with the real EURO/USD\nchange rate. By using the test of local Poisson hypothesis, we show that this\nminimal model leads to clustering and \"declustering\" in the volatility signal,\ntypical of the real market data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0601047v1"
    },
    {
        "title": "A nonextensive approach to the dynamics of financial observables",
        "authors": [
            "Silvio M. Duarte Queiros",
            "Luis G. Moyano",
            "Jeferson de Souza",
            "Constantino Tsallis"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We present results about financial market observables, specifically returns\nand traded volumes. They are obtained within the current nonextensive\nstatistical mechanical framework based on the entropy\n$S_{q}=k\\frac{1-\\sum\\limits_{i=1}^{W} p_{i} ^{q}}{1-q} (q\\in \\Re)$ ($S_{1}\n\\equiv S_{BG}=-k\\sum\\limits_{i=1}^{W}p_{i} \\ln p_{i}$). More precisely, we\npresent stochastic dynamical mechanisms which mimic probability density\nfunctions empirically observed. These mechanisms provide possible\ninterpretations for the emergence of the entropic indices $q$ in the time\nevolution of the corresponding observables. In addition to this, through\nmulti-fractal analysis of return time series, we verify that the dual relation\n$q_{stat}+q_{sens}=2$ is numerically satisfied, $q_{stat}$ and $q_{sens}$ being\nassociated to the probability density function and to the sensitivity to\ninitial conditions respectively. This type of simple relation, whose\nunderstanding remains ellusive, has been empirically verified in various other\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0601222v1"
    },
    {
        "title": "Structure and evolution of online social relationships: Heterogeneity in\n  warm discussions",
        "authors": [
            "K. -I. Goh",
            "Y. -H. Eom",
            "H. Jeong",
            "B. Kahng",
            "D. Kim"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  With the advancement in the information age, people are using electronic\nmedia more frequently for communications, and social relationships are also\nincreasingly resorting to online channels. While extensive studies on\ntraditional social networks have been carried out, little has been done on\nonline social network. Here we analyze the structure and evolution of online\nsocial relationships by examining the temporal records of a bulletin board\nsystem (BBS) in a university. The BBS dataset comprises of 1,908 boards, in\nwhich a total of 7,446 students participate. An edge is assigned to each\ndialogue between two students, and it is defined as the appearance of the name\nof a student in the from- and to-field in each message. This yields a weighted\nnetwork between the communicating students with an unambiguous group\nassociation of individuals. In contrast to a typical community network, where\nintracommunities (intercommunities) are strongly (weakly) tied, the BBS network\ncontains hub members who participate in many boards simultaneously but are\nstrongly tied, that is, they have a large degree and betweenness centrality and\nprovide communication channels between communities. On the other hand,\nintracommunities are rather homogeneously and weakly connected. Such a\nstructure, which has never been empirically characterized in the past, might\nprovide a new perspective on social opinion formation in this digital era.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0601223v1"
    },
    {
        "title": "Modularity and community structure in networks",
        "authors": [
            "M. E. J. Newman"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Many networks of interest in the sciences, including a variety of social and\nbiological networks, are found to divide naturally into communities or modules.\nThe problem of detecting and characterizing this community structure has\nattracted considerable recent attention. One of the most sensitive detection\nmethods is optimization of the quality function known as \"modularity\" over the\npossible divisions of a network, but direct application of this method using,\nfor instance, simulated annealing is computationally costly. Here we show that\nthe modularity can be reformulated in terms of the eigenvectors of a new\ncharacteristic matrix for the network, which we call the modularity matrix, and\nthat this reformulation leads to a spectral algorithm for community detection\nthat returns results of better quality than competing methods in noticeably\nshorter running times. We demonstrate the algorithm with applications to\nseveral network data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0602124v1"
    },
    {
        "title": "Relation between Bid-Ask Spread, Impact and Volatility in Double Auction\n  Markets",
        "authors": [
            "Matthieu Wyart",
            "Jean-Philippe Bouchaud",
            "Julien Kockelkoren",
            "Marc Potters",
            "Michele Vettorazzo"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We show that the cost of market orders and the profit of infinitesimal\nmarket-making or -taking strategies can be expressed in terms of directly\nobservable quantities, namely the spread and the lag-dependent impact function.\nImposing that any market taking or liquidity providing strategies is at best\nmarginally profitable, we obtain a linear relation between the bid-ask spread\nand the instantaneous impact of market orders, in good agreement with our\nempirical observations on electronic markets. We then use this relation to\njustify a strong, and hitherto unnoticed, empirical correlation between the\nspread and the volatility_per trade_, with R^2s exceeding 0.9. This correlation\nsuggests both that the main determinant of the bid-ask spread is adverse\nselection, and that most of the volatilitycomes from trade impact. We argue\nthat the role of the time-horizon appearing in the definition of costs is\ncrucial and that long-range correlations in the order flow, overlooked in\nprevious studies, must be carefully factored in. We find that the spread is\nsignificantly larger on the nyse, a liquid market with specialists, where\nmonopoly rents appear to be present.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0603084v3"
    },
    {
        "title": "Transcending The Least Squares",
        "authors": [
            "Fyodor V. Tkachov"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The method of quasi-optimal weights provides a comprehensive, asymptotically\noptimal, transparent and flexible alternative to the least squares method. The\noptimality holds for a general non-linear, non-gaussian case.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0604127v3"
    },
    {
        "title": "Wavelets and Solar Magnetic Activity I: Wavelets on the Edge",
        "authors": [
            "Robert W. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The traditional continuous wavelet transform is plagued by the\ncone-of-influence, ie wavelets which extend past either end of a finite\ntimeseries return transform coefficients which tend to decrease as more of the\nwavelet is truncated. These coefficients may be corrected simply by rescaling\nthe remaining wavelet. The corrected wavelet transform displays no\ncone-of-influence and maintains reconstruction as either edge is approached. As\nan application and example, we present the corrected wavelet transform of the\n(derectified) yearly International Sunspot Number, R_i, as a measure of solar\nmagnetic activity, and compare the yearly solar magnetic power with Oerlemans'\nglacial global temperature reconstruction.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0604211v4"
    },
    {
        "title": "Finding community structure in networks using the eigenvectors of\n  matrices",
        "authors": [
            "M. E. J. Newman"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We consider the problem of detecting communities or modules in networks,\ngroups of vertices with a higher-than-average density of edges connecting them.\nPrevious work indicates that a robust approach to this problem is the\nmaximization of the benefit function known as \"modularity\" over possible\ndivisions of a network. Here we show that this maximization process can be\nwritten in terms of the eigenspectrum of a matrix we call the modularity\nmatrix, which plays a role in community detection similar to that played by the\ngraph Laplacian in graph partitioning calculations. This result leads us to a\nnumber of possible algorithms for detecting community structure, as well as\nseveral other results, including a spectral measure of bipartite structure in\nnetworks and a new centrality measure that identifies those vertices that\noccupy central positions within the communities to which they belong. The\nalgorithms and measures proposed are illustrated with applications to a variety\nof real-world complex networks.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605087v3"
    },
    {
        "title": "Comparison of weighted and unweighted histograms",
        "authors": [
            "N. D. Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Two modifications of the chi square test for comparing usual(unweighted) and\nweighted histograms and two weighted histograms are proposed. Numerical\nexamples illustrate an application of the tests for the histograms with\ndifferent statistics of events. Proposed tests can be used for the comparison\nof experimental data histograms against simulated data histograms and two\nsimulated data histograms.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605123v1"
    },
    {
        "title": "About the proof of the so called exact classical confidence intervals.\n  Where is the trick?",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  In this note I go through the `proof' of frequentistic confidence intervals\nand show what it logically implies concerning the value of a physical quantity\ngiven an experimental observation (nothing).\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605140v2"
    },
    {
        "title": "Optimal approximations of power-laws with exponentials",
        "authors": [
            "Thierry Bochud",
            "Damien Challet"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We propose an explicit recursive method to approximate a power-law with a\nfinite sum of weighted exponentials. Applications to moving averages with long\nmemory are discussed in relationship with stochastic volatility models.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605149v2"
    },
    {
        "title": "Correlation matrix decomposition of WIG20 intraday fluctuations",
        "authors": [
            "R. Rak",
            "S. Drozdz",
            "J. Kwapien",
            "P. Oswiecimka"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Using the correlation matrix formalism we study the temporal aspects of the\nWarsaw Stock Market evolution as represented by the WIG20 index. The high\nfrequency (1 min) WIG20 recordings over the time period between January 2001\nand October 2005 are used. The entries of the correlation matrix considered\nhere connect different distinct periods of the stock market dynamics, like days\nor weeks. Such a methodology allows to decompose the price fluctuations into\nthe orthogonal eigensignals that quantify different modes of the underlying\ndynamics. The magnitudes of the corresponding eigenvalues reflect the strengths\nof such modes. One observation made in this paper is that strength of the daily\ntrend in the WIG20 dynamics systematically decreases when going from 2001 to\n2005. Another is that large events in the return fluctuations are primarily\nassociated with a few most collective eigensignals.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0606041v3"
    },
    {
        "title": "Of Songs and Men: a Model for Multiple Choice with Herding",
        "authors": [
            "Christian Borghesi",
            "Jean-Philippe Bouchaud"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We propose a generic model for multiple choice situations in the presence of\nherding and compare it with recent empirical results from a Web-based music\nmarket experiment. The model predicts a phase transition between a weak\nimitation phase and a strong imitation, `fashion' phase, where choices are\ndriven by peer pressure and the ranking of individual preferences is strongly\ndistorted at the aggregate level. The model can be calibrated to reproduce the\nmain experimental results of Salganik et al. (Science, 311, pp. 854-856\n(2006)); we show in particular that the value of the social influence parameter\ncan be estimated from the data. In one of the experimental situation, this\nvalue is found to be close to the critical value of the model.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0606224v1"
    },
    {
        "title": "Trend arbitrage, bid-ask spread and market dynamics",
        "authors": [
            "Nikolai Zaitsev"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Microstructure of market dynamics is studied through analysis of tick price\ndata. Linear trend is introduced as a tool for such analysis. Trend arbitrage\ninequality is developed and tested. The inequality sets limiting relationship\nbetween trend, bid-ask spread, market reaction and average update frequency of\nprice information. Average time of market reaction is measured from market\ndata. This parameter is interpreted as a constant value of the stock exchange\nand is attributed to the latency of exchange reaction to actions of traders.\nThis latency and cost of trade are shown to be the main limit of bid-ask\nspread. Data analysis also suggests some relationships between trend, bid-ask\nspread and average frequency of price update process.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607076v1"
    },
    {
        "title": "Non-Parametric Extraction of Implied Asset Price Distributions",
        "authors": [
            "Jerome V. Healy",
            "Maurice Dixon",
            "Brian J. Read",
            "Fang Fang Cai"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Extracting the risk neutral density (RND) function from option prices is well\ndefined in principle, but is very sensitive to errors in practice. For risk\nmanagement, knowledge of the entire RND provides more information for\nValue-at-Risk (VaR) calculations than implied volatility alone [1]. Typically,\nRNDs are deduced from option prices by making a distributional assumption, or\nrelying on implied volatility [2]. We present a fully non-parametric method for\nextracting RNDs from observed option prices. The aim is to obtain a continuous,\nsmooth, monotonic, and convex pricing function that is twice differentiable.\nThus, irregularities such as negative probabilities that afflict many existing\nRND estimation techniques are reduced. Our method employs neural networks to\nobtain a smoothed pricing function, and a central finite difference\napproximation to the second derivative to extract the required gradients.\n  This novel technique was successfully applied to a large set of FTSE 100\ndaily European exercise (ESX) put options data and as an Ansatz to the\ncorresponding set of American exercise (SEI) put options. The results of paired\nt-tests showed significant differences between RNDs extracted from ESX and SEI\noption data, reflecting the distorting impact of early exercise possibility for\nthe latter. In particular, the results for skewness and kurtosis suggested\ndifferent shapes for the RNDs implied by the two types of put options. However,\nboth ESX and SEI data gave an unbiased estimate of the realised FTSE 100\nclosing prices on the options' expiration date. We confirmed that estimates of\nvolatility from the RNDs of both types of option were biased estimates of the\nrealised volatility at expiration, but less so than the LIFFE tabulated\nat-the-money implied volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607240v1"
    },
    {
        "title": "Beyond the average: detecting global singular nodes from local features\n  in complex networks",
        "authors": [
            "Luciano da F. Costa",
            "Marcus Kaiser",
            "Claus Hilgetag"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Deviations from the average can provide valuable insights about the\norganization of natural systems. This article extends this important principle\nto the more systematic identification and analysis of singular local\nconnectivity patterns in complex networks. Four measurements quantifying\ndifferent and complementary features of the connectivity around each node are\ncalculated and multivariate statistical methods are then applied in order to\nidentify outliers. The potential of the presented concepts and methodology is\nillustrated with respect to a word association network.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0607272v1"
    },
    {
        "title": "Information filtering via Iterative Refinement",
        "authors": [
            "P. Laureti",
            "L. Moret",
            "Y. -C. Zhang",
            "Y. -K. Yu"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  With the explosive growth of accessible information, expecially on the\nInternet, evaluation-based filtering has become a crucial task. Various systems\nhave been devised aiming to sort through large volumes of information and\nselect what is likely to be more relevant. In this letter we analyse a new\nranking method, where the reputation of information providers is determined\nself-consistently.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608166v1"
    },
    {
        "title": "Updating Probabilities",
        "authors": [
            "Ariel Caticha",
            "Adom Giffin"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We show that Skilling's method of induction leads to a unique general theory\nof inductive inference, the method of Maximum relative Entropy (ME). The main\ntool for updating probabilities is the logarithmic relative entropy; other\nentropies such as those of Renyi or Tsallis are ruled out. We also show that\nBayes updating is a special case of ME updating and thus, that the two are\ncompletely compatible.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608185v1"
    },
    {
        "title": "The art of fitting financial time series with Levy stable distributions",
        "authors": [
            "Enrico Scalas",
            "Kyungsik Kim"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  This paper illustrates a procedure for fitting financial data with\n$\\alpha$-stable distributions. After using all the available methods to\nevaluate the distribution parameters, one can qualitatively select the best\nestimate and run some goodness-of-fit tests on this estimate, in order to\nquantitatively assess its quality. It turns out that, for the two investigated\ndata sets (MIB30 and DJIA from 2000 to present), an $\\alpha$-stable fit of\nlog-returns is reasonably good.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608224v1"
    },
    {
        "title": "Maximizing Modularity is hard",
        "authors": [
            "U. Brandes",
            "D. Delling",
            "M. Gaertler",
            "R. Goerke",
            "M. Hoefer",
            "Z. Nikoloski",
            "D. Wagner"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Several algorithms have been proposed to compute partitions of networks into\ncommunities that score high on a graph clustering index called modularity.\nWhile publications on these algorithms typically contain experimental\nevaluations to emphasize the plausibility of results, none of these algorithms\nhas been shown to actually compute optimal partitions. We here settle the\nunknown complexity status of modularity maximization by showing that the\ncorresponding decision version is NP-complete in the strong sense. As a\nconsequence, any efficient, i.e. polynomial-time, algorithm is only heuristic\nand yields suboptimal partitions on many instances.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608255v2"
    },
    {
        "title": "Coupled continuous time random walks in finance",
        "authors": [
            "Mark M. Meerschaert",
            "Enrico Scalas"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Continuous time random walks (CTRWs) are used in physics to model anomalous\ndiffusion, by incorporating a random waiting time between particle jumps. In\nfinance, the particle jumps are log-returns and the waiting times measure delay\nbetween transactions. These two random variables (log-return and waiting time)\nare typically not independent. For these coupled CTRW models, we can now\ncompute the limiting stochastic process (just like Brownian motion is the limit\nof a simple random walk), even in the case of heavy tailed (power-law) price\njumps and/or waiting times. The probability density functions for this limit\nprocess solve fractional partial differential equations. In some cases, these\nequations can be explicitly solved to yield descriptions of long-term price\nchanges, based on a high-resolution model of individual trades that includes\nthe statistical dependence between waiting times and the subsequent\nlog-returns. In the heavy tailed case, this involves operator stable space-time\nrandom vectors that generalize the familiar stable models. In this paper, we\nwill review the fundamental theory and present two applications with\ntick-by-tick stock and futures data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0608281v1"
    },
    {
        "title": "Fear and its implications for stock markets",
        "authors": [
            "Ingve Simonsen",
            "Peter Toke Heden Ahlgren",
            "Mogens H. Jensen",
            "Raul Donangelo",
            "Kim Sneppen"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The value of stocks, indices and other assets, are examples of stochastic\nprocesses with unpredictable dynamics. In this paper, we discuss asymmetries in\nshort term price movements that can not be associated with a long term positive\ntrend. These empirical asymmetries predict that stock index drops are more\ncommon on a relatively short time scale than the corresponding raises. We\npresent several empirical examples of such asymmetries. Furthermore, a simple\nmodel featuring occasional short periods of synchronized dropping prices for\nall stocks constituting the index is introduced with the aim of explaining\nthese facts. The collective negative price movements are imagined triggered by\nexternal factors in our society, as well as internal to the economy, that\ncreate fear of the future among investors. This is parameterized by a ``fear\nfactor'' defining the frequency of synchronized events. It is demonstrated that\nsuch a simple fear factor model can reproduce several empirical facts\nconcerning index asymmetries. It is also pointed out that in its simplest form,\nthe model has certain shortcomings.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0609046v2"
    },
    {
        "title": "Probability of stochastic processes and spacetime geometry",
        "authors": [
            "E. Canessa"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We made a first attempt to associate a probabilistic description of\nstochastic processes like birth-death processes with spacetime geometry in the\nSchwarzschild metrics on distance scales from the macro- to the micro-domains.\nWe idealize an ergodic system in which system states communicate through a\ncurved path composed of transition arrows where each arrow corresponds to a\npositive, analogous birth or death rate.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0610211v1"
    },
    {
        "title": "Mixture models and exploratory analysis in networks",
        "authors": [
            "M. E. J. Newman",
            "E. A. Leicht"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Networks are widely used in the biological, physical, and social sciences as\na concise mathematical representation of the topology of systems of interacting\ncomponents. Understanding the structure of these networks is one of the\noutstanding challenges in the study of complex systems. Here we describe a\ngeneral technique for detecting structural features in large-scale network data\nwhich works by dividing the nodes of a network into classes such that the\nmembers of each class have similar patterns of connection to other nodes. Using\nthe machinery of probabilistic mixture models and the expectation-maximization\nalgorithm, we show that it is possible to detect, without prior knowledge of\nwhat we are looking for, a very broad range of types of structure in networks.\nWe give a number of examples demonstrating how the method can be used to shed\nlight on the properties of real-world networks, including social and\ninformation networks.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0611158v3"
    },
    {
        "title": "Forecasting extreme events in collective dynamics: an analytic signal\n  approach to detecting discrete scale invariance",
        "authors": [
            "G. M. Viswanathan"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  A challenging problem in physics concerns the possibility of forecasting rare\nbut extreme phenomena such as large earthquakes, financial market crashes, and\nmaterial rupture. A promising line of research involves the early detection of\nprecursory log-periodic oscillations to help forecast extreme events in\ncollective phenomena where discrete scale invariance plays an important role.\nHere I investigate two distinct approaches towards the general problem of how\nto detect log-periodic oscillations in arbitrary time series without prior\nknowledge of the location of the moveable singularity. I first show that the\nproblem has a definite solution in Fourier space, however the technique\ninvolved requires an unrealistically large signal to noise ratio. I then show\nthat the quadrature signal obtained via analytic continuation onto the\nimaginary axis, using the Hilbert transform, necessarily retains the\nlog-periodicities found in the original signal. This finding allows the\ndevelopment of a new method of detecting log-periodic oscillations that relies\non calculation of the instantaneous phase of the analytic signal. I illustrate\nthe method by applying it to the well documented stock market crash of 1987.\nFinally, I discuss the relevance of these findings for parametric rather than\nnonparametric estimation of critical times.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0611281v2"
    },
    {
        "title": "A mechanism to derive multi-power law functions: an application in the\n  econophysics framework",
        "authors": [
            "A. M. Scarfone"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  It is generally recognized that economical systems, and more in general\ncomplex systems, are characterized by power law distributions. Sometime, these\ndistributions show a changing of the slope in the tail so that, more\nappropriately, they show a multi-power law behavior. We present a method to\nderive analytically a two-power law distribution starting from a single power\nlaw function recently obtained, in the frameworks of the generalized\nstatistical mechanics based on the Sharma-Taneja-Mittal information measure. In\norder to test the method, we fit the cumulative distribution of personal income\nand gross domestic production of several countries, obtaining a good agreement\nfor a wide range of data.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0612231v1"
    },
    {
        "title": "Fluctuations in time intervals of financial data from the view point of\n  the Gini index",
        "authors": [
            "Naoya Sazuka",
            "Jun-ichi Inoue"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We propose an approach to explain fluctuations in time intervals of financial\nmarkets data from the view point of the Gini index. We show the explicit form\nof the Gini index for a Weibull distribution which is a good candidate to\ndescribe the first passage time of foreign exchange rate. The analytical\nexpression of the Gini index gives a very close value with that of empirical\ndata analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0701008v1"
    },
    {
        "title": "Waiting time analysis of foreign currency exchange rates: Beyond the\n  renewal-reward theorem",
        "authors": [
            "Naoya Sazuka",
            "Jun-ichi Inoue"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We evaluate the average waiting time between observing the price of financial\nmarkets and the next price change, especially in an on-line foreign exchange\ntrading service for individual customers via the internet. Basic technical idea\nof our present work is dependent on the so-called renewal-reward theorem.\nAssuming that stochastic processes of the market price changes could be\nregarded as a renewal process, we use the theorem to calculate the average\nwaiting time of the process. In the conventional derivation of the theorem, it\nis apparently hard to evaluate the higher order moments of the waiting time. To\novercome this type of difficulties, we attempt to derive the waiting time\ndistribution Omega(s) directly for arbitrary time interval distribution (first\npassage time distribution) of the stochastic process P_{W}(tau) and observation\ntime distribution P_{O}(t) of customers. Our analysis enables us to evaluate\nnot only the first moment (the average waiting time) but also any order of the\nhigher moments of the waiting time. Moreover, in our formalism, it is possible\nto model the observation of the price on the internet by the customers in terms\nof the observation time distribution P_{O}(t). We apply our analysis to the\nstochastic process of the on-line foreign exchange rate for individual\ncustomers from the Sony bank and compare the moments with the empirical data\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0702003v1"
    },
    {
        "title": "Correlation of coming limit price with order book in stock markets",
        "authors": [
            "Jun-ichi Maskawa"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We examine the correlation of the limit price with the order book, when a\nlimit order comes. We analyzed the Rebuild Order Book of Stock Exchange\nElectronic Trading Service, which is the centralized order book market of\nLondon Stock Exchange. As a result, the limit price is broadly distributed\naround the best price according to a power-law, and it isn't randomly drawn\nfrom the distribution, but has a strong correlation with the size of cumulative\nunexecuted limit orders on the price. It was also found that the limit price,\non the coarse-grained price scale, tends to gather around the price which has a\nlarge size of cumulative unexecuted limit orders.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0702029v1"
    },
    {
        "title": "Method of reconstructing a moving pulse",
        "authors": [
            "Stephen J. Howard",
            "R. D. Horton",
            "D. Q. Hwang",
            "R. W. Evans",
            "S. J. Brockington",
            "J. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We present a method of analyzing a set of N time signals f_i(t) that consist\nof local measurements of the same physical observable taken at N sequential\nlocations Z_i along the length of an experimental device. The result is an\nalgorithm for reconstructing an approximation F(z,t) of the field f(z,t) in the\ninaccessible regions between the points of measurement. We also explore the\nconditions needed for this approximation to hold, and test the algorithm under\na variety of conditions. We apply this method to analyze the magnetic field\nmeasurements taken on the Compact Toroid Injection eXperiment (CTIX) plasma\naccelerator; providing a direct means of visualizing experimental data,\nquantifying global properties, and benchmarking simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0702187v2"
    },
    {
        "title": "The Laplace-Jaynes approach to induction",
        "authors": [
            "P. G. L. Porta Mana",
            "A. Månsson",
            "G. Björk"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  An approach to induction is presented, based on the idea of analysing the\ncontext of a given problem into `circumstances'. This approach, fully Bayesian\nin form and meaning, provides a complement or in some cases an alternative to\nthat based on de Finetti's representation theorem and on the notion of infinite\nexchangeability. In particular, it gives an alternative interpretation of those\nformulae that apparently involve `unknown probabilities' or `propensities'.\nVarious advantages and applications of the presented approach are discussed,\nespecially in comparison to that based on exchangeability. Generalisations are\nalso discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0703126v2"
    },
    {
        "title": "SigSpec - I. Frequency- and Phase-Resolved Significance in Fourier Space",
        "authors": [
            "P. Reegen"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Identifying frequencies with low signal-to-noise ratios in time series of\nstellar photometry and spectroscopy, and measuring their amplitude ratios and\npeak widths accurately, are critical goals for asteroseismology. These are also\nchallenges for time series with gaps or whose data are not sampled at a\nconstant rate, even with modern Discrete Fourier Transform (DFT) software. Also\nthe False-Alarm Probability introduced by Lomb and Scargle is an approximation\nwhich becomes less reliable in time series with longer data gaps. A rigorous\nstatistical treatment of how to determine the significance of a peak in a DFT,\ncalled SigSpec, is presented here. SigSpec is based on an analytical solution\nof the probability that a DFT peak of a given amplitude does not arise from\nwhite noise in a non-equally spaced data set. The underlying Probability\nDensity Function (PDF) of the amplitude spectrum generated by white noise can\nbe derived explicitly if both frequency and phase are incorporated into the\nsolution. In this paper, I define and evaluate an unbiased statistical\nestimator, the \"spectral significance\", which depends on frequency, amplitude,\nand phase in the DFT, and which takes into account the time-domain sampling. I\nalso compare this estimator to results from other well established techniques\nand demonstrate the effectiveness of SigSpec with a few examples of ground- and\nspace-based photometric data, illustratring how SigSpec deals with the effects\nof noise and time-domain sampling in determining significant frequencies.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0703160v1"
    },
    {
        "title": "Properties of a simple bilinear stochastic model: estimation and\n  predictability",
        "authors": [
            "D. Sornette",
            "V. F. Pisarenko"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We analyze the properties of arguably the simplest bilinear stochastic\nmultiplicative process, proposed as a model of financial returns and of other\ncomplex systems combining both nonlinearity and multiplicative noise. By\nconstruction, it has no linear predictability (zero two-point correlation) but\na certain nonlinear predictability (non-zero three-point correlation). It can\nthus be considered as a paradigm for testing the existence of a possible\nnonlinear predictbility in a given time series. We present a rather exhaustive\nstudy of the process, including its ability to produce fat-tailed distribution\nfrom Gaussian innovations, the unstable characteristics of the inversion of the\nkey nonlinear parameters and of the two initial conditions necessary for the\nimplementation of a prediction scheme and an analysis of the associated\nsuper-exponential sensitivity of the inversion of the innovations in the\npresence of a large impluse. Our study emphasizes the conditions under which a\ndegree of predictability can be achieved and describes a number of different\nattempts, which overall illuminates the properties of the process. In\nconclusion, notwithstanding its remarkable simplicity, the bilinear stochastic\nprocess exhibits remarkably rich and complex behavior, which makes it a serious\ncandidate for the modeling of financial times series and of other complex\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0703217v1"
    },
    {
        "title": "Scale-free avalanches in the multifractal random walk",
        "authors": [
            "M. Bartolozzi"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Avalanches, or Avalanche-like, events are often observed in the dynamical\nbehaviour of many complex systems which span from solar flaring to the Earth's\ncrust dynamics and from traffic flows to financial markets. Self-organized\ncriticality (SOC) is one of the most popular theories able to explain this\nintermittent charge/discharge behaviour. Despite a large amount of theoretical\nwork, empirical tests for SOC are still in their infancy. In the present paper\nwe address the common problem of revealing SOC from a simple time series\nwithout having much information about the underlying system. As a working\nexample we use a modified version of the multifractal random walk originally\nproposed as a model for the stock market dynamics. The study reveals, despite\nthe lack of the typical ingredients of SOC, an avalanche-like dynamics similar\nto that of many physical systems. While, on one hand, the results confirm the\nrelevance of cascade models in representing turbulent-like phenomena, on the\nother, they also raise the question about the current state of reliability of\nSOC inference from time series analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.4329v1"
    },
    {
        "title": "Kullback-Leibler distance as a measure of the information filtered from\n  multivariate data",
        "authors": [
            "Michele Tumminello",
            "Fabrizio Lillo",
            "Rosario Nunzio Mantegna"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We show that the Kullback-Leibler distance is a good measure of the\nstatistical uncertainty of correlation matrices estimated by using a finite set\nof data. For correlation matrices of multivariate Gaussian variables we\nanalytically determine the expected values of the Kullback-Leibler distance of\na sample correlation matrix from a reference model and we show that the\nexpected values are known also when the specific model is unknown. We propose\nto make use of the Kullback-Leibler distance to estimate the information\nextracted from a correlation matrix by correlation filtering procedures. We\nalso show how to use this distance to measure the stability of filtering\nprocedures with respect to statistical uncertainty. We explain the\neffectiveness of our method by comparing four filtering procedures, two of them\nbeing based on spectral analysis and the other two on hierarchical clustering.\nWe compare these techniques as applied both to simulations of factor models and\nempirical data. We investigate the ability of these filtering procedures in\nrecovering the correlation matrix of models from simulations. We discuss such\nan ability in terms of both the heterogeneity of model parameters and the\nlength of data series. We also show that the two spectral techniques are\ntypically more informative about the sample correlation matrix than techniques\nbased on hierarchical clustering, whereas the latter are more stable with\nrespect to statistical uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.0168v1"
    },
    {
        "title": "Power-law distributions in empirical data",
        "authors": [
            "Aaron Clauset",
            "Cosma Rohilla Shalizi",
            "M. E. J. Newman"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Power-law distributions occur in many situations of scientific interest and\nhave significant consequences for our understanding of natural and man-made\nphenomena. Unfortunately, the detection and characterization of power laws is\ncomplicated by the large fluctuations that occur in the tail of the\ndistribution -- the part of the distribution representing large but rare events\n-- and by the difficulty of identifying the range over which power-law behavior\nholds. Commonly used methods for analyzing power-law data, such as\nleast-squares fitting, can produce substantially inaccurate estimates of\nparameters for power-law distributions, and even in cases where such methods\nreturn accurate answers they are still unsatisfactory because they give no\nindication of whether the data obey a power law at all. Here we present a\nprincipled statistical framework for discerning and quantifying power-law\nbehavior in empirical data. Our approach combines maximum-likelihood fitting\nmethods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic\nand likelihood ratios. We evaluate the effectiveness of the approach with tests\non synthetic data and give critical comparisons to previous approaches. We also\napply the proposed methods to twenty-four real-world data sets from a range of\ndifferent disciplines, each of which has been conjectured to follow a power-law\ndistribution. In some cases we find these conjectures to be consistent with the\ndata while in others the power law is ruled out.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.1062v2"
    },
    {
        "title": "Modularity and community detection in bipartite networks",
        "authors": [
            "Michael J. Barber"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The modularity of a network quantifies the extent, relative to a null model\nnetwork, to which vertices cluster into community groups. We define a null\nmodel appropriate for bipartite networks, and use it to define a bipartite\nmodularity. The bipartite modularity is presented in terms of a modularity\nmatrix B; some key properties of the eigenspectrum of B are identified and used\nto describe an algorithm for identifying modules in bipartite networks. The\nalgorithm is based on the idea that the modules in the two parts of the network\nare dependent, with each part mutually being used to induce the vertices for\nthe other part into the modules. We apply the algorithm to real-world network\ndata, showing that the algorithm successfully identifies the modular structure\nof bipartite networks.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.1616v3"
    },
    {
        "title": "Robustness of community structure in networks",
        "authors": [
            "Brian Karrer",
            "Elizaveta Levina",
            "M. E. J. Newman"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The discovery of community structure is a common challenge in the analysis of\nnetwork data. Many methods have been proposed for finding community structure,\nbut few have been proposed for determining whether the structure found is\nstatistically significant or whether, conversely, it could have arisen purely\nas a result of chance. In this paper we show that the significance of community\nstructure can be effectively quantified by measuring its robustness to small\nperturbations in network structure. We propose a suitable method for perturbing\nnetworks and a measure of the resulting change in community structure and use\nthem to assess the significance of community structure in a variety of\nnetworks, both real and computer generated.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.2108v1"
    },
    {
        "title": "Extending the optimum interval method",
        "authors": [
            "S. Yellin"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The optimum interval method for finding an upper limit of a one-dimensionally\ndistributed signal in the presence of an unknown background is extended to the\ncase of high statistics. There is also some discussion of how the method can be\nextended to the multiple dimensional case.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.2701v1"
    },
    {
        "title": "A Bayesian Approach to Network Modularity",
        "authors": [
            "Jake M. Hofman",
            "Chris H. Wiggins"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We present an efficient, principled, and interpretable technique for\ninferring module assignments and for identifying the optimal number of modules\nin a given network. We show how several existing methods for finding modules\ncan be described as variant, special, or limiting cases of our work, and how\nthe method overcomes the resolution limit problem, accurately recovering the\ntrue number of modules. Our approach is based on Bayesian methods for model\nselection which have been used with success for almost a century, implemented\nusing a variational technique developed only in the past decade. We apply the\ntechnique to synthetic and real networks and outline how the method naturally\nallows selection among competing models.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.3512v3"
    },
    {
        "title": "Shrinkage and spectral filtering of correlation matrices: a comparison\n  via the Kullback-Leibler distance",
        "authors": [
            "M. Tumminello",
            "F. Lillo",
            "R. N. Mantegna"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  The problem of filtering information from large correlation matrices is of\ngreat importance in many applications. We have recently proposed the use of the\nKullback-Leibler distance to measure the performance of filtering algorithms in\nrecovering the underlying correlation matrix when the variables are described\nby a multivariate Gaussian distribution. Here we use the Kullback-Leibler\ndistance to investigate the performance of filtering methods based on Random\nMatrix Theory and on the shrinkage technique. We also present some results on\nthe application of the Kullback-Leibler distance to multivariate data which are\nnon Gaussian distributed.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.0576v1"
    },
    {
        "title": "Demographic growth and the distribution of language sizes",
        "authors": [
            "Damian H. Zanette"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  It is argued that the present log-normal distribution of language sizes is,\nto a large extent, a consequence of demographic dynamics within the population\nof speakers of each language. A two-parameter stochastic multiplicative process\nis proposed as a model for the population dynamics of individual languages, and\napplied over a period spanning the last ten centuries. The model disregards\nlanguage birth and death. A straightforward fitting of the two parameters,\nwhich statistically characterize the population growth rate, predicts a\ndistribution of language sizes in excellent agreement with empirical data.\nNumerical simulations, and the study of the size distribution within language\nfamilies, validate the assumptions at the basis of the model.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.1511v1"
    },
    {
        "title": "Centrality measures and thermodynamic formalism for complex networks",
        "authors": [
            "Jean-Charles Delvenne",
            "Anne-Sophie Libert"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  In the study of small and large networks it is customary to perform a simple\nrandom walk, where the random walker jumps from one node to one of its\nneighbours with uniform probability. The properties of this random walk are\nintimately related to the combinatorial properties of the network. In this\npaper we propose to use the Ruelle-Bowens random walk instead, whose\nprobability transitions are chosen in order to maximise the entropy rate of the\nwalk on an unweighted graph. If the graph is weighted, then a free energy is\noptimised instead of entropy rate.\n  Specifically, we introduce a centrality measure for large networks, which is\nthe stationary distribution attained by the the Ruelle-Bowens random walk; we\nname it Entropy Rank. We introduce a more general version, able to deal with\ndisconnected networks, under the name of Free Energy Rank. We compare the\nproperties of those centrality measures with the classic PageRank and HITS on\nboth toy and real-life examples, in particular their robustness to small\nmodifications of the network. It is observed that our centrality measures have\na better discriminating power than PageRank, being able to distinguish clearly\npages that PageRank holds for almost equally interesting, and is more sensitive\nto the medium-scale details of the graph.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.3972v3"
    },
    {
        "title": "Dynamics of Non-Conservative Voters",
        "authors": [
            "R. Lambiotte",
            "S. Redner"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We study a family of opinion formation models in one dimension where the\npropensity for a voter to align with its local environment depends non-linearly\non the fraction of disagreeing neighbors. Depending on this non-linearity in\nthe voting rule, the population may exhibit a bias toward zero magnetization or\ntoward consensus, and the average magnetization is generally not conserved. We\nuse a decoupling approximation to truncate the equation hierarchy for\nmulti-point spin correlations and thereby derive the probability to reach a\nfinal state of a given consensus as a function of the initial magnetization.\nThe case when voters are influenced by more distant voters is also considered\nby investigating the Sznajd model.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.0364v3"
    },
    {
        "title": "Unsupervised and semi-supervised clustering by message passing:\n  Soft-constraint affinity propagation",
        "authors": [
            "Michele Leone",
            " Sumedha",
            "Martin Weigt"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Soft-constraint affinity propagation (SCAP) is a new statistical-physics\nbased clustering technique. First we give the derivation of a simplified\nversion of the algorithm and discuss possibilities of time- and\nmemory-efficient implementations. Later we give a detailed analysis of the\nperformance of SCAP on artificial data, showing that the algorithm efficiently\nunveils clustered and hierarchical data structures. We generalize the algorithm\nto the problem of semi-supervised clustering, where data are already partially\nlabeled, and clustering assigns labels to previously unlabeled points. SCAP\nuses both the geometrical organization of the data and the available labels\nassigned to few points in a computationally efficient way, as is shown on\nartificial and biological benchmark data.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.1165v2"
    },
    {
        "title": "Applying Bayesian Neural Networks to Event Reconstruction in Reactor\n  Neutrino Experiments",
        "authors": [
            "Ye Xu",
            "Weiwei Xu",
            "Yixiong Meng",
            "Kaien Zhu",
            "Wei Xu"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  A toy detector has been designed to simulate central detectors in reactor\nneutrino experiments in the paper. The electron samples from the Monte-Carlo\nsimulation of the toy detector have been reconstructed by the method of\nBayesian neural networks (BNN) and the standard algorithm, a maximum likelihood\nmethod (MLD), respectively. The result of the event reconstruction using BNN\nhas been compared with the one using MLD. Compared to MLD, the uncertainties of\nthe electron vertex are not improved, but the energy resolutions are\nsignificantly improved using BNN. And the improvement is more obvious for the\nhigh energy electrons than the low energy ones.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.4042v3"
    },
    {
        "title": "Goodness of fit tests for weighted histograms",
        "authors": [
            "N. D. Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Weighted histogram in Monte-Carlo simulations is often used for the\nestimation of a probability density function. It is obtained as a result of\nrandom experiment with random events that have weights. In this paper the bin\ncontents of weighted histogram are considered as a sum of random variables with\nrandom number of terms. Goodness of fit tests for weighted histograms and for\nweighted histograms with unknown normalization are proposed. Sizes and powers\nof the tests are investigated numerically.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.4250v2"
    },
    {
        "title": "Active Brownian Motion Models and Applications to Ratchets",
        "authors": [
            "Alessandro Fiasconaro",
            "Werner Ebeling",
            "Ewa Gudowska-Nowak"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We give an overview over recent studies on the model of Active Brownian\nMotion (ABM) coupled to reservoirs providing free energy which may be converted\ninto kinetic energy of motion. First, we present an introduction to a general\nconcept of active Brownian particles which are capable to take up energy from\nthe source and transform part of it in order to perform various activities. In\nthe second part of our presentation we consider applications of ABM to ratchet\nsystems with different forms of differentiable potentials. Both analytical and\nnumerical evaluations are discussed for three cases of sinusoidal,\nstaircase-like and Mateos ratchet potentials, also with the additional loads\nmodeled by tilted potential structure. In addition, stochastic character of the\nkinetics is investigated by considering perturbation by Gaussian white noise\nwhich is shown to be responsible for driving the directionality of the\nasymptotic flux in the ratchet. This \\textit{stochastically driven\ndirectionality} effect is visualized as a strong nonmonotonic dependence of the\nstatistics of the right versus left trajectories of motion leading to a net\ncurrent of particles. Possible applications of the ratchet systems to molecular\nmotors are also briefly discussed\n",
        "pdf_link": "http://arxiv.org/pdf/0801.4838v2"
    },
    {
        "title": "Fourier-Based Spectral Analysis with Adaptive Resolution",
        "authors": [
            "Andrey Khilko"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Despite being the most popular methods of data analysis, Fourier-based\ntechniques suffer from the problem of static resolution that is currently\nbelieved to be a fundamental limitation of the Fourier Transform. Although\nalternative solutions overcome this limitation, none provide the simplicity,\nversatility, and convenience of the Fourier analysis. The lack of convenience\noften prevents these alternatives from replacing classical spectral methods -\neven in applications that suffer from the limitation of static resolution.\n  This work demonstrates that, contrary to the generally accepted belief, the\nFourier Transform can be generalized to the case of adaptive resolution. The\ngeneralized transform provides backward compatibility with classical spectral\ntechniques and introduces minimal computational overhead.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.1348v2"
    },
    {
        "title": "Bayesian considerations on the multiverse explanation of cosmic\n  fine-tuning",
        "authors": [
            "V. Palonen"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The fundamental laws and constants of our universe seem to be finely tuned\nfor life. The various multiverse hypotheses are popular explanations for the\nfine tuning. This paper reviews the four main suggestions on inference in the\npresence of possible multiple universes and observer selection effects. Basic\nidentities from probability theory and previously unnoticed conditional\ndependencies of the propositions involved are used to decide among the\nalternatives. In the case of cosmic fine-tuning, information about the\nobservation is not independent of the hypothesis. It follows that the\nobservation should be used as data when comparing hypotheses. Hence, approaches\nthat use the observation only as background information are incorrect. It is\nalso shown that in some cases the self-sampling assumption by Bostrom leads to\nprobabilities greater than one, leaving the approach inconsistent. The \"some\nuniverse\" (SU) approach is found wanting. Several reasons are given on why the\n\"this universe\" (TU) approach seems to be correct. Lastly, the converse\nselection effect by White is clarified by showing formally that the converse\ncondition leads to SU and its absence to TU. The overall result is that,\nbecause multiverse hypotheses do not predict the fine-tuning for this universe\nany better than a single universe hypothesis, the multiverse hypotheses fail as\nexplanations for cosmic fine-tuning. Conversely, the fine-tuning data does not\nsupport the multiverse hypotheses.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.4013v2"
    },
    {
        "title": "Multifractal detrended cross-correlation analysis for two nonstationary\n  signals",
        "authors": [
            "Wei-Xing Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  It is ubiquitous in natural and social sciences that two variables, recorded\ntemporally or spatially in a complex system, are cross-correlated and possess\nmultifractal features. We propose a new method called multifractal detrended\ncross-correlation analysis (MF-DXA) to investigate the multifractal behaviors\nin the power-law cross-correlations between two records in one or higher\ndimensions. The method is validated with cross-correlated 1D and 2D binomial\nmeasures and multifractal random walks. Application to two financial time\nseries is also illustrated.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.2773v1"
    },
    {
        "title": "Testing Consistency of Two Histograms",
        "authors": [
            "Frank C. Porter"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Several approaches to testing the hypothesis that two histograms are drawn\nfrom the same distribution are investigated. We note that single-sample\ncontinuous distribution tests may be adapted to this two-sample grouped data\nsituation. The difficulty of not having a fully-specified null hypothesis is an\nimportant consideration in the general case, and care is required in estimating\nprobabilities with ``toy'' Monte Carlo simulations. The performance of several\ncommon tests is compared; no single test performs best in all situations.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.0380v1"
    },
    {
        "title": "Identification of community structure in networks with convex\n  optimization",
        "authors": [
            "Roland Hildebrand"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We reformulate the problem of modularity maximization over the set of\npartitions of a network as a conic optimization problem over the completely\npositive cone, converting it from a combinatorial optimization problem to a\nconvex continuous one. A semidefinite relaxation of this conic program then\nallows to compute upper bounds on the maximum modularity of the network. Based\non the solution of the corresponding semidefinite program, we design a\nrandomized algorithm generating partitions of the network with suboptimal\nmodularities. We apply this algorithm to several benchmark networks,\ndemonstrating that it is competitive in accuracy with the best algorithms\npreviously known. We use our method to provide the first proof of optimality of\na partition for a real-world network.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.1896v2"
    },
    {
        "title": "On discrete stochastic processes with long-lasting time dependence",
        "authors": [
            "Silvio M. Duarte Queiros"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  In this manuscript, we analytically and numerically study statistical\nproperties of an heteroskedastic process based on the celebrated ARCH generator\nof random variables whose variance is defined by a memory of\n$q_{m}$-exponencial, form ($e_{q_{m}=1}^{x}=e^{x}$). Specifically, we inspect\nthe self-correlation function of squared random variables as well as the\nkurtosis. In addition, by numerical procedures, we infer the stationary\nprobability density function of both of the heteroskedastic random variables\nand the variance, the multiscaling properties, the first-passage times\ndistribution, and the dependence degree. Finally, we introduce an asymmetric\nvariance version of the model that enables us to reproduce the so-called\nleverage effect in financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.2617v2"
    },
    {
        "title": "The Ups and Downs of Modeling Financial Time Series with Wiener Process\n  Mixtures",
        "authors": [
            "Damien Challet",
            "Pier Paolo Peirano"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Starting from inhomogeneous time scaling and linear decorrelation between\nsuccessive price returns, Baldovin and Stella recently proposed a way to build\na model describing the time evolution of a financial index. We first make it\nfully explicit by using Student distributions instead of power law-truncated\nL\\'evy distributions; we also show that the analytic tractability of the model\nextends to the larger class of symmetric generalized hyperbolic distributions\nand provide a full computation of their multivariate characteristic functions;\nmore generally, the stochastic processes arising in this framework are\nrepresentable as mixtures of Wiener processes. The Baldovin and Stella model,\nwhile mimicking well volatility relaxation phenomena such as the Omori law,\nfails to reproduce other stylized facts such as the leverage effect or some\ntime reversal asymmetries. We discuss how to modify the dynamics of this\nprocess in order to reproduce real data more accurately.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.4163v3"
    },
    {
        "title": "Ergodic Properties of Fractional Brownian-Langevin Motion",
        "authors": [
            "Weihua Deng",
            "Eli Barkai"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We investigate the time average mean square displacement\n$\\overline{\\delta^2}(x(t))=\\int_0^{t-\\Delta}[x(t^\\prime+\\Delta)-x(t^\\prime)]^2\ndt^\\prime/(t-\\Delta)$ for fractional Brownian and Langevin motion. Unlike the\npreviously investigated continuous time random walk model $\\overline{\\delta^2}$\nconverges to the ensemble average $<x^2 > \\sim t^{2 H}$ in the long measurement\ntime limit. The convergence to ergodic behavior is however slow, and\nsurprisingly the Hurst exponent $H=3/4$ marks the critical point of the speed\nof convergence. When $H<3/4$, the ergodicity breaking parameter ${EB} = {Var}\n(\\overline{\\delta^2}) / < \\overline{\\delta^2} >^2\\sim k(H) \\cdot\\Delta\\cdot\nt^{-1}$, when $H=3/4$, ${EB} \\sim (9/16)(\\ln t) \\cdot\\Delta \\cdot t^{-1}$, and\nwhen $3/4<H <1, {EB} \\sim k(H)\\Delta^{4-4H} t^{4H-4}$. In the ballistic limit\n$H \\to 1$ ergodicity is broken and ${EB} \\sim 2$. The critical point $H=3/4$ is\nmarked by the divergence of the coefficient $k(H)$. Fractional Brownian motion\nas a model for recent experiments of sub-diffusion of mRNA in the cell is\nbriefly discussed and comparison with the continuous time random walk model is\nmade.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.2430v1"
    },
    {
        "title": "Hierarchical Bayesian sparse image reconstruction with application to\n  MRFM",
        "authors": [
            "Nicolas Dobigeon",
            "Alfred O. Hero",
            "Jean-Yves Tourneret"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  This paper presents a hierarchical Bayesian model to reconstruct sparse\nimages when the observations are obtained from linear transformations and\ncorrupted by an additive white Gaussian noise. Our hierarchical Bayes model is\nwell suited to such naturally sparse image applications as it seamlessly\naccounts for properties such as sparsity and positivity of the image via\nappropriate Bayes priors. We propose a prior that is based on a weighted\nmixture of a positive exponential distribution and a mass at zero. The prior\nhas hyperparameters that are tuned automatically by marginalization over the\nhierarchical Bayesian model. To overcome the complexity of the posterior\ndistribution, a Gibbs sampling strategy is proposed. The Gibbs samples can be\nused to estimate the image to be recovered, e.g. by maximizing the estimated\nposterior distribution. In our fully Bayesian approach the posteriors of all\nthe parameters are available. Thus our algorithm provides more information than\nother previously proposed sparse reconstruction methods that only give a point\nestimate. The performance of our hierarchical Bayesian sparse reconstruction\nmethod is illustrated on synthetic and real data collected from a tobacco virus\nsample using a prototype MRFM instrument.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.3650v2"
    },
    {
        "title": "X-ray tracing using Geant4",
        "authors": [
            "Ernst-Jan Buis",
            "Giuseppe Vacanti"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  We describe an extension to the Geant4 software package that allows it to be\nused as a general purpose X-ray tracing package. We demonstrate its use by\nbuilding a model of the X-ray optics of the XMM-Newton, calculating its\neffective area, and comparing the results with the published calibration\ncurves.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.1273v4"
    },
    {
        "title": "Microdynamics in stationary complex networks",
        "authors": [
            "Aurelien Gautreau",
            "Alain Barrat",
            "Marc Barthelemy"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Many complex systems, including networks, are not static but can display\nstrong fluctuations at various time scales. Characterizing the dynamics in\ncomplex networks is thus of the utmost importance in the understanding of these\nnetworks and of the dynamical processes taking place on them. In this article,\nwe study the example of the US airport network in the time period 1990-2000. We\nshow that even if the statistical distributions of most indicators are\nstationary, an intense activity takes place at the local (`microscopic') level,\nwith many disappearing/appearing connections (links) between airports. We find\nthat connections have a very broad distribution of lifetimes, and we introduce\na set of metrics to characterize the links' dynamics. We observe in particular\nthat the links which disappear have essentially the same properties as the ones\nwhich appear, and that links which connect airports with very different traffic\nare very volatile. Motivated by this empirical study, we propose a model of\ndynamical networks, inspired from previous studies on firm growth, which\nreproduces most of the empirical observations both for the stationary\nstatistical distributions and for the dynamical properties.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.1051v1"
    },
    {
        "title": "Evolutionary Construction of Geographical Networks with Nearly Optimal\n  Robustness and Efficient Routing Properties",
        "authors": [
            "Yukio Hayashi"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Robust and efficient design of networks on a realistic geographical space is\none of the important issues for the realization of dependable communication\nsystems. In this paper, based on a percolation theory and a geometric graph\nproperty, we investigate such a design from the following viewpoints: 1)\nnetwork evolution according to a spatially heterogeneous population, 2)\ntrimodal low degrees for the tolerant connectivity against both failures and\nattacks, and 3) decentralized routing within short paths. Furthermore, we point\nout the weakened tolerance by geographical constraints on local cycles, and\npropose a practical strategy by adding a small fraction of shortcut links\nbetween randomly chosen nodes in order to improve the robustness to a similar\nlevel to that of the optimal bimodal networks with a larger degree\n$O(\\sqrt{N})$ for the network size $N$. These properties will be useful for\nconstructing future ad-hoc networks in wide-area communications.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.2827v3"
    },
    {
        "title": "Enhanced Wavelet Analysis of Solar Magnetic Activity with Comparison to\n  Global Temperature and the Central England Temperature Record",
        "authors": [
            "Robert W. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The continuous wavelet transform may be enhanced by deconvolution with the\nwavelet response function. After correcting for the cone-of-influence, the\npower spectral density of the solar magnetic record as given by the derectified\nyearly sunspot number is calculated, revealing a spectrum of odd harmonics of\nthe fundamental Hale cycle, and the integrated instant power is compared to a\nreconstruction of global temperature in a normalized scatter plot displaying a\npositive correlation after the turn of the twentieth century. Comparison of the\nspectrum with that obtained from the Central England Temperature record\nsuggests that some features are shared while others are not, and the scatter\nplot again indicates a possible correlation.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2438v2"
    },
    {
        "title": "Content-based and Algorithmic Classifications of Journals: Perspectives\n  on the Dynamics of Scientific Communication and Indexer Effects",
        "authors": [
            "Ismael Rafols",
            "Loet Leydesdorff"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  The aggregated journal-journal citation matrix -based on the Journal Citation\nReports (JCR) of the Science Citation Index- can be decomposed by indexers\nand/or algorithmically. In this study, we test the results of two recently\navailable algorithms for the decomposition of large matrices against two\ncontent-based classifications of journals: the ISI Subject Categories and the\nfield/subfield classification of Glaenzel & Schubert (2003). The content-based\nschemes allow for the attribution of more than a single category to a journal,\nwhereas the algorithms maximize the ratio of within-category citations over\nbetween-category citations in the aggregated category-category citation matrix.\nBy adding categories, indexers generate between-category citations, which may\nenrich the database, for example, in the case of inter-disciplinary\ndevelopments. The consequent indexer effects are significant in sparse areas of\nthe matrix more than in denser ones. Algorithmic decompositions, on the other\nhand, are more heavily skewed towards a relatively small number of categories,\nwhile this is deliberately counter-acted upon in the case of content-based\nclassifications. Because of the indexer effects, science policy studies and the\nsociology of science should be careful when using content-based\nclassifications, which are made for bibliographic disclosure, and not for the\npurpose of analyzing latent structures in scientific communications. Despite\nthe large differences among them, the four classification schemes enable us to\ngenerate surprisingly similar maps of science at the global level. Erroneous\nclassifications are cancelled as noise at the aggregate level, but may disturb\nthe evaluation locally.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.4332v1"
    },
    {
        "title": "Identifying Relevant Eigenimages - a Random Matrix Approach",
        "authors": [
            "Yu Ding",
            "Yiu-Cho Chung",
            "Kun Huang",
            "Orlando P. Simonetti"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  Dimensional reduction of high dimensional data can be achieved by keeping\nonly the relevant eigenmodes after principal component analysis. However,\ndifferentiating relevant eigenmodes from the random noise eigenmodes is\nproblematic. A new method based on the random matrix theory and a statistical\ngoodness-of-fit test is proposed in this paper. It is validated by numerical\nsimulations and applied to real-time magnetic resonance cardiac cine images.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.4618v1"
    },
    {
        "title": "Superstatistical fluctuations in time series: Applications to\n  share-price dynamics and turbulence",
        "authors": [
            "Erik Van der Straeten",
            "Christian Beck"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We report a general technique to study a given experimental time series with\nsuperstatistics. Crucial for the applicability of the superstatistics concept\nis the existence of a parameter $\\beta$ that fluctuates on a large time scale\nas compared to the other time scales of the complex system under consideration.\nThe proposed method extracts the main superstatistical parameters out of a\ngiven data set and examines the validity of the superstatistical model\nassumptions. We test the method thoroughly with surrogate data sets. Then the\napplicability of the superstatistical approach is illustrated using real\nexperimental data. We study two examples, velocity time series measured in\nturbulent Taylor-Couette flows and time series of log returns of the closing\nprices of some stock market indices.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.2271v2"
    },
    {
        "title": "Bayesian reasoning in cosmology",
        "authors": [
            "Jakub Mielczarek",
            "Marek Szydlowski",
            "Pawel Tambor"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We discuss epistemological and methodological aspects of the Bayesian\napproach in astrophysics and cosmology. The introduction to the Bayesian\nframework is given for a further discussion concerning the Bayesian inference\nin physics. The interplay between the modern cosmology, Bayesian statistics,\nand philosophy of science is presented. We consider paradoxes of confirmation,\nlike Goodman's paradox, appearing in the Bayesian theory of confirmation. As in\nGoodman's paradox the Bayesian inference is susceptible to some epistemic\nlimitations in the logic of induction. However Goodman's paradox applied to\ncosmological hypotheses seems to be resolved due to the evolutionary character\nof cosmology and accumulation new empirical evidences. We argue that the\nBayesian framework is useful in the context of falsificability of quantum\ncosmological models, as well as contemporary dark energy and dark matter\nproblem.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.4075v2"
    },
    {
        "title": "Extracting Spooky-activation-at-a-distance from Considerations of\n  Entanglement",
        "authors": [
            "P. D. Bruza",
            "K. Kitto",
            "D. Nelson",
            "C. McEvoy"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Following an early claim by Nelson & McEvoy \\cite{Nelson:McEvoy:2007}\nsuggesting that word associations can display `spooky action at a distance\nbehaviour', a serious investigation of the potentially quantum nature of such\nassociations is currently underway. This paper presents a simple quantum model\nof a word association system. It is shown that a quantum model of word\nentanglement can recover aspects of both the Spreading Activation equation and\nthe Spooky-activation-at-a-distance equation, both of which are used to model\nthe activation level of words in human memory.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.4375v1"
    },
    {
        "title": "Waiting time dynamics of priority-queue networks",
        "authors": [
            "Byungjoon Min",
            "K. -I. Goh",
            "I. -M. Kim"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We study the dynamics of priority-queue networks, generalizations of the\nbinary interacting priority queue model introduced by Oliveira and Vazquez\n[Physica A {\\bf 388}, 187 (2009)]. We found that the original AND-type protocol\nfor interacting tasks is not scalable for the queue networks with loops because\nthe dynamics becomes frozen due to the priority conflicts. We then consider a\nscalable interaction protocol, an OR-type one, and examine the effects of the\nnetwork topology and the number of queues on the waiting time distributions of\nthe priority-queue networks, finding that they exhibit power-law tails in all\ncases considered, yet with model-dependent power-law exponents. We also show\nthat the synchronicity in task executions, giving rise to priority conflicts in\nthe priority-queue networks, is a relevant factor in the queue dynamics that\ncan change the power-law exponent of the waiting time distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.0704v2"
    },
    {
        "title": "Methods for detection and characterization of signals in noisy data with\n  the Hilbert-Huang Transform",
        "authors": [
            "Alexander Stroeer",
            "John K. Cannizzo",
            "Jordan B. Camp",
            "Nicolas Gagarin"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  The Hilbert-Huang Transform is a novel, adaptive approach to time series\nanalysis that does not make assumptions about the data form. Its adaptive,\nlocal character allows the decomposition of non-stationary signals with\nhightime-frequency resolution but also renders it susceptible to degradation\nfrom noise. We show that complementing the HHT with techniques such as\nzero-phase filtering, kernel density estimation and Fourier analysis allows it\nto be used effectively to detect and characterize signals with low signal to\nnoise ratio.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.4616v1"
    },
    {
        "title": "Information erasure lurking behind measures of complexity",
        "authors": [
            "Karoline Wiesner",
            "Mile Gu",
            "Elisabeth Rieper",
            "Vlatko Vedral"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Complex systems are found in most branches of science. It is still argued how\nto best quantify their complexity and to what end. One prominent measure of\ncomplexity (the statistical complexity) has an operational meaning in terms of\nthe amount of resources needed to forecasting a system's behaviour. Another one\n(the effective measure complexity, aka excess entropy) is a measure of mutual\ninformation stored in the system proper. We show that for any given system the\ntwo measures differ by the amount of information erased during forecasting. We\ninterpret the difference as inefficiency of a given model. We find a bound to\nthe ratio of the two measures defined as information-processing efficiency, in\nanalogy to the second law of thermodynamics. This new link between two\nprominent measures of complexity provides a quantitative criterion for good\nmodels of complex systems, namely those with little information erasure.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.2918v2"
    },
    {
        "title": "Effective and Efficient Similarity Index for Link Prediction of Complex\n  Networks",
        "authors": [
            "Linyuan Lv",
            "Ci-Hang Jin",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Predictions of missing links of incomplete networks like protein-protein\ninteraction networks or very likely but not yet existent links in evolutionary\nnetworks like friendship networks in web society can be considered as a\nguideline for further experiments or valuable information for web users. In\nthis paper, we introduce a local path index to estimate the likelihood of the\nexistence of a link between two nodes. We propose a network model with\ncontrollable density and noise strength in generating links, as well as collect\ndata of six real networks. Extensive numerical simulations on both modeled\nnetworks and real networks demonstrated the high effectiveness and efficiency\nof the local path index compared with two well-known and widely used indices,\nthe common neighbors and the Katz index. Indeed, the local path index provides\ncompetitively accurate predictions as the Katz index while requires much less\nCPU time and memory space, which is therefore a strong candidate for potential\npractical applications in data mining of huge-size networks.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.3558v2"
    },
    {
        "title": "Chi-Square Tests for Comparing Weighted Histograms",
        "authors": [
            "N. D. Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Weighted histograms in Monte Carlo simulations are often used for the\nestimation of probability density functions. They are obtained as a result of\nrandom experiments with random events that have weights. In this paper, the bin\ncontents of a weighted histogram are considered as a sum of random variables\nwith a random number of terms. Generalizations of the classical chi-square test\nfor comparing weighted histograms was proposed. Numerical examples illustrate\nan application of the tests for the histograms with different statistics of\nevents and different weighted functions. The proposed tests can be used for the\ncomparison of experimental data histograms with simulated data histograms, as\nwell as for the two simulated data histograms.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.4221v3"
    },
    {
        "title": "Understanding the spreading patterns of mobile phone viruses",
        "authors": [
            "P. Wang",
            "M. Gonzalez",
            "C. A. Hidalgo",
            "A. -L. Barabasi"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We model the mobility of mobile phone users to study the fundamental\nspreading patterns characterizing a mobile virus outbreak. We find that while\nBluetooth viruses can reach all susceptible handsets with time, they spread\nslowly due to human mobility, offering ample opportunities to deploy antiviral\nsoftware. In contrast, viruses utilizing multimedia messaging services could\ninfect all users in hours, but currently a phase transition on the underlying\ncall graph limits them to only a small fraction of the susceptible users. These\nresults explain the lack of a major mobile virus breakout so far and predict\nthat once a mobile operating system's market share reaches the phase transition\npoint, viruses will pose a serious threat to mobile communications.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.4567v1"
    },
    {
        "title": "Entropic Priors and Bayesian Model Selection",
        "authors": [
            "Brendon J. Brewer",
            "Matthew J. Francis"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We demonstrate that the principle of maximum relative entropy (ME), used\njudiciously, can ease the specification of priors in model selection problems.\nThe resulting effect is that models that make sharp predictions are\ndisfavoured, weakening the usual Bayesian \"Occam's Razor\". This is illustrated\nwith a simple example involving what Jaynes called a \"sure thing\" hypothesis.\nJaynes' resolution of the situation involved introducing a large number of\nalternative \"sure thing\" hypotheses that were possible before we observed the\ndata. However, in more complex situations, it may not be possible to explicitly\nenumerate large numbers of alternatives. The entropic priors formalism produces\nthe desired result without modifying the hypothesis space or requiring explicit\nenumeration of alternatives; all that is required is a good model for the prior\npredictive distribution for the data. This idea is illustrated with a simple\nrigged-lottery example, and we outline how this idea may help to resolve a\nrecent debate amongst cosmologists: is dark energy a cosmological constant, or\nhas it evolved with time in some way? And how shall we decide, when the data\nare in?\n",
        "pdf_link": "http://arxiv.org/pdf/0906.5609v2"
    },
    {
        "title": "Effect of user tastes on personalized recommendation",
        "authors": [
            "Jian-Guo Liu",
            "Tao Zhou",
            "Qiang Guo",
            "Bing-Hong Wang",
            "Yi-Cheng Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this paper, based on a weighted projection of the user-object bipartite\nnetwork, we study the effects of user tastes on the mass-diffusion-based\npersonalized recommendation algorithm, where a user's tastes or interests are\ndefined by the average degree of the objects he has collected. We argue that\nthe initial recommendation power located on the objects should be determined by\nboth of their degree and the users' tastes. By introducing a tunable parameter,\nthe user taste effects on the configuration of initial recommendation power\ndistribution are investigated. The numerical results indicate that the\npresented algorithm could improve the accuracy, measured by the average ranking\nscore, more importantly, we find that when the data is sparse, the algorithm\nshould give more recommendation power to the objects whose degrees are close to\nthe users' tastes, while when the data becomes dense, it should assign more\npower on the objects whose degrees are significantly different from user's\ntastes.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.1224v2"
    },
    {
        "title": "Degree correlation effect of bipartite network on personalized\n  recommendation",
        "authors": [
            "Jian-Guo Liu",
            "Tao Zhou",
            "Zhao-Guo Xuan",
            "Hong-An Che",
            "Bing-Hong Wang",
            "Yi-Cheng Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this paper, by introducing a new user similarity index base on the\ndiffusion process, we propose a modified collaborative filtering (MCF)\nalgorithm, which has remarkably higher accuracy than the standard collaborative\nfiltering. In the proposed algorithm, the degree correlation between users and\nobjects is taken into account and embedded into the similarity index by a\ntunable parameter. The numerical simulation on a benchmark data set shows that\nthe algorithmic accuracy of the MCF, measured by the average ranking score, is\nfurther improved by 18.19% in the optimal case. In addition, two significant\ncriteria of algorithmic performance, diversity and popularity, are also taken\ninto account. Numerical results show that the presented algorithm can provide\nmore diverse and less popular recommendations, for example, when the\nrecommendation list contains 10 objects, the diversity, measured by the hamming\ndistance, is improved by 21.90%.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.1228v1"
    },
    {
        "title": "Limits, discovery and cut optimization for a Poisson process with\n  uncertainty in background and signal efficiency: TRolke 2.0",
        "authors": [
            "J. Lundberg",
            "J. Conrad",
            "W. Rolke",
            "A. Lopez"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  A C++ class was written for the calculation of frequentist confidence\nintervals using the profile likelihood method. Seven combinations of Binomial,\nGaussian, Poissonian and Binomial uncertainties are implemented. The package\nprovides routines for the calculation of upper and lower limits, sensitivity\nand related properties. It also supports hypothesis tests which take\nuncertainties into account. It can be used in compiled C++ code, in Python or\ninteractively via the ROOT analysis framework.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.3450v2"
    },
    {
        "title": "N-tuple Zipf Analysis and Modeling for Language, Computer Program and\n  DNA",
        "authors": [
            "Xiaocong Gan",
            "Dahui Wang",
            "Zhangang Han"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  n-tuple power law widely exists in language, computer program code, DNA and\nmusic. After a vast amount of Zipf analyses of n-tuple power law from empirical\ndata, we propose a model to explain the n-tuple power law feature existed in\nthese information translational carriers. Our model is a preferential selection\napproach inspired by Simon's model which explained scaling law of single symbol\nin a sequence Zipf analysis. The kernel mechanism is neat and simple in our\nmodel. It can be simply described as a randomly copy and paste process, that\nis, randomly select a random segment from current sequence and attach it to the\nend repeatedly. The simulation of our model shows that n-tuple power law exists\nin model generated data. Furthermore, two estimation equations: the Zipf\nexponent and the minimal length of n-tuple for power law appears all correspond\nto empirical data well. Our model can also reproduce the symmetry breaking\nprocess of ATGC number differences in DNA data.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.0500v1"
    },
    {
        "title": "A statistical model for the relation between exoplanets and their host\n  stars",
        "authors": [
            "E. Martinez-Gomez",
            "G. J. Babu"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  A general model is proposed to explain the relation between the extrasolar\nplanets (or exoplanets) detected until June 2008 and the main characteristics\nof their host stars through statistical techniques. The main goal is to\nestablish a mathematical relation among the set of variables which better\ndescribe the physical characteristics of the host star and the planet itself.\nThe host star is characterized by its distance, age, effective temperature,\nmass, metallicity, radius and magnitude. The exoplanet is described through its\nphysical parameters (radius and mass) and its orbital parameters (distance,\nperiod, eccentricity, inclination and major semiaxis). As a first approach we\nconsider that only the mass of the exoplanet is being determined by the\nphysical properties of its host star. The proposed model is then validated\nthrough statistical analysis. Finally we discuss the categorical behavior of\nthe dependent variable through binary models.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.4056v1"
    },
    {
        "title": "A Quantitative Measure of Theoretical Scientific Merit",
        "authors": [
            "Bruce Knuteson"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Program review in the physical sciences may benefit from a framework within\nwhich to quantitatively discuss the scientific merit of a proposed theoretical\nprogram of research, and to assess the scientific merit of a particular\ntheoretical paper. This article interprets a previously proposed measure of\nexperimental scientific merit in a manner appropriate for quantifying the\nscientific merit of completed and proposed theoretical research. With this\ninterpretation, the resulting figure of merit represents a proposal for a\nquantitative measure of total scientific merit.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.2361v1"
    },
    {
        "title": "Empirical analysis of web-based user-object bipartite networks",
        "authors": [
            "Mingsheng Shang",
            "Linyuan Lu",
            "Yi-Cheng Zhang",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Understanding the structure and evolution of web-based user-object networks\nis a significant task since they play a crucial role in e-commerce nowadays.\nThis Letter reports the empirical analysis on two large-scale web sites,\naudioscrobbler.com and del.icio.us, where users are connected with music groups\nand bookmarks, respectively. The degree distributions and degree-degree\ncorrelations for both users and objects are reported. We propose a new index,\nnamed collaborative clustering coefficient, to quantify the clustering behavior\nbased on the collaborative selection. Accordingly, the clustering properties\nand clustering-degree correlations are investigated. We report some novel\nphenomena well characterizing the selection mechanism of web users and outline\nthe relevance of these phenomena to the information recommendation problem.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.4938v2"
    },
    {
        "title": "Cracking the Liu key exchange protocol in its most secure state with\n  Lorentzian spectra",
        "authors": [
            "Lazar L. Kish",
            "Bruce Zhang",
            "Laszlo B. Kish"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  We have found a security risk in the Liu's cypher based on random signals and\nfeedback, when it utilizes a large class of noises for communication in its\nmost secure state, the steady state. For the vulnerability to exist, the noise\nmust have a spectrum which can be transformed to white-like noise by linear\nfiltering. For the cracking, we utilize the natural properties of power density\nspectra and autocorrelation functions. We introduce and demonstrate the method\nfor Lorentzian spectra. Some of the implications of the results concern the\ntransient operation during changing bits, where the modulation products of\nnoise cannot be band-limited therefore the cypher is vulnerable. We propose the\napplication of line filters to provide a proper spectral shape and to improve\nthe security.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.0665v5"
    },
    {
        "title": "Application of the rule-growing algorithm RIPPER to particle physics\n  analysis",
        "authors": [
            "Markward Britsch",
            "Nikolai Gagunashvili",
            "Michael Schmelling"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  A large hadron machine like the LHC with its high track multiplicities always\nasks for powerful tools that drastically reduce the large background while\nselecting signal events efficiently. Actually such tools are widely needed and\nused in all parts of particle physics. Regarding the huge amount of data that\nwill be produced at the LHC, the process of training as well as the process of\napplying these tools to data, must be time efficient. Such tools can be\nmultivariate analysis -- also called data mining -- tools. In this contribution\nwe present the results for the application of the multivariate analysis, rule\ngrowing algorithm RIPPER on a problem of particle selection. It turns out that\nthe meta-methods bagging and cost-sensitivity are essential for the quality of\nthe outcome. The results are compared to other multivariate analysis\ntechniques.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.1729v1"
    },
    {
        "title": "Has the world economy reached its globalization limit?",
        "authors": [
            "Janusz Miskiewicz",
            "Marcel Ausloos"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  The economy globalization measure problem is discussed. Four macroeconomic\nindices of twenty among the \"richest\" countries are examined. Four types of\n\"distances\" are calculated.Two types of networks are next constructed for each\ndistance measure definition. It is shown that the globalization process can be\nbest characterised by an entropy measure, based on entropy Manhattan distance.\nIt is observed that a globalization maximum was reached in the interval\n1970-2000. More recently a deglobalization process is observed.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.3695v1"
    },
    {
        "title": "Photogrammetry and ballistic analysis of a high-flying projectile in the\n  STS-124 space shuttle launch",
        "authors": [
            "Philip T. Metzger",
            "John E. Lane",
            "Robert A. Carilli",
            "Jason M. Long",
            "Kathy L. Shawn"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  A method combining photogrammetry with ballistic analysis is demonstrated to\nidentify flying debris in a rocket launch environment. Debris traveling near\nthe STS-124 Space Shuttle was captured on cameras viewing the launch pad within\nthe first few seconds after launch. One particular piece of debris caught the\nattention of investigators studying the release of flame trench fire bricks\nbecause its high trajectory could indicate a flight risk to the Space Shuttle.\nDigitized images from two pad perimeter high-speed 16-mm film cameras were\nprocessed using photogrammetry software based on a multi-parameter optimization\ntechnique. Reference points in the image were found from 3D CAD models of the\nlaunch pad and from surveyed points on the pad. The three-dimensional reference\npoints were matched to the equivalent two-dimensional camera projections by\noptimizing the camera model parameters using a gradient search optimization\ntechnique. Using this method of solving the triangulation problem, the xyz\nposition of the object's path relative to the reference point coordinate system\nwas found for every set of synchronized images. This trajectory was then\ncompared to a predicted trajectory while performing regression analysis on the\nballistic coefficient and other parameters. This identified, with a high degree\nof confidence, the object's material density and thus its probable origin\nwithin the launch pad environment. Future extensions of this methodology may\nmake it possible to diagnose the underlying causes of debris-releasing events\nin near-real time, thus improving flight safety.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.4357v1"
    },
    {
        "title": "Musical Genres: Beating to the Rhythms of Different Drums",
        "authors": [
            "Debora C. Correa",
            "Jose H. Saito",
            "Luciano da F. Costa"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Online music databases have increased signicantly as a consequence of the\nrapid growth of the Internet and digital audio, requiring the development of\nfaster and more efficient tools for music content analysis. Musical genres are\nwidely used to organize music collections. In this paper, the problem of\nautomatic music genre classification is addressed by exploring rhythm-based\nfeatures obtained from a respective complex network representation. A Markov\nmodel is build in order to analyse the temporal sequence of rhythmic notation\nevents. Feature analysis is performed by using two multivariate statistical\napproaches: principal component analysis(unsupervised) and linear discriminant\nanalysis (supervised). Similarly, two classifiers are applied in order to\nidentify the category of rhythms: parametric Bayesian classifier under gaussian\nhypothesis (supervised), and agglomerative hierarchical clustering\n(unsupervised). Qualitative results obtained by Kappa coefficient and the\nobtained clusters corroborated the effectiveness of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.3842v1"
    },
    {
        "title": "Collaboration in sensor network research: an in-depth longitudinal\n  analysis of assortative mixing patterns",
        "authors": [
            "Alberto Pepe",
            "Marko A. Rodriguez"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Many investigations of scientific collaboration are based on statistical\nanalyses of large networks constructed from bibliographic repositories. These\ninvestigations often rely on a wealth of bibliographic data, but very little or\nno other information about the individuals in the network, and thus, fail to\nillustrate the broader social and academic landscape in which collaboration\ntakes place. In this article, we perform an in-depth longitudinal analysis of a\nrelatively small network of scientific collaboration (N = 291) constructed from\nthe bibliographic record of a research center involved in the development and\napplication of sensor network and wireless technologies. We perform a\npreliminary analysis of selected structural properties of the network,\ncomputing its range, configuration and topology. We then support our\npreliminary statistical analysis with an in-depth temporal investigation of the\nassortative mixing of selected node characteristics, unveiling the researchers'\npropensity to collaborate preferentially with others with a similar academic\nprofile. Our qualitative analysis of mixing patterns offers clues as to the\nnature of the scientific community being modeled in relation to its\norganizational, disciplinary, institutional, and international arrangements of\ncollaboration.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.4223v2"
    },
    {
        "title": "Edge Adapted Wavelets, Solar Magnetic Activity, and Climate Change",
        "authors": [
            "Robert W. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  The continuous wavelet transform is adapted to account for signal truncation\nthrough renormalization and by modifying the shape of the analyzing window.\nComparison is made of the instant and integrated wavelet power with previous\nalgorithms. The edge adapted and renormalized admissible wavelet transforms are\nused to estimate the level of solar magnetic activity from the sunspot record.\nThe solar activity is compared to Oerlemans' temperature reconstruction and to\nthe Central England Temperature record. A correlation is seen for years between\n1610 and 1990, followed by a strong deviation as the recently observed\ntemperature increases.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.4663v1"
    },
    {
        "title": "Repeater-assisted Zeno effect in classical stochastic processes",
        "authors": [
            "Shi-Jian Gu",
            "Li-Gang Wang",
            "Zhi-Guo Wang",
            "Hai-Qing Lin"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  As a classical state, for instance a digitized image, is transferred through\na classical channel, it decays inevitably with the distance due to the\nsurroundings' interferences. However, if there are enough number of repeaters,\nwhich can both check and recover the state's information continuously, the\nstate's decay rate will be significantly suppressed, then a classical Zeno\neffect might occur. Such a physical process is purely classical and without any\ninterferences of living beings, therefore, it manifests that the Zeno effect is\nno longer a patent of quantum mechanics, but does exist in classical stochastic\nprocesses.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.5388v1"
    },
    {
        "title": "Line Graphs of Weighted Networks for Overlapping Communities",
        "authors": [
            "T. S. Evans",
            "R. Lambiotte"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  In this paper, we develop the idea to partition the edges of a weighted graph\nin order to uncover overlapping communities of its nodes. Our approach is based\non the construction of different types of weighted line graphs, i.e. graphs\nwhose nodes are the links of the original graph, that encapsulate differently\nthe relations between the edges. Weighted line graphs are argued to provide an\nalternative, valuable representation of the system's topology, and are shown to\nhave important applications in community detection, as the usual node partition\nof a line graph naturally leads to an edge partition of the original graph.\nThis identification allows us to use traditional partitioning methods in order\nto address the long-standing problem of the detection of overlapping\ncommunities. We apply it to the analysis of different social and geographical\nnetworks.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.4389v2"
    },
    {
        "title": "Segmentation algorithm for non-stationary compound Poisson processes",
        "authors": [
            "Bence Toth",
            "Fabrizio Lillo",
            "J. Doyne Farmer"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We introduce an algorithm for the segmentation of a class of regime switching\nprocesses. The segmentation algorithm is a non parametric statistical method\nable to identify the regimes (patches) of the time series. The process is\ncomposed of consecutive patches of variable length, each patch being described\nby a stationary compound Poisson process, i.e. a Poisson process where each\ncount is associated to a fluctuating signal. The parameters of the process are\ndifferent in each patch and therefore the time series is non stationary. Our\nmethod is a generalization of the algorithm introduced by Bernaola-Galvan, et\nal., Phys. Rev. Lett., 87, 168105 (2001). We show that the new algorithm\noutperforms the original one for regime switching compound Poisson processes.\nAs an application we use the algorithm to segment the time series of the\ninventory of market members of the London Stock Exchange and we observe that\nour method finds almost three times more patches than the original one.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.2549v2"
    },
    {
        "title": "On the Correct Use of Statistical Tests: Reply to \"Lies, damned lies and\n  statistics (in Geology)\"",
        "authors": [
            "D. Sornette",
            "V. Pisarenko"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  In a Forum published in EOS Transactions AGU (2009) entitled \"Lies, damned\nlies and statistics (in Geology)\", Vermeesch (2009) claims that \"statistical\nsignificant is not the same as geological significant\", in other words,\nstatistical tests may be misleading. In complete contradiction, we affirm that\nstatistical tests are always informative. We detail the several mistakes of\nVermeesch in his initial paper and in his comments to our reply. The present\ntext is developed in the hope that it can serve as an illuminating pedagogical\nexercise for students and lecturers to learn more about the subtleties,\nrichness and power of the science of statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.4158v4"
    },
    {
        "title": "Analytical continuation of imaginary axis data using maximum entropy",
        "authors": [
            "O. Gunnarsson",
            "M. W. Haverkort",
            "G. Sangiovanni"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We study the maximum entropy (MaxEnt) approach for analytical continuation of\nspectral data from imaginary times to real frequencies. The total error is\ndivided in a statistical error, due to the noise in the input data, and a\nsystematic error, due to deviations of the default function, used in the MaxEnt\napproach, from the exact spectrum. We find that the MaxEnt approach in its\nclassical formulation can lead to a nonoptimal balance between the two types of\nerrors, leading to an unnecessary large statistical error. The statistical\nerror can be reduced by splitting up the data in several batches, performing a\nMaxEnt calculation for each batch and averaging. This can outweigh an increase\nin the systematic error resulting from this approach. The output from the\nMaxEnt result can be used as a default function for a new MaxEnt calculation.\nSuch iterations often lead to worse results due to an increase in the\nstatistical error. By splitting up the data in batches, the statistical error\nis reduced and and the increase resulting from iterations can be outweighed by\na decrease in the systematic error. Finally we consider a linearized version to\nobtain a better understanding of the method.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.4351v1"
    },
    {
        "title": "Influence of solar magnetic activity on the North American temperature\n  record",
        "authors": [
            "Robert W. Johnson"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The effect of solar magnetic activity on the yearly mean average temperature\nis extracted from the historical record for much of North America. The level of\nsolar activity is derived from the international sunspot number by the\nrenormalized continuous wavelet transform using the Morlet basis to provide a\nrunning estimate of the power associated with the magnetic cycle. The solar\nactivity gives the abscissa for a scatter plot of temperature for each station,\nfrom which the solar dependence and mean temperature are extracted. These\nparameters are then plotted against the latitude, longitude, and elevation for\neach station, revealing a dependence of their values on geophysical location. A\nmechanism to explain the latitudinal variation of the solar dependence is\nsuggested.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.0078v1"
    },
    {
        "title": "On the meaning of the h-index",
        "authors": [
            "S. Redner"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The h-index -- the value for which an individual has published at least h\npapers with at least h citations -- has become a popular metric to assess the\ncitation impact of scientists. As already noted in the original work of Hirsch\nand as evidenced from data of a representative sample of physicists, sqrt{c}\nscales as h, where c is the total number citations to an individual. Thus\nsqrt{c} appears to be equivalent to the h index. As a further check of this\nequivalence, the distribution of the ratio s=sqrt{c}/2h for this sample is\nsharply peaked about 1. The outliers in this distribution reveal fundamentally\ndifferent types of individual publication records.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.0878v1"
    },
    {
        "title": "Interdependent networks: Reducing the coupling strength leads to a\n  change from a first to second order percolation transition",
        "authors": [
            "Roni Parshani",
            "Sergey V. Buldyrev",
            "Shlomo Havlin"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We study a system composed from two interdependent networks A and B, where a\nfraction of the nodes in network A depends on the nodes of network B and a\nfraction of the nodes in network B depends on the nodes of network A. Due to\nthe coupling between the networks when nodes in one network fail they cause\ndependent nodes in the other network to also fail. This invokes an iterative\ncascade of failures in both networks. When a critical fraction of nodes fail\nthe iterative process results in a percolation phase transition that completely\nfragments both networks. We show both analytically and numerically that\nreducing the coupling between the networks leads to a change from a first order\npercolation phase transition to a second order percolation transition at a\ncritical point. The scaling of the percolation order parameter near the\ncritical point is characterized by the critical exponent beta=1.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.3989v1"
    },
    {
        "title": "Detecting the Most Unusual Part of Two and Three-dimensional Digital\n  Images",
        "authors": [
            "Kostadin Koroutchev",
            "Elka Korutcheva"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The purpose of this paper is to introduce an algorithm that can detect the\nmost unusual part of a digital image in probabilistic setting. The most unusual\npart of a given shape is defined as a part of the image that has the maximal\ndistance to all non intersecting shapes with the same form. The method is\ntested on two and three-dimensional images and has shown very good results\nwithout any predefined model. A version of the method independent of the\ncontrast of the image is considered and is found to be useful for finding the\nmost unusual part (and the most similar part) of the image conditioned on given\nimage. The results can be used to scan large image databases, as for example\nmedical databases.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.0527v2"
    },
    {
        "title": "Detecting communities of triangles in complex networks using spectral\n  optimization",
        "authors": [
            "Belkacem Serrour",
            "Alex Arenas",
            "Sergio Gomez"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The study of the sub-structure of complex networks is of major importance to\nrelate topology and functionality. Many efforts have been devoted to the\nanalysis of the modular structure of networks using the quality function known\nas modularity. However, generally speaking, the relation between topological\nmodules and functional groups is still unknown, and depends on the semantic of\nthe links. Sometimes, we know in advance that many connections are transitive\nand, as a consequence, triangles have a specific meaning. Here we propose the\nstudy of the modular structure of networks considering triangles as the\nbuilding blocks of modules. The method generalizes the standard modularity and\nuses spectral optimization to find its maximum. We compare the partitions\nobtained with those resulting from the optimization of the standard modularity\nin several real networks. The results show that the information reported by the\nanalysis of modules of triangles complements the information of the classical\nmodularity analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.3658v1"
    },
    {
        "title": "Spreading Dynamics Following Bursty Human Activity Patterns",
        "authors": [
            "Byungjoon Min",
            "K. -I. Goh",
            "Alexei Vazquez"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We study the susceptible-infected model with power-law waiting time\ndistributions $P(\\tau)\\sim \\tau^{-\\alpha}$, as a model of spreading dynamics\nunder heterogeneous human activity patterns. We found that the average number\nof new infections $n(t)$ at time $t$ decays as a power law in the long time\nlimit, $n(t) \\sim t^{-\\beta}$, leading to extremely slow revalence decay.We\nalso found that the exponent in the spreading dynamics, $\\beta$, is related to\nthat in the waiting time distribution, $\\alpha$, in a way depending on the\ninteractions between agents but is insensitive to the network topology. These\nobservations are well supported by both the theoretical predictions and the\nlong prevalence decay time in real social spreading phenomena. Our results\nunify individual activity patterns with macroscopic collective dynamics at the\nnetwork level.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.2643v3"
    },
    {
        "title": "Noise-based deterministic logic and computing: a brief survey",
        "authors": [
            "Laszlo B. Kish",
            "Sunil P. Khatri",
            "Sergey M. Bezrukov",
            "Ferdinand Peper",
            "Zoltan Gingl",
            "Tamas Horvath"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  A short survey is provided about our recent explorations of the young topic\nof noise-based logic. After outlining the motivation behind noise-based\ncomputation schemes, we present a short summary of our ongoing efforts in the\nintroduction, development and design of several noise-based deterministic\nmultivalued logic schemes and elements. In particular, we describe classical,\ninstantaneous, continuum, spike and random-telegraph-signal based schemes with\napplications such as circuits that emulate the brain's functioning and string\nverification via a slow communication channel.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.5282v1"
    },
    {
        "title": "A Bayesian Foundation for Physical Theories",
        "authors": [
            "Roberto C. Alamino"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Bayesian probability theory is used as a framework to develop a formalism for\nthe scientific method based on principles of inductive reasoning. The formalism\nallows for precise definitions of the key concepts in theories of physics and\nalso leads to a well-defined procedure to select one or more theories among a\nfamily of (well-defined) candidates by ranking them according to their\nposterior probability distributions, which result from Bayes's theorem by\nincorporating to an initial prior the information extracted from a dataset,\nultimately defined by experimental evidence. Examples with different levels of\ncomplexity are given and three main applications to basic cosmological\nquestions are analysed: (i) typicality of human observers, (ii) the multiverse\nhypothesis and, extremely briefly, some few observations about (iii) the\nanthropic principle. Finally, it is demonstrated that this formulation can\naddress problems that were out of the scope of scientific research until now by\npresenting the isolated worlds problem and its resolution via the presented\nframework.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.1635v3"
    },
    {
        "title": "A Test for Equality of Distributions in High Dimensions",
        "authors": [
            "Wolfgang Rolke",
            "Angel Lopez"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We present a method which tests whether or not two datasets (one of which\ncould be Monte Carlo generated) might come from the same distribution. Our\nmethod works in arbitrarily high dimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.2877v1"
    },
    {
        "title": "Coherent Patterns in Nuclei and in Financial Markets",
        "authors": [
            "S. Drozdz",
            "J. Kwapien",
            "J. Speth"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  In the area of traditional physics the atomic nucleus belongs to the most\ncomplex systems. It involves essentially all elements that characterize\ncomplexity including the most distinctive one whose essence is a permanent\ncoexistence of coherent patterns and of randomness. From a more\ninterdisciplinary perspective, these are the financial markets that represent\nan extreme complexity. Here, based on the matrix formalism, we set some\nparallels between several characteristics of complexity in the above two\nsystems. We, in particular, refer to the concept - historically originating\nfrom nuclear physics considerations - of the random matrix theory and\ndemonstrate its utility in quantifying characteristics of the coexistence of\nchaos and collectivity also for the financial markets. In this later case we\nshow examples that illustrate mapping of the matrix formulation into the\nconcepts originating from the graph theory. Finally, attention is drawn to some\nnovel aspects of the financial coherence which opens room for speculation if\nanalogous effects can be detected in the atomic nuclei or in other strongly\ninteracting Fermi systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.1105v1"
    },
    {
        "title": "Uniqueness transition in noisy phase retrieval",
        "authors": [
            "Veit Elser",
            "Stefan Eisebitt"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Previous criteria for the feasibility of reconstructing phase information\nfrom intensity measurements, both in x-ray crystallography and more recently in\ncoherent x-ray imaging, have been based on the Maxwell constraint counting\nprinciple. We propose a new criterion, based on Shannon's mutual information,\nthat is better suited for noisy data or contrast that has strong priors not\nwell modeled by continuous variables. A natural application is magnetic domain\nimaging, where the criterion for uniqueness in the reconstruction takes the\nform that the number of photons, per pixel of contrast in the image, exceeds a\ncertain minimum. Detailed studies of a simple model show that the uniqueness\ntransition is of the type exhibited by spin glasses.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.4693v1"
    },
    {
        "title": "Extracting directed information flow networks: an application to\n  genetics and semantics",
        "authors": [
            "A. P. Masucci",
            "A. Kalampokis",
            "V. M. Eguíluz",
            "E. Hernández-García"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We introduce a general method to infer the directional information flow\nbetween populations whose elements are described by n-dimensional vectors of\nsymbolic attributes. The method is based on the Jensen-Shannon divergence and\non the Shannon entropy and has a wide range of application. We show here the\nresults of two applications: first extracting the network of genetic flow\nbetween the meadows of the seagrass Poseidonia Oceanica, where the meadow\nelements are specified by sets of microsatellite markers, then we extract the\nsemantic flow network from a set of Wikipedia pages, showing the semantic\nchannels between different areas of knowledge.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.4797v2"
    },
    {
        "title": "First-Digit Law in Nonextensive Statistics",
        "authors": [
            "Lijing Shao",
            "Bo-Qiang Ma"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Nonextensive statistics, characterized by a nonextensive parameter $q$, is a\npromising and practically useful generalization of the Boltzmann statistics to\ndescribe power-law behaviors from physical and social observations. We here\nexplore the unevenness of the first digit distribution of nonextensive\nstatistics analytically and numerically. We find that the first-digit\ndistribution follows Benford's law and fluctuates slightly in a periodical\nmanner with respect to the logarithm of the temperature. The fluctuation\ndecreases when $q$ increases, and the result converges to Benford's law exactly\nas $q$ approaches 2. The relevant regularities between nonextensive statistics\nand Benford's law are also presented and discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.2699v1"
    },
    {
        "title": "Automatic detection and tracking of dust particles in a RF plasma sheath",
        "authors": [
            "Y. Zayachuk",
            "F. Brochard",
            "S. Bardin",
            "J-L. Briançon",
            "R. Hugon",
            "J. Bougdira"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  A method enabling automatic detection and tracking of large amounts of\nindividual dust particles in plasmas is presented. Individual trajectories can\nbe found with a good spatiotemporal resolution, even without applying any\nexternal light source to facilitate detection. Main advantages of this method\nis a high portability and the possibility of making statistical analyses of the\ntrajectories of a large amount of non uniformly size distributed particles,\nunder challenging illumination conditions and with low time and computational\nresources. In order to evaluate the efficiency of different detection and\ntracking strategies statistically, an experimental benchmark is proposed, and\ncompleted by numerical simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.3432v1"
    },
    {
        "title": "The critical effect of dependency groups on the function of networks",
        "authors": [
            "Roni Parshani",
            "Sergey V. Buldyrev",
            "Shlomo Havlin"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Current network models assume one type of links to define the relations\nbetween the network entities. However, many real networks can only be correctly\ndescribed using two different types of relations. Connectivity links that\nenable the nodes to function cooperatively as a network and dependency links\nthat bind the failure of one network element to the failure of other network\nelements. Here we present for the first time an analytical framework for\nstudying the robustness of networks that include both connectivity and\ndependency links. We show that the synergy between the two types of failures\nleads to an iterative process of cascading failures that has a devastating\neffect on the network stability and completely alters the known assumptions\nregarding the robustness of networks. We present exact analytical results for\nthe dramatic change in the network behavior when introducing dependency links.\nFor a high density of dependency links the network disintegrates in a form of a\nfirst order phase transition while for a low density of dependency links the\nnetwork disintegrates in a second order transition. Moreover, opposed to\nnetworks containing only connectivity links where a broader degree distribution\nresults in a more robust network, when both types of links are present a broad\ndegree distribution leads to higher vulnerability.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.4498v1"
    },
    {
        "title": "Inter-similarity between coupled networks",
        "authors": [
            "Roni Parshani",
            "Celine Rozenblat",
            "Daniele Ietri",
            "Cesar Ducruet",
            "Shlomo Havlin"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Recent studies have shown that a system composed from several randomly\ninterdependent networks is extremely vulnerable to random failure. However,\nreal interdependent networks are usually not randomly interdependent, rather a\npair of dependent nodes are coupled according to some regularity which we coin\ninter-similarity. For example, we study a system composed from an\ninterdependent world wide port network and a world wide airport network and\nshow that well connected ports tend to couple with well connected airports. We\nintroduce two quantities for measuring the level of inter-similarity between\nnetworks (i) Inter degree-degree correlation (IDDC) (ii) Inter-clustering\ncoefficient (ICC). We then show both by simulation models and by analyzing the\nport-airport system that as the networks become more inter-similar the system\nbecomes significantly more robust to random failure.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.4506v1"
    },
    {
        "title": "Correlated couplings and robustness of coupled networks",
        "authors": [
            "Won-kuk Cho",
            "K. -I. Goh",
            "I. -M. Kim"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Most real-world complex systems can be modelled by coupled networks with\nmultiple layers. How and to what extent the pattern of couplings between\nnetwork layers may influence the interlaced structure and function of coupled\nnetworks are not clearly understood. Here we study the impact of correlated\ninter-layer couplings on the network robustness of coupled networks using\npercolation concept. We found that the positive correlated inter-layer coupling\nenhaces network robustness in the sense that it lowers the percolation\nthreshold of the interlaced network than the negative correlated coupling case.\nAt the same time, however, positive inter-layer correlation leads to smaller\ngiant component size in the well-connected region, suggesting potential\ndisadvantage for network connectivity, as demonstrated also with some\nreal-world coupled network structures.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.4971v1"
    },
    {
        "title": "Robustness of a Network of Networks",
        "authors": [
            "Jianxi Gao",
            "Sergey V. Buldyrev",
            "Shlomo Havlin",
            "H. Eugene Stanley"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Almost all network research has been focused on the properties of a single\nnetwork that does not interact and depends on other networks. In reality, many\nreal-world networks interact with other networks. Here we develop an analytical\nframework for studying interacting networks and present an exact percolation\nlaw for a network of $n$ interdependent networks. In particular, we find that\nfor $n$ Erd\\H{o}s-R\\'{e}nyi networks each of average degree $k$, the giant\ncomponent, $P_{\\infty}$, is given by $P_{\\infty}=p[1-\\exp(-kP_{\\infty})]^n$\nwhere $1-p$ is the initial fraction of removed nodes. Our general result\ncoincides for $n=1$ with the known Erd\\H{o}s-R\\'{e}nyi second-order phase\ntransition for a single network. For any $n \\geq 2$ cascading failures occur\nand the transition becomes a first-order percolation transition. The new law\nfor $P_{\\infty}$ shows that percolation theory that is extensively studied in\nphysics and mathematics is a limiting case ($n=1$) of a more general general\nand different percolation law for interdependent networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.5829v1"
    },
    {
        "title": "A population-based approach to background discrimination in particle\n  physics",
        "authors": [
            "Federico Colecchia"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Background properties in experimental particle physics are typically\nestimated using control samples corresponding to large numbers of events. This\ncan provide precise knowledge of average background distributions, but\ntypically does not consider the effect of fluctuations in a data set of\ninterest. A novel approach based on mixture model decomposition is presented as\na way to estimate the effect of fluctuations on the shapes of probability\ndistributions in a given data set, with a view to improving on the knowledge of\nbackground distributions obtained from control samples. Events are treated as\nheterogeneous populations comprising particles originating from different\nprocesses, and individual particles are mapped to a process of interest on a\nprobabilistic basis. The proposed approach makes it possible to extract from\nthe data information about the effect of fluctuations that would otherwise be\nlost using traditional methods based on high-statistics control samples. A\nfeasibility study on Monte Carlo is presented, together with a comparison with\nexisting techniques. Finally, the prospects for the development of tools for\nintensive offline analysis of individual events at the Large Hadron Collider\nare discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.6051v5"
    },
    {
        "title": "Modeling the structure and evolution of discussion cascades",
        "authors": [
            "Vicenç Gómez",
            "Hilbert J. Kappen",
            "Andreas Kaltenbrunner"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We analyze the structure and evolution of discussion cascades in four popular\nwebsites: Slashdot, Barrapunto, Meneame and Wikipedia. Despite the big\nheterogeneities between these sites, a preferential attachment (PA) model with\nbias to the root can capture the temporal evolution of the observed trees and\nmany of their statistical properties, namely, probability distributions of the\nbranching factors (degrees), subtree sizes and certain correlations. The\nparameters of the model are learned efficiently using a novel maximum\nlikelihood estimation scheme for PA and provide a figurative interpretation\nabout the communication habits and the resulting discussion cascades on the\nfour different websites.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.0673v3"
    },
    {
        "title": "Entropic Inference",
        "authors": [
            "Ariel Caticha"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  In this tutorial we review the essential arguments behing entropic inference.\nWe focus on the epistemological notion of information and its relation to the\nBayesian beliefs of rational agents. The problem of updating from a prior to a\nposterior probability distribution is tackled through an eliminative induction\nprocess that singles out the logarithmic relative entropy as the unique tool\nfor inference. The resulting method of Maximum relative Entropy (ME), includes\nas special cases both MaxEnt and Bayes' rule, and therefore unifies the two\nthemes of these workshops -- the Maximum Entropy and the Bayesian methods --\ninto a single general inference scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.0723v1"
    },
    {
        "title": "Irreversible Aggregation and Network Renormalization",
        "authors": [
            "Seung-Woo Son",
            "Golnoosh Bizhani",
            "Claire Christensen",
            "Peter Grassberger",
            "Maya Paczuski"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Irreversible aggregation is revisited in view of recent work on\nrenormalization of complex networks. Its scaling laws and phase transitions are\nrelated to percolation transitions seen in the latter. We illustrate our points\nby giving the complete solution for the probability to find any given state in\nan aggregation process $(k+1)X\\to X$, given a fixed number of unit mass\nparticles in the initial state. Exactly the same probability distributions and\nscaling are found in one dimensional systems (a trivial network) and well-mixed\nsolutions. This reveals that scaling laws found in renormalization of complex\nnetworks do not prove that they are self-similar.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.0944v2"
    },
    {
        "title": "Classifying extremely imbalanced data sets",
        "authors": [
            "Markward Britsch",
            "Nikolai Gagunashvili",
            "Michael Schmelling"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Imbalanced data sets containing much more background than signal instances\nare very common in particle physics, and will also be characteristic for the\nupcoming analyses of LHC data. Following up the work presented at ACAT 2008, we\nuse the multivariate technique presented there (a rule growing algorithm with\nthe meta-methods bagging and instance weighting) on much more imbalanced data\nsets, especially a selection of D0 decays without the use of particle\nidentification. It turns out that the quality of the result strongly depends on\nthe number of background instances used for training. We discuss methods to\nexploit this in order to improve the results significantly, and how to handle\nand reduce the size of large training sets without loss of result quality in\ngeneral. We will also comment on how to take into account statistical\nfluctuation in receiver operation characteristic curves (ROC) for comparing\nclassifier methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.6224v1"
    },
    {
        "title": "Universal patterns in sound amplitudes of songs and music genres",
        "authors": [
            "R. S. Mendes",
            "H. V. Ribeiro",
            "F. C. M. Freire",
            "A. A. Tateishi",
            "E. K. Lenzi"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We report a statistical analysis over more than eight thousand songs.\nSpecifically, we investigate the probability distribution of the normalized\nsound amplitudes. Our findings seems to suggest a universal form of\ndistribution which presents a good agreement with a one-parameter stretched\nGaussian. We also argue that this parameter can give information on music\ncomplexity, and consequently it goes towards classifying songs as well as music\ngenres. Additionally, we present statistical evidences that correlation aspects\nof the songs are directly related with the non-Gaussian nature of their sound\namplitude distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.0142v1"
    },
    {
        "title": "Identification of overlapping communities and their hierarchy by locally\n  calculating community-changing resolution levels",
        "authors": [
            "Frank Havemann",
            "Michael Heinz",
            "Alexander Struck",
            "Jochen Gläser"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We propose a new local, deterministic and parameter-free algorithm that\ndetects fuzzy and crisp overlapping communities in a weighted network and\nsimultaneously reveals their hierarchy. Using a local fitness function, the\nalgorithm greedily expands natural communities of seeds until the whole graph\nis covered. The hierarchy of communities is obtained analytically by\ncalculating resolution levels at which communities grow rather than numerically\nby testing different resolution levels. This analytic procedure is not only\nmore exact than its numerical alternatives such as LFM and GCE but also much\nfaster. Critical resolution levels can be identified by searching for intervals\nin which large changes of the resolution do not lead to growth of communities.\nWe tested our algorithm on benchmark graphs and on a network of 492 papers in\ninformation science. Combined with a specific post-processing, the algorithm\ngives much more precise results on LFR benchmarks with high overlap compared to\nother algorithms and performs very similar to GCE.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.1269v1"
    },
    {
        "title": "Measuring maximal eigenvalue distribution of Wishart random matrices\n  with coupled lasers",
        "authors": [
            "Moti Fridman",
            "Rami Pugatch",
            "Micha Nixon",
            "Asher A. Friesem",
            "Nir Davidson"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We determined the probability distribution of the combined output power from\ntwenty five coupled fiber lasers and show that it agrees well with the\nTracy-Widom, Majumdar-Vergassola and Vivo-Majumdar-Bohigas distributions of the\nlargest eigenvalue of Wishart random matrices with no fitting parameters. This\nwas achieved with $500,000$ measurements of the combined output power from the\nfiber lasers, that continuously changes with variations of the fiber lasers\nlengths. We show experimentally that for small deviations of the combined\noutput power over its mean value the Tracy-Widom distribution is correct, while\nfor large deviations the Majumdar-Vergassola and Vivo-Majumdar-Bohigas\ndistributions are correct.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.1282v3"
    },
    {
        "title": "First Experiences with LHC Grid Computing and Distributed Analysis",
        "authors": [
            "Ian Fisk"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  In this presentation the experiences of the LHC experiments using grid\ncomputing were presented with a focus on experience with distributed analysis.\nAfter many years of development, preparation, exercises, and validation the LHC\n(Large Hadron Collider) experiments are in operations. The computing\ninfrastructure has been heavily utilized in the first 6 months of data\ncollection. The general experience of exploiting the grid infrastructure for\norganized processing and preparation is described, as well as the successes\nemploying the infrastructure for distributed analysis. At the end the expected\nevolution and future plans are outlined.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.1980v1"
    },
    {
        "title": "Topics in statistical data analysis for high-energy physics",
        "authors": [
            "G. Cowan"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  These lectures concern two topics that are becoming increasingly important in\nthe analysis of High Energy Physics (HEP) data: Bayesian statistics and\nmultivariate methods. In the Bayesian approach we extend the interpretation of\nprobability to cover not only the frequency of repeatable outcomes but also to\ninclude a degree of belief. In this way we are able to associate probability\nwith a hypothesis and thus to answer directly questions that cannot be\naddressed easily with traditional frequentist methods. In multivariate analysis\nwe try to exploit as much information as possible from the characteristics that\nwe measure for each event to distinguish between event types. In particular we\nwill look at a method that has gained popularity in HEP in recent years: the\nboosted decision tree (BDT).\n",
        "pdf_link": "http://arxiv.org/pdf/1012.3589v1"
    },
    {
        "title": "Generalized Methods and Solvers for Noise Removal from Piecewise\n  Constant Signals",
        "authors": [
            "Max A. Little",
            "Nick S. Jones"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Removing noise from piecewise constant (PWC) signals, is a challenging signal\nprocessing problem arising in many practical contexts. For example, in\nexploration geosciences, noisy drill hole records need separating into\nstratigraphic zones, and in biophysics, jumps between molecular dwell states\nneed extracting from noisy fluorescence microscopy signals. Many PWC denoising\nmethods exist, including total variation regularization, mean shift clustering,\nstepwise jump placement, running medians, convex clustering shrinkage and\nbilateral filtering; conventional linear signal processing methods are\nfundamentally unsuited however. This paper shows that most of these methods are\nassociated with a special case of a generalized functional, minimized to\nachieve PWC denoising. The minimizer can be obtained by diverse solver\nalgorithms, including stepwise jump placement, convex programming, finite\ndifferences, iterated running medians, least angle regression, regularization\npath following, and coordinate descent. We introduce novel PWC denoising\nmethods, which, for example, combine global mean shift clustering with local\ntotal variation smoothing. Head-to-head comparisons between these methods are\nperformed on synthetic data, revealing that our new methods have a useful role\nto play. Finally, overlaps between the methods of this paper and others such as\nwavelet shrinkage, hidden Markov models, and piecewise smooth filtering are\ntouched on.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.5095v2"
    },
    {
        "title": "Trip Length Distribution Under Multiplicative Spatial Models of Supply\n  and Demand: Theory and Sensitivity Analysis",
        "authors": [
            "Daniele Veneziano",
            "Marta C. Gonzalez"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We propose new probabilistic models for the spatial distribution of supply\nand demand and use the models to determine how the trip length distribution is\naffected by the relative shortage or excess of supply, the spatial clustering\nof supply and demand, and the degree of attraction or repulsion between supply\nand demand at different spatial scales. The models have a multiplicative\nstructure and in certain cases possess scale invariance properties. Using\ndetailed population data in metropolitan US regions validates the demand model.\nThe trip length distribution, evaluated under destination choice models of the\nintervening opportunities type, has quasi-analytic form.We take advantage of\nthis feature to study the sensitivity of the trip length distribution to\nparameters of the demand, supply and destination choice models. We find that\ntrip length is affected in important but different ways by the spatial density\nof potential destinations, the dependence among their attractiveness levels,\nand the correlation between supply and demand at different spatial scales.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.3719v1"
    },
    {
        "title": "Statistical Mechanics of Semi-Supervised Clustering in Sparse Graphs",
        "authors": [
            "Greg Ver Steeg",
            "Aram Galstyan",
            "Armen E. Allahverdyan"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We theoretically study semi-supervised clustering in sparse graphs in the\npresence of pairwise constraints on the cluster assignments of nodes. We focus\non bi-cluster graphs, and study the impact of semi-supervision for varying\nconstraint density and overlap between the clusters. Recent results for\nunsupervised clustering in sparse graphs indicate that there is a critical\nratio of within-cluster and between-cluster connectivities below which clusters\ncannot be recovered with better than random accuracy. The goal of this paper is\nto examine the impact of pairwise constraints on the clustering accuracy. Our\nresults suggests that the addition of constraints does not provide automatic\nimprovement over the unsupervised case. When the density of the constraints is\nsufficiently small, their only impact is to shift the detection threshold while\npreserving the criticality. Conversely, if the density of (hard) constraints is\nabove the percolation threshold, the criticality is suppressed and the\ndetection threshold disappears.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.4227v3"
    },
    {
        "title": "Node-weighted measures for complex networks with spatially embedded,\n  sampled, or differently sized nodes",
        "authors": [
            "Jobst Heitzig",
            "Jonathan F. Donges",
            "Yong Zou",
            "Norbert Marwan",
            "Jürgen Kurths"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  When network and graph theory are used in the study of complex systems, a\ntypically finite set of nodes of the network under consideration is frequently\neither explicitly or implicitly considered representative of a much larger\nfinite or infinite region or set of objects of interest. The selection\nprocedure, e.g., formation of a subset or some kind of discretization or\naggregation, typically results in individual nodes of the studied network\nrepresenting quite differently sized parts of the domain of interest. This\nheterogeneity may induce substantial bias and artifacts in derived network\nstatistics. To avoid this bias, we propose an axiomatic scheme based on the\nidea of node splitting invariance to derive consistently weighted variants of\nvarious commonly used statistical network measures. The practical relevance and\napplicability of our approach is demonstrated for a number of example networks\nfrom different fields of research, and is shown to be of fundamental importance\nin particular in the study of spatially embedded functional networks derived\nfrom time series as studied in, e.g., neuroscience and climatology.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.4757v2"
    },
    {
        "title": "A Complex Networks Approach for Data Clustering",
        "authors": [
            "Francisco A. Rodrigues",
            "Guilherme Ferraz de Arruda",
            "Luciano da Fontoura Costa"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Many methods have been developed for data clustering, such as k-means,\nexpectation maximization and algorithms based on graph theory. In this latter\ncase, graphs are generally constructed by taking into account the Euclidian\ndistance as a similarity measure, and partitioned using spectral methods.\nHowever, these methods are not accurate when the clusters are not well\nseparated. In addition, it is not possible to automatically determine the\nnumber of clusters. These limitations can be overcome by taking into account\nnetwork community identification algorithms. In this work, we propose a\nmethodology for data clustering based on complex networks theory. We compare\ndifferent metrics for quantifying the similarity between objects and take into\naccount three community finding techniques. This approach is applied to two\nreal-world databases and to two sets of artificially generated data. By\ncomparing our method with traditional clustering approaches, we verify that the\nproximity measures given by the Chebyshev and Manhattan distances are the most\nsuitable metrics to quantify the similarity between objects. In addition, the\ncommunity identification method based on the greedy optimization provides the\nsmallest misclassification rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.5141v1"
    },
    {
        "title": "Construction of Ito model for Geoelectrical Signals",
        "authors": [
            "Zbigniew Czechowski",
            "Luciano Telesca"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Ito stochastic differential equation governs one-dimensional diffusive Markov\nprocess. Geoelectrical signals measured in seismic areas can be considered as\nthe result of competitive and collective interactions among system elements.\nThe Ito equation may constitute a good macroscopic model of such phenomenon in\nwhich microscopic interactions are adequately averaged. The present study shows\nhow to construct Ito model for a geoelectrical time series measured in a\nseismic area of southern Italy. Our results reveal that Ito model describes\nquite well the whole time series, but performs better when one considers\nfragments of the data set with lower variability range (absent or rare large\nfluctuations) . Our findings show that generally detrended geoelectrical time\nseries can be considered as an approximation of the Markov diffusion process.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.5501v1"
    },
    {
        "title": "Harmonic Oscillator in Heat Bath: Exact simulation of\n  time-lapse-recorded data, exact analytical benchmark statistics",
        "authors": [
            "Simon F. Norrelykke",
            "Henrik Flyvbjerg"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The stochastic dynamics of the damped harmonic oscillator in a heat bath is\nsimulated with an algorithm that is exact for time steps of arbitrary size.\nExact analytical results are given for correlation functions and power spectra\nin the form they acquire when computed from experimental time-lapse recordings.\nThree applications are discussed: (i) Effects of finite sampling-rate and\n-time, described exactly here, are similar for other stochastic dynamical\nsystems-e.g. motile micro-organisms and their time-lapse recorded trajectories.\n(ii) The same statistics is satisfied by any experimental system to the extent\nit is interpreted as a damped harmonic oscillator at finite temperature-such as\nan AFM cantilever. (iii) Three other models of fundamental interest are\nlimiting cases of the damped harmonic oscillator at finite temperature; it\nconsequently bridges their differences and describes effects of finite sampling\nrate and sampling time for these models as well. Finally, we give a brief\ndiscussion of nondimensionalization.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.0524v1"
    },
    {
        "title": "Generalized Measures of Information Transfer",
        "authors": [
            "Paul L. Williams",
            "Randall D. Beer"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Transfer entropy provides a general tool for analyzing the magnitudes and\ndirections---but not the \\emph{kinds}---of information transfer in a system. We\nextend transfer entropy in two complementary ways. First, we distinguish\nstate-dependent from state-independent transfer, based on whether a source's\ninfluence depends on the state of the target. Second, for multiple sources, we\ndistinguish between unique, redundant, and synergistic transfer. The new\nmeasures are demonstrated on several systems that extend examples from previous\nliterature.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.1507v1"
    },
    {
        "title": "On the criticality of inferred models",
        "authors": [
            "Iacopo Mastromatteo",
            "Matteo Marsili"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Advanced inference techniques allow one to reconstruct the pattern of\ninteraction from high dimensional data sets. We focus here on the statistical\nproperties of inferred models and argue that inference procedures are likely to\nyield models which are close to a phase transition. On one side, we show that\nthe reparameterization invariant metrics in the space of probability\ndistributions of these models (the Fisher Information) is directly related to\nthe model's susceptibility. As a result, distinguishable models tend to\naccumulate close to critical points, where the susceptibility diverges in\ninfinite systems. On the other, this region is the one where the estimate of\ninferred parameters is most stable. In order to illustrate these points, we\ndiscuss inference of interacting point processes with application to financial\ndata and show that sensible choices of observation time-scales naturally yield\nmodels which are close to criticality.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.1624v2"
    },
    {
        "title": "Urban road networks -- Spatial networks with universal geometric\n  features? A case study on Germany's largest cities",
        "authors": [
            "Sonic H. Y. Chan",
            "Reik V. Donner",
            "Stefan Lämmer"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Urban road networks have distinct geometric properties that are partially\ndetermined by their (quasi-) two-dimensional structure. In this work, we study\nthese properties for 20 of the largest German cities. We find that the\nsmall-scale geometry of all examined road networks is extremely similar. The\nobject-size distributions of road segments and the resulting cellular\nstructures are characterised by heavy tails. As a specific feature, a large\ndegree of rectangularity is observed in all networks, with link angle\ndistributions approximately described by stretched exponential functions. We\npresent a rigorous statistical analysis of the main geometric characteristics\nand discuss their mutual interrelationships. Our results demonstrate the\nfundamental importance of cost-efficiency constraints for in time evolution of\nurban road networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.3584v1"
    },
    {
        "title": "Mathematica with ROOT",
        "authors": [
            "Ken Hsieh",
            "Thomas G. Throwe",
            "Sebastian White"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We present an open-source Mathematica importer for CERN ROOT files. Taking\nadvantage of Mathematica's import/export plug-in mechanism, the importer offers\na simple, unified interface that cleanly wraps around its MathLink-based core\nthat links the ROOT libraries with Mathematica. Among other tests for accuracy\nand efficiency, the importer has also been tested on a large (~5 Gbyte) file\nstructure, D3PD, used by the ATLAS experiment for offline analysis without\nproblems. In addition to describing the installation and usage of the importer,\nwe discuss how the importer may be further improved and customized. A link to\nthe package can be found at:\nhttp://library.wolfram.com/infocenter/Articles/7793/ and a related presentation\nis at: http://cd-docdb.fnal.gov/cgi-bin/DisplayMeeting?conferenceid=522\n",
        "pdf_link": "http://arxiv.org/pdf/1102.5068v2"
    },
    {
        "title": "Analytical maximum-likelihood method to detect patterns in real networks",
        "authors": [
            "Tiziano Squartini",
            "Diego Garlaschelli"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In order to detect patterns in real networks, randomized graph ensembles that\npreserve only part of the topology of an observed network are systematically\nused as fundamental null models. However, their generation is still\nproblematic. The existing approaches are either computationally demanding and\nbeyond analytic control, or analytically accessible but highly approximate.\nHere we propose a solution to this long-standing problem by introducing an\nexact and fast method that allows to obtain expectation values and standard\ndeviations of any topological property analytically, for any binary, weighted,\ndirected or undirected network. Remarkably, the time required to obtain the\nexpectation value of any property is as short as that required to compute the\nsame property on the single original network. Our method reveals that the null\nbehavior of various correlation properties is different from what previously\nbelieved, and highly sensitive to the particular network considered. Moreover,\nour approach shows that important structural properties (such as the modularity\nused in community detection problems) are currently based on incorrect\nexpressions, and provides the exact quantities that should replace them.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.0701v2"
    },
    {
        "title": "Bayesian versus frequentist upper limits",
        "authors": [
            "Christian Röver",
            "Chris Messenger",
            "Reinhard Prix"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  While gravitational waves have not yet been measured directly, data analysis\nfrom detection experiments commonly includes an upper limit statement. Such\nupper limits may be derived via a frequentist or Bayesian approach; the\ntheoretical implications are very different, and on the technical side, one\nnotable difference is that one case requires maximization of the likelihood\nfunction over parameter space, while the other requires integration. Using a\nsimple example (detection of a sinusoidal signal in white Gaussian noise), we\ninvestigate the differences in performance and interpretation, and the effect\nof the \"trials factor\", or \"look-elsewhere effect\".\n",
        "pdf_link": "http://arxiv.org/pdf/1103.2987v3"
    },
    {
        "title": "Uncovering missing links with cold ends",
        "authors": [
            "Yu-Xiao Zhu",
            "Linyuan Lü",
            "Qian-Ming Zhang",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  To evaluate the performance of prediction of missing links, the known data\nare randomly divided into two parts, the training set and the probe set. We\nargue that this straightforward and standard method may lead to terrible bias,\nsince in real biological and information networks, missing links are more\nlikely to be links connecting low-degree nodes. We therefore study how to\nuncover missing links with low-degree nodes, namely links in the probe set are\nof lower degree products than a random sampling. Experimental analysis on ten\nlocal similarity indices and four disparate real networks reveals a surprising\nresult that the Leicht-Holme-Newman index [E. A. Leicht, P. Holme, and M. E. J.\nNewman, Phys. Rev. E 73, 026120 (2006)] performs the best, although it was\nknown to be one of the worst indices if the probe set is a random sampling of\nall links. We further propose an parameter-dependent index, which considerably\nimproves the prediction accuracy. Finally, we show the relevance of the\nproposed index on three real sampling methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.0395v2"
    },
    {
        "title": "Comparison of methods to extract an asymmetry parameter from data",
        "authors": [
            "Jörg Pretz"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Several methods to extract an asymmetry parameter in an event distribution\nfunction are discussed and compared in terms of statistical precision and\napplicability. These methods are: simple counting rate asymmetries, event\nweighting procedures and the unbinned extended maximum likelihood method. It is\nknown that weighting methods reach the same figure of merit (FOM) as the\nlikelihood method in the limit of vanishing asymmetries. This article presents\nan improved weighting procedure reaching the FOM of the likelihood method for\narbitrary asymmetries. Cases where the maximum likelihood method is not\napplicable are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.1038v2"
    },
    {
        "title": "Can intrinsic noise induce various resonant peaks?",
        "authors": [
            "J. J. Torres",
            "J. Marro",
            "J. F. Mejias"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We theoretically describe how weak signals may be efficiently transmitted\nthroughout more than one frequency range in noisy excitable media by kind of\nstochastic multiresonance. This serves us here to reinterpret recent\nexperiments in neuroscience, and to suggest that many other systems in nature\nmight be able to exhibit several resonances. In fact, the observed behavior\nhappens in our (network) model as a result of competition between (1) changes\nin the transmitted signals as if the units were varying their activation\nthreshold, and (2) adaptive noise realized in the model as rapid\nactivity-dependent fluctuations of the connection intensities. These two\nconditions are indeed known to characterize heterogeneously networked systems\nof excitable units, e.g., sets of neurons and synapses in the brain. Our\nresults may find application also in the design of detector devices.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.1202v1"
    },
    {
        "title": "Exploring Human Mobility Patterns Based on Location Information of US\n  Flights",
        "authors": [
            "Bin Jiang",
            "Tao Jia"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  A range of early studies have been conducted to illustrate human mobility\npatterns using different tracking data, such as dollar notes, cell phones and\ntaxicabs. Here, we explore human mobility patterns based on massive tracking\ndata of US flights. Both topological and geometric properties are examined in\ndetail. We found that topological properties, such as traffic volume (between\nairports) and degree of connectivity (of individual airports), including both\nin- and outdegrees, follow a power law distribution but not a geometric\nproperty like travel lengths. The travel lengths exhibit an exponential\ndistribution rather than a power law with an exponential cutoff as previous\nstudies illustrated. We further simulated human mobility on the established\ntopologies of airports with various moving behaviors and found that the\nmobility patterns are mainly attributed to the underlying binary topology of\nairports and have little to do with other factors, such as moving behaviors and\ngeometric distances. Apart from the above findings, this study adopts the\nhead/tail division rule, which is regularity behind any heavy-tailed\ndistribution for extracting individual airports. The adoption of this rule for\ndata processing constitutes another major contribution of this paper.\n  Keywords: scaling of geographic space, head/tail division rule, power law,\ngeographic information, agent-based simulations\n",
        "pdf_link": "http://arxiv.org/pdf/1104.4578v2"
    },
    {
        "title": "From brain to earth and climate systems: Small-world interaction\n  networks or not?",
        "authors": [
            "Stephan Bialonski",
            "Marie-Therese Horstmann",
            "Klaus Lehnertz"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We consider recent reports on small-world topologies of interaction networks\nderived from the dynamics of spatially extended systems that are investigated\nin diverse scientific fields such as neurosciences, geophysics, or meteorology.\nWith numerical simulations that mimic typical experimental situations we have\nidentified an important constraint when characterizing such networks:\nindications of a small-world topology can be expected solely due to the spatial\nsampling of the system along with commonly used time series analysis based\napproaches to network characterization.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.2257v1"
    },
    {
        "title": "Asymmetric random matrices: What do we need them for?",
        "authors": [
            "Stanislaw Drozdz",
            "Jaroslaw Kwapien",
            "Andreas A. Ioannides"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Complex systems are typically represented by large ensembles of observations.\nCorrelation matrices provide an efficient formal framework to extract\ninformation from such multivariate ensembles and identify in a quantifiable way\npatterns of activity that are reproducible with statistically significant\nfrequency compared to a reference chance probability, usually provided by\nrandom matrices as fundamental reference. The character of the problem and\nespecially the symmetries involved must guide the choice of random matrices to\nbe used for the definition of a baseline reference. For standard correlation\nmatrices this is the Wishart ensemble of symmetric random matrices. The real\nworld complexity however often shows asymmetric information flows and therefore\nmore general correlation matrices are required to adequately capture the\nasymmetry. Here we first summarize the relevant theoretical concepts. We then\npresent some examples of human brain activity where asymmetric time-lagged\ncorrelations are evident and hence highlight the need for further theoretical\ndevelopments.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.0390v1"
    },
    {
        "title": "Impact of Heterogeneous Human Activities on Epidemic Spreading",
        "authors": [
            "Zimo Yang",
            "Ai-Xiang Cui",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Recent empirical observations suggest a heterogeneous nature of human\nactivities. The heavy-tailed inter-event time distribution at population level\nis well accepted, while whether the individual acts in a heterogeneous way is\nstill under debate. Motivated by the impact of temporal heterogeneity of human\nactivities on epidemic spreading, this paper studies the susceptible-infected\nmodel on a fully mixed population, where each individual acts in a completely\nhomogeneous way but different individuals have different mean activities.\nExtensive simulations show that the heterogeneity of activities at population\nlevel remarkably affects the speed of spreading, even though each individual\nbehaves regularly. Further more, the spreading speed of this model is more\nsensitive to the change of system heterogeneity compared with the model\nconsisted of individuals acting with heavy-tailed inter-event time\ndistribution. This work refines our understanding of the impact of\nheterogeneous human activities on epidemic spreading.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.3554v1"
    },
    {
        "title": "The Asymptotic Mandelbrot Law of Some Evolution Networks",
        "authors": [
            "Li Li"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In this letter, we study some evolution networks that grow with linear\npreferential attachment. Based upon some recent results on the quotient Gamma\nfunction, we give a rigorous proof of the asymptotic Mandelbrot law for the\ndegree distribution $p_k \\propto (k + c)^{-\\gamma}$ in certain conditions. We\nalso analytically derive the best fitting values for the scaling exponent\n$\\gamma$ and the shifting coefficient $c$.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.3740v2"
    },
    {
        "title": "A random walk on image patches",
        "authors": [
            "Kye M. Taylor",
            "Francois G. Meyer"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In this paper we address the problem of understanding the success of\nalgorithms that organize patches according to graph-based metrics. Algorithms\nthat analyze patches extracted from images or time series have led to\nstate-of-the art techniques for classification, denoising, and the study of\nnonlinear dynamics. The main contribution of this work is to provide a\ntheoretical explanation for the above experimental observations. Our approach\nrelies on a detailed analysis of the commute time metric on prototypical graph\nmodels that epitomize the geometry observed in general patch graphs. We prove\nthat a parametrization of the graph based on commute times shrinks the mutual\ndistances between patches that correspond to rapid local changes in the signal,\nwhile the distances between patches that correspond to slow local changes\nexpand. In effect, our results explain why the parametrization of the set of\npatches based on the eigenfunctions of the Laplacian can concentrate patches\nthat correspond to rapid local changes, which would otherwise be shattered in\nthe space of patches. While our results are based on a large sample analysis,\nnumerical experimentations on synthetic and real data indicate that the results\nhold for datasets that are very small in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.0414v1"
    },
    {
        "title": "Bayesian experimental design for the active nitridation of graphite by\n  atomic nitrogen",
        "authors": [
            "Gabriel Terejanu",
            "Rochan R. Upadhyay",
            "Kenji Miki"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The problem of optimal data collection to efficiently learn the model\nparameters of a graphite nitridation experiment is studied in the context of\nBayesian analysis using both synthetic and real experimental data. The paper\nemphasizes that the optimal design can be obtained as a result of an\ninformation theoretic sensitivity analysis. Thus, the preferred design is where\nthe statistical dependence between the model parameters and observables is the\nhighest possible. In this paper, the statistical dependence between random\nvariables is quantified by mutual information and estimated using a k-nearest\nneighbor based approximation. It is shown, that by monitoring the inference\nprocess via measures such as entropy or Kullback-Leibler divergence, one can\ndetermine when to stop the data collection process. The methodology is applied\nto select the most informative designs on both a simulated data set and on an\nexperimental data set, previously published in the literature. It is also shown\nthat the sequential Bayesian analysis used in the experimental design can also\nbe useful in detecting conflicting information between measurements and model\npredictions.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.1445v1"
    },
    {
        "title": "The use of statistical methods for the search for new physics at the LHC\n  (in Russian)",
        "authors": [
            "S. I. Bityukov",
            "N. V. Krasnikov"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We review statistical methods used for the search for new physics at LHC.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.3974v1"
    },
    {
        "title": "Three dimensional structure from intensity correlations",
        "authors": [
            "Veit Elser"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We develop the analysis of x-ray intensity correlations from dilute ensembles\nof identical particles in a number of ways. First, we show that the 3D particle\nstructure can be determined if the particles can be aligned with respect to a\nsingle axis having a known angle with respect to the incident beam. Second, we\nclarify the phase problem in this setting and introduce a data reduction scheme\nthat assesses the integrity of the data even before the particle reconstruction\nis attempted. Finally, we describe an algorithm that reconstructs intensity and\nparticle density simultaneously, thereby making maximal use of the available\nconstraints.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.4030v1"
    },
    {
        "title": "Mandelbrot Law of Evolving Networks",
        "authors": [
            "Xue-Zao Ren",
            "Zimo Yang",
            "Bing-Hong Wang",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Degree distributions of many real networks are known to follow the Mandelbrot\nlaw, which can be considered as an extension of the power law and is determined\nby not only the power-law exponent, but also the shifting coefficient. Although\nthe shifting coefficient highly affects the shape of distribution, it receives\nless attention in the literature and in fact, mainstream analytical method\nbased on backward or forward difference will lead to considerable deviations to\nits value. In this Letter, we show that the degree distribution of a growing\nnetwork with linear preferential attachment approximately follows the\nMandelbrot law. We propose an analytical method based on a recursive formula\nthat can obtain a more accurate expression of the shifting coefficient.\nSimulations demonstrate the advantages of our method. This work provides a\npossible mechanism leading to the Mandelbrot law of evolving networks, and\nrefines the mainstream analytical methods for the shifting coefficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.4649v2"
    },
    {
        "title": "Temporal motifs in time-dependent networks",
        "authors": [
            "Lauri Kovanen",
            "Márton Karsai",
            "Kimmo Kaski",
            "János Kertész",
            "Jari Saramäki"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Temporal networks are commonly used to represent systems where connections\nbetween elements are active only for restricted periods of time, such as\nnetworks of telecommunication, neural signal processing, biochemical reactions\nand human social interactions. We introduce the framework of temporal motifs to\nstudy the mesoscale topological-temporal structure of temporal networks in\nwhich the events of nodes do not overlap in time. Temporal motifs are classes\nof similar event sequences, where the similarity refers not only to topology\nbut also to the temporal order of the events. We provide a mapping from event\nsequences to colored directed graphs that enables an efficient algorithm for\nidentifying temporal motifs. We discuss some aspects of temporal motifs,\nincluding causality and null models, and present basic statistics of temporal\nmotifs in a large mobile call network.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.5646v2"
    },
    {
        "title": "Optimizing the energy threshold of light detectors coupled to\n  luminescent bolometers",
        "authors": [
            "G. Piperno",
            "S. Pirro",
            "M. Vignati"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Bolometers have proven to be good detectors for the search of neutrinoless\ndouble beta decay. By operating at cryogenic temperatures, they feature\nexcellent energy resolution and low background. The detection of the possible\nlight emitted when particles interact in the bolometer is a promising method to\nlower the background of the experiments. The different amount of light emitted\nin beta/gamma and alpha interactions, whether due to scintillation or Cerenkov\nemission, allows to discriminate the two interaction types. Because of the\ncryogenic environment, light detectors are often bolometers. In this work we\npresent a software algorithm to lower the energy threshold of bolometric light\ndetectors coupled to luminescent bolometers. The application to data from Ge\nlight detectors coupled to ZnMoO4 and TeO2 bolometers shows that the energy\nthreshold can be lowered substantially, increasing the discrimination power\nwhen the amount of emitted light is small.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.5679v1"
    },
    {
        "title": "Percentile rank scores are congruous indicators of relative performance,\n  or aren't they?",
        "authors": [
            "Ronald Rousseau"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Percentile ranks and the I3 indicator were introduced by Bornmann,\nLeydesdorff, Mutz and Opthof. These two notions are based on the concept of\npercentiles (or quantiles) for discrete data. As several definitions for these\nnotions exist we propose one that we think is suitable in this context. Next we\nshow that if the notion of relative congruous indicators is carefully defined\nthen percentile rank scores are congruous indicators of relative performance.\nThe I3 indicator is a strictly congruous indicator of absolute performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.1860v1"
    },
    {
        "title": "Robustness of a Tree-like Network of Interdependent Networks",
        "authors": [
            "Jianxi Gao",
            "S. V. Buldyrev",
            "S. Havlin",
            "H. E. Stanley"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In reality, many real-world networks interact with and depend on other\nnetworks. We develop an analytical framework for studying interacting networks\nand present an exact percolation law for a network of $n$ interdependent\nnetworks (NON). We present a general framework to study the dynamics of the\ncascading failures process at each step caused by an initial failure occurring\nin the NON system. We study and compare both $n$ coupled Erd\\H{o}s-R\\'{e}nyi\n(ER) graphs and $n$ coupled random regular (RR) graphs. We found recently [Gao\net. al. arXive:1010.5829] that for an NON composed of $n$ ER networks each of\naverage degree $k$, the giant component, $P_{\\infty}$, is given by\n$P_{\\infty}=p[1-\\exp(-kP_{\\infty})]^n$ where $1-p$ is the initial fraction of\nremoved nodes. Our general result coincides for $n=1$ with the known\nErd\\H{o}s-R\\'{e}nyi second-order phase transition at a threshold, $p=p_c$, for\na single network. For $n=2$ the general result for $P_{\\infty}$ corresponds to\nthe $n=2$ result [Buldyrev et. al., Nature, 464, (2010)]. Similar to the ER\nNON, for $n=1$ the percolation transition at $p_c$, is of second order while\nfor any $n>1$ it is of first order. The first order percolation transition in\nboth ER and RR (for $n>1$) is accompanied by cascading failures between the\nnetworks due to their interdependencies. However, we find that the robustness\nof $n$ coupled RR networks of degree $k$ is dramatically higher compared to the\n$n$ coupled ER networks of average degree $k$. While for ER NON there exists a\ncritical minimum average degree $k=k_{\\min}$, that increases with $n$, below\nwhich the system collapses, there is no such analogous $k_{\\min}$ for RR NON\nsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5515v1"
    },
    {
        "title": "Partial wave analysis at BES III harnessing the power of GPUs",
        "authors": [
            "Niklaus Berger"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Partial wave analysis is a core tool in hadron spectroscopy. With the high\nstatistics data available at facilities such as the Beijing Spectrometer III,\nthis procedure becomes computationally very expensive. We have successfully\nimplemented a framework for performing partial wave analysis on graphics\nprocessors. We discuss the implementation, the parallel computing frameworks\nemployed and the performance achieved, with a focus on the recent transition to\nthe OpenCL framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5673v1"
    },
    {
        "title": "Optimal Data Split Methodology for Model Validation",
        "authors": [
            "Rebecca Morrison",
            "Corey Bryant",
            "Gabriel Terejanu",
            "Kenji Miki",
            "Serge Prudhomme"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The decision to incorporate cross-validation into validation processes of\nmathematical models raises an immediate question - how should one partition the\ndata into calibration and validation sets? We answer this question\nsystematically: we present an algorithm to find the optimal partition of the\ndata subject to certain constraints. While doing this, we address two critical\nissues: 1) that the model be evaluated with respect to predictions of a given\nquantity of interest and its ability to reproduce the data, and 2) that the\nmodel be highly challenged by the validation set, assuming it is properly\ninformed by the calibration set. This framework also relies on the interaction\nbetween the experimentalist and/or modeler, who understand the physical system\nand the limitations of the model; the decision-maker, who understands and can\nquantify the cost of model failure; and the computational scientists, who\nstrive to determine if the model satisfies both the modeler's and decision\nmaker's requirements. We also note that our framework is quite general, and may\nbe applied to a wide range of problems. Here, we illustrate it through a\nspecific example involving a data reduction model for an ICCD camera from a\nshock-tube experiment located at the NASA Ames Research Center (ARC).\n",
        "pdf_link": "http://arxiv.org/pdf/1108.6043v1"
    },
    {
        "title": "Pheromone Static Routing Strategy for Complex Networks",
        "authors": [
            "Xiang Ling",
            "Henry Y. K. Lau",
            "Rui Jiang",
            "Mao-Bin Hu"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In this paper, we adopt the concept of pheromone to generate a set of static\npaths that can reach the performance of global dynamic routing strategy [Phys.\nRev. E 81, 016113(2010)]. In the test stage, pheromone is dropped to the nodes\nby packets forwarded by the global dynamic routing strategy. After that, static\npaths are generated according to the density of pheromone. The output paths can\ngreatly improve traffic systems' overall capacity on different network\nstructures, including scale-free networks, small-world networks and random\ngraphs. Because the paths are static, the system needs much less computational\nresource than the global dynamic routing strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.6119v1"
    },
    {
        "title": "A Student-t based filter for robust signal detection",
        "authors": [
            "Christian Röver"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The search for gravitational-wave signals in detector data is often hampered\nby the fact that many data analysis methods are based on the theory of\nstationary Gaussian noise, while actual measurement data frequently exhibit\nclear departures from these assumptions. Deriving methods from models more\nclosely reflecting the data's properties promises to yield more sensitive\nprocedures. The commonly used matched filter is such a detection method that\nmay be derived via a Gaussian model. In this paper we propose a generalized\nmatched-filtering technique based on a Student-t distribution that is able to\naccount for heavier-tailed noise and is robust against outliers in the data. On\nthe technical side, it generalizes the matched filter's least-squares method to\nan iterative, or adaptive, variation. In a simplified Monte Carlo study we show\nthat when applied to simulated signals buried in actual interferometer noise it\nleads to a higher detection rate than the usual (\"Gaussian\") matched filter.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.0442v3"
    },
    {
        "title": "Persistent Data Layout and Infrastructure for Efficient Selective\n  Retrieval of Event Data in ATLAS",
        "authors": [
            "Peter van Gemmeren",
            "David Malon"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The ATLAS detector at CERN has completed its first full year of recording\ncollisions at 7 TeV, resulting in billions of events and petabytes of data. At\nthese scales, physicists must have the capability to read only the data of\ninterest to their analyses, with the importance of efficient selective access\nincreasing as data taking continues. ATLAS has developed a sophisticated\nevent-level metadata infrastructure and supporting I/O framework allowing event\nselections by explicit specification, by back navigation, and by selection\nqueries to a TAG database via an integrated web interface. These systems and\ntheir performance have been reported on elsewhere. The ultimate success of such\na system, however, depends significantly upon the efficiency of selective event\nretrieval. Supporting such retrieval can be challenging, as ATLAS stores its\nevent data in column-wise orientation using ROOT trees for a number of reasons,\nincluding compression considerations, histogramming use cases, and more. For\n2011 data, ATLAS will utilize new capabilities in ROOT to tune the persistent\nstorage layout of event data, and to significantly speed up selective event\nreading. The new persistent layout strategy and its implications for I/O\nperformance are described in this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.3119v1"
    },
    {
        "title": "Strategy of Competition between Two Groups based on a Contrarian Opinion\n  Model",
        "authors": [
            "Qian Li",
            "Lidia A. Braunstein",
            "Shlomo Havlin",
            "H. Eugene Stanley"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We introduce a contrarian opinion (CO) model in which a fraction p of\ncontrarians within a group holds a strong opinion opposite to the opinion held\nby the rest of the group. At the initial stage, stable clusters of two\nopinions, A and B exist. Then we introduce contrarians which hold a strong B\nopinion into the opinion A group. Through their interactions, the contrarians\nare able to decrease the size of the largest A opinion cluster, and even\ndestroy it. We see this kind of method in operation, e.g when companies send\nfree new products to potential customers in order to convince them to adopt the\nproduct and influence others. We study the CO model, using two different\nstrategies, on both ER and scale-free networks. In strategy I, the contrarians\nare positioned at random. In strategy II, the contrarians are chosen to be the\nhighest degrees nodes. We find that for both strategies the size of the largest\nA cluster decreases to zero as p increases as in a phase transition. At a\ncritical threshold value p_c the system undergoes a second-order phase\ntransition that belongs to the same universality class of mean field\npercolation. We find that even for an ER type model, where the degrees of the\nnodes are not so distinct, strategy II is significantly more effctive in\nreducing the size of the largest A opinion cluster and, at very small values of\np, the largest A opinion cluster is destroyed.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.4305v1"
    },
    {
        "title": "Can the evolution of music be analyzed in a quantitative manner?",
        "authors": [
            "Vilson Vieira",
            "Renato Fabbri",
            "Gonzalo Travieso",
            "Luciano da Fontoura Costa"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We propose a methodology to study music development by applying multivariate\nstatistics on composers characteristics. Seven representative composers were\nconsidered in terms of eight main musical features. Grades were assigned to\neach characteristic and their correlations were analyzed. A bootstrap method\nwas applied to simulate hundreds of artificial composers influenced by the\nseven representatives chosen. Afterwards we quantify non-numeric relations like\ndialectics, opposition and innovation. Composers differences on style and\ntechnique were represented as geometrical distances in the feature space,\nmaking it possible to quantify, for example, how much Bach and Stockhausen\ndiffer from other composers or how much Beethoven influenced Brahms. In\naddition, we compared the results with a prior investigation on philosophy.\nOpposition, strong on philosophy, was not remarkable on music. Supporting an\nobservation already considered by music theorists, strong influences were\nidentified between composers by the quantification of dialectics, implying\ninheritance and suggesting a stronger master-disciple evolution when compared\nto the philosophy analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.4653v2"
    },
    {
        "title": "An OPERA inspired classical model reproducing superluminal velocities",
        "authors": [
            "Bogusław Broda"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In the context of the sensational results concerning superluminal velocities,\nannounced recently by the OPERA Collaboration, we have proposed a classical\nmodel yielding a statistically calculated measured velocity of a beam, higher\nthan the velocity of the particles constituting the beam. The two key elements\nof our model, necessary and sufficient to obtain this curious result, are a\ntime-dependent \"transmission\" function and statistical method of the\nmaximum-likelihood estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.0644v3"
    },
    {
        "title": "Anthropic decision theory",
        "authors": [
            "Stuart Armstrong"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  This paper sets out to resolve how agents ought to act in the Sleeping Beauty\nproblem and various related anthropic (self-locating belief) problems, not\nthrough the calculation of anthropic probabilities, but through finding the\ncorrect decision to make. It creates an anthropic decision theory (ADT) that\ndecides these problems from a small set of principles. By doing so, it\ndemonstrates that the attitude of agents with regards to each other (selfish or\naltruistic) changes the decisions they reach, and that it is very important to\ntake this into account. To illustrate ADT, it is then applied to two major\nanthropic problems and paradoxes, the Presumptuous Philosopher and Doomsday\nproblems, thus resolving some issues about the probability of human extinction.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.6437v3"
    },
    {
        "title": "Non-Asymptotic Analysis of Tangent Space Perturbation",
        "authors": [
            "Daniel N. Kaslovsky",
            "Francois G. Meyer"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Constructing an efficient parameterization of a large, noisy data set of\npoints lying close to a smooth manifold in high dimension remains a fundamental\nproblem. One approach consists in recovering a local parameterization using the\nlocal tangent plane. Principal component analysis (PCA) is often the tool of\nchoice, as it returns an optimal basis in the case of noise-free samples from a\nlinear subspace. To process noisy data samples from a nonlinear manifold, PCA\nmust be applied locally, at a scale small enough such that the manifold is\napproximately linear, but at a scale large enough such that structure may be\ndiscerned from noise. Using eigenspace perturbation theory and non-asymptotic\nrandom matrix theory, we study the stability of the subspace estimated by PCA\nas a function of scale, and bound (with high probability) the angle it forms\nwith the true tangent space. By adaptively selecting the scale that minimizes\nthis bound, our analysis reveals an appropriate scale for local tangent plane\nrecovery. We also introduce a geometric uncertainty principle quantifying the\nlimits of noise-curvature perturbation for stable recovery. With the purpose of\nproviding perturbation bounds that can be used in practice, we propose plug-in\nestimates that make it possible to directly apply the theoretical results to\nreal data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.4601v4"
    },
    {
        "title": "Fisher information as a performance metric for locally optimum\n  processing",
        "authors": [
            "Fabing Duan",
            "Francois Chapeau-Blondeau",
            "Derek Abbott"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  For a known weak signal in additive white noise, the asymptotic performance\nof a locally optimum processor (LOP) is shown to be given by the Fisher\ninformation (FI) of a standardized even probability density function (PDF) of\nnoise in three cases: (i) the maximum signal-to-noise ratio (SNR) gain for a\nperiodic signal; (ii) the optimal asymptotic relative efficiency (ARE) for\nsignal detection; (iii) the best cross-correlation gain (CG) for signal\ntransmission. The minimal FI is unity, corresponding to a Gaussian PDF, whereas\nthe FI is certainly larger than unity for any non-Gaussian PDFs. In the sense\nof a realizable LOP, it is found that the dichotomous noise PDF possesses an\ninfinite FI for known weak signals perfectly processed by the corresponding\nLOP. The significance of FI lies in that it provides a upper bound for the\nperformance of locally optimum processing.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.5679v1"
    },
    {
        "title": "Null Models of Economic Networks: The Case of the World Trade Web",
        "authors": [
            "Giorgio Fagiolo",
            "Tiziano Squartini",
            "Diego Garlaschelli"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  In all empirical-network studies, the observed properties of economic\nnetworks are informative only if compared with a well-defined null model that\ncan quantitatively predict the behavior of such properties in constrained\ngraphs. However, predictions of the available null-model methods can be derived\nanalytically only under assumptions (e.g., sparseness of the network) that are\nunrealistic for most economic networks like the World Trade Web (WTW). In this\npaper we study the evolution of the WTW using a recently-proposed family of\nnull network models. The method allows to analytically obtain the expected\nvalue of any network statistic across the ensemble of networks that preserve on\naverage some local properties, and are otherwise fully random. We compare\nexpected and observed properties of the WTW in the period 1950-2000, when\neither the expected number of trade partners or total country trade is kept\nfixed and equal to observed quantities. We show that, in the binary WTW,\nnode-degree sequences are sufficient to explain higher-order network properties\nsuch as disassortativity and clustering-degree correlation, especially in the\nlast part of the sample. Conversely, in the weighted WTW, the observed sequence\nof total country imports and exports are not sufficient to predict higher-order\npatterns of the WTW. We discuss some important implications of these findings\nfor international-trade models.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2895v2"
    },
    {
        "title": "Semi-Supervised Anomaly Detection - Towards Model-Independent Searches\n  of New Physics",
        "authors": [
            "Mikael Kuusela",
            "Tommi Vatanen",
            "Eric Malmi",
            "Tapani Raiko",
            "Timo Aaltonen",
            "Yoshikazu Nagai"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Most classification algorithms used in high energy physics fall under the\ncategory of supervised machine learning. Such methods require a training set\ncontaining both signal and background events and are prone to classification\nerrors should this training data be systematically inaccurate for example due\nto the assumed MC model. To complement such model-dependent searches, we\npropose an algorithm based on semi-supervised anomaly detection techniques,\nwhich does not require a MC training sample for the signal data. We first model\nthe background using a multivariate Gaussian mixture model. We then search for\ndeviations from this model by fitting to the observations a mixture of the\nbackground model and a number of additional Gaussians. This allows us to\nperform pattern recognition of any anomalous excess over the background. We\nshow by a comparison to neural network classifiers that such an approach is a\nlot more robust against misspecification of the signal MC than supervised\nclassification. In cases where there is an unexpected signal, a neural network\nmight fail to correctly identify it, while anomaly detection does not suffer\nfrom such a limitation. On the other hand, when there are no systematic errors\nin the training data, both methods perform comparably.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.3329v3"
    },
    {
        "title": "A measure of centrality based on the spectrum of the Laplacian",
        "authors": [
            "Scott D. Pauls",
            "Daniel Remondini"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We introduce a family of new centralities, the k-spectral centralities.\nk-Spectral centrality is a measurement of importance with respect to the\ndeformation of the graph Laplacian associated with the graph. Due to this\nconnection, k-spectral centralities have various interpretations in terms of\nspectrally determined information.\n  We explore this centrality in the context of several examples. While for\nsparse unweighted networks 1-spectral centrality behaves similarly to other\nstandard centralities, for dense weighted networks they show different\nproperties. In summary, the k-spectral centralities provide a novel and useful\nmeasurement of relevance (for single network elements as well as whole\nsubnetworks) distinct from other known measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.4758v1"
    },
    {
        "title": "Complex network classification using partially self-avoiding\n  deterministic walks",
        "authors": [
            "Wesley Nunes Gonçalves",
            "Alexandre Souto Martinez",
            "Odemir Martinez Bruno"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Complex networks have attracted increasing interest from various fields of\nscience. It has been demonstrated that each complex network model presents\nspecific topological structures which characterize its connectivity and\ndynamics. Complex network classification rely on the use of representative\nmeasurements that model topological structures. Although there are a large\nnumber of measurements, most of them are correlated. To overcome this\nlimitation, this paper presents a new measurement for complex network\nclassification based on partially self-avoiding walks. We validate the\nmeasurement on a data set composed by 40.000 complex networks of four\nwell-known models. Our results indicate that the proposed measurement improves\ncorrect classification of networks compared to the traditional ones.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.5625v2"
    },
    {
        "title": "Constrained Randomisation of Weighted Networks",
        "authors": [
            "Gerrit Ansmann",
            "Klaus Lehnertz"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We propose a Markov chain method to efficiently generate 'surrogate networks'\nthat are random under the constraint of given vertex strengths. With these\nstrength-preserving surrogates and with edge-weight-preserving surrogates we\ninvestigate the clustering coefficient and the average shortest path length of\nfunctional networks of the human brain as well as of the International Trade\nNetworks. We demonstrate that surrogate networks can provide additional\ninformation about network-specific characteristics and thus help interpreting\nempirical weighted networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.0638v1"
    },
    {
        "title": "Optimal Allocation of Interconnecting Links in Cyber-Physical Systems:\n  Interdependence, Cascading Failures and Robustness",
        "authors": [
            "Osman Yagan",
            "Dajun Qian",
            "Junshan Zhang",
            "Douglas Cochran"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We consider a cyber-physical system consisting of two interacting networks,\ni.e., a cyber-network overlaying a physical-network. It is envisioned that\nthese systems are more vulnerable to attacks since node failures in one network\nmay result in (due to the interdependence) failures in the other network,\ncausing a cascade of failures that would potentially lead to the collapse of\nthe entire infrastructure. The robustness of interdependent systems against\nthis sort of catastrophic failure hinges heavily on the allocation of the\n(interconnecting) links that connect nodes in one network to nodes in the other\nnetwork. In this paper, we characterize the optimum inter-link allocation\nstrategy against random attacks in the case where the topology of each\nindividual network is unknown. In particular, we analyze the \"regular\"\nallocation strategy that allots exactly the same number of bi-directional\ninter-network links to all nodes in the system. We show, both analytically and\nexperimentally, that this strategy yields better performance (from a network\nresilience perspective) compared to all possible strategies, including\nstrategies using random allocation, unidirectional inter-links, etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.2698v2"
    },
    {
        "title": "Closed Contour Fractal Dimension Estimation by the Fourier Transform",
        "authors": [
            "João B. Florindo",
            "Odemir M. Bruno"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  This work proposes a novel technique for the numerical calculus of the\nfractal dimension of fractal objects which can be represented as a closed\ncontour. The proposed method maps the fractal contour onto a complex signal and\ncalculates its fractal dimension using the Fourier transform. The Fourier power\nspectrum is obtained and an exponential relation is verified between the power\nand the frequency. From the parameter (exponent) of the relation, it is\nobtained the fractal dimension. The method is compared to other classical\nfractal dimension estimation methods in the literature, e. g.,\nBouligand-Minkowski, box-couting and classical Fourier. The comparison is\nachieved by the calculus of the fractal dimension of fractal contours whose\ndimensions are well-known analytically. The results showed the high precision\nand robustness of the proposed technique.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3097v1"
    },
    {
        "title": "Fractal Descriptors in the Fourier Domain Applied to Color Texture\n  Analysis",
        "authors": [
            "João Batista Florindo",
            "Odemir Martinez Bruno"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The present work proposes the development of a novel method to provide\ndescriptors for colored texture images. The method consists in two steps. In\nthe first, we apply a linear transform in the color space of the image aiming\nat highlighting spatial structuring relations among the color of pixels. In a\nsecond moment, we apply a multiscale approach to the calculus of fractal\ndimension based on Fourier transform. From this multiscale operation, we\nextract the descriptors used to discriminate the texture represented in digital\nimages. The accuracy of the method is verified in the classification of two\ncolor texture datasets, by comparing the performance of the proposed technique\nto other classical and state-of-the-art methods for color texture analysis. The\nresults showed an advantage of almost 3% of the proposed technique over the\nsecond best approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3133v2"
    },
    {
        "title": "Multiscale Fractal Descriptors Applied to Nanoscale Images",
        "authors": [
            "João B. Florindo",
            "Mariana S. Sikora",
            "Ernesto C. Pereira",
            "Odemir M. Bruno"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  This work proposes the application of fractal descriptors to the analysis of\nnanoscale materials under different experimental conditions. We obtain\ndescriptors for images from the sample applying a multiscale transform to the\ncalculation of fractal dimension of a surface map of such image. Particularly,\nwe have used the}Bouligand-Minkowski fractal dimension. We applied these\ndescriptors to discriminate between two titanium oxide films prepared under\ndifferent experimental conditions. Results demonstrate the discrimination power\nof proposed descriptors in such kind of application.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3410v1"
    },
    {
        "title": "Potential Theory for Directed Networks",
        "authors": [
            "Qian-Ming Zhang",
            "Linyuan Lü",
            "Wen-Qiang Wang",
            "Yu-Xiao Zhu",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Uncovering factors underlying the network formation is a long-standing\nchallenge for data mining and network analysis. In particular, the microscopic\norganizing principles of directed networks are less understood than those of\nundirected networks. This article proposes a hypothesis named potential theory,\nwhich assumes that every directed link corresponds to a decrease of a unit\npotential and subgraphs with definable potential values for all nodes are\npreferred. Combining the potential theory with the clustering and homophily\nmechanisms, it is deduced that the Bi-fan structure consisting of 4 nodes and 4\ndirected links is the most favored local structure in directed networks. Our\nhypothesis receives strongly positive supports from extensive experiments on 15\ndirected networks drawn from disparate fields, as indicated by the most\naccurate and robust performance of Bi-fan predictor within the link prediction\nframework. In summary, our main contribution is twofold: (i) We propose a new\nmechanism for the local organization of directed networks; (ii) We design the\ncorresponding link prediction algorithm, which can not only testify our\nhypothesis, but also find out direct applications in missing link prediction\nand friendship recommendation.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.2709v2"
    },
    {
        "title": "Scaling Laws in Human Language",
        "authors": [
            "Linyuan Lu",
            "Zi-Ke Zhang",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Zipf's law on word frequency is observed in English, French, Spanish,\nItalian, and so on, yet it does not hold for Chinese, Japanese or Korean\ncharacters. A model for writing process is proposed to explain the above\ndifference, which takes into account the effects of finite vocabulary size.\nExperiments, simulations and analytical solution agree well with each other.\nThe results show that the frequency distribution follows a power law with\nexponent being equal to 1, at which the corresponding Zipf's exponent diverges.\nActually, the distribution obeys exponential form in the Zipf's plot. Deviating\nfrom the Heaps' law, the number of distinct words grows with the text length in\nthree stages: It grows linearly in the beginning, then turns to a logarithmical\nform, and eventually saturates. This work refines previous understanding about\nZipf's law and Heaps' law in language systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.2903v1"
    },
    {
        "title": "Information flow in a network model and the law of diminishing marginal\n  returns",
        "authors": [
            "Daniele Marinazzo",
            "Mario Pellicoro",
            "Guorong Wu",
            "Leonardo Angelini",
            "Sebastiano Stramaglia"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We analyze a simple dynamical network model which describes the limited\ncapacity of nodes to process the input information. For a suitable choice of\nthe parameters, the information flow pattern is characterized by exponential\ndistribution of the incoming information and a fat-tailed distribution of the\noutgoing information, as a signature of the law of diminishing marginal\nreturns. The analysis of a real EEG data-set shows that similar phenomena may\nbe relevant for brain signals.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.5041v2"
    },
    {
        "title": "Hyperspectral Unmixing Overview: Geometrical, Statistical, and Sparse\n  Regression-Based Approaches",
        "authors": [
            "José M. Bioucas-Dias",
            "Antonio Plaza",
            "Nicolas Dobigeon",
            "Mario Parente",
            "Qian Du",
            "Paul Gader",
            "Jocelyn Chanussot"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Imaging spectrometers measure electromagnetic energy scattered in their\ninstantaneous field view in hundreds or thousands of spectral channels with\nhigher spectral resolution than multispectral cameras. Imaging spectrometers\nare therefore often referred to as hyperspectral cameras (HSCs). Higher\nspectral resolution enables material identification via spectroscopic analysis,\nwhich facilitates countless applications that require identifying materials in\nscenarios unsuitable for classical spectroscopic analysis. Due to low spatial\nresolution of HSCs, microscopic material mixing, and multiple scattering,\nspectra measured by HSCs are mixtures of spectra of materials in a scene. Thus,\naccurate estimation requires unmixing. Pixels are assumed to be mixtures of a\nfew materials, called endmembers. Unmixing involves estimating all or some of:\nthe number of endmembers, their spectral signatures, and their abundances at\neach pixel. Unmixing is a challenging, ill-posed inverse problem because of\nmodel inaccuracies, observation noise, environmental conditions, endmember\nvariability, and data set size. Researchers have devised and investigated many\nmodels searching for robust, stable, tractable, and accurate unmixing\nalgorithms. This paper presents an overview of unmixing methods from the time\nof Keshava and Mustard's unmixing tutorial [1] to the present. Mixing models\nare first discussed. Signal-subspace, geometrical, statistical, sparsity-based,\nand spatial-contextual unmixing algorithms are described. Mathematical problems\nand potential solutions are described. Algorithm characteristics are\nillustrated experimentally.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.6294v2"
    },
    {
        "title": "Perturbation of the Eigenvectors of the Graph Laplacian: Application to\n  Image Denoising",
        "authors": [
            "Francois G. Meyer",
            "Xilin Shen"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The original contributions of this paper are twofold: a new understanding of\nthe influence of noise on the eigenvectors of the graph Laplacian of a set of\nimage patches, and an algorithm to estimate a denoised set of patches from a\nnoisy image. The algorithm relies on the following two observations: (1) the\nlow-index eigenvectors of the diffusion, or graph Laplacian, operators are very\nrobust to random perturbations of the weights and random changes in the\nconnections of the patch-graph; and (2) patches extracted from smooth regions\nof the image are organized along smooth low-dimensional structures in the\npatch-set, and therefore can be reconstructed with few eigenvectors.\nExperiments demonstrate that our denoising algorithm outperforms the denoising\ngold-standards.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.6666v1"
    },
    {
        "title": "Network Physiology reveals relations between network topology and\n  physiological function",
        "authors": [
            "Amir Bashan",
            "Ronny P. Bartsch",
            "Jan W. Kantelhardt",
            "Shlomo Havlin",
            "Plamen Ch. Ivanov"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The human organism is an integrated network where complex physiologic\nsystems, each with its own regulatory mechanisms, continuously interact, and\nwhere failure of one system can trigger a breakdown of the entire network.\nIdentifying and quantifying dynamical networks of diverse systems with\ndifferent types of interactions is a challenge. Here, we develop a framework to\nprobe interactions among diverse systems, and we identify a physiologic\nnetwork. We find that each physiologic state is characterized by a specific\nnetwork structure, demonstrating a robust interplay between network topology\nand function. Across physiologic states the network undergoes topological\ntransitions associated with fast reorganization of physiologic interactions on\ntime scales of a few minutes, indicating high network flexibility in response\nto perturbations. The proposed system-wide integrative approach may facilitate\nthe development of a new field, Network Physiology.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.0242v1"
    },
    {
        "title": "RooStats for Searches",
        "authors": [
            "Grégory Schott"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The RooStats toolkit, which is distributed with the ROOT software package,\nprovides a large collection of software tools that implement statistical\nmethods commonly used by the High Energy Physics community. The toolkit is\nbased on RooFit, a high-level data analysis modeling package that implements\nvarious methods of statistical data analysis. RooStats enforces a clear mapping\nof statistical concepts to C++ classes and methods and emphasizes the ability\nto easily combine analyses within and across experiments. We present an\noverview of the RooStats toolkit, describe some of the methods used for\nhypothesis testing and estimation of confidence intervals and finally discuss\nsome of the latest developments.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.1547v1"
    },
    {
        "title": "Noise correlation and decorrelation in arrays of bolometric detectors",
        "authors": [
            "C. Mancini-Terracciano",
            "M. Vignati"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Bolometers are phonon mediated detectors used in particle physics experiments\nto search for rare processes, such as neutrinoless double beta decay and dark\nmatter interactions. They feature an excellent energy resolution, which is a\nfew keV over an energy range extending from a few keV up to several MeV.\nNevertheless the resolution can be limited by the noise induced by vibrations\nof the mechanical apparatus. In arrays of bolometers part of this noise is\ncorrelated among different detectors and can be removed using a multichannel\ndecorrelation algorithm. In this paper we present a decorrelation method and\nits application to data from the CUORICINO experiment, an array of 62 TeO2\nbolometers.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.1782v1"
    },
    {
        "title": "Microcanonical processing methodology for ECG and intracardial\n  potential: application to atrial fibrillation",
        "authors": [
            "Oriol Pont",
            "Michel Haïssaguerre",
            "Hussein Yahia",
            "Nicolas Derval",
            "Mélèze Hocini"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Cardiac diseases are the principal cause of human morbidity and mortality in\nthe western world. The electric potential of the heart is a highly complex\nsignal emerging as a result of nontrivial flow conduction, hierarchical\nstructuring and multiple regulation mechanisms. Its proper accurate analysis\nbecomes of crucial importance in order to detect and treat arrhythmias or other\nabnormal dynamics that could lead to life-threatening conditions. To achieve\nthis, advanced nonlinear processing methods are needed: one example here is the\ncase of recent advances in the Microcanonical Multiscale Formalism. The aim of\nthe present paper is to recapitulate those advances and extend the analyses\nperformed, specially looking at the case of atrial fibrillation. We show that\nboth ECG and intracardial potential signals can be described in a model-free\nway as a fast dynamics combined with a slow dynamics. Sharp differences in the\nkey parameters of the fast dynamics appear in different regimes of transition\nbetween atrial fibrillation and healthy cases. Therefore, this type of analysis\ncould be used for automated early warning, also in the treatment of atrial\nfibrillation particularly to guide radiofrequency ablation procedures.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.4084v1"
    },
    {
        "title": "Implicit particle methods and their connection with variational data\n  assimilation",
        "authors": [
            "Ethan Atkins",
            "Matthias Morzfeld",
            "Alexandre J. Chorin"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The implicit particle filter is a sequential Monte Carlo method for data\nassimilation that guides the particles to the high-probability regions via a\nsequence of steps that includes minimizations. We present a new and more\ngeneral derivation of this approach and extend the method to particle smoothing\nas well as to data assimilation for perfect models. We show that the\nminimizations required by implicit particle methods are similar to the ones one\nencounters in variational data assimilation and explore the connection of\nimplicit particle methods with variational data assimilation. In particular, we\nargue that existing variational codes can be converted into implicit particle\nmethods at a low cost, often yielding better estimates, that are also equipped\nwith quantitative measures of the uncertainty. A detailed example is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.1830v1"
    },
    {
        "title": "A Symplectic Method to Generate Multivariate Normal Distributions",
        "authors": [
            "Christian Baumgarten"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The AMAS group at the Paul Scherrer Institute developed an object oriented\nlibrary for high performance simulation of high intensity ion beam transport\nwith space charge. Such particle-in-cell (PIC) simulations require a method to\ngenerate multivariate particle distributions as starting conditions.\n  In a preceeding publications it has been shown that the generators of\nsymplectic transformations in two dimensions are a subset of the real Dirac\nmatrices (RDMs) and that few symplectic transformations are required to\ntransform a quadratic Hamiltonian into diagonal form.\n  Here we argue that the use of RDMs is well suited for the generation of\nmultivariate normal distributions with arbitrary covariances. A direct and\nsimple argument supporting this claim is that this is the \"natural\" way how\nsuch distributions are formed. The transport of charged particle beams may\nserve as an example: An uncorrelated gaussian distribution of particles\nstarting at some initial position of the accelerator is subject to linear\ndeformations when passing through various beamline elements. These deformations\ncan be described by symplectic transformations.\n  Hence, if it is possible to derive the symplectic transformations that bring\nup these covariances, it is also possible to produce arbitrary multivariate\nnormal distributions without Cholesky decomposition. The method allows the use\nof arbitrary uncoupled distributions. The functional form of the coupled\nmultivariate distributions however depends in the general case on the type of\nthe used random number generator. Only gaussian generators always yield\ngaussian multivariate distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.3601v1"
    },
    {
        "title": "A Development Environment for Visual Physics Analysis",
        "authors": [
            "H. -P. Bretz",
            "M. Brodski",
            "M. Erdmann",
            "R. Fischer",
            "A. Hinzmann",
            "T. Klimkovich",
            "D. Klingebiel",
            "M. Komm",
            "J. Lingemann",
            "G. Müller",
            "T. Münzer",
            "M. Rieger",
            "J. Steggemann",
            "T. Winchen"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The Visual Physics Analysis (VISPA) project integrates different aspects of\nphysics analyses into a graphical development environment. It addresses the\ntypical development cycle of (re-)designing, executing and verifying an\nanalysis. The project provides an extendable plug-in mechanism and includes\nplug-ins for designing the analysis flow, for running the analysis on batch\nsystems, and for browsing the data content. The corresponding plug-ins are\nbased on an object-oriented toolkit for modular data analysis. We introduce the\nmain concepts of the project, describe the technical realization and\ndemonstrate the functionality in example applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.4912v2"
    },
    {
        "title": "Recognition of stable distribution with Levy index alpha close to 2",
        "authors": [
            "Krzysztof Burnecki",
            "Agnieszka Wyłomańska",
            "Aleksei Beletskii",
            "Vsevolod Gonchar",
            "Aleksei Chechkin"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We address the problem of recognizing alpha-stable Levy distribution with\nLevy index close to 2 from experimental data. We are interested in the case\nwhen the sample size of available data is not large, thus the power law\nasymptotics of the distribution is not clearly detectable, and the shape of\nempirical probability density function is close to a Gaussian. We propose a\ntesting procedure combining a simple visual test based on empirical fourth\nmoment with the Anderson-Darling and Jarque-Bera statistical tests and we check\nthe efficiency of the method on simulated data. Furthermore, we apply our\nmethod to the analysis of turbulent plasma density and potential fluctuations\nmeasured in the stellarator type fusion device and demonstrate that the\nphenomenon of L-H transition occurring in this device is accompanied by the\ntransition from Levy to Gaussian fluctuation statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.6458v1"
    },
    {
        "title": "Cascading Failures in Interdependent Lattice Networks: The Critical Role\n  of the Length of Dependency Links",
        "authors": [
            "Wei Li",
            "Amir Bashan",
            "Sergey V. Buldyrev",
            "H. Eugene Stanley",
            "Shlomo Havlin"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We study the cascading failures in a system composed of two interdependent\nsquare lattice networks A and B placed on the same Cartesian plane, where each\nnode in network A depends on a node in network B randomly chosen within a\ncertain distance $r$ from the corresponding node in network A and vice versa.\nOur results suggest that percolation for small $r$ below $r_{\\rm max}\\approx 8$\n(lattice units) is a second-order transition, and for larger $r$ is a\nfirst-order transition. For $r<r_{\\rm max}$, the critical threshold increases\nlinearly with $r$ from 0.593 at $r=0$ and reaches a maximum, 0.738 for\n$r=r_{\\rm max}$ and then gradually decreases to 0.683 for $r=\\infty$. Our\nanalytical considerations are in good agreement with simulations. Our study\nsuggests that interdependent infrastructures embedded in Euclidean space become\nmost vulnerable when the distance between interdependent nodes is in the\nintermediate range, which is much smaller than the size of the system.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.0224v1"
    },
    {
        "title": "Parameter Estimation Through Ignorance",
        "authors": [
            "Hailiang Du",
            "Leonard A. Smith"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Dynamical modelling lies at the heart of our understanding of physical\nsystems. Its role in science is deeper than mere operational forecasting, in\nthat it allows us to evaluate the adequacy of the mathematical structure of our\nmodels. Despite the importance of model parameters, there is no general method\nof parameter estimation outside linear systems. A new relatively simple method\nof parameter estimation for nonlinear systems is presented, based on variations\nin the accuracy of probability forecasts. It is illustrated on the Logistic\nMap, the Henon Map and the 12-D Lorenz96 flow, and its ability to outperform\nlinear least squares in these systems is explored at various noise levels and\nsampling rates. As expected, it is more effective when the forecast error\ndistributions are non-Gaussian. The new method selects parameter values by\nminimizing a proper, local skill score for continuous probability forecasts as\na function of the parameter values. This new approach is easier to implement in\npractice than alternative nonlinear methods based on the geometry of attractors\nor the ability of the model to shadow the observations. New direct measures of\ninadequacy in the model, the \"Implied Ignorance\" and the information deficit\nare introduced.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.1268v1"
    },
    {
        "title": "The extreme vulnerability of interdependent spatially embedded networks",
        "authors": [
            "Amir Bashan",
            "Yehiel Berezin",
            "Sergey V. Buldyrev",
            "Shlomo Havlin"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Recent studies show that in interdependent networks a very small failure in\none network may lead to catastrophic consequences. Above a critical fraction of\ninterdependent nodes, even a single node failure can invoke cascading failures\nthat may abruptly fragment the system, while below this \"critical dependency\"\n(CD) a failure of few nodes leads only to small damage to the system. So far,\nthe research has been focused on interdependent random networks without space\nlimitations. However, many real systems, such as power grids and the Internet,\nare not random but are spatially embedded. Here we analytically and numerically\nanalyze the stability of systems consisting of interdependent spatially\nembedded networks modeled as lattice networks. Surprisingly, we find that in\nlattice systems, in contrast to non-embedded systems, there is no CD and\n\\textit{any} small fraction of interdependent nodes leads to an abrupt\ncollapse. We show that this extreme vulnerability of very weakly coupled\nlattices is a consequence of the critical exponent describing the percolation\ntransition of a single lattice. Our results are important for understanding the\nvulnerabilities and for designing robust interdependent spatial embedded\nnetworks.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2062v1"
    },
    {
        "title": "Pippi - painless parsing, post-processing and plotting of posterior and\n  likelihood samples",
        "authors": [
            "Pat Scott"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Interpreting samples from likelihood or posterior probability density\nfunctions is rarely as straightforward as it seems it should be. Producing\npublication-quality graphics of these distributions is often similarly painful.\nIn this short note I describe pippi, a simple, publicly-available package for\nparsing and post-processing such samples, as well as generating high-quality\nPDF graphics of the results. Pippi is easily and extensively configurable and\ncustomisable, both in its options for parsing and post-processing samples, and\nin the visual aspects of the figures it produces. I illustrate some of these\nusing an existing supersymmetric global fit, performed in the context of a\ngamma-ray search for dark matter. Pippi can be downloaded and followed at\nhttp://github.com/patscott/pippi .\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2245v3"
    },
    {
        "title": "The H1 Data Preservation Project",
        "authors": [
            "David M. South",
            "Michael Steder"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The H1 data preservation project was started in 2009 as part of the global\ndata preservation initiative in high-energy physics, DPHEP. In order to retain\nthe full potential for future improvements, the H1 Collaboration aims for level\n4 of the DPHEP recommendations, which requires the full simulation and\nreconstruction chain as well as the data to be preserved for future analysis. A\nmajor goal of the H1 project is therefore to provide secure, long-lived and\nvalidated access to the H1 data and analysis software, which is realised in\ncollaboration with DESY-IT using virtualisation techniques. By implementing\nsuch a system, it is hoped that the lifetime of the unique ep collision data\nfrom HERA will be extended, providing the possibility for novel analysis in the\nfuture. The preservation of the data and software is performed alongside a\nconsolidation programme of digital and non-digital documentation, some of which\ndates back to the early 1980s. A new organisational model of the H1\nCollaboration, reflecting the change to the long term phase, is to be adopted\nin July 2012.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.5200v1"
    },
    {
        "title": "What is Statistics?; The Answer by Quantum Language",
        "authors": [
            "Shiro Ishikawa"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Since the problem: \"What is statistics?\" is most fundamental in sceince, in\norder to solve this problem, there is every reason to believe that we have to\nstart from the proposal of a worldview. Recently we proposed measurement theory\n(i.e., quantum language, or the linguistic interpretation of quantum\nmechanics), which is characterized as the linguistic turn of the Copenhagen\ninterpretation of quantum mechanics. This turn from physics to language does\nnot only extend quantum theory to classical theory but also yield the quantum\nmechanical world view (i.e., the (quantum) linguistic world view, and thus, a\nform of quantum thinking, in other words, quantum philosophy). Thus, we believe\nthat the quantum lingistic formulation of statistics gives an answer to the\nquestion: \"What is statistics?\". In this paper, this will be done through the\nstudies of inference interval, statistical hypothesis testing, Fisher maximum\nlikelihood method, Bayes method and regression analysis in meaurement theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.0407v1"
    },
    {
        "title": "On the origin of long-range correlations in texts",
        "authors": [
            "Eduardo G. Altmann",
            "Giampaolo Cristadoro",
            "Mirko Degli Esposti"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The complexity of human interactions with social and natural phenomena is\nmirrored in the way we describe our experiences through natural language. In\norder to retain and convey such a high dimensional information, the statistical\nproperties of our linguistic output has to be highly correlated in time. An\nexample are the robust observations, still largely not understood, of\ncorrelations on arbitrary long scales in literary texts. In this paper we\nexplain how long-range correlations flow from highly structured linguistic\nlevels down to the building blocks of a text (words, letters, etc..). By\ncombining calculations and data analysis we show that correlations take form of\na bursty sequence of events once we approach the semantically relevant topics\nof the text. The mechanisms we identify are fairly general and can be equally\napplied to other hierarchical settings.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.0658v1"
    },
    {
        "title": "A Simple Non-Markovian Computational Model of the Statistics of Soccer\n  Leagues: Emergence and Scaling effects",
        "authors": [
            "Roberto da Silva",
            "Mendeli Vainstein",
            "Luis Lamb",
            "Sandra Prado"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We propose a novel algorithm that outputs the final standings of a soccer\nleague, based on a simple dynamics that mimics a soccer tournament. In our\nmodel, a team is created with a defined potential(ability) which is updated\nduring the tournament according to the results of previous games. The updated\npotential modifies a teams' future winning/losing probabilities. We show that\nthis evolutionary game is able to reproduce the statistical properties of final\nstandings of actual editions of the Brazilian tournament (Brasileir\\~{a}o).\nHowever, other leagues such as the Italian and the Spanish tournaments have\nnotoriously non-Gaussian traces and cannot be straightforwardly reproduced by\nthis evolutionary non-Markovian model. A complete understanding of these\nphenomena deserves much more attention, but we suggest a simple explanation\nbased on data collected in Brazil: Here several teams were crowned champion in\nprevious editions corroborating that the champion typically emerges from random\nfluctuations that partly preserves the gaussian traces during the tournament.\nOn the other hand, in the Italian and Spanish leagues only a few teams in\nrecent history have won their league tournaments. These leagues are based on\nmore robust and hierarchical structures established even before the beginning\nof the tournament. For the sake of completeness, we also elaborate a totally\nGaussian model (which equalizes the winning, drawing, and losing probabilities)\nand we show that the scores of the \"Brasileir\\~{a}o\" cannot be reproduced. Such\naspects stress that evolutionary aspects are not superfluous in our modeling.\nFinally, we analyse the distortions of our model in situations where a large\nnumber of teams is considered, showing the existence of a transition from a\nsingle to a double peaked histogram of the final classification scores. An\ninteresting scaling is presented for different sized tournaments.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.1848v1"
    },
    {
        "title": "Stochastic description of geometric phase for polarized waves in random\n  media",
        "authors": [
            "Jérémie Boulanger",
            "Nicolas le Bihan",
            "Vincent Rossetto"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We present a stochastic description of multiple scattering of polarized waves\nin the regime of forward scattering. In this regime, if the source is\npolarized, polarization survives along a few transport mean free paths, making\nit possible to measure an outgoing polarization distribution. We solve the\ndirect problem using compound Poisson processes on the rotation group SO(3) and\nnon-commutative harmonic analysis. The obtained solution generalizes previous\nworks in multiple scattering theory and is used to design an algorithm solving\nthe inverse problem of estimating the scattering properties of the medium from\nthe observations. This technique applies to thin disordered layers, spatially\nfluctuating media and multiple scattering systems and is based on the\npolarization but not on the signal amplitude. We suggest that it can be used as\na non invasive testing method.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.2569v1"
    },
    {
        "title": "Inference of Extreme Synchrony with an Entropy Measure on a Bipartite\n  Network",
        "authors": [
            "Aki-Hiro Sato"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  This article proposes a method to quantify the structure of a bipartite graph\nusing a network entropy per link. The network entropy of a bipartite graph with\nrandom links is calculated both numerically and theoretically. As an\napplication of the proposed method to analyze collective behavior, the affairs\nin which participants quote and trade in the foreign exchange market are\nquantified. The network entropy per link is found to correspond to the\nmacroeconomic situation. A finite mixture of Gumbel distributions is used to\nfit the empirical distribution for the minimum values of network entropy per\nlink in each week. The mixture of Gumbel distributions with parameter estimates\nby segmentation procedure is verified by the Kolmogorov--Smirnov test. The\nfinite mixture of Gumbel distributions that extrapolate the empirical\nprobability of extreme events has explanatory power at a statistically\nsignificant level.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.4860v4"
    },
    {
        "title": "An improved method for measuring muon energy using the truncated mean of\n  dE/dx",
        "authors": [
            " IceCube collaboration",
            "R. Abbasi",
            "Y. Abdou",
            "M. Ackermann",
            "J. Adams",
            "J. A. Aguilar",
            "M. Ahlers",
            "D. Altmann",
            "K. Andeen",
            "J. Auffenberg",
            "X. Bai",
            "M. Baker",
            "S. W. Barwick",
            "V. Baum",
            "R. Bay",
            "K. Beattie",
            "J. J. Beatty",
            "S. Bechet",
            "J. Becker Tjus",
            "K. -H. Becker",
            "M. Bell",
            "M. L. Benabderrahmane",
            "S. BenZvi",
            "J. Berdermann",
            "P. Berghaus",
            "D. Berley",
            "E. Bernardini",
            "D. Bertrand",
            "D. Z. Besson",
            "D. Bindig",
            "M. Bissok",
            "E. Blaufuss",
            "J. Blumenthal",
            "D. J. Boersma",
            "C. Bohm",
            "D. Bose",
            "S. Böser",
            "O. Botner",
            "L. Brayeur",
            "A. M. Brown",
            "R. Bruijn",
            "J. Brunner",
            "S. Buitink",
            "M. Carson",
            "J. Casey",
            "M. Casier",
            "D. Chirkin",
            "B. Christy",
            "F. Clevermann",
            "S. Cohen",
            "D. F. Cowen",
            "A. H. Cruz Silva",
            "M. Danninger",
            "J. Daughhetee",
            "J. C. Davis",
            "C. De Clercq",
            "F. Descamps",
            "P. Desiati",
            "G. de Vries-Uiterweerd",
            "T. DeYoung",
            "J. C. Díaz-Vélez",
            "J. Dreyer",
            "J. P. Dumm",
            "M. Dunkman",
            "R. Eagan",
            "J. Eisch",
            "C. Elliott",
            "R. W. Ellsworth",
            "O. Engdegård",
            "S. Euler",
            "P. A. Evenson",
            "O. Fadiran",
            "A. R. Fazely",
            "A. Fedynitch",
            "J. Feintzeig",
            "T. Feusels",
            "K. Filimonov",
            "C. Finley",
            "T. Fischer-Wasels",
            "S. Flis",
            "A. Franckowiak",
            "R. Franke",
            "K. Frantzen",
            "T. Fuchs",
            "T. K. Gaisser",
            "J. Gallagher",
            "L. Gerhardt",
            "L. Gladstone",
            "T. Glüsenkamp",
            "A. Goldschmidt",
            "J. A. Goodman",
            "D. Góra",
            "D. Grant",
            "A. Groß",
            "S. Grullon",
            "M. Gurtner",
            "C. Ha",
            "A. Haj Ismail",
            "A. Hallgren",
            "F. Halzen",
            "K. Hanson",
            "D. Heereman",
            "P. Heimann",
            "D. Heinen",
            "K. Helbing",
            "R. Hellauer",
            "S. Hickford",
            "G. C. Hill",
            "K. D. Hoffman",
            "R. Hoffmann",
            "A. Homeier",
            "K. Hoshina",
            "W. Huelsnitz",
            "P. O. Hulth",
            "K. Hultqvist",
            "S. Hussain",
            "A. Ishihara",
            "E. Jacobi",
            "J. Jacobsen",
            "G. S. Japaridze",
            "O. Jlelati",
            "A. Kappes",
            "T. Karg",
            "A. Karle",
            "J. Kiryluk",
            "F. Kislat",
            "J. Kläs",
            "S. R. Klein",
            "S. Klepser",
            "J. -H. Köhne",
            "G. Kohnen",
            "H. Kolanoski",
            "L. Köpke",
            "C. Kopper",
            "S. Kopper",
            "D. J. Koskinen",
            "M. Kowalski",
            "M. Krasberg",
            "G. Kroll",
            "J. Kunnen",
            "N. Kurahashi",
            "T. Kuwabara",
            "M. Labare",
            "K. Laihem",
            "H. Landsman",
            "M. J. Larson",
            "R. Lauer",
            "M. Lesiak-Bzdak",
            "J. Lünemann",
            "J. Madsen",
            "R. Maruyama",
            "K. Mase",
            "H. S. Matis",
            "A. McDermott",
            "F. McNally",
            "K. Meagher",
            "M. Merck",
            "P. Mészáros",
            "T. Meures",
            "S. Miarecki",
            "E. Middell",
            "N. Milke",
            "J. Miller",
            "L. Mohrmann",
            "T. Montaruli",
            "R. Morse",
            "S. M. Movit",
            "R. Nahnhauer",
            "U. Naumann",
            "P. Nießen",
            "S. C. Nowicki",
            "D. R. Nygren",
            "A. Obertacke",
            "S. Odrowski",
            "A. Olivas",
            "M. Olivo",
            "A. O'Murchadha",
            "S. Panknin",
            "L. Paul",
            "J. A. Pepper",
            "C. Pérez de los Heros",
            "D. Pieloth",
            "N. Pirk",
            "J. Posselt",
            "P. B. Price",
            "G. T. Przybylski",
            "L. Rädel",
            "K. Rawlins",
            "P. Redl",
            "E. Resconi",
            "W. Rhode",
            "M. Ribordy",
            "M. Richman",
            "B. Riedel",
            "J. P. Rodrigues",
            "J. Roth",
            "F. Rothmaier",
            "C. Rott",
            "C. Roucelle",
            "T. Ruhe",
            "B. Ruzybayev",
            "D. Ryckbosch",
            "S. M. Saba",
            "T. Salameh",
            "H. -G. Sander",
            "M. Santander",
            "S. Sarkar",
            "K. Schatto",
            "M. Scheel",
            "F. Scheriau",
            "T. Schmidt",
            "M. Schmitz",
            "S. Schoenen",
            "S. Schöneberg",
            "L. Schönherr",
            "A. Schönwald",
            "A. Schukraft",
            "L. Schulte",
            "O. Schulz",
            "D. Seckel",
            "S. H. Seo",
            "Y. Sestayo",
            "S. Seunarine",
            "L. Shulman",
            "M. W. E. Smith",
            "M. Soiron",
            "D. Soldin",
            "G. M. Spiczak",
            "C. Spiering",
            "M. Stamatikos",
            "T. Stanev",
            "A. Stasik",
            "T. Stezelberger",
            "R. G. Stokstad",
            "A. Stößl",
            "S. Stoyanov",
            "E. A. Strahler",
            "R. Ström",
            "K-H. Sulanke",
            "G. W. Sullivan",
            "H. Taavola",
            "I. Taboada",
            "A. Tamburro",
            "S. Ter-Antonyan",
            "S. Tilav",
            "P. A. Toale",
            "S. Toscano",
            "M. Usner",
            "D. van der Drift",
            "N. van Eijndhoven",
            "A. Van Overloop",
            "J. van Santen",
            "M. Vehring",
            "M. Voge",
            "C. Walck",
            "T. Waldenmaier",
            "M. Wallraff",
            "M. Walter",
            "R. Wasserman",
            "Ch. Weaver",
            "C. Wendt",
            "S. Westerhoff",
            "N. Whitehorn",
            "K. Wiebe",
            "C. H. Wiebusch",
            "D. R. Williams",
            "H. Wissing",
            "M. Wolf",
            "T. R. Wood",
            "K. Woschnagg",
            "C. Xu",
            "D. L. Xu",
            "X. W. Xu",
            "J. P. Yanez",
            "G. Yodh",
            "S. Yoshida",
            "P. Zarzhitsky",
            "J. Ziemann",
            "A. Zilles",
            "M. Zoll"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The measurement of muon energy is critical for many analyses in large\nCherenkov detectors, particularly those that involve separating\nextraterrestrial neutrinos from the atmospheric neutrino background. Muon\nenergy has traditionally been determined by measuring the specific energy loss\n(dE/dx) along the muon's path and relating the dE/dx to the muon energy.\nBecause high-energy muons (E_mu > 1 TeV) lose energy randomly, the spread in\ndE/dx values is quite large, leading to a typical energy resolution of 0.29 in\nlog10(E_mu) for a muon observed over a 1 km path length in the IceCube\ndetector. In this paper, we present an improved method that uses a truncated\nmean and other techniques to determine the muon energy. The muon track is\ndivided into separate segments with individual dE/dx values. The elimination of\nsegments with the highest dE/dx results in an overall dE/dx that is more\nclosely correlated to the muon energy. This method results in an energy\nresolution of 0.22 in log10(E_mu), which gives a 26% improvement. This\ntechnique is applicable to any large water or ice detector and potentially to\nlarge scintillator or liquid argon detectors.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.3430v2"
    },
    {
        "title": "Power-law distributions in binned empirical data",
        "authors": [
            "Yogesh Virkar",
            "Aaron Clauset"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Many man-made and natural phenomena, including the intensity of earthquakes,\npopulation of cities and size of international wars, are believed to follow\npower-law distributions. The accurate identification of power-law patterns has\nsignificant consequences for correctly understanding and modeling complex\nsystems. However, statistical evidence for or against the power-law hypothesis\nis complicated by large fluctuations in the empirical distribution's tail, and\nthese are worsened when information is lost from binning the data. We adapt the\nstatistically principled framework for testing the power-law hypothesis,\ndeveloped by Clauset, Shalizi and Newman, to the case of binned data. This\napproach includes maximum-likelihood fitting, a hypothesis test based on the\nKolmogorov--Smirnov goodness-of-fit statistic and likelihood ratio tests for\ncomparing against alternative explanations. We evaluate the effectiveness of\nthese methods on synthetic binned data with known structure, quantify the loss\nof statistical power due to binning, and apply the methods to twelve real-world\nbinned data sets with heavy-tailed patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.3524v2"
    },
    {
        "title": "Comparing the performance of FA, DFA and DMA using different synthetic\n  long-range correlated time series",
        "authors": [
            "Ying-Hui Shao",
            "Gao Feng Gu",
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou",
            "Didier Sornette"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Notwithstanding the significant efforts to develop estimators of long-range\ncorrelations (LRC) and to compare their performance, no clear consensus exists\non what is the best method and under which conditions. In addition, synthetic\ntests suggest that the performance of LRC estimators varies when using\ndifferent generators of LRC time series. Here, we compare the performances of\nfour estimators [Fluctuation Analysis (FA), Detrended Fluctuation Analysis\n(DFA), Backward Detrending Moving Average (BDMA), and centred Detrending Moving\nAverage (CDMA)]. We use three different generators [Fractional Gaussian Noises,\nand two ways of generating Fractional Brownian Motions]. We find that CDMA has\nthe best performance and DFA is only slightly worse in some situations, while\nFA performs the worst. In addition, CDMA and DFA are less sensitive to the\nscaling range than FA. Hence, CDMA and DFA remain \"The Methods of Choice\" in\ndetermining the Hurst index of time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.4158v1"
    },
    {
        "title": "Reciprocity of weighted networks",
        "authors": [
            "Tiziano Squartini",
            "Francesco Picciolo",
            "Franco Ruzzenenti",
            "Diego Garlaschelli"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  All types of networks arise as intricate combinations of dyadic building\nblocks formed by pairs of vertices. In directed networks, the dyadic patterns\nare entirely determined by reciprocity, i.e. the tendency to form, or to avoid,\nmutual links. Reciprocity has dramatic effects on every networks dynamical\nprocesses and the emergence of structures like motifs and communities. The\nbinary reciprocity has been extensively studied: that of weighted networks is\nstill poorly understood. We introduce a general approach to it, by defining\nquantities capturing the observed patterns (from dyad-specific to\nvertex-specific and network-wide) and introducing analytically solved models\n(Exponential Random Graphs-type). Counter-intuitively, the previous reciprocity\nmeasures based on the similarity of the mutual links-weights are uninformative.\nBy contrast, our measures can classify different weighted networks, track the\ntemporal evolution of a networks reciprocity, identify patterns. We show that\nin some networks the local reciprocity structure can be inferred from the\nglobal one.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.4208v2"
    },
    {
        "title": "Anchoring Bias in Online Voting",
        "authors": [
            "Zimo Yang",
            "Zi-Ke Zhang",
            "Tao Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Voting online with explicit ratings could largely reflect people's\npreferences and objects' qualities, but ratings are always irrational, because\nthey may be affected by many unpredictable factors like mood, weather, as well\nas other people's votes. By analyzing two real systems, this paper reveals a\nsystematic bias embedding in the individual decision-making processes, namely\npeople tend to give a low rating after a low rating, as well as a high rating\nfollowing a high rating. This so-called \\emph{anchoring bias} is validated via\nextensive comparisons with null models, and numerically speaking, the extent of\nbias decays with interval voting number in a logarithmic form. Our findings\ncould be applied in the design of recommender systems and considered as\nimportant complementary materials to previous knowledge about anchoring effects\non financial trades, performance judgements, auctions, and so on.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.0057v1"
    },
    {
        "title": "Dynamical networks reconstructed from time series",
        "authors": [
            "Zoran Levnajić"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Novel method of reconstructing dynamical networks from empirically measured\ntime series is proposed. By examining the variable--derivative correlation of\nnetwork node pairs, we derive a simple equation that directly yields the\nadjacency matrix, assuming the intra-network interaction functions to be known.\nWe illustrate the method on a simple example, and discuss the dependence of the\nreconstruction precision on the properties of time series. Our method is\napplicable to any network, allowing for reconstruction precision to be\nmaximized, and errors to be estimated.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.0219v3"
    },
    {
        "title": "Information content versus word length in random typing",
        "authors": [
            "Ramon Ferrer-i-Cancho",
            "Fermín Moscoso del Prado Martín"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Recently, it has been claimed that a linear relationship between a measure of\ninformation content and word length is expected from word length optimization\nand it has been shown that this linearity is supported by a strong correlation\nbetween information content and word length in many languages (Piantadosi et\nal. 2011, PNAS 108, 3825-3826). Here, we study in detail some connections\nbetween this measure and standard information theory. The relationship between\nthe measure and word length is studied for the popular random typing process\nwhere a text is constructed by pressing keys at random from a keyboard\ncontaining letters and a space behaving as a word delimiter. Although this\nrandom process does not optimize word lengths according to information content,\nit exhibits a linear relationship between information content and word length.\nThe exact slope and intercept are presented for three major variants of the\nrandom typing process. A strong correlation between information content and\nword length can simply arise from the units making a word (e.g., letters) and\nnot necessarily from the interplay between a word and its context as proposed\nby Piantadosi et al. In itself, the linear relation does not entail the results\nof any optimization process.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.1751v1"
    },
    {
        "title": "Potential fitting biases resulting from grouping data into variable\n  width bins",
        "authors": [
            "S. Towers"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  When reading peer-reviewed scientific literature describing any analysis of\nempirical data, it is natural and correct to proceed with the underlying\nassumption that experiments have made good faith efforts to ensure that their\nanalyses yield unbiased results. However, particle physics experiments are\nexpensive and time consuming to carry out, thus if an analysis has inherent\nbias (even if unintentional), much money and effort can be wasted trying to\nreplicate or understand the results, particularly if the analysis is\nfundamental to our understanding of the universe.\n  In this note we discuss the significant biases that can result from data\nbinning schemes. As we will show, if data are binned such that they provide the\nbest comparison to a particular (but incorrect) model, the resulting model\nparameter estimates when fitting to the binned data can be significantly\nbiased, leading us to too often accept the model hypothesis when it is not in\nfact true. When using binned likelihood or least squares methods there is of\ncourse no a priori requirement that data bin sizes need to be constant, but we\nshow that fitting to data grouped into variable width bins is particularly\nprone to produce biased results if the bin boundaries are chosen to optimize\nthe comparison of the binned data to a wrong model. The degree of bias that can\nbe achieved simply with variable binning can be surprisingly large.\n  Fitting the data with an unbinned likelihood method, when possible to do so,\nis the best way for researchers to show their analyses are not biased by\nbinning effects. Failing that, equal bin widths should be employed as a\ncross-check of the fitting analysis whenever possible.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.2690v1"
    },
    {
        "title": "Quasi-real-time analysis of dynamic near field scattering data using a\n  graphics processing unit",
        "authors": [
            "Giovanni Cerchiari",
            "Fabrizio Croccolo",
            "Frédéric Cardinaux",
            "Frank Scheffold"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We present an implementation of the analysis of dynamic near field scattering\n(NFS) data using a graphics processing unit (GPU). We introduce an optimized\ndata management scheme thereby limiting the number of operations required.\nOverall, we reduce the processing time from hours to minutes, for typical\nexperimental conditions. Previously the limiting step in such experiments, the\nprocessing time is now comparable to the data acquisition time. Our approach is\napplicable to various dynamic NFS methods, including shadowgraph, Schlieren and\ndifferential dynamic microscopy.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.3362v1"
    },
    {
        "title": "Algebraic characterization of binary graphs",
        "authors": [
            "Tiziano Squartini"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  One of the fundamental concepts in the statistical mechanics field is that of\nensemble. Ensembles of graphs are collections of graphs, defined according to\ncertain rules. The two most used ensembles in network theory are the\nmicrocanonical and the grandcanonical (whose definitions mimick the classical\nones, originally proposed by Boltzmann and Gibbs), even if the latter is far\nmore used than the former to carry on the analytical calculations. For binary\n(undirected or directed) networks, the grandcanonical ensemble is defined by\nconsidering all the graphs with the same number of vertices and a variable\nnumber of links, ranging from 0 to the maximum: N(N-1)/2 for binary, undirected\ngraphs and N(N-1) for binary, directed graphs. Even if it is commonly used\nalmost exclusively as a tool to calculate the average of some topological\nquantity of interest, its structure is so rich to deserve an analysis on its\nown. In this paper a logic-algebraic characterization of the grandcanonical\nensemble of binary graphs is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.5565v1"
    },
    {
        "title": "Modified frequentist determination of confidence intervals for Poisson\n  distribution",
        "authors": [
            "S. I. Bitioukov",
            "N. V. Krasnikov"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We propose modified frequentist definitions for the determination of\nconfidence intervals for the case of Poisson statistics. We require that\n1-\\beta^{'} \\geq \\sum_{n=o}^{n_{obs}+k} P(n|\\lambda) \\geq \\alpha^{'}. We show\nthat this definition is equivalent to the Bayesian method with prior\n\\pi(\\lambda) \\sim \\lambda^{k}. Other generalizations are also considered. In\nparticular, we propose modified symmetric frequentist definition which\ncorresponds to the Bayes approach with the prior function \\pi(\\lambda) \\sim\n1/2(1 + \\frac{n_{obs}}{\\lambda}). Modified frequentist definitions for the case\nof nonzero background are proposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.6545v1"
    },
    {
        "title": "Quantifying Causal Coupling Strength: A Lag-specific Measure For\n  Multivariate Time Series Related To Transfer Entropy",
        "authors": [
            "Jakob Runge",
            "Jobst Heitzig",
            "Norbert Marwan",
            "Jürgen Kurths"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  While it is an important problem to identify the existence of causal\nassociations between two components of a multivariate time series, a topic\naddressed in Runge et al. (2012), it is even more important to assess the\nstrength of their association in a meaningful way. In the present article we\nfocus on the problem of defining a meaningful coupling strength using\ninformation theoretic measures and demonstrate the short-comings of the\nwell-known mutual information and transfer entropy. Instead, we propose a\ncertain time-delayed conditional mutual information, the momentary information\ntransfer (MIT), as a measure of association that is general, causal and\nlag-specific, reflects a well interpretable notion of coupling strength and is\npractically computable. MIT is based on the fundamental concept of source\nentropy, which we utilize to yield a notion of coupling strength that is,\ncompared to mutual information and transfer entropy, well interpretable, in\nthat for many cases it solely depends on the interaction of the two components\nat a certain lag. In particular, MIT is thus in many cases able to exclude the\nmisleading influence of autodependency within a process in an\ninformation-theoretic way. We formalize and prove this idea analytically and\nnumerically for a general class of nonlinear stochastic processes and\nillustrate the potential of MIT on climatological data.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.2748v2"
    },
    {
        "title": "Constraints on Yield Parameters in Extended Maximum Likelihood Fits",
        "authors": [
            "Till Moritz Karbach",
            "Maximilian Schlupp"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The method of extended maximum likelihood is a well known concept of\nparameter estimation. One can implement external knowledge on the unknown\nparameters by multiplying the likelihood by constraint terms. In this note, we\nemphasize that this is also true for yield parameters in an extended maximum\nlikelihood fit, which is widely used in the particle physics community. We\nrecommend a way to generate pseudo-experiments in presence of constraint terms\non yield parameters, and point to pitfalls inside the RooFit framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.7141v1"
    },
    {
        "title": "Surprisingly Rational: Probability theory plus noise explains biases in\n  judgment",
        "authors": [
            "Fintan Costello",
            "Paul Watts"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The systematic biases seen in people's probability judgments are typically\ntaken as evidence that people do not reason about probability using the rules\nof probability theory, but instead use heuristics which sometimes yield\nreasonable judgments and sometimes systematic biases. This view has had a major\nimpact in economics, law, medicine, and other fields; indeed, the idea that\npeople cannot reason with probabilities has become a widespread truism. We\npresent a simple alternative to this view, where people reason about\nprobability according to probability theory but are subject to random variation\nor noise in the reasoning process. In this account the effect of noise is\ncancelled for some probabilistic expressions: analysing data from two\nexperiments we find that, for these expressions, people's probability judgments\nare strikingly close to those required by probability theory. For other\nexpressions this account produces systematic deviations in probability\nestimates. These deviations explain four reliable biases in human probabilistic\nreasoning (conservatism, subadditivity, conjunction and disjunction fallacies).\nThese results suggest that people's probability judgments embody the rules of\nprobability theory, and that biases in those judgments are due to the effects\nof random noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.0501v3"
    },
    {
        "title": "The relation between frequentist confidence intervals and Bayesian\n  credible intervals",
        "authors": [
            "S. I. Bitioukov",
            "N. V. Krasnikov"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We investigate the relation between frequentist and Bayesian approaches.\nNamely, we find the \"frequentist\" Bayes prior \\pi_{f}(\\lambda,x_{obs}) =\n-\\frac{\\int_{-\\infty}^{x_{obs}}\\frac{\\partial f(x,\\lambda)}{\\partial\n\\lambda}dx}{f(x_{obs},\\lambda)} (here f(x,\\lambda) is the probability density)\nfor which the results of frequentist and Bayes approaches to the determination\nof confidence intervals coincide. In many cases (but not always) the\n\"frequentist\" prior which reproduces frequentist results coincides with the\nJeffreys prior.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.3343v1"
    },
    {
        "title": "Signal recognition and adapted filtering by non-commutative tomography",
        "authors": [
            "Carlos Aguirre",
            "R. Vilela Mendes"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Tomograms, a generalization of the Radon transform to arbitrary pairs of\nnon-commuting operators, are positive bilinear transforms with a rigorous\nprobabilistic interpretation which provide a full characterization of the\nsignal and are robust in the presence of noise. Tomograms based on the\ntime-frequency operator pair, were used in the past for component separation\nand denoising. Here we show how, by the construction of an operator pair\nadapted to the signal, meaningful information with good time resolution is\nextracted even in very noisy situations.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.5986v1"
    },
    {
        "title": "Persistence and periodicity in a dynamic proximity network",
        "authors": [
            "Aaron Clauset",
            "Nathan Eagle"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The topology of social networks can be understood as being inherently\ndynamic, with edges having a distinct position in time. Most characterizations\nof dynamic networks discretize time by converting temporal information into a\nsequence of network \"snapshots\" for further analysis. Here we study a highly\nresolved data set of a dynamic proximity network of 66 individuals. We show\nthat the topology of this network evolves over a very broad distribution of\ntime scales, that its behavior is characterized by strong periodicities driven\nby external calendar cycles, and that the conversion of inherently\ncontinuous-time data into a sequence of snapshots can produce highly biased\nestimates of network structure. We suggest that dynamic social networks exhibit\na natural time scale \\Delta_{nat}, and that the best conversion of such dynamic\ndata to a discrete sequence of networks is done at this natural rate.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.7343v1"
    },
    {
        "title": "Towards the full information chain theory: question difficulty",
        "authors": [
            "Eugene Perevalov",
            "David Grace"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  A general problem of optimal information acquisition for its use in decision\nmaking problems is considered. This motivates the need for developing\nquantitative measures of information sources' capabilities for supplying\naccurate information depending on the particular content of the latter. In this\narticle, the notion of a real valued difficulty functional for questions\nidentified with partitions of problem parameter space is introduced and the\noverall form of this functional is derived that satisfies a particular system\nof reasonable postulates. It is found that, in an isotropic case, the resulting\ndifficulty functional depends on a single scalar function on the parameter\nspace that can be interpreted -- using parallels with classical thermodynamics\n-- as a temperature-like quantity, with the question difficulty itself playing\nthe role of thermal energy. Quantitative relationships between difficulty\nfunctionals of different questions are also explored.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.2693v2"
    },
    {
        "title": "Towards the full information chain theory: answer depth and source\n  models",
        "authors": [
            "Eugene Perevalov",
            "David Grace"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  A problem of optimal information acquisition for its use in general decision\nmaking problems is considered. This motivates the need for developing\nquantitative measures of information sources' capabilities for supplying\naccurate information depending on the particular content of the latter. A\ncompanion article developed the notion of a question difficulty functional for\nquestions concerning input data for a decision making problem. Here, answers\nwhich an information source may provide in response to such questions are\nconsidered. In particular, a real valued answer depth functional measuring the\ndegree of accuracy of such answers is introduced and its overall form is\nderived under the assumption of isotropic knowledge structure of the\ninformation source. Additionally, information source models that relate answer\ndepth to question difficulty are discussed. It turns out to be possible to\nintroduce a notion of an information source capacity as the highest value of\nthe answer depth the source is capable of providing.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.2696v2"
    },
    {
        "title": "Multifractal analysis of sentence lengths in English literary texts",
        "authors": [
            "Iwona Grabska-Gradzińska",
            "Andrzej Kulig",
            "Jarosław Kwapień",
            "Paweł Oświęcimka",
            "Stanisław Drożdż"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  This paper presents analysis of 30 literary texts written in English by\ndifferent authors. For each text, there were created time series representing\nlength of sentences in words and analyzed its fractal properties using two\nmethods of multifractal analysis: MFDFA and WTMM. Both methods showed that\nthere are texts which can be considered multifractal in this representation but\na majority of texts are not multifractal or even not fractal at all. Out of 30\nbooks, only a few have so-correlated lengths of consecutive sentences that the\nanalyzed signals can be interpreted as real multifractals. An interesting\ndirection for future investigations would be identifying what are the specific\nfeatures which cause certain texts to be multifractal and other to be\nmonofractal or even not fractal at all.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.3171v1"
    },
    {
        "title": "Parsimonious module inference in large networks",
        "authors": [
            "Tiago P. Peixoto"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  We investigate the detectability of modules in large networks when the number\nof modules is not known in advance. We employ the minimum description length\n(MDL) principle which seeks to minimize the total amount of information\nrequired to describe the network, and avoid overfitting. According to this\ncriterion, we obtain general bounds on the detectability of any prescribed\nblock structure, given the number of nodes and edges in the sampled network. We\nalso obtain that the maximum number of detectable blocks scales as $\\sqrt{N}$,\nwhere $N$ is the number of nodes in the network, for a fixed average degree\n$<k>$. We also show that the simplicity of the MDL approach yields an efficient\nmultilevel Monte Carlo inference algorithm with a complexity of $O(\\tau N\\log\nN)$, if the number of blocks is unknown, and $O(\\tau N)$ if it is known, where\n$\\tau$ is the mixing time of the Markov chain. We illustrate the application of\nthe method on a large network of actors and films with over $10^6$ edges, and a\ndissortative, bipartite block structure.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.4794v4"
    },
    {
        "title": "Entropic Inference: some pitfalls and paradoxes we can avoid",
        "authors": [
            "Ariel Caticha"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  The method of maximum entropy has been very successful but there are cases\nwhere it has either failed or led to paradoxes that have cast doubt on its\ngeneral legitimacy. My more optimistic assessment is that such failures and\nparadoxes provide us with valuable learning opportunities to sharpen our skills\nin the proper way to deploy entropic methods. The central theme of this paper\nrevolves around the different ways in which constraints are used to capture the\ninformation that is relevant to a problem. This leads us to focus on four\nepistemically different types of constraints. I propose that the failure to\nrecognize the distinctions between them is a prime source of errors. I\nexplicitly discuss two examples. One concerns the dangers involved in replacing\nexpected values with sample averages. The other revolves around\nmisunderstanding ignorance. I discuss the Friedman-Shimony paradox as it is\nmanifested in the three-sided die problem and also in its original\nthermodynamic formulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.6967v1"
    },
    {
        "title": "Markov Chain Order estimation with Conditional Mutual Information",
        "authors": [
            "Maria Papapetrou",
            "Dimitris Kugiumtzis"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We introduce the Conditional Mutual Information (CMI) for the estimation of\nthe Markov chain order. For a Markov chain of $K$ symbols, we define CMI of\norder $m$, $I_c(m)$, as the mutual information of two variables in the chain\nbeing $m$ time steps apart, conditioning on the intermediate variables of the\nchain. We find approximate analytic significance limits based on the estimation\nbias of CMI and develop a randomization significance test of $I_c(m)$, where\nthe randomized symbol sequences are formed by random permutation of the\ncomponents of the original symbol sequence. The significance test is applied\nfor increasing $m$ and the Markov chain order is estimated by the last order\nfor which the null hypothesis is rejected. We present the appropriateness of\nCMI-testing on Monte Carlo simulations and compare it to the Akaike and\nBayesian information criteria, the maximal fluctuation method (Peres-Shields\nestimator) and a likelihood ratio test for increasing orders using\n$\\phi$-divergence. The order criterion of CMI-testing turns out to be superior\nfor orders larger than one, but its effectiveness for large orders depends on\ndata availability. In view of the results from the simulations, we interpret\nthe estimated orders by the CMI-testing and the other criteria on genes and\nintergenic regions of DNA chains.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.0148v1"
    },
    {
        "title": "Deux approches differentes dans l'evaluation analytique du potentiel\n  endommageant des tremblements de terre: application et validation pour les\n  forts seismes de Vrancea, Roumanie",
        "authors": [
            "Iolanda-Gabriela Craifaleanu",
            "Ioan Sorin Borcia"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The paper presents a comparison between the results of the analytical\nassessments of the damage potential of seismic ground motions, based on two\ndifferent parameters: the Park-Ang damage index and the Sandi instrumental\nseismic intensity. The Park Ang damage index is largely used for structural\ndamage assessment due to its simplicity and its calibration against\nexperimental data. The instrumental intensity has the advantage of being\ncalibrated with respect to the EMS 98 intensity scale. Based on the processing\nof ground motions recorded during the strong earthquakes generated by the\nVrancea seismic source in 1977 (moment magnitude Mw=7,4) and 1986 (Mw=7,1), the\nvalues of the considered parameters are determined and maps of their spatial\ndistribution are generated. The results of the two approaches are compared to\none another, as well as with the information provided by the reports concerning\nthe effective damage observed in buildings due to the considered earthquakes.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.0897v1"
    },
    {
        "title": "Coupling between time series: a network view",
        "authors": [
            "Saeed Mehraban",
            "Amirhossein Shirazi",
            "Maryam Zamani",
            "Gholamreza Jafari"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Recently, the visibility graph has been introduced as a novel view for\nanalyzing time series, which maps it to a complex network. In this paper, we\nintroduce new algorithm of visibility, \"cross-visibility\", which reveals the\nconjugation of two coupled time series. The correspondence between the two time\nseries is mapped to a network, \"the cross-visibility graph\", to demonstrate the\ncorrelation between them. We applied the algorithm to several correlated and\nuncorrelated time series, generated by the linear stationary ARFIMA process.\nThe results demonstrate that the cross-visibility graph associated with\ncorrelated time series with power-law auto-correlation is scale-free. If the\ntime series are uncorrelated, the degree distribution of their cross-visibility\nnetwork deviates from power-law. For more clarifying the process, we applied\nthe algorithm to real-world data from the financial trades of two companies,\nand observed significant small-scale coupling in their dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.1010v1"
    },
    {
        "title": "Effective number of samples and pseudo-random nonlinear distortions in\n  digital OFDM coded signal",
        "authors": [
            "Adam Rudziński"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  This paper concerns theoretical modeling of degradation of signal with OFDM\ncoding caused by pseudo-random nonlinear distortions introduced by an\nanalog-to-digital or digital-to-analog converter. A new quantity, effective\nnumber of samples, is defined and used for derivation of accurate expressions\nfor autocorrelation function and the total power of the distortions. The\nderivation is based on probabilistic model of the signal and its transition\nprobability. It is shown, that for digital (discrete and quantized) signals the\neffective number of samples replaces the total number of samples and is the\nproper quantity defining their properties.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.1065v2"
    },
    {
        "title": "Bayes and Frequentism: a Particle Physicist's perspective",
        "authors": [
            "Louis Lyons"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  In almost every scientific field, an experiment involves collecting data and\nthen analysing it. The analysis stage will often consist in trying to extract\nsome physical parameter and estimating its uncertainty; this is known as\nParameter Determination. An example would be the determination of the mass of\nthe top quark, from data collected from high energy proton-proton collisions. A\ndifferent aim is to choose between two possible hypotheses. For example, are\ndata on the recession speed s of distant galaxies proportional to their\ndistance d, or do they fit better to a model where the expansion of the\nUniverse is accelerating?\n  There are two fundamental approaches to such statistical analyses - Bayesian\nand Frequentist. This article discusses the way they differ in their approach\nto probability, and then goes on to consider how this affects the way they deal\nwith Parameter Determination and Hypothesis Testing. The examples are taken\nfrom every-day life and from Particle Physics.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.1273v1"
    },
    {
        "title": "Towards the full information chain theory: expected loss and information\n  relevance",
        "authors": [
            "Eugene Perevalov",
            "David Grace"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  When additional information sources are available, an important question for\nan agent solving a certain problem is how to optimally use the information the\nsources are capable of providing. A framework that relates information accuracy\non the source side to information relevance on the problem side is proposed. An\noptimal information acquisition problem is formulated as that of question\nselection to maximize the loss reduction for the problem solved by the agent. A\nduality relationship between pseudoenergy (accuracy related) quantities on the\nsource side and loss (relevance related) quantities on the problem side is\nobserved.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2020v1"
    },
    {
        "title": "An Evidential Interpretation of the 1st and 2nd Laws of Thermodynamics",
        "authors": [
            "V. J. Vieland"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  I argue here that both the first and second laws of thermodynamics, generally\nunderstood to be quintessentially physical in nature, can be equally well\ndescribed as being about certain types of information without the need to\ninvoke physical manifestations for information. In particular, I show that the\nstatistician's familiar likelihood principle is a general conservation\nprinciple on a par with the first law, and that likelihood itself involves a\nform of irrecoverable information loss that can be expressed in the form of\n(one version of) the second law. Each of these principles involves a particular\ntype of information, and requires its own form of bookkeeping to properly\naccount for information accumulation. I illustrate both sets of books with a\nsimple coin-tossing (binomial) experiment. In thermodynamics, absolute\ntemperature T is the link that relates energy-based and entropy-based\nbookkeeping systems. I consider the information-based analogue of this link,\ndenoted here as E, and show that E has a meaningful interpretation in its own\nright in connection with statistical inference. These results contribute to a\ngrowing body of theory at the intersection of thermodynamics, information\ntheory and statistical inference, and suggest a novel framework in which E\nitself for the first time plays a starring role.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2150v2"
    },
    {
        "title": "Escape Time Characterization of Pendular Fabry-Perot",
        "authors": [
            "P. Addesso",
            "V. Pierro",
            "G. Filatrella"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We show that an escape from the potential minimum of Fabry-Perot\ninterferometers can be detected measuring the associated sudden change of\nreflectivity. We demonstrate that the loss of information that occurs retaining\nonly the sequence of escapes, rather than the full trajectory, can be very mild\nand can lead to an effective method to reveal the noise intensity or the\npresence of a coherent signal.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2653v1"
    },
    {
        "title": "Information-related complexity: a problem-oriented approach",
        "authors": [
            "Eugene Perevalov",
            "David Grace"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  A general notion of information-related complexity applicable to both natural\nand man-made systems is proposed. The overall approach is to explicitly\nconsider a rational agent performing a certain task with a quantifiable degree\nof success. The complexity is defined as the minimum (quasi-)quantity of\ninformation that's necessary to complete the task to the given extent --\nmeasured by the corresponding loss. The complexity so defined is shown to\ngeneralize the existing notion of statistical complexity when the system in\nquestion can be described by a discrete-time stochastic process. The proposed\ndefinition also applies, in particular, to optimization and decision making\nproblems under uncertainty in which case it gives the agent a useful measure of\nthe problem's \"susceptibility\" to additional information and allows for an\nestimation of the potential value of the latter.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.4211v1"
    },
    {
        "title": "Towards the full information chain theory: solution methods for optimal\n  information acquisition problem",
        "authors": [
            "Eugene Perevalov",
            "David Grace"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  When additional information sources are available in decision making problems\nthat allow stochastic optimization formulations, an important question is how\nto optimally use the information the sources are capable of providing. A\nframework that relates information accuracy determined by the source's\nknowledge structure to its relevance determined by the problem being solved was\nproposed in a companion paper. There, the problem of optimal information\nacquisition was formulated as that of minimization of the expected loss of the\nsolution subject to constraints dictated by the information source knowledge\nstructure and depth. Approximate solution methods for this problem are\ndeveloped making use of probability metrics method and its application for\nscenario reduction in stochastic optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.0070v1"
    },
    {
        "title": "A method for statistical comparison of histograms",
        "authors": [
            "Sergey Bityukov",
            "Nikolai Krasnikov",
            "Alexander Nikitenko",
            "Vera Smirnova"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We propose an approach for testing the hypothesis that two realizations of\nthe random variables in the form of histograms are taken from the same\nstatistical population (i.e. that two histograms are drawn from the same\ndistribution). The approach is based on the notion \"significance of deviation\".\nOur approach allows also to estimate the statistical difference between two\nhistograms.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.2651v4"
    },
    {
        "title": "Explaining Zipf's Law via Mental Lexicon",
        "authors": [
            "Armen E. Allahverdyan",
            "Weibing Deng",
            "Q. A. Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The Zipf's law is the major regularity of statistical linguistics that served\nas a prototype for rank-frequency relations and scaling laws in natural\nsciences. Here we show that the Zipf's law -- together with its applicability\nfor a single text and its generalizations to high and low frequencies including\nhapax legomena -- can be derived from assuming that the words are drawn into\nthe text with random probabilities. Their apriori density relates, via the\nBayesian statistics, to general features of the mental lexicon of the author\nwho produced the text.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.4383v1"
    },
    {
        "title": "xrayutilities: A versatile tool for reciprocal space conversion of\n  scattering data recorded with linear and area detectors",
        "authors": [
            "Dominik Kriegner",
            "Eugen Wintersberger",
            "Julian Stangl"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We present general algorithms to convert scattering data of linear and area\ndetectors recorded in various scattering geometries to reciprocal space\ncoordinates. The presented algorithms work for any goniometer configuration\nincluding popular four-circle, six-circle and kappa goniometers. We avoid the\nuse of commonly employed approximations and therefore provide algorithms which\nwork also for large detectors at small sample detector distances. A recipe for\ndetermining the necessary detector parameters including mostly ignored\nmisalignments is given. The algorithms are implemented in a freely available\nopen-source package.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.1732v1"
    },
    {
        "title": "Nonlinear unmixing of hyperspectral images: models and algorithms",
        "authors": [
            "Nicolas Dobigeon",
            "Jean-Yves Tourneret",
            "Cédric Richard",
            "José C. M. Bermudez",
            "Stephen McLaughlin",
            "Alfred O. Hero"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  When considering the problem of unmixing hyperspectral images, most of the\nliterature in the geoscience and image processing areas relies on the widely\nused linear mixing model (LMM). However, the LMM may be not valid and other\nnonlinear models need to be considered, for instance, when there are\nmulti-scattering effects or intimate interactions. Consequently, over the last\nfew years, several significant contributions have been proposed to overcome the\nlimitations inherent in the LMM. In this paper, we present an overview of\nrecent advances in nonlinear unmixing modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.1875v2"
    },
    {
        "title": "Stochastic Recovery Of Sparse Signals From Random Measurements",
        "authors": [
            "M. Andrecut"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Sparse signal recovery from a small number of random measurements is a well\nknown NP-hard to solve combinatorial optimization problem, with important\napplications in signal and image processing. The standard approach to the\nsparse signal recovery problem is based on the basis pursuit method. This\napproach requires the solution of a large convex optimization problem, and\ntherefore suffers from high computational complexity. Here, we discuss a\nstochastic optimization method, as a low-complexity alternative to the basis\npursuit approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.2058v1"
    },
    {
        "title": "Fractional Gaussian noise criterion for correlations characterization: a\n  random-matrix-theory inspired perspective",
        "authors": [
            "Tayeb Jamali",
            "Hamed Saberi",
            "G. R. Jafari"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We introduce a particular construction of an autocorrelation matrix of a time\nseries and its analysis based on the random-matrix theory ideas that is capable\nof unveiling the type of correlations information which is inaccessible to the\nstraight analysis of the autocorrelation function. Exploiting the well-studied\nhierarchy of the fractional Gaussian noise (fGn), an \\emph{in situ} criterion\nfor the sake of a quantitative comparison with the autocorrelation data is\noffered. We illustrate the applicability of our method by two paradigmatic\nexamples from the orthodox context of the stock markets and the turbulence.\nQuite strikingly, a remarkable agreement with the fGn is achieved\nnotwithstanding the non-Gaussianity in returns of the stock market. In the\nlatter context, on the contrary, a significant deviation from an fGn is\nobserved despite a Gaussian distribution of the velocity profile of the\nturbulence.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.4895v2"
    },
    {
        "title": "Continuous-Time Random Walk with multi-step memory: An application to\n  market dynamics",
        "authors": [
            "Tomasz Gubiec",
            "Ryszard Kutner"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  A novel version of the Continuous-Time Random Walk (CTRW) model with memory\nis developed. This memory means the dependence between arbitrary number of\nsuccessive jumps of the process, while waiting times between jumps are\nconsidered as i.i.d. random variables. The dependence was found by analysis of\nempirical histograms for the stochastic process of a single share price on a\nmarket within the high frequency time scale, and justified theoretically by\nconsidering bid-ask bounce mechanism containing some delay characteristic for\nany double-auction market. Our model turns out to be exactly analytically\nsolvable, which enables a direct comparison of its predictions with their\nempirical counterparts, for instance, with empirical velocity autocorrelation\nfunction. Thus this paper significantly extends the capabilities of the CTRW\nformalism.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.6797v3"
    },
    {
        "title": "Eigenvalue Spectra of Modular Networks",
        "authors": [
            "Tiago P. Peixoto"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  A large variety of dynamical processes that take place on networks can be\nexpressed in terms of the spectral properties of some linear operator which\nreflects how the dynamical rules depend on the network topology. Often such\nspectral features are theoretically obtained by considering only local node\nproperties, such as degree distributions. Many networks, however, possess\nlarge-scale modular structures that can drastically influence their spectral\ncharacteristics, and which are neglected in such simplified descriptions. Here\nwe obtain in a unified fashion the spectrum of a large family of operators,\nincluding the adjacency, Laplacian and normalized Laplacian matrices, for\nnetworks with generic modular structure, in the limit of large degrees. We\nfocus on the conditions necessary for the merging of the isolated eigenvalues\nwith the continuous band of the spectrum, after which the planted modular\nstructure can no longer be easily detected by spectral methods. This is a\ncrucial transition point which determines when a modular structure is strong\nenough to affect a given dynamical process. We show that this transition\nhappens in general at different points for the different matrices, and hence\nthe detectability threshold can vary significantly depending on the operator\nchosen. Equivalently, the sensitivity to the modular structure of the different\ndynamical processes associated with each matrix will be different, given the\nsame large-scale structure present in the network. Furthermore, we show that,\nwith the exception of the Laplacian matrix, the different transitions coalesce\ninto the same point for the special case where the modules are homogeneous, but\nseparate otherwise.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.2507v2"
    },
    {
        "title": "Analysis of data in the form of graphs",
        "authors": [
            "Karthikeyan Rajendran",
            "Ioannis G. Kevrekidis"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We discuss the problem of extending data mining approaches to cases in which\ndata points arise in the form of individual graphs. Being able to find the\nintrinsic low-dimensionality in ensembles of graphs can be useful in a variety\nof modeling contexts, especially when coarse-graining the detailed graph\ninformation is of interest. One of the main challenges in mining graph data is\nthe definition of a suitable pairwise similarity metric in the space of graphs.\nWe explore two practical solutions to solving this problem: one based on\nfinding subgraph densities, and one using spectral information. The approach is\nillustrated on three test data sets (ensembles of graphs); two of these are\nobtained from standard graph generating algorithms, while the graphs in the\nthird example are sampled as dynamic snapshots from an evolving network\nsimulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3524v1"
    },
    {
        "title": "Low-Rank Separated Representation Surrogates of High-Dimensional\n  Stochastic Functions: Application in Bayesian Inference",
        "authors": [
            "AbdoulAhad Validi"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  This study introduces a non-intrusive approach in the context of low-rank\nseparated representation to construct a surrogate of high-dimensional\nstochastic functions, e.g., PDEs/ODEs, in order to decrease the computational\ncost of Markov Chain Monte Carlo simulations in Bayesian inference. The\nsurrogate model is constructed via a regularized alternative least-square\nregression with Tikhonov regularization using a roughening matrix computing the\ngradient of the solution, in conjunction with a perturbation-based error\nindicator to detect optimal model complexities. The model approximates a vector\nof a continuous solution at discrete values of a physical variable. The\nrequired number of random realizations to achieve a successful approximation\nlinearly depends on the function dimensionality. The computational cost of the\nmodel construction is quadratic in the number of random inputs, which\npotentially tackles the curse of dimensionality in high-dimensional stochastic\nfunctions. Furthermore, this vector valued separated representation-based\nmodel, in comparison to the available scalar-valued case, leads to a\nsignificant reduction in the cost of approximation by an order of magnitude\nequal to the vector size. The performance of the method is studied through its\napplication to three numerical examples including a 41-dimensional elliptic PDE\nand a 21-dimensional cavity flow.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.5374v1"
    },
    {
        "title": "Enhanced reconstruction of weighted networks from strengths and degrees",
        "authors": [
            "Rossana Mastrandrea",
            "Tiziano Squartini",
            "Giorgio Fagiolo",
            "Diego Garlaschelli"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Network topology plays a key role in many phenomena, from the spreading of\ndiseases to that of financial crises. Whenever the whole structure of a network\nis unknown, one must resort to reconstruction methods that identify the least\nbiased ensemble of networks consistent with the partial information available.\nA challenging case, frequently encountered due to privacy issues in the\nanalysis of interbank flows and Big Data, is when there is only local\n(node-specific) aggregate information available. For binary networks, the\nrelevant ensemble is one where the degree (number of links) of each node is\nconstrained to its observed value. However, for weighted networks the problem\nis much more complicated. While the naive approach prescribes to constrain the\nstrengths (total link weights) of all nodes, recent counter-intuitive results\nsuggest that in weighted networks the degrees are often more informative than\nthe strengths. This implies that the reconstruction of weighted networks would\nbe significantly enhanced by the specification of both strengths and degrees, a\ncomputationally hard and bias-prone procedure. Here we solve this problem by\nintroducing an analytical and unbiased maximum-entropy method that works in the\nshortest possible time and does not require the explicit generation of\nreconstructed samples. We consider several real-world examples and show that,\nwhile the strengths alone give poor results, the additional knowledge of the\ndegrees yields accurately reconstructed networks. Information-theoretic\ncriteria rigorously confirm that the degree sequence, as soon as it is\nnon-trivial, is irreducible to the strength sequence. Our results have strong\nimplications for the analysis of motifs and communities and whenever the\nreconstructed ensemble is required as a null model to detect higher-order\npatterns.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.2104v2"
    },
    {
        "title": "Detecting spatial homogeneity in the world trade web with Detrended\n  Fluctuation Analysis",
        "authors": [
            "Riccardo Chiarucci",
            "Franco Ruzzenenti",
            "Maria I. Loffredo"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  In a spatially embedded network, that is a network where nodes can be\nuniquely determined in a system of coordinates, links' weights might be\naffected by metric distances coupling every pair of nodes (dyads). In order to\nassess to what extent metric distances affect relationships (link's weights) in\na spatially embedded network, we propose a methodology based on DFA (Detrended\nFluctuation Analysis). DFA is a well developed methodology to evaluate\nautocorrelations and estimate long-range behaviour in time series. We argue it\ncan be further extended to spatially ordered series in order to assess\nautocorrelations in values. A scaling exponent of 0.5 (uncorrelated data) would\nthereby signal a perfect homogeneous space embedding the network. We apply the\nproposed methodology to the World Trade Web (WTW) during the years 1949-2000\nand we find, in some contrast with predictions of gravity models, a declining\ninfluence of distances on trading relationships.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.0526v3"
    },
    {
        "title": "Structural Changes in Data Communication in Wireless Sensor Networks",
        "authors": [
            "Raquel S. Cabral",
            "Andre L. L. Aquino",
            "Alejandro C. Frery",
            "Osvaldo A. Rosso",
            "Jaime A. Ramírez"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Wireless sensor networks are an important technology for making distributed\nautonomous measures in hostile or inaccessible environments. Among the\nchallenges they pose, the way data travel among them is a relevant issue since\ntheir structure is quite dynamic. The operational topology of such devices can\noften be described by complex networks. In this work, we assess the variation\nof measures commonly employed in the complex networks literature applied to\nwireless sensor networks. Four data communication strategies were considered:\ngeometric, random, small-world, and scale-free models, along with the shortest\npath length measure. The sensitivity of this measure was analyzed with respect\nto the following perturbations: insertion and removal of nodes in the geometric\nstrategy; and insertion, removal and rewiring of links in the other models. The\nassessment was performed using the normalized Kullback-Leibler divergence and\nHellinger distance quantifiers, both deriving from the Information Theory\nframework. The results reveal that the shortest path length is sensitive to\nperturbations.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.3009v1"
    },
    {
        "title": "Optimal scales in weighted networks",
        "authors": [
            "Diego Garlaschelli",
            "Sebastian E. Ahnert",
            "Thomas M. A. Fink",
            "Guido Caldarelli"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The analysis of networks characterized by links with heterogeneous intensity\nor weight suffers from two long-standing problems of arbitrariness. On one\nhand, the definitions of topological properties introduced for binary graphs\ncan be generalized in non-unique ways to weighted networks. On the other hand,\neven when a definition is given, there is no natural choice of the (optimal)\nscale of link intensities (e.g. the money unit in economic networks). Here we\nshow that these two seemingly independent problems can be regarded as\nintimately related, and propose a common solution to both. Using a formalism\nthat we recently proposed in order to map a weighted network to an ensemble of\nbinary graphs, we introduce an information-theoretic approach leading to the\nleast biased generalization of binary properties to weighted networks, and at\nthe same time fixing the optimal scale of link intensities. We illustrate our\nmethod on various social and economic networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.4259v1"
    },
    {
        "title": "Discovering the Significance of 5 sigma",
        "authors": [
            "Louis Lyons"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We discuss the traditional criterion for discovery in Particle Physics of\nrequiring a significance corresponding to at least 5 sigma; and whether a more\nnuanced approach might be better.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.1284v1"
    },
    {
        "title": "An extension to GUM methodology: degrees-of-freedom calculations for\n  correlated multidimensional estimates",
        "authors": [
            "R. Willink",
            "B. D. Hall"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The Guide to the Expression of Uncertainty in Measurement advocates the use\nof an 'effective number of degrees of freedom' for the calculation of an\ninterval of measurement uncertainty. However, it does not describe how this\nnumber is to be calculated when (i) the measurand is a vector quantity or (ii)\nwhen the errors in the estimates of the quantities defining the measurand (the\n'input quantities') are not incurred independently. An appropriate analysis for\na vector-valued measurand has been described (Metrologia 39 (2002) 361-9), and\na method for a one-dimensional measurand with dependent errors has also been\ngiven (Metrologia 44 (2007) 340-9). This paper builds on those analyses to\npresent a method for the situation where the problem is multidimensional and\ninvolves correlated errors. The result is an explicit general procedure that\nreduces to simpler procedures where appropriate. The example studied is from\nthe field of radio-frequency metrology, where measured quantities are often\ncomplex-valued and can be regarded as vectors of two elements.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.0343v1"
    },
    {
        "title": "Benford's Law: Textbook Exercises and Multiple-choice Testbanks",
        "authors": [
            "Aaron D. Slepkov",
            "Kevin B. Ironside",
            "David DiBattista"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Benford's Law describes the finding that the distribution of leading (or\nleftmost) digits of innumerable datasets follows a well-defined logarithmic\ntrend, rather than an intuitive uniformity. In practice this means that the\nmost common leading digit is 1, with an expected frequency of 30.1%, and the\nleast common is 9, with an expected frequency of 4.6%. The history and\ndevelopment of Benford's Law is inexorably linked to physics, yet there has\nbeen a dearth of physics-related Benford datasets reported in the literature.\nCurrently, the most common application of Benford's Law is in detecting number\ninvention and tampering such as found in accounting-, tax-, and voter-fraud. We\ndemonstrate that answers to end-of-chapter exercises in physics and chemistry\ntextbooks conform to Benford's Law. Subsequently, we investigate whether this\nfact can be used to gain advantage over random guessing in multiple-choice\ntests, and find that while testbank answers in introductory physics closely\nconform to Benford's Law, the testbank is nonetheless secure against such a\nBenford's attack for banal reasons.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.4787v1"
    },
    {
        "title": "Benford's Law and Distractors in Multiple Choice Exams",
        "authors": [
            "Fred M. Hoppe"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Suppose that in a multiple choice examination the leading digit of the\ncorrect options follows Benford's Law, while the the leading digit of the\ndistractors are uniform. Consider a strategy for guessing at answers that\nselects the option with the lowest leading digit with ties broken at random. We\nprovide an expression for the probability that this strategy selects the\ncorrect option.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.7606v2"
    },
    {
        "title": "Global inequality in energy consumption from 1980 to 2010",
        "authors": [
            "Scott Lawrence",
            "Qin Liu",
            "Victor M. Yakovenko"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We study the global probability distribution of energy consumption per capita\naround the world using data from the U.S. Energy Information Administration\n(EIA) for 1980-2010. We find that the Lorenz curves have moved up during this\ntime period, and the Gini coefficient G has decreased from 0.66 in 1980 to 0.55\nin 2010, indicating a decrease in inequality. The global probability\ndistribution of energy consumption per capita in 2010 is close to the\nexponential distribution with G=0.5. We attribute this result to the\nglobalization of the world economy, which mixes the world and brings it closer\nto the state of maximal entropy. We argue that global energy production is a\nlimited resource that is partitioned among the world population. The most\nprobable partition is the one that maximizes entropy, thus resulting in the\nexponential distribution function. A consequence of the latter is the law of\n1/3: the top 1/3 of the world population consumes 2/3 of produced energy. We\nalso find similar results for the global probability distribution of CO2\nemissions per capita.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.6443v2"
    },
    {
        "title": "Reconstructing the intermittent dynamics of the torque in wind turbines",
        "authors": [
            "Pedro G. Lind",
            "Matthias Wächter",
            "Joachim Peinke"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We apply a framework introduced in the late nineties to analyze load\nmeasurements in off-shore wind energy converters (WEC). The framework is\nborrowed from statistical physics and properly adapted to the analysis of\nmultivariate data comprising wind velocity, power production and torque\nmeasurements, taken at one single WEC. In particular, we assume that wind\nstatistics drives the fluctuations of the torque produced in the wind turbine\nand show how to extract an evolution equation of the Langevin type for the\ntorque driven by the wind velocity. It is known that the intermittent nature of\nthe atmosphere, i.e. of the wind field, is transferred to the power production\nof a wind energy converter and consequently to the shaft torque. We show that\nthe derived stochastic differential equation quantifies the dynamical coupling\nof the measured fluctuating properties as well as it reproduces the\nintermittency observed in the data. Finally, we discuss our approach in the\nlight of turbine monitoring, a particular important issue in off-shore wind\nfarms.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.2063v1"
    },
    {
        "title": "Cluster analysis of weighted bipartite networks: a new copula-based\n  approach",
        "authors": [
            "Alessandro Chessa",
            "Irene Crimaldi",
            "Massimo Riccaboni",
            "Luca Trapin"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  In this work we are interested in identifying clusters of \"positional\nequivalent\" actors, i.e. actors who play a similar role in a system. In\nparticular, we analyze weighted bipartite networks that describes the\nrelationships between actors on one side and features or traits on the other,\ntogether with the intensity level to which actors show their features. The main\ncontribution of our work is twofold. First, we develop a methodological\napproach that takes into account the underlying multivariate dependence among\ngroups of actors. The idea is that positions in a network could be defined on\nthe basis of the similar intensity levels that the actors exhibit in expressing\nsome features, instead of just considering relationships that actors hold with\neach others. Second, we propose a new clustering procedure that exploits the\npotentiality of copula functions, a mathematical instrument for the\nmodelization of the stochastic dependence structure. Our clustering algorithm\ncan be applied both to binary and real-valued matrices. We validate it with\nsimulations and applications to real-world data.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.2590v2"
    },
    {
        "title": "Cycle flow based module detection in directed recurrence networks",
        "authors": [
            "Ralf Banisch",
            "Nataša Djurdjevac Conrad"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We present a new cycle flow based method for finding fuzzy partitions of\nweighted directed networks coming from time series data. We show that this\nmethod overcomes essential problems of most existing clustering approaches,\nwhich tend to ignore important directional information by considering only\none-step, one-directional node connections. Our method introduces a novel\nmeasure of communication between nodes using multi-step, bidirectional\ntransitions encoded by a cycle decomposition of the probability flow. Symmetric\nproperties of this measure enable us to construct an undirected graph that\ncaptures information flow of the original graph seen by the data and apply\nclustering methods designed for undirected graphs. Finally, we demonstrate our\nalgorithm by analyzing earthquake time series data, which naturally induce\n(time-)directed networks. This article has been published originally in EPL,\nDOI: 10.1209/0295-5075/108/68008. This version differs from the published\nversion by minor formatting details.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.2813v2"
    },
    {
        "title": "Defining a Trend for a Time Series Which Makes Use of the Intrinsic\n  Time-Scale Decomposition",
        "authors": [
            "Juan M. Restrepo",
            "Shankar C. Venkataramani",
            "Darin Comeau",
            "Hermann Flaschka"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We propose criteria that define a trend for time series with inherent\nmulti-scale features. We call this trend the {\\it tendency} of a time series.\nThe tendency is defined empirically by a set of criteria and captures the\nlarge-scale temporal variability of the original signal as well as the most\nfrequent events in its histogram. Among other properties, the tendency has a\nvariance no larger than that of the original signal; the histogram of the\ndifference between the original signal and the tendency is as symmetric as\npossible; and with reduced complexity, the tendency captures essential features\nof the signal.\n  To find the tendency we first use the Intrinsic Time-Scale Decomposition\n(ITD) of the signal, introduced in 2007 by Frei and Osorio, to produce a set of\ncandidate tendencies. We then apply the criteria to each of the candidates to\nsingle out the one that best agrees with them.\n  While the criteria for the tendency are independent of the signal\ndecomposition scheme, it is found that the ITD is a simple and stable\nmethodology, well suited for multi-scale signals. The ITD is a relatively new\ndecomposition and little is known about its outcomes. In this study we take the\nfirst steps towards a probabilistic model of the ITD analysis of random time\nseries. This analysis yields details concerning the universality and scaling\nproperties of the components of the decomposition.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.3827v1"
    },
    {
        "title": "Flood avalanches in a semiarid basin with a dense reservoir network",
        "authors": [
            "Samuel J. Peter",
            "J. C. de Araújo",
            "N. A. M. Araújo",
            "H. J. Herrmann"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  This study investigates flood avalanches in a dense reservoir network in the\nsemiarid north-eastern Brazil. The population living in this area strongly\ndepends on the availability of the water from this network. Water is stored\nduring intense wet-season rainfall events and evaporates from the reservoir\nsurface during the dry season. These seasonal changes are the driving forces\nbehind the water dynamics in the network. The reservoir network and its\nconnectivity properties during flood avalanches are investigated with a model\ncalled ResNetM, which simulates each reservoir explicitly. It runs on the basis\nof daily calculated water balances for each reservoir. A spilling reservoir\ncontributes with water to the reservoir downstream, which can trigger\navalanches affecting, in some cases, large fractions of the network. The main\nfocus is on the study of the relation between the total amount of water stored\nand the largest observable cluster of connected reservoirs that overspill in\nthe same day. It is shown that the thousands of small and middle-sized\nreservoirs are eminent for the retention of water upstream the large ones.\nTherefore, they prevent large clusters at a low level of water. Concerning\nconnectivity measures, the actual reservoir network, which evolved without an\nintegrated plan, performed better (i.e., generated smaller avalanches for\nsimilar amount of stored water) than numerous stochastically generated\nartificial reservoir networks on the same river network.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.4232v1"
    },
    {
        "title": "Resolving Histogram Binning Dilemmas with Binless and Binfull Algorithms",
        "authors": [
            "Abram Krislock",
            "Nathan Krislock"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The histogram is an analysis tool in widespread use within many sciences,\nwith high energy physics as a prime example. However, there exists an inherent\nbias in the choice of binning for the histogram, with different choices\npotentially leading to different interpretations. This paper aims to eliminate\nthis bias using two \"debinning\" algorithms. Both algorithms generate an\nobserved cumulative distribution function from the data, and use it to\nconstruct a representation of the underlying probability distribution function.\nThe strengths and weaknesses of these two algorithms are compared and\ncontrasted. The applicability and future prospects of these algorithms is also\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.4958v1"
    },
    {
        "title": "Another Look at Confidence Intervals: Proposal for a More Relevant and\n  Transparent Approach",
        "authors": [
            "Steven D. Biller",
            "Scott M. Oser"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The behaviors of various confidence/credible interval constructions are\nexplored, particularly in the region of low statistics where methods diverge\nmost. We highlight a number of challenges, such as the treatment of nuisance\nparameters, and common misconceptions associated with such constructions. An\ninformal survey of the literature suggests that confidence intervals are not\nalways defined in relevant ways and are too often misinterpreted and/or\nmisapplied. This can lead to seemingly paradoxical behaviours and flawed\ncomparisons regarding the relevance of experimental results. We therefore\nconclude that there is a need for a more pragmatic strategy which recognizes\nthat, while it is critical to objectively convey the information content of the\ndata, there is also a strong desire to derive bounds on models and a natural\ninstinct to interpret things this way. Accordingly, we attempt to put aside\nphilosophical biases in favor of a practical view to propose a more transparent\nand self-consistent approach that better addresses these issues.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.5010v2"
    },
    {
        "title": "Detection of Nuclear Sources in Search Applications using Dynamic\n  Quantum Clustering of Spectral Data",
        "authors": [
            "Marvin Weinstein",
            "Alexander Heifetz",
            "Raymond Klann"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  In a search scenario, nuclear background spectra are continuously measured in\nshort acquisition intervals with a mobile detector-spectrometer. Detecting\nsources from measured data is difficult because of low signal to noise ratio\n(S/N) of spectra, large and highly varying background due to naturally\noccurring radioactive material (NORM), and line broadening due to limited\nspectral resolution of nuclear detector. We have invented a method for\ndetection of sources using clustering of spectral data. Our method takes\nadvantage of the physical fact that a source not only produces counts in the\nregion of its spectral emission, but also has the effect on the entire detector\nspectrum via Compton continuum. This allows characterizing the low S/N spectrum\nwithout distinct isotopic lines using multiple data features. We have shown\nthat noisy spectra with low S/N can be grouped by overall spectral shape\nsimilarity using a data clustering technique called Dynamic Quantum Clustering\n(DQC). The spectra in the same cluster can then be averaged to enhance S/N of\nthe isotopic spectral line. This would allow for increased accuracy of isotopic\nidentification and lower false alarm rate. Our method was validated in a\nproof-of principle study using a data set of spectra measured in one-second\nintervals with Sodium Iodide detector. The data set consisted of over 7000\nspectra obtained in urban background measurements, and approximately 70\nmeasurements of Cs-137 and Co-60 sources. Using DQC analysis, we have observed\nthat all spectra containing Cs-137 and Co-60 signal cluster away from the\nbackground.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.0746v1"
    },
    {
        "title": "Iterative Method for Generating Correlated Binary Sequences",
        "authors": [
            "O. V. Usatenko",
            "S. S. Melnik",
            "S. S. Apostolov",
            "N. M. Makarov",
            "A. A. Krokhin"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We propose a new efficient iterative method for generating random correlated\nbinary sequences with prescribed correlation function. The method is based on\nconsecutive linear modulations of initially uncorrelated sequence into a\ncorrelated one. Each step of modulation increases the correlations until the\ndesired level has been reached. Robustness and efficiency for the proposed\nalgorithm are tested by generating sequences with inverse power-law\ncorrelations. The substantial increase in the strength of correlation in the\niterative method with respect to the single-step filtering generation is shown\nfor all studied correlation functions. Our results can be used for design of\ndisordered superlattices, waveguides, and surfaces with selective transport\nproperties.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.2656v1"
    },
    {
        "title": "Bayesian signal reconstruction for 1-bit compressed sensing",
        "authors": [
            "Yingying Xu",
            "Yoshiyuki Kabashima",
            "Lenka Zdeborova"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The 1-bit compressed sensing framework enables the recovery of a sparse\nvector x from the sign information of each entry of its linear transformation.\nDiscarding the amplitude information can significantly reduce the amount of\ndata, which is highly beneficial in practical applications. In this paper, we\npresent a Bayesian approach to signal reconstruction for 1-bit compressed\nsensing, and analyze its typical performance using statistical mechanics.\nUtilizing the replica method, we show that the Bayesian approach enables better\nreconstruction than the L1-norm minimization approach, asymptotically\nsaturating the performance obtained when the non-zero entries positions of the\nsignal are known. We also test a message passing algorithm for signal\nreconstruction on the basis of belief propagation. The results of numerical\nexperiments are consistent with those of the theoretical analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.3782v2"
    },
    {
        "title": "New likelihoods for shape analysis",
        "authors": [
            "Sylvain Fichet"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We introduce a new kind of likelihood function based on the sequence of\nmoments of the data distribution. Both binned and unbinned data samples are\ndiscussed, and the multivariate case is also derived. Building on this approach\nwe lay out the formalism of shape analysis for signal searches. In addition to\nmoment-based likelihoods, standard likelihoods and approximate statistical\ntests are provided. Enough material is included to make the paper\nself-contained from the perspective of shape analysis. We argue that the\nmoment-based likelihoods can advantageously replace unbinned standard\nlikelihoods for the search of non-local signals, by avoiding the step of\nfitting Monte-Carlo generated distributions. This benefit increases with the\nnumber of variables simultaneously analyzed. The moment-based signal search is\nexemplified and tested in various 1D toy models mimicking typical high-energy\nsignal--background configurations. Moment-based techniques should be\nparticularly appropriate for the searches for effective operators at the LHC.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.1865v2"
    },
    {
        "title": "Bayesian parameter estimation of core collapse supernovae using\n  gravitational wave simulations",
        "authors": [
            "Matthew C. Edwards",
            "Renate Meyer",
            "Nelson Christensen"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Using the latest numerical simulations of rotating stellar core collapse, we\npresent a Bayesian framework to extract the physical information encoded in\nnoisy gravitational wave signals. We fit Bayesian principal component\nregression models with known and unknown signal arrival times to reconstruct\ngravitational wave signals, and subsequently fit known astrophysical parameters\non the posterior means of the principal component coefficients using a linear\nmodel. We predict the ratio of rotational kinetic energy to gravitational\nenergy of the inner core at bounce by sampling from the posterior predictive\ndistribution, and find that these predictions are generally very close to the\ntrue parameter values, with $90\\%$ credible intervals $\\sim 0.04$ and $\\sim\n0.06$ wide for the known and unknown arrival time models respectively. Two\nsupervised machine learning methods are implemented to classify precollapse\ndifferential rotation, and we find that these methods discriminate rapidly\nrotating progenitors particularly well. We also introduce a constrained\noptimization approach to model selection to find an optimal number of principal\ncomponents in the signal reconstruction step. Using this approach, we select 14\nprincipal components as the most parsimonious model.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.7549v1"
    },
    {
        "title": "Characterization of graphs for protein structure modeling and\n  recognition of solubility",
        "authors": [
            "Lorenzo Livi",
            "Alessandro Giuliani",
            "Alireza Sadeghian"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  This paper deals with the relations among structural, topological, and\nchemical properties of the E.Coli proteome from the vantage point of the\nsolubility/aggregation propensity of proteins. Each E.Coli protein is initially\nrepresented according to its known folded 3D shape. This step consists in\nrepresenting the available E.Coli proteins in terms of graphs. We first analyze\nthose graphs by considering pure topological characterizations, i.e., by\nanalyzing the mass fractal dimension and the distribution underlying both\nshortest paths and vertex degrees. Results confirm the general architectural\nprinciples of proteins. Successively, we focus on the statistical properties of\na representation of such graphs in terms of vectors composed of several\nnumerical features, which we extracted from their structural representation. We\nfound that protein size is the main discriminator for the solubility, while\nhowever there are other factors that help explaining the solubility degree. We\nfinally analyze such data through a novel one-class classifier, with the aim of\ndiscriminating among very and poorly soluble proteins. Results are encouraging\nand consolidate the potential of pattern recognition techniques when employed\nto describe complex biological systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.8033v5"
    },
    {
        "title": "Information topology identifies emergent model classes",
        "authors": [
            "Mark K. Transtrum",
            "Gus Hart",
            "Peng Qiu"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  We develop a language for describing the relationship among observations,\nmathematical models, and the underlying principles from which they are derived.\nUsing Information Geometry, we consider geometric properties of statistical\nmodels for different observations. As observations are varied, the model\nmanifold may be stretched, compressed, or even collapsed. Observations that\npreserve the structural identifiability of the parameters also preserve certain\ntopological features (such as edges and corners) that characterize the model's\nunderlying physical principles. We introduce Information Topology in analogy\nwith information geometry as characterizing the \"abstract model\" of which\nstatistical models are realizations. Observations that change the topology,\ni.e., \"manifold collapse,\" require a modification of the abstract model in\norder to construct identifiable statistical models. Often, the essential\ntopological feature is a hierarchical structure of boundaries (faces, edges,\ncorners, etc.) which we represent as a hierarchical graph known as a Hasse\ndiagram. Low-dimensional elements of this diagram are simple models that\ndescribe the dominant behavioral modes, what we call emergent model classes.\nObservations that preserve the Hasse diagram are diffeomorphically related and\nform a group, the collection of which form a partially ordered set. All\npossible observations have a semi-group structure. For hierarchical models, we\nconsider how the topology of simple models is embedded in that of larger\nmodels. When emergent model classes are unstable to the introduction of new\nparameters, we classify the new parameters as relevant. Conversely, the\nemergent model classes are stable to the introduction of irrelevant parameters.\nIn this way, information topology provides a general language for exploring\nrepresentations of physical systems and their relationships to observations.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.6203v2"
    },
    {
        "title": "Bayesian estimation of the multifractality parameter for image texture\n  using a Whittle approximation",
        "authors": [
            "Sébastien Combrexelle",
            "Herwig Wendt",
            "Nicolas Dobigeon",
            "Jean-Yves Tourneret",
            "Steve McLaughlin",
            "Patrice Abry"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Texture characterization is a central element in many image processing\napplications. Multifractal analysis is a useful signal and image processing\ntool, yet, the accurate estimation of multifractal parameters for image texture\nremains a challenge. This is due in the main to the fact that current\nestimation procedures consist of performing linear regressions across frequency\nscales of the two-dimensional (2D) dyadic wavelet transform, for which only a\nfew such scales are computable for images. The strongly non-Gaussian nature of\nmultifractal processes, combined with their complicated dependence structure,\nmakes it difficult to develop suitable models for parameter estimation. Here,\nwe propose a Bayesian procedure that addresses the difficulties in the\nestimation of the multifractality parameter. The originality of the procedure\nis threefold: The construction of a generic semi-parametric statistical model\nfor the logarithm of wavelet leaders; the formulation of Bayesian estimators\nthat are associated with this model and the set of parameter values admitted by\nmultifractal theory; the exploitation of a suitable Whittle approximation\nwithin the Bayesian model which enables the otherwise infeasible evaluation of\nthe posterior distribution associated with the model. Performance is assessed\nnumerically for several 2D multifractal processes, for several image sizes and\na large range of process parameters. The procedure yields significant benefits\nover current benchmark estimators in terms of estimation performance and\nability to discriminate between the two most commonly used classes of\nmultifractal process models. The gains in performance are particularly\npronounced for small image sizes, notably enabling for the first time the\nanalysis of image patches as small as 64x64 pixels.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.4871v2"
    },
    {
        "title": "Automatic fault detection on BIPV systems without solar irradiation data",
        "authors": [
            "Jonathan Leloux",
            "Luis Narvarte",
            "Alberto Luna",
            "Adrien Desportes"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  BIPV systems are small PV generation units spread out over the territory, and\nwhose characteristics are very diverse. This makes difficult a cost-effective\nprocedure for monitoring, fault detection, performance analyses, operation and\nmaintenance. As a result, many problems affecting BIPV systems go undetected.\nIn order to carry out effective automatic fault detection procedures, we need a\nperformance indicator that is reliable and that can be applied on many PV\nsystems at a very low cost. The existing approaches for analyzing the\nperformance of PV systems are often based on the Performance Ratio (PR), whose\naccuracy depends on good solar irradiation data, which in turn can be very\ndifficult to obtain or cost-prohibitive for the BIPV owner. We present an\nalternative fault detection procedure based on a performance indicator that can\nbe constructed on the sole basis of the energy production data measured at the\nBIPV systems. This procedure does not require the input of operating conditions\ndata, such as solar irradiation, air temperature, or wind speed. The\nperformance indicator, called Performance to Peers (P2P), is constructed from\nspatial and temporal correlations between the energy output of neighboring and\nsimilar PV systems. This method was developed from the analysis of the energy\nproduction data of approximately 10,000 BIPV systems located in Europe. The\nresults of our procedure are illustrated on the hourly, daily and monthly data\nmonitored during one year at one BIPV system located in the South of Belgium.\nOur results confirm that it is possible to carry out automatic fault detection\nprocedures without solar irradiation data. P2P proves to be more stable than PR\nmost of the time, and thus constitutes a more reliable performance indicator\nfor fault detection procedures.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.6946v1"
    },
    {
        "title": "Target Density Normalization for Markov Chain Monte Carlo Algorithms",
        "authors": [
            "Allen Caldwell",
            "Chang Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Techniques for evaluating the normalization integral of the target density\nfor Markov Chain Monte Carlo algorithms are described and tested numerically.\nIt is assumed that the Markov Chain algorithm has converged to the target\ndistribution and produced a set of samples from the density. These are used to\nevaluate sample mean, harmonic mean and Laplace algorithms for the calculation\nof the integral of the target density. A clear preference for the sample mean\nalgorithm applied to a reduced support region is found, and guidelines are\ngiven for implementation.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.7149v2"
    },
    {
        "title": "Breakdown of statistical inference from some random experiments",
        "authors": [
            "Marian Kupczynski",
            "Hans De Raedt"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Many experiments can be interpreted in terms of random processes operating\naccording to some internal protocols. When experiments are costly or cannot be\nrepeated only one or a few finite samples are available. In this paper we study\ndata generated by pseudo-random computer experiments operating according to\nparticular internal protocols. We show that the standard statistical analysis\nperformed on a sample, containing 100000 data points or more, may sometimes be\nhighly misleading and statistical errors largely underestimated. Our results\nconfirm in a dramatic way the dangers of standard asymptotic statistical\ninference if a sample is not homogenous. We demonstrate that analyzing various\nsubdivisions of samples by multiple chi-square tests and chi-square frequency\ngraphs is very effective in detecting sample inhomogeneity. Therefore to assure\ncorrectness of the statistical inference the above mentioned chi-square tests\nand other non-parametric sample homogeneity tests should be incorporated in any\nstatistical analysis of experimental data. If such tests are not performed the\nreported conclusions and estimates of the errors cannot be trusted.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.7424v4"
    },
    {
        "title": "Identifying Coupling Structure in Complex Systems through the Optimal\n  Causation Entropy Principle",
        "authors": [
            "Jie Sun",
            "Carlo Cafaro",
            "Erik M. Bollt"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Inferring the coupling structure of complex systems from time series data in\ngeneral by means of statistical and information-theoretic techniques is a\nchallenging problem in applied science. The reliability of statistical\ninferences requires the construction of suitable information-theoretic measures\nthat take into account both direct and indirect influences, manifest in the\nform of information flows, between the components within the system. In this\nwork, we present an application of the optimal causation entropy (oCSE)\nprinciple to identify the coupling structure of a synthetic biological system,\nthe repressilator. Specifically, when the system reaches an equilibrium state,\nwe use a stochastic perturbation approach to extract time series data that\napproximate a linear stochastic process. Then, we present and jointly apply the\naggregative discovery and progressive removal algorithms based on the oCSE\nprinciple to infer the coupling structure of the system from the measured data.\nFinally, we show that the success rate of our coupling inferences not only\nimproves with the amount of available data, but it also increases with a higher\nfrequency of sampling and is especially immune to false positives.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.5350v1"
    },
    {
        "title": "Unification of field theory and maximum entropy methods for learning\n  probability densities",
        "authors": [
            "Justin B. Kinney"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The need to estimate smooth probability distributions (a.k.a. probability\ndensities) from finite sampled data is ubiquitous in science. Many approaches\nto this problem have been described, but none is yet regarded as providing a\ndefinitive solution. Maximum entropy estimation and Bayesian field theory are\ntwo such approaches. Both have origins in statistical physics, but the\nrelationship between them has remained unclear. Here I unify these two methods\nby showing that every maximum entropy density estimate can be recovered in the\ninfinite smoothness limit of an appropriate Bayesian field theory. I also show\nthat Bayesian field theory estimation can be performed without imposing any\nboundary conditions on candidate densities, and that the infinite smoothness\nlimit of these theories recovers the most common types of maximum entropy\nestimates. Bayesian field theory is thus seen to provide a natural test of the\nvalidity of the maximum entropy null hypothesis. Bayesian field theory also\nreturns a lower entropy density estimate when the maximum entropy hypothesis is\nfalsified. The computations necessary for this approach can be performed\nrapidly for one-dimensional data, and software for doing this is provided.\nBased on these results, I argue that Bayesian field theory is poised to provide\na definitive solution to the density estimation problem in one dimension.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.5371v5"
    },
    {
        "title": "Blurring Out Cosmic Puzzles",
        "authors": [
            "Yann Benétreau-Dupin"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The Doomsday argument and anthropic reasoning are two puzzling examples of\nprobabilistic confirmation. In both cases, a lack of knowledge apparently\nyields surprising conclusions. Since they are formulated within a Bayesian\nframework, they constitute a challenge to Bayesianism. Several attempts, some\nsuccessful, have been made to avoid these conclusions, but some versions of\nthese arguments cannot be dissolved within the framework of orthodox\nBayesianism. I show that adopting an imprecise framework of probabilistic\nreasoning allows for a more adequate representation of ignorance in Bayesian\nreasoning and explains away these puzzles.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.4382v1"
    },
    {
        "title": "Achievement of Preassigned Spectra in the Synthesis of Band-Pass\n  Constant-Envelope Signals by Rapidly Hopping through Discrete Frequencies",
        "authors": [
            "Sergio Callegari"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Spread-spectrum signals are increasingly adopted in fields including\ncommunications, testing of electronic systems, Electro-Magnetic Compatibility\n(EMC) enhancement, ultrasonic non-destructive testing. This paper considers the\nsynthesis of constant-envelope band-pass wave-forms with preassigned spectra\nvia an FM technique using only a limited number of frequencies. In particular,\nan optimization-based approach for the selection of appropriate modulation\nparameters and statistical features of the modulating waveform is proposed. By\nexample, it is shown that the design problem generally admits multiple local\noptima, but can still be managed with relative ease since the local optima can\ntypically be scanned by changing the initial setting of a single parameter.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.6357v1"
    },
    {
        "title": "Difficulties applying recent blind source separation techniques to EEG\n  and MEG",
        "authors": [
            "Kevin H. Knuth"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  High temporal resolution measurements of human brain activity can be\nperformed by recording the electric potentials on the scalp surface\n(electroencephalography, EEG), or by recording the magnetic fields near the\nsurface of the head (magnetoencephalography, MEG). The analysis of the data is\nproblematic due to the fact that multiple neural generators may be\nsimultaneously active and the potentials and magnetic fields from these sources\nare superimposed on the detectors. It is highly desirable to un-mix the data\ninto signals representing the behaviors of the original individual generators.\nThis general problem is called blind source separation and several recent\ntechniques utilizing maximum entropy, minimum mutual information, and maximum\nlikelihood estimation have been applied. These techniques have had much success\nin separating signals such as natural sounds or speech, but appear to be\nineffective when applied to EEG or MEG signals. Many of these techniques\nimplicitly assume that the source distributions have a large kurtosis, whereas\nan analysis of EEG/MEG signals reveals that the distributions are multimodal.\nThis suggests that more effective separation techniques could be designed for\nEEG and MEG signals.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.05068v1"
    },
    {
        "title": "Convergent Bayesian formulations of blind source separation and\n  electromagnetic source estimation",
        "authors": [
            "Kevin H. Knuth",
            "Herbert G. Vaughan Jr"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We consider two areas of research that have been developing in parallel over\nthe last decade: blind source separation (BSS) and electromagnetic source\nestimation (ESE). BSS deals with the recovery of source signals when only\nmixtures of signals can be obtained from an array of detectors and the only\nprior knowledge consists of some information about the nature of the source\nsignals. On the other hand, ESE utilizes knowledge of the electromagnetic\nforward problem to assign source signals to their respective generators, while\ninformation about the signals themselves is typically ignored. We demonstrate\nthat these two techniques can be derived from the same starting point using the\nBayesian formalism. This suggests a means by which new algorithms can be\ndeveloped that utilize as much relevant information as possible. We also\nbriefly mention some preliminary work that supports the value of integrating\ninformation used by these two techniques and review the kinds of information\nthat may be useful in addressing the ESE problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.05069v1"
    },
    {
        "title": "Fractal Dimension Computation From Equal Mass Partitions",
        "authors": [
            "Yui Shiozawa",
            "Bruce N. Miller",
            "Jean-Louis Rouet"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  While the numerical methods which utilizes partitions of equal-size,\nincluding the box-counting method, remain the most popular choice for computing\nthe generalized dimension of multifractal sets, two mass- oriented methods are\ninvestigated by applying them to the one-dimensional generalized Cantor set. We\nshow that both mass-oriented methods generate relatively good results for\ngeneralized dimensions for important cases where the box-counting method is\nknown to fail. Both the strengths and limitations of the methods are also\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.05273v1"
    },
    {
        "title": "From graphs to signals and back: Identification of network structures\n  using spectral analysis",
        "authors": [
            "Ronan Hamon",
            "Pierre Borgnat",
            "Patrick Flandrin",
            "Céline Robardet"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Many systems comprising entities in interactions can be represented as\ngraphs, whose structure gives significant insights about how these systems\nwork. Network theory has undergone further developments, in particular in\nrelation to detection of communities in graphs, to catch this structure.\nRecently, an approach has been proposed to transform a graph into a collection\nof signals: Using a multidimensional scaling technique on a distance matrix\nrepresenting relations between vertices of the graph, points in a Euclidean\nspace are obtained and interpreted as signals, indexed by the vertices. In this\narticle, we propose several extensions to this approach, developing a framework\nto study graph structures using signal processing tools. We first extend the\ncurrent methodology, enabling us to highlight connections between properties of\nsignals and graph structures, such as communities, regularity or randomness, as\nwell as combinations of those. A robust inverse transformation method is next\ndescribed, taking into account possible changes in the signals compared to\noriginal ones. This technique uses, in addition to the relationships between\nthe points in the Euclidean space, the energy of each signal, coding the\ndifferent scales of the graph structure. These contributions open up new\nperspectives in the study of graphs, by enabling processing of graphs through\nthe processing of the corresponding collection of signals, using reliable tools\nfrom signal processing. A technique of denoising of a graph by filtering of the\ncorresponding signals is then described, suggesting considerable potential of\nthe approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.04697v3"
    },
    {
        "title": "Bayesian field theoretic reconstruction of bond potential and bond\n  mobility in single molecule force spectroscopy",
        "authors": [
            "Joshua C. Chang",
            "Pak-Wing Fok",
            "Tom Chou"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Quantifying the forces between and within macromolecules is a necessary first\nstep in understanding the mechanics of molecular structure, protein folding,\nand enzyme function and performance. In such macromolecular settings, dynamic\nsingle-molecule force spectroscopy (DFS) has been used to distort bonds. The\nresulting responses, in the form of rupture forces, work applied, and\ntrajectories of displacements, have been used to reconstruct bond potentials.\nSuch approaches often rely on simple parameterizations of one-dimensional bond\npotentials, assumptions on equilibrium starting states, and/or large amounts of\ntrajectory data. Parametric approaches typically fail at inferring\ncomplex-shaped bond potentials with multiple minima, while piecewise estimation\nmay not guarantee smooth results with the appropriate behavior at large\ndistances. Existing techniques, particularly those based on work theorems, also\ndo not address spatial variations in the diffusivity that may arise from\nspatially inhomogeneous coupling to other degrees of freedom in the\nmacromolecule, thereby presenting an incomplete picture of the overall bond\ndynamics. To solve these challenges, we have developed a comprehensive\nempirical Bayesian approach that incorporates data and regularization terms\ndirectly into a path integral. All experiemental and statistical parameters in\nour method are estimated empirically directly from the data. Upon testing our\nmethod on simulated data, our regularized approach requires fewer data and\nallows simultaneous inference of both complex bond potentials and diffusivity\nprofiles.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.06415v1"
    },
    {
        "title": "Safe Leads and Lead Changes in Competitive Team Sports",
        "authors": [
            "A. Clauset",
            "M. Kogan",
            "S. Redner"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We investigate the time evolution of lead changes within individual games of\ncompetitive team sports. Exploiting ideas from the theory of random walks, the\nnumber of lead changes within a single game follows a Gaussian distribution. We\nshow that the probability that the last lead change and the time of the largest\nlead size are governed by the same arcsine law, a bimodal distribution that\ndiverges at the start and at the end of the game. We also determine the\nprobability that a given lead is \"safe\" as a function of its size $L$ and game\ntime $t$. Our predictions generally agree with comprehensive data on more than\n1.25 million scoring events in roughly 40,000 games across four professional or\nsemi-professional team sports, and are more accurate than popular heuristics\ncurrently used in sports analytics.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.03509v1"
    },
    {
        "title": "Bertrand `paradox' reloaded (with details on transformations of\n  variables, an introduction to Monte Carlo simulation and an inferential\n  variation of the problem)",
        "authors": [
            "Giulio D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  This note is mainly to point out, if needed, that uncertainty about models\nand their parameters has little to do with a `paradox'. The proposed `solution'\nis to formulate practical questions instead of seeking refuge into abstract\nprinciples. (And, in order to be concrete, some details on how to calculate the\nprobability density functions of the chord lengths are provided, together with\nsome comments on simulations and an appendix on the inferential aspects of the\nproblem.)\n",
        "pdf_link": "http://arxiv.org/pdf/1504.01361v1"
    },
    {
        "title": "Stochastic determination of matrix determinants",
        "authors": [
            "Sebastian Dorn",
            "Torsten A. Enßlin"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Matrix determinants play an important role in data analysis, in particular\nwhen Gaussian processes are involved. Due to currently exploding data volumes,\nlinear operations - matrices - acting on the data are often not accessible\ndirectly but are only represented indirectly in form of a computer routine.\nSuch a routine implements the transformation a data vector undergoes under\nmatrix multiplication. While efficient probing routines to estimate a matrix's\ndiagonal or trace, based solely on such computationally affordable\nmatrix-vector multiplications, are well known and frequently used in signal\ninference, there is no stochastic estimate for its determinant. We introduce a\nprobing method for the logarithm of a determinant of a linear operator. Our\nmethod rests upon a reformulation of the log-determinant by an integral\nrepresentation and the transformation of the involved terms into stochastic\nexpressions. This stochastic determinant determination enables large-size\napplications in Bayesian inference, in particular evidence calculations, model\ncomparison, and posterior determination.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.02661v2"
    },
    {
        "title": "Predicting sports scoring dynamics with restoration and anti-persistence",
        "authors": [
            "Leto Peel",
            "Aaron Clauset"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Professional team sports provide an excellent domain for studying the\ndynamics of social competitions. These games are constructed with simple,\nwell-defined rules and payoffs that admit a high-dimensional set of possible\nactions and nontrivial scoring dynamics. The resulting gameplay and efforts to\npredict its evolution are the object of great interest to both sports\nprofessionals and enthusiasts. In this paper, we consider two online prediction\nproblems for team sports:~given a partially observed game Who will score next?\nand ultimately Who will win? We present novel interpretable generative models\nof within-game scoring that allow for dependence on lead size (restoration) and\non the last team to score (anti-persistence). We then apply these models to\ncomprehensive within-game scoring data for four sports leagues over a ten year\nperiod. By assessing these models' relative goodness-of-fit we shed new light\non the underlying mechanisms driving the observed scoring dynamics of each\nsport. Furthermore, in both predictive tasks, the performance of our models\nconsistently outperforms baselines models, and our models make quantitative\nassessments of the latent team skill, over time.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.05872v1"
    },
    {
        "title": "Denoise in the pseudopolar grid Fourier space using exact inverse\n  pseudopolar Fourier transform",
        "authors": [
            "Jun Wei Fan"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  In this paper I show a matrix method to calculate the exact inverse\npseudopolar grid Fourier transform, and use this transform to do noise removals\nin the k space of pseudopolar grids. I apply the Gaussian filter to this\npseudopolar grid and find the advantages of the noise removals are very\nexcellent by using pseudopolar grid, and finally I show the Cartesian grid\ndenoise for comparisons. The results present the signal to noise ratio and the\nvariance are much better when doing noise removals in the pseudopolar grid than\nthe Cartesian grid. The noise removals of pseudopolar grid or Cartesian grid\nare both in the k space, and all these noises are added in the real space.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.07060v4"
    },
    {
        "title": "Intermediate-Level Crossings of a First-Passage Path",
        "authors": [
            "Uttam Bhat",
            "S. Redner"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We investigate some simple and surprising properties of a one-dimensional\nBrownian trajectory with diffusion coefficient $D$ that starts at the origin\nand reaches $X$ either: (i) at time $T$ or (ii) for the first time at time $T$.\nWe determine the most likely location of the first-passage trajectory from\n$(0,0)$ to $(X,T)$ and its distribution at any intermediate time $t<T$. A\nfirst-passage path typically starts out by being repelled from its final\nlocation when $X^2/DT\\ll 1$. We also determine the distribution of times when\nthe trajectory first crosses and last crosses an arbitrary intermediate\nposition $x<X$. The distribution of first-crossing times may be unimodal or\nbimodal, depending on whether $X^2/DT\\ll 1$ or $X^2/DT\\gg 1$. The form of the\nfirst-crossing probability in the bimodal regime is qualitatively similar to,\nbut more singular than, the well-known arcsine law.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.01184v2"
    },
    {
        "title": "Effects of polynomial trends on detrending moving average analysis",
        "authors": [
            "Ying-Hui Shao",
            "Gao-Feng Gu",
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The detrending moving average (DMA) algorithm is one of the best performing\nmethods to quantify the long-term correlations in nonstationary time series.\nMany long-term correlated time series in real systems contain various trends.\nWe investigate the effects of polynomial trends on the scaling behaviors and\nthe performances of three widely used DMA methods including backward algorithm\n(BDMA), centered algorithm (CDMA) and forward algorithm (FDMA). We derive a\ngeneral framework for polynomial trends and obtain analytical results for\nconstant shifts and linear trends. We find that the behavior of the CDMA method\nis not influenced by constant shifts. In contrast, linear trends cause a\ncrossover in the CDMA fluctuation functions. We also find that constant shifts\nand linear trends cause crossovers in the fluctuation functions obtained from\nthe BDMA and FDMA methods. When a crossover exists, the scaling behavior at\nsmall scales comes from the intrinsic time series while that at large scales is\ndominated by the constant shifts or linear trends. We also derive analytically\nthe expressions of crossover scales and show that the crossover scale depends\non the strength of the polynomial trend, the Hurst index, and in some cases\n(linear trends for BDMA and FDMA) the length of the time series. In all cases,\nthe BDMA and the FDMA behave almost the same under the influence of constant\nshifts or linear trends. Extensive numerical experiments confirm excellently\nthe analytical derivations. We conclude that the CDMA method outperforms the\nBDMA and FDMA methods in the presence of polynomial trends.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.02750v1"
    },
    {
        "title": "Estimation of the directional parameter of the offset exponential and\n  normal distributions in three-dimensional space using the sample mean",
        "authors": [
            "Yaroslav Nikitenko"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The directional precision of the sample mean estimator was calculated\nanalytically for the offset exponential and normal distributions in\nthree-dimensional space both for a finite sample and for limiting cases. It was\nshown that the spherical projection of the sample mean of the shifted\nexponential distribution has connections with modified Bessel functions and\nwith hypergeometric functions. It was shown explicitly how the distribution of\nthe sample mean of the exponential pdf converges near the mode to the normal\ndistribution. Approximation formulae for the distribution of the sample mean of\nthe shifted exponential distribution and for its directional precision and for\nthe precision of the estimation of the direction of shift of the normal\ndistribution were obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.04647v1"
    },
    {
        "title": "The development of an information criterion for Change-Point Analysis",
        "authors": [
            "Paul A. Wiggins",
            "Colin H. LaMont"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Change-point analysis is a flexible and computationally tractable tool for\nthe analysis of times series data from systems that transition between discrete\nstates and whose observables are corrupted by noise. The change-point algorithm\nis used to identify the time indices (change points) at which the system\ntransitions between these discrete states. We present a unified\ninformation-based approach to testing for the existence of change points. This\nnew approach reconciles two previously disparate approaches to Change-Point\nAnalysis (frequentist and information-based) for testing transitions between\nstates. The resulting method is statistically principled, parameter and prior\nfree and widely applicable to a wide range of change-point problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.05572v1"
    },
    {
        "title": "A Comparison of Limit Setting Methods for the On-Off Problem",
        "authors": [
            "Wolfgang A. Rolke"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We study the frequentist properties of confidence intervals for the On-Off\nproblem. The methods include all those in common use today. We derive explicit\nformulas for the limits and calculate the true coverage and the expected\nlengths of these methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.07027v3"
    },
    {
        "title": "Generalization of the Van Cittert--Zernike theorem: observers moving\n  with respect to sources",
        "authors": [
            "Daniel Braun",
            "Younes Monjid",
            "Bernard Rougé",
            "Yann Kerr"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The use of the Van Cittert--Zernike theorem for the formulation of the\nvisibility function in satellite-based Earth observation with passive\nradiometers does not take into account the relative motion of the observer (the\nsatellite antenna) with respect to sources of the electro-magnetic fields at\nthe surface of the Earth. The motion of the observer leads on the one hand to a\nmore complex signal due to a pixel-dependent Doppler shift that is neglected in\nthe standard derivation of the Van Cittert--Zernike theorem, but on the other\nhand one may hope that it could be employed for a temporal aperture synthesis,\nwhere virtual baselines are created through the motion of the satellite. Here,\nwe generalize the formulation of the aperture synthesis concept to the case of\nobservers moving with respect to the sources, and to the correlation of fields\nmeasured at times that differ by the travel time of the observer along a\nvirtual baseline. Our derivation is based on first principles, starting with\nthe wave propagation in the Earth reference frame of electro-magnetic fields\narising from incoherent current sources, and Lorentz transforming the fields\ninto the reference frame of the satellite. Our detailed study leads to the\nremarkable conclusion that the delay time due to observer motion cancels\nexactly the Doppler effect. This justifies the neglect of the Doppler effect in\nexisting imaging systems based on the standard Van Cittert--Zernike theorem.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.04764v1"
    },
    {
        "title": "The Pandora Software Development Kit for Pattern Recognition",
        "authors": [
            "J. S. Marshall",
            "M. A. Thomson"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The development of automated solutions to pattern recognition problems is\nimportant in many areas of scientific research and human endeavour. This paper\ndescribes the implementation of the Pandora Software Development Kit, which\naids the process of designing, implementing and running pattern recognition\nalgorithms. The Pandora Application Programming Interfaces ensure simple\nspecification of the building-blocks defining a pattern recognition problem.\nThe logic required to solve the problem is implemented in algorithms. The\nalgorithms request operations to create or modify data structures and the\noperations are performed by the Pandora framework. This design promotes an\napproach using many decoupled algorithms, each addressing specific topologies.\nDetails of algorithms addressing two pattern recognition problems in High\nEnergy Physics are presented: reconstruction of events at a high-energy e+e-\nlinear collider and reconstruction of cosmic ray or neutrino events in a liquid\nargon time projection chamber.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.05348v2"
    },
    {
        "title": "A simple application of FIC to model selection",
        "authors": [
            "Paul A. Wiggins"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We have recently proposed a new information-based approach to model\nselection, the Frequentist Information Criterion (FIC), that reconciles\ninformation-based and frequentist inference. The purpose of this current paper\nis to provide a simple example of the application of this criterion and a\ndemonstration of the natural emergence of model complexities with both AIC-like\n($N^0$) and BIC-like ($\\log N$) scaling with observation number $N$. The\napplication developed is deliberately simplified to make the analysis\nanalytically tractable.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.06129v1"
    },
    {
        "title": "Advanced statistical methods for eye movement analysis and modeling: a\n  gentle introduction",
        "authors": [
            "Giuseppe Boccignone"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  In this Chapter we show that by considering eye movements, and in particular,\nthe resulting sequence of gaze shifts, a stochastic process, a wide variety of\ntools become available for analyses and modelling beyond conventional\nstatistical methods. Such tools encompass random walk analyses and more complex\ntechniques borrowed from the pattern recognition and machine learning fields.\n  After a brief, though critical, probabilistic tour of current computational\nmodels of eye movements and visual attention, we lay down the basis for gaze\nshift pattern analysis. To this end, the concepts of Markov Processes, the\nWiener process and related random walks within the Gaussian framework of the\nCentral Limit Theorem will be introduced. Then, we will deliberately violate\nfundamental assumptions of the Central Limit Theorem to elicit a larger\nperspective, rooted in statistical physics, for analysing and modelling eye\nmovements in terms of anomalous, non-Gaussian, random walks and modern foraging\ntheory.\n  Eventually, by resorting to machine learning techniques, we discuss how the\nanalyses of movement patterns can develop into the inference of hidden patterns\nof the mind: inferring the observer's task, assessing cognitive impairments,\nclassifying expertise.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.07194v4"
    },
    {
        "title": "Detection of phase transition in generalized Pólya urn in information\n  cascade experiment",
        "authors": [
            "Masafumi Hino",
            "Yosuke Irie",
            "Masato Hisakado",
            "Taiki Takahashi",
            "Shintaro Mori"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We propose a method of detecting a phase transition in a generalized P\\'olya\nurn in an information cascade experiment. The method is based on the asymptotic\nbehavior of the correlation $C(t)$ between the first subject's choice and the\n$t+1$-th subject's choice, the limit value of which, $c\\equiv \\lim_{t\\to\n\\infty}C(t)$, is the order parameter of the phase transition. To verify the\nmethod, we perform a voting experiment using two-choice questions. An urn X is\nchosen at random from two urns A and B, which contain red and blue balls in\ndifferent configurations. Subjects sequentially guess whether X is A or B using\ninformation about the prior subjects' choices and the color of a ball randomly\ndrawn from X. The color tells the subject which is X with probability $q$. We\nset $q\\in \\{5/9,6/9,7/9,8/9\\}$ by controlling the configurations of red and\nblue balls in A and B. The (average) lengths of the sequence of the subjects\nare 63, 63, 54.0, and 60.5 for $q\\in \\{5/9,6/9,7/9,8/9\\}$, respectively. We\ndescribe the sequential voting process by a nonlinear P\\'olya urn model. The\nmodel suggests the possibility of a phase transition when $q$ changes. We show\nthat $c>0\\,\\,\\,(=0)$ for $q=5/9,6/9\\,\\,\\,(7/9,8/9 )$ and detect the phase\ntransition using the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.07269v2"
    },
    {
        "title": "Detector resolution correction for width of intermediate states in three\n  particle decays",
        "authors": [
            "Igor Denisenko",
            "Igor Boyko"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We propose a method that allows to take into account detector resolution in\nthe partial wave analysis event-by-event fit as a special case. Implementation\nof the method is discussed and the applicability of the method is studied for\nthe $J/\\psi \\to K^{*\\pm}K^{\\mp} \\to K^+K^-\\pi^0$ and $J/\\psi \\to K_2(1430)^\\pm\nK^{\\mp} \\to K^+K^-\\pi^0$ decays.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.01229v1"
    },
    {
        "title": "Exploring laser-driven quantum phenomena from a time-frequency analysis\n  perspective: A comprehensive study",
        "authors": [
            "Yae-lin Sheu",
            "Hau-tieng Wu",
            "Liang-Yan Hsu"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Time-frequency (TF) analysis is a powerful tool for exploring ultrafast\ndynamics in atoms and molecules. While some TF methods have demonstrated their\nusefulness and potential in several of quantum systems, a systematic comparison\namong these methods is still lacking. To this end, we compare a series of\nclassical and contemporary TF methods by taking hydrogen atom in a strong laser\nfield as a benchmark. In addition, several TF methods such as Cohen class\ndistribution other than the Wigner-Ville distribution, reassignment methods,\nand the empirical mode decomposition method are first introduced to exploration\nof ultrafast dynamics. Among these TF methods, the synchrosqueezing transform\nsuccessfully illustrates the physical mechanisms in the multiphoton ionization\nregime and in the tunneling ionization regime. Furthermore, an empirical\nprocedure to analyze an unknown complicated quantum system is provided,\nindicating the versatility of TF analysis as a new viable venue for exploring\nquantum dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.01976v1"
    },
    {
        "title": "Resolving Weak Sources within a Dense Array using a Network Approach",
        "authors": [
            "Nima Riahi",
            "Peter Gerstoft"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  A non-parametric technique to identify weak sources within dense sensor\narrays is developed using a network approach. No knowledge about the\npropagation medium is needed except that signal strengths decay to\ninsignificant levels within a scale that is shorter than the aperture. We then\nreinterpret the spatial covariance matrix of a wave field as a matrix whose\nsupport is a connectivity matrix of a network of vertices (sensors) connected\ninto communities. These communities correspond to sensor clusters associated\nwith individual sources. We estimate the support of the covariance matrix from\nlimited-time data using a robust hypothesis test combined with a physical\ndistance criterion. The latter ensures sufficient network sparsity to prevent\nvertex communities from forming by chance. We verify the approach on simulated\ndata and quantify its reliability. The method is then applied to data from a\ndense 5200 element geophone array that blanketed 7$\\times$10 km of the city of\nLong Beach (CA). The analysis exposes a helicopter traversing the array, oil\nproduction facilities, and reveals that low-frequency events tend to occur near\nroads.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05116v1"
    },
    {
        "title": "Frequentist limit setting in effective field theories",
        "authors": [
            "Kristian Damlund Gregersen",
            "Jørgen Beck Hansen"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The original frequentist approach for computing confidence intervals involves\nthe construction of the confidence belt which provides a mapping of the\nobservation in data into a subset of values for the parameter. There are\ndifferent prescriptions for constructing the confidence belt, here we use the\none provided by Feldman and Cousins. Alternative methods based on the\nfrequentist idea exist, including the delta likelihood method, the $CL_s$\nmethod and a method here referred to as the $p$-value method, which have all\nbeen commonly used in high energy experiments. The purpose of this article is\nto draw attention to a series of potential problems when applying these\nalternative methods to the important case where the predicted signal depends\nquadratically on the parameter of interest, a situation which is common in high\nenergy physics as it covers scenarios encountered in effective theories. These\ninclude anomalous Higgs couplings and anomalous trilinear and quartic gauge\ncouplings. It is found that the alternative methods, contrary to the original\nmethod using the confidence belt, encode the goodness-of-fit into the\nconfidence intervals and potentially over-constrain the parameter.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01808v3"
    },
    {
        "title": "Resolution enhancement by extrapolation of coherent diffraction images:\n  A quantitative study on the limits and a numerical study of non-binary and\n  phase objects",
        "authors": [
            "Tatiana Latychevskaia",
            "Yuriy Chushkin",
            "Hans-Werner Fink"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  In coherent diffractive imaging (CDI) the resolution of the reconstructed\nobject is limited by the numerical aperture of the experimental setup. We\npresent here a theoretical and numerical study for achieving super-resolution\nby post-extrapolation of coherent diffraction images, such as diffraction\npatterns or holograms. We demonstrate that a diffraction pattern can\nunambiguously be extrapolated from only a fraction of the entire pattern and\nthat the ratio of the extrapolated signal to the originally available signal is\nlinearly proportional to the oversampling ratio. While there could be in\nprinciple other methods to achieve extrapolation, we devote our discussion to\nemploying iterative phase retrieval methods and demonstrate their limits. We\npresent two numerical studies; namely the extrapolation of diffraction patterns\nof non-binary and that of phase objects together with a discussion of the\noptimal extrapolation procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.01654v3"
    },
    {
        "title": "Coupled uncertainty provided by a multifractal random walker",
        "authors": [
            "Z. Koohi Lai",
            "S. Vasheghani Farahani",
            "S. M. S. Movahed",
            "G. R. Jafari"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  The aim here is to study the concept of pairing multifractality between time\nseries possessing non-Gaussian distributions. The increasing number of rare\nevents creates \"criticality\". We show how the pairing between two series is\naffected by rare events, which we call \"coupled criticality\". A method is\nproposed for studying the coupled criticality born out of the interaction\nbetween two series, using the bivariate multifractal random walk (BiMRW). This\nmethod allows studying dependence of the coupled criticality on the criticality\nof each individual system. This approach is applied to data sets of gold and\noil markets, and inflation and unemployment.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.03040v1"
    },
    {
        "title": "Online Unmixing of Multitemporal Hyperspectral Images accounting for\n  Spectral Variability",
        "authors": [
            "Pierre-Antoine Thouvenin",
            "Nicolas Dobigeon",
            "Jean-Yves Tourneret"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Hyperspectral unmixing is aimed at identifying the reference spectral\nsignatures composing an hyperspectral image and their relative abundance\nfractions in each pixel. In practice, the identified signatures may vary\nspectrally from an image to another due to varying acquisition conditions, thus\ninducing possibly significant estimation errors. Against this background,\nhyperspectral unmixing of several images acquired over the same area is of\nconsiderable interest. Indeed, such an analysis enables the endmembers of the\nscene to be tracked and the corresponding endmember variability to be\ncharacterized. Sequential endmember estimation from a set of hyperspectral\nimages is expected to provide improved performance when compared to methods\nanalyzing the images independently. However, the significant size of\nhyperspectral data precludes the use of batch procedures to jointly estimate\nthe mixture parameters of a sequence of hyperspectral images. Provided that\neach elementary component is present in at least one image of the sequence, we\npropose to perform an online hyperspectral unmixing accounting for temporal\nendmember variability. The online hyperspectral unmixing is formulated as a\ntwo-stage stochastic program, which can be solved using a stochastic\napproximation. The performance of the proposed method is evaluated on synthetic\nand real data. A comparison with independent unmixing algorithms finally\nillustrates the interest of the proposed strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.05893v3"
    },
    {
        "title": "A note on the best invariant estimation of continuous probability\n  distributions under mean square loss",
        "authors": [
            "Thomas Schürmann"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We consider the nonparametric estimation problem of continuous probability\ndistribution functions. For the integrated mean square error we provide the\nstatistic corresponding to the best invariant estimator proposed by Aggarwal\n(1955) and Ferguson (1967). The table of critical values is computed and a\nnumerical power comparison of the statistic with the traditional Cram\\'{e}r-von\nMises statistic is done for several representative distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.06419v2"
    },
    {
        "title": "Data-driven detrending of nonstationary fractal time series with echo\n  state networks",
        "authors": [
            "Enrico Maiorino",
            "Filippo Maria Bianchi",
            "Lorenzo Livi",
            "Antonello Rizzi",
            "Alireza Sadeghian"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  In this paper, we propose a novel data-driven approach for removing trends\n(detrending) from nonstationary, fractal and multifractal time series. We\nconsider real-valued time series relative to measurements of an underlying\ndynamical system that evolves through time. We assume that such a dynamical\nprocess is predictable to a certain degree by means of a class of recurrent\nnetworks called Echo State Network (ESN), which are capable to model a generic\ndynamical process. In order to isolate the superimposed (multi)fractal\ncomponent of interest, we define a data-driven filter by leveraging on the ESN\nprediction capability to identify the trend component of a given input time\nseries. Specifically, the (estimated) trend is removed from the original time\nseries and the residual signal is analyzed with the multifractal detrended\nfluctuation analysis procedure to verify the correctness of the detrending\nprocedure. In order to demonstrate the effectiveness of the proposed technique,\nwe consider several synthetic time series consisting of different types of\ntrends and fractal noise components with known characteristics. We also process\na real-world dataset, the sunspot time series, which is well-known for its\nmultifractal features and has recently gained attention in the complex systems\nfield. Results demonstrate the validity and generality of the proposed\ndetrending method based on ESNs.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.07146v2"
    },
    {
        "title": "Phase Retrieval with One or Two Diffraction Patterns by Alternating\n  Projection with Null Initialization",
        "authors": [
            "Pengwen Chen",
            "Albert Fannjiang",
            "Gi-Ren Liu"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Alternating projection (AP) of various forms, including the Parallel AP\n(PAP), Real-constrained AP (RAP) and the Serial AP (SAP), are proposed to solve\nphase retrieval with at most two coded diffraction patterns. The proofs of\ngeometric convergence are given with sharp bounds on the rates of convergence\nin terms of a spectral gap condition.\n  To compensate for the local nature of convergence, the null initialization is\nproposed for initial guess and proved to produce asymptotically accurate\ninitialization for the case of Gaussian random measurement. Numerical\nexperiments show that the null initialization produces more accurate initial\nguess than the spectral initialization and that AP converges faster to the true\nobject than other iterative schemes for non-convex optimization such as the\nWirtinger Flow. In numerical experiments, AP with the null initialization\nconverges globally to the true object.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.07379v3"
    },
    {
        "title": "A Bayesian Consistent Dual Ensemble Kalman Filter for State-Parameter\n  Estimation in Subsurface Hydrology",
        "authors": [
            "Boujemaa Ait-El-Fquih",
            "Mohamad El Gharamti",
            "Ibrahim Hoteit"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Ensemble Kalman filtering (EnKF) is an efficient approach to addressing\nuncertainties in subsurface groundwater models. The EnKF sequentially\nintegrates field data into simulation models to obtain a better\ncharacterization of the model's state and parameters. These are generally\nestimated following joint and dual filtering strategies, in which, at each\nassimilation cycle, a forecast step by the model is followed by an update step\nwith incoming observations. The Joint-EnKF directly updates the augmented\nstate-parameter vector while the Dual-EnKF employs two separate filters, first\nestimating the parameters and then estimating the state based on the updated\nparameters. In this paper, we reverse the order of the forecast-update steps\nfollowing the one-step-ahead (OSA) smoothing formulation of the Bayesian\nfiltering problem, based on which we propose a new dual EnKF scheme, the\nDual-EnKF$_{\\rm OSA}$. Compared to the Dual-EnKF, this introduces a new update\nstep to the state in a fully consistent Bayesian framework, which is shown to\nenhance the performance of the dual filtering approach without any significant\nincrease in the computational cost. Numerical experiments are conducted with a\ntwo-dimensional synthetic groundwater aquifer model to assess the performance\nand robustness of the proposed Dual-EnKF$_{\\rm OSA}$, and to evaluate its\nresults against those of the Joint- and Dual-EnKFs. The proposed scheme is able\nto successfully recover both the hydraulic head and the aquifer conductivity,\nfurther providing reliable estimates of their uncertainties. Compared with the\nstandard Joint- and Dual-EnKFs, the proposed scheme is found more robust to\ndifferent assimilation settings, such as the spatial and temporal distribution\nof the observations, and the level of noise in the data. Based on our\nexperimental setups, it yields up to 25% more accurate state and parameters\nestimates.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.02178v1"
    },
    {
        "title": "Entropy-based Time-Varying Window Width Selection for Nonlinear type\n  Time-Frequency Analysis",
        "authors": [
            "Yae-lin Sheu",
            "Liang-Yan Hsu",
            "Pi-Tai Chou",
            "Hau-tieng Wu"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We propose a time-varying optimal window width (TVOWW) selection scheme to\noptimize the performance of several nonlinear-type time-frequency analyses,\nincluding the reassignment method, and the synchrosqueezing transform (SST) and\nits variations. A window rendering the most concentrated distribution in the\ntime-frequency representation (TFR) is regarded as the optimal window. The\nTVOWW selection scheme is particularly useful for signals that comprise\nfast-varying instantaneous frequencies and small spectral gaps. To demonstrate\nthe efficacy of the method, in addition to analyzing a synthetic signal, we\nstudy an atomic time-varying dipole moment driven by two-color mid-infrared\nlaser fields in attosecond physics.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.04811v3"
    },
    {
        "title": "Superplot: a graphical interface for plotting and analysing MultiNest\n  output",
        "authors": [
            "Andrew Fowlie",
            "Michael Hugh Bardsley"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We present an application, Superplot, for calculating and plotting\nstatistical quantities relevant to parameter inference from a \"chain\" of\nsamples drawn from a parameter space, produced by e.g. MultiNest. A simple\ngraphical interface allows one to browse a chain of many variables quickly, and\nmake publication quality plots of, inter alia, one- and two-dimensional profile\nlikelihood, posterior pdf (with kernel density estimation), confidence\nintervals and credible regions. In this short manual, we document installation\nand basic usage, and define all statistical quantities and conventions. The\ncode is fully compatible with Linux, Windows and Mac OSX. Furthermore, if\npreferred, all functionality is available through the command line rather than\na graphical interface.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.00555v4"
    },
    {
        "title": "Reversible Markov chain estimation using convex-concave programming",
        "authors": [
            "Benjamin Trendelkamp-Schroer",
            "Hao Wu",
            "Frank Noe"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We present a convex-concave reformulation of the reversible Markov chain\nestimation problem and outline an efficient numerical scheme for the solution\nof the resulting problem based on a primal-dual interior point method for\nmonotone variational inequalities. Extensions to situations in which\ninformation about the stationary vector is available can also be solved via the\nconvex- concave reformulation. The method can be generalized and applied to the\ndiscrete transition matrix reweighting analysis method to perform inference\nfrom independent chains with specified couplings between the stationary\nprobabilities. The proposed approach offers a significant speed-up compared to\na fixed-point iteration for a number of relevant applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01640v1"
    },
    {
        "title": "Determination of the edge of criticality in echo state networks through\n  Fisher information maximization",
        "authors": [
            "Lorenzo Livi",
            "Filippo Maria Bianchi",
            "Cesare Alippi"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  It is a widely accepted fact that the computational capability of recurrent\nneural networks is maximized on the so-called \"edge of criticality\". Once the\nnetwork operates in this configuration, it performs efficiently on a specific\napplication both in terms of (i) low prediction error and (ii) high short-term\nmemory capacity. Since the behavior of recurrent networks is strongly\ninfluenced by the particular input signal driving the dynamics, a universal,\napplication-independent method for determining the edge of criticality is still\nmissing. In this paper, we aim at addressing this issue by proposing a\ntheoretically motivated, unsupervised method based on Fisher information for\ndetermining the edge of criticality in recurrent neural networks. It is proven\nthat Fisher information is maximized for (finite-size) systems operating in\nsuch critical regions. However, Fisher information is notoriously difficult to\ncompute and either requires the probability density function or the conditional\ndependence of the system states with respect to the model parameters. The paper\ntakes advantage of a recently-developed non-parametric estimator of the Fisher\ninformation matrix and provides a method to determine the critical region of\necho state networks, a particular class of recurrent networks. The considered\ncontrol parameters, which indirectly affect the echo state network performance,\nare explored to identify those configurations lying on the edge of criticality\nand, as such, maximizing Fisher information and computational performance.\nExperimental results on benchmarks and real-world data demonstrate the\neffectiveness of the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.03685v2"
    },
    {
        "title": "Write error rate of spin-transfer-torque random access memory including\n  micromagnetic effects using rare event enhancement",
        "authors": [
            "Urmimala Roy",
            "Tanmoy Pramanik",
            "Leonard F. Register",
            "Sanjay K. Banerjee"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Spin-transfer-torque random access memory (STT-RAM) is a promising candidate\nfor the next-generation of random-access-memory due to improved scalability,\nread-write speeds and endurance. However, the write pulse duration must be long\nenough to ensure a low write error rate (WER), the probability that a bit will\nremain unswitched after the write pulse is turned off, in the presence of\nstochastic thermal effects. WERs on the scale of 10$^{-9}$ or lower are\ndesired. Within a macrospin approximation, WERs can be calculated analytically\nusing the Fokker-Planck method to this point and beyond. However, dynamic\nmicromagnetic effects within the bit can affect and lead to faster switching.\nSuch micromagnetic effects can be addressed via numerical solution of the\nstochastic Landau-Lifshitz-Gilbert-Slonczewski (LLGS) equation. However,\ndetermining WERs approaching 10$^{-9}$ would require well over 10$^{9}$ such\nindependent simulations, which is infeasible. In this work, we explore\ncalculation of WER using \"rare event enhancement\" (REE), an approach that has\nbeen used for Monte Carlo simulation of other systems where rare events\nnevertheless remain important. Using a prototype REE approach tailored to the\nSTT-RAM switching physics, we demonstrate reliable calculation of a WER to\n10$^{-9}$ with sets of only approximately 10$^{3}$ ongoing stochastic LLGS\nsimulations, and the apparent ability to go further.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.08512v2"
    },
    {
        "title": "A Noise-Robust Method with Smoothed \\ell_1/\\ell_2 Regularization for\n  Sparse Moving-Source Mapping",
        "authors": [
            "Mai Quyen Pham",
            "Benoit Oudompheng",
            "Jérôme I. Mars",
            "Barbara Nicolas"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The method described here performs blind deconvolution of the beamforming\noutput in the frequency domain. To provide accurate blind deconvolution,\nsparsity priors are introduced with a smooth \\ell_1/\\ell_2 regularization term.\nAs the mean of the noise in the power spectrum domain is dependent on its\nvariance in the time domain, the proposed method includes a variance estimation\nstep, which allows more robust blind deconvolution. Validation of the method on\nboth simulated and real data, and of its performance, are compared with two\nwell-known methods from the literature: the deconvolution approach for the\nmapping of acoustic sources, and sound density modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.03450v1"
    },
    {
        "title": "Unsupervised single-particle deep clustering via statistical manifold\n  learning",
        "authors": [
            "Jiayi Wu",
            "Yong-Bei Ma",
            "Charles Congdon",
            "Bevin Brett",
            "Shuobing Chen",
            "Qi Ouyang",
            "Youdong Mao"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Motivation: Structural heterogeneity in single-particle cryo-electron\nmicroscopy (cryo-EM) data represents a major challenge for high-resolution\nstructure determination. Unsupervised classification may serve as the first\nstep in the assessment of structural heterogeneity. Traditional algorithms for\nunsupervised classification, such as K-means clustering and maximum likelihood\noptimization, may classify images into wrong classes with decreasing\nsignal-to-noise-ratio (SNR) in the image data, yet demand increased cost in\ncomputation. Overcoming these limitations requires further development on\nclustering algorithms for high-performance cryo-EM data analysis. Results: Here\nwe introduce a statistical manifold learning algorithm for unsupervised\nsingle-particle deep clustering. We show that statistical manifold learning\nimproves classification accuracy by about 40% in the absence of input\nreferences for lower SNR data. Applications to several experimental datasets\nsuggest that our deep clustering approach can detect subtle structural\ndifference among classes. Through code optimization over the Intel\nhigh-performance computing (HPC) processors, our software implementation can\ngenerate thousands of reference-free class averages within several hours from\nhundreds of thousands of single-particle cryo-EM images, which allows\nsignificant improvement in ab initio 3D reconstruction resolution and quality.\nOur approach has been successfully applied in several structural determination\nprojects. We expect that it provides a powerful computational tool in analyzing\nhighly heterogeneous structural data and assisting in computational\npurification of single-particle datasets for high-resolution reconstruction.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.04539v2"
    },
    {
        "title": "Computationally Efficient Calculations of Target Performance of the\n  Normalized Matched Filter Detector for Hydrocoustic Signals",
        "authors": [
            "Roee Diamant"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Detection of hydroacoustic transmissions is a key enabling technology in\napplications such as depth measurements, detection of objects, and undersea\nmapping. To cope with the long channel delay spread and the low signal-to-noise\nratio, hydroacoustic signals are constructed with a large time-bandwidth\nproduct, $N$. A promising detector for hydroacoustic signals is the normalized\nmatched filter (NMF). For the NMF, the detection threshold depends only on $N$,\nthereby obviating the need to estimate the characteristics of the sea ambient\nnoise which are time-varying and hard to estimate. While previous works\nanalyzed the characteristics of the normalized matched filter (NMF), for\nhydroacoustic signals with large $N$ values the expressions available are\ncomputationally complicated to evaluate. Specifically for hydroacoustic signals\nof large $N$ values, this paper presents approximations for the probability\ndistribution of the NMF. These approximations are found extremely accurate in\nnumerical simulations. We also outline a computationally efficient method to\ncalculate the receiver operating characteristic (ROC) which is required to\ndetermine the detection threshold. Results from an experiment conducted in the\nMediterranean sea at depth of 900~m agree with the analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.06416v1"
    },
    {
        "title": "Wave-shape function analysis -- when cepstrum meets time-frequency\n  analysis",
        "authors": [
            "Chen-Yun Lin",
            "Li Su",
            "Hau-tieng Wu"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We propose to combine cepstrum and nonlinear time-frequency (TF) analysis to\nstudy mutiple component oscillatory signals with time-varying frequency and\namplitude and with time-varying non-sinusoidal oscillatory pattern. The concept\nof cepstrum is applied to eliminate the wave-shape function influence on the TF\nanalysis, and we propose a new algorithm, named de-shape synchrosqueezing\ntransform (de-shape SST). The mathematical model, adaptive non-harmonic model,\nis introduced and the de-shape SST algorithm is theoretically analyzed. In\naddition to simulated signals, several different physiological, musical and\nbiological signals are analyzed to illustrate the proposed algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.01805v2"
    },
    {
        "title": "Track reconstruction through the application of the Legendre Transform\n  on ellipses",
        "authors": [
            "T. Alexopoulos",
            "Y. Bristogiannis",
            "S. Leontsinis"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We propose a pattern recognition method that identifies the common tangent\nlines of a set of ellipses. The detection of the tangent lines is attained by\napplying the Legendre transform on a given set of ellipses. As context, we\nconsider a hypothetical detector made out of layers of chambers, each of which\nreturns an ellipse as an output signal. The common tangent of these ellipses\nrepresents the trajectory of a charged particle crossing the detector. The\nproposed method is evaluated using ellipses constructed from Monte Carlo\ngenerated tracks.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.04738v1"
    },
    {
        "title": "Classification of Big Data with Application to Imaging Genetics",
        "authors": [
            "Magnus O. Ulfarsson",
            "Frosti Palsson",
            "Jakob Sigurdsson",
            "Johannes R. Sveinsson"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Big data applications, such as medical imaging and genetics, typically\ngenerate datasets that consist of few observations n on many more variables p,\na scenario that we denote as p>>n. Traditional data processing methods are\noften insufficient for extracting information out of big data. This calls for\nthe development of new algorithms that can deal with the size, complexity, and\nthe special structure of such datasets. In this paper, we consider the problem\nof classifying p>>n data and propose a classification method based on linear\ndiscriminant analysis (LDA). Traditional LDA depends on the covariance estimate\nof the data, but when p>>n the sample covariance estimate is singular. The\nproposed method estimates the covariance by using a sparse version of noisy\nprincipal component analysis (nPCA). The use of sparsity in this setting aims\nat automatically selecting variables that are relevant for classification. In\nexperiments, the new method is compared to state-of-the art methods for big\ndata problems using both simulated datasets and imaging genetics datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.04932v1"
    },
    {
        "title": "Multifractal analysis of three-dimensional grayscale images:\n  Characterization of natural porous structures",
        "authors": [
            "Lorenzo Milazzo",
            "Radoslaw Pajor"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  A multifractal analysis (MFA) is performed on three-dimensional grayscale\nimages associated with natural porous structures (soil samples). First,\ncomputed tomography (CT) scans are carried out on the samples to generate 3D\ngrayscale images. Then, a preliminary analysis is conducted to evaluate key\nquantities associated with the porosity, such as void fraction, pore volume,\nconnectivity, and surface area. Finally, the samples are successfully\nidentified and separated into two different structure families by using the\nMFA. A new software (Munari) to carry out the MFA of 3D grayscale images is\nalso presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.06009v1"
    },
    {
        "title": "Simultaneous Estimation of Noise Variance and Number of Peaks in\n  Bayesian Spectral Deconvolution",
        "authors": [
            "Satoru Tokuda",
            "Kenji Nagata",
            "Masato Okada"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The heuristic identification of peaks from noisy complex spectra often leads\nto misunderstanding of the physical and chemical properties of matter. In this\npaper, we propose a framework based on Bayesian inference, which enables us to\nseparate multipeak spectra into single peaks statistically and consists of two\nsteps. The first step is estimating both the noise variance and the number of\npeaks as hyperparameters based on Bayes free energy, which generally is not\nanalytically tractable. The second step is fitting the parameters of each peak\nfunction to the given spectrum by calculating the posterior density, which has\na problem of local minima and saddles since multipeak models are nonlinear and\nhierarchical. Our framework enables the escape from local minima or saddles by\nusing the exchange Monte Carlo method and calculates Bayes free energy via the\nmultiple histogram method. We discuss a simulation demonstrating how efficient\nour framework is and show that estimating both the noise variance and the\nnumber of peaks prevents overfitting, overpenalizing, and misunderstanding the\nprecision of parameter estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.07590v2"
    },
    {
        "title": "Adaptive Signal Detection and Parameter Estimation in Unknown Colored\n  Gaussian Noise",
        "authors": [
            "Bo Tang",
            "Haibo He",
            "Steven Kay"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  This paper considers the general signal detection and parameter estimation\nproblem in the presence of colored Gaussian noise disturbance. By modeling the\ndisturbance with an autoregressive process, we present three signal detectors\nwith different unknown parameters under the general framework of binary\nhypothesis testing. The closed form of parameter estimates and the asymptotic\ndistributions of these three tests are also given. Given two examples of\nfrequency modulated signal detection problem and time series moving object\ndetection problem, the simulation results demonstrate the effectiveness of\nthree presented detectors.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.08259v1"
    },
    {
        "title": "Online Decorrelation of Humidity and Temperature in Chemical Sensors for\n  Continuous Monitoring",
        "authors": [
            "Ramon Huerta",
            "Thiago S. Mosqueiro",
            "Jordi Fonollosa",
            "Nikolai F Rulkov",
            "Irene Rodriguez-Lujan"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  A method for online decorrelation of chemical sensor signals from the effects\nof environmental humidity and temperature variations is proposed. The goal is\nto improve the accuracy of electronic nose measurements for continuous\nmonitoring by processing data from simultaneous readings of environmental\nhumidity and temperature. The electronic nose setup built for this study\nincluded eight metal-oxide sensors, temperature and humidity sensors with a\nwireless communication link to external computer. This wireless electronic nose\nwas used to monitor air for two years in the residence of one of the authors\nand it collected data continuously during 537 days with a sampling rate of 1\nsamples per second. To estimate the effects of variations in air humidity and\ntemperature on the chemical sensors signals, we used a standard energy band\nmodel for an n-type metal-oxide (MOX) gas sensor. The main assumption of the\nmodel is that variations in sensor conductivity can be expressed as a nonlinear\nfunction of changes in the semiconductor energy bands in the presence of\nexternal humidity and temperature variations. Fitting this model to the\ncollected data, we confirmed that the most statistically significant factors\nare humidity changes and correlated changes of temperature and humidity. This\nsimple model achieves excellent accuracy with a coefficient of determination\n$R^2$ close to 1. To show how the humidity-temperature correction model works\nfor gas discrimination, we constructed a model for online discrimination among\nbanana, wine and baseline response. This shows that pattern recognition\nalgorithms improve performance and reliability by including the filtered signal\nof the chemical sensors.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.01719v2"
    },
    {
        "title": "Improving randomness characterization through Bayesian model selection",
        "authors": [
            "Rafael Díaz Hernández Rojas",
            "Aldo Solís",
            "Alí M. Angulo Martínez",
            "Alfred B. U'Ren",
            "Jorge G. Hirsch",
            "Matteo Marsili",
            "Isaac Pérez Castillo"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Nowadays random number generation plays an essential role in technology with\nimportant applications in areas ranging from cryptography, which lies at the\ncore of current communication protocols, to Monte Carlo methods, and other\nprobabilistic algorithms. In this context, a crucial scientific endeavour is to\ndevelop effective methods that allow the characterization of random number\ngenerators. However, commonly employed methods either lack formality (e.g. the\nNIST test suite), or are inapplicable in principle (e.g. the characterization\nderived from the Algorithmic Theory of Information (ATI)). In this letter we\npresent a novel method based on Bayesian model selection, which is both\nrigorous and effective, for characterizing randomness in a bit sequence. We\nderive analytic expressions for a model's likelihood which is then used to\ncompute its posterior probability distribution. Our method proves to be more\nrigorous than NIST's suite and the Borel-Normality criterion and its\nimplementation is straightforward. We have applied our method to an\nexperimental device based on the process of spontaneous parametric\ndownconversion, implemented in our laboratory, to confirm that it behaves as a\ngenuine quantum random number generator (QRNG). As our approach relies on\nBayesian inference, which entails model generalizability, our scheme transcends\nindividual sequence analysis, leading to a characterization of the source of\nthe random sequences itself.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05119v3"
    },
    {
        "title": "Reweighting with Boosted Decision Trees",
        "authors": [
            "A. Rogozhnikov"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Machine learning tools are commonly used in modern high energy physics (HEP)\nexperiments. Different models, such as boosted decision trees (BDT) and\nartificial neural networks (ANN), are widely used in analyses and even in the\nsoftware triggers.\n  In most cases, these are classification models used to select the \"signal\"\nevents from data. Monte Carlo simulated events typically take part in training\nof these models. While the results of the simulation are expected to be close\nto real data, in practical cases there is notable disagreement between\nsimulated and observed data. In order to use available simulation in training,\ncorrections must be introduced to generated data. One common approach is\nreweighting - assigning weights to the simulated events. We present a novel\nmethod of event reweighting based on boosted decision trees. The problem of\nchecking the quality of reweighting step in analyses is also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05806v1"
    },
    {
        "title": "Discriminating image textures with the multiscale two-dimensional\n  complexity-entropy causality plane",
        "authors": [
            "Luciano Zunino",
            "Haroldo V. Ribeiro"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The aim of this paper is to further explore the usefulness of the\ntwo-dimensional complexity-entropy causality plane as a texture image\ndescriptor. A multiscale generalization is introduced in order to distinguish\nbetween different roughness features of images at small and large spatial\nscales. Numerically generated two-dimensional structures are initially\nconsidered for illustrating basic concepts in a controlled framework. Then,\nmore realistic situations are studied. Obtained results allow us to confirm\nthat intrinsic spatial correlations of images are successfully unveiled by\nimplementing this multiscale symbolic information-theory approach.\nConsequently, we conclude that the proposed representation space is a versatile\nand practical tool for identifying, characterizing and discriminating image\ntextures.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.01625v2"
    },
    {
        "title": "Mathematical Properties of Numerical Inversion for Jet Calibrations",
        "authors": [
            "Aviv Cukierman",
            "Benjamin Nachman"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Numerical inversion is a general detector calibration technique that is\nindependent of the underlying spectrum. This procedure is formalized and\nimportant statistical properties are presented, using high energy jets at the\nLarge Hadron Collider as an example setting. In particular, numerical inversion\nis inherently biased and common approximations to the calibrated jet energy\ntend to over-estimate the resolution. Analytic approximations to the closure\nand calibrated resolutions are demonstrated to effectively predict the full\nforms under realistic conditions. Finally, extensions of numerical inversion\nare presented which can reduce the inherent biases. These methods will be\nincreasingly important to consider with degraded resolution at low jet energies\ndue to a much higher instantaneous luminosity in the near future.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.05195v2"
    },
    {
        "title": "Determination of signal-to-noise ratio on the base of\n  information-entropic analysis",
        "authors": [
            "Z. Zh. Zhanabaev",
            "S. N. Akhtanov",
            "E. T. Kozhagulov",
            "B. A Karibayev"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In this paper we suggest a new algorithm for determination of signal-to-noise\nratio (SNR). SNR is a quantitative measure widely used in science and\nengineering. Generally, methods for determination of SNR are based on using of\nexperimentally defined power of noise level, or some conditional noise\ncriterion which can be specified for signal processing. In the present work we\ndescribe method for determination of SNR of chaotic and stochastic signals at\nunknown power levels of signal and noise. For this aim we use information as\ndifference between unconditional and conditional entropy. Our theoretical\nresults are confirmed by results of analysis of signals which can be described\nby nonlinear maps and presented as overlapping of harmonic and stochastic\nsignals.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.09212v1"
    },
    {
        "title": "A model independent safeguard for unbinned Likelihood",
        "authors": [
            "Nadav Priel",
            "Ludwig Rauch",
            "Hagar Landsman",
            "Alessandro Manfredini",
            "Ranny Budnik"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We present a universal method to include residual un-modeled background shape\nuncertainties in likelihood based statistical tests for high energy physics and\nastroparticle physics. This approach provides a simple and natural protection\nagainst mismodeling, thus lowering the chances of a false discovery or of an\nover constrained confidence interval, and allows a natural transition to\nunbinned space. Unbinned likelihood allows optimal usage of information for the\ndata and the models, and enhances the sensitivity.\n  We show that the asymptotic behavior of the test statistic can be regained in\ncases where the model fails to describe the true background behavior, and\npresent 1D and 2D case studies for model-driven and data-driven background\nmodels. The resulting penalty on sensitivities follows the actual discrepancy\nbetween the data and the models, and is asymptotically reduced to zero with\nincreasing knowledge.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.02643v2"
    },
    {
        "title": "Nonparametric Bayesian inference of the microcanonical stochastic block\n  model",
        "authors": [
            "Tiago P. Peixoto"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  A principled approach to characterize the hidden structure of networks is to\nformulate generative models, and then infer their parameters from data. When\nthe desired structure is composed of modules or \"communities\", a suitable\nchoice for this task is the stochastic block model (SBM), where nodes are\ndivided into groups, and the placement of edges is conditioned on the group\nmemberships. Here, we present a nonparametric Bayesian method to infer the\nmodular structure of empirical networks, including the number of modules and\ntheir hierarchical organization. We focus on a microcanonical variant of the\nSBM, where the structure is imposed via hard constraints, i.e. the generated\nnetworks are not allowed to violate the patterns imposed by the model. We show\nhow this simple model variation allows simultaneously for two important\nimprovements over more traditional inference approaches: 1. Deeper Bayesian\nhierarchies, with noninformative priors replaced by sequences of priors and\nhyperpriors, that not only remove limitations that seriously degrade the\ninference on large networks, but also reveal structures at multiple scales; 2.\nA very efficient inference algorithm that scales well not only for networks\nwith a large number of nodes and edges, but also with an unlimited number of\nmodules. We show also how this approach can be used to sample modular\nhierarchies from the posterior distribution, as well as to perform model\nselection. We discuss and analyze the differences between sampling from the\nposterior and simply finding the single parameter estimate that maximizes it.\nFurthermore, we expose a direct equivalence between our microcanonical approach\nand alternative derivations based on the canonical SBM.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.02703v4"
    },
    {
        "title": "Support Vector Machines and Generalisation in HEP",
        "authors": [
            "A. Bethani",
            "A. J. Bevan",
            "J. Hays",
            "T. J. Stevenson"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We review the concept of support vector machines (SVMs) and discuss examples\nof their use. One of the benefits of SVM algorithms, compared with neural\nnetworks and decision trees is that they can be less susceptible to over\nfitting than those other algorithms are to over training. This issue is related\nto the generalisation of a multivariate algorithm (MVA); a problem that has\noften been overlooked in particle physics. We discuss cross validation and how\nthis can be used to improve the generalisation of a MVA in the context of High\nEnergy Physics analyses. The examples presented use the Toolkit for\nMultivariate Analysis (TMVA) based on ROOT and describe our improvements to the\nSVM functionality and new tools introduced for cross validation within this\nframework.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.09932v1"
    },
    {
        "title": "Emergence of Compositional Representations in Restricted Boltzmann\n  Machines",
        "authors": [
            "Jérôme Tubiana",
            "Rémi Monasson"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Extracting automatically the complex set of features composing real\nhigh-dimensional data is crucial for achieving high performance in\nmachine--learning tasks. Restricted Boltzmann Machines (RBM) are empirically\nknown to be efficient for this purpose, and to be able to generate distributed\nand graded representations of the data. We characterize the structural\nconditions (sparsity of the weights, low effective temperature, nonlinearities\nin the activation functions of hidden units, and adaptation of fields\nmaintaining the activity in the visible layer) allowing RBM to operate in such\na compositional phase. Evidence is provided by the replica analysis of an\nadequate statistical ensemble of random RBMs and by RBM trained on the\nhandwritten digits dataset MNIST.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.06759v2"
    },
    {
        "title": "When \"Optimal Filtering\" Isn't",
        "authors": [
            "J. W. Fowler",
            "B. K. Alpert",
            "W. B. Doriese",
            "J. Hays-Wehle",
            "Y. -I. Joe",
            "K. M. Morgan",
            "G. C. O'Neil",
            "C. D. Reintsema",
            "D. R. Schmidt",
            "J. N. Ullom",
            "D. S. Swetz"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The so-called \"optimal filter\" analysis of a microcalorimeter's x-ray pulses\nis statistically optimal only if all pulses have the same shape, regardless of\nenergy. The shapes of pulses from a nonlinear detector can and do depend on the\npulse energy, however. A pulse-fitting procedure that we call \"tangent\nfiltering\" accounts for the energy dependence of the shape and should therefore\nachieve superior energy resolution. We take a geometric view of the\npulse-fitting problem and give expressions to predict how much the energy\nresolution stands to benefit from such a procedure. We also demonstrate the\nmethod with a case study of K-line fluorescence from several 3d transition\nmetals. The method improves the resolution from 4.9 eV to 4.2 eV at the Cu\nK$\\alpha$ line (8.0keV).\n",
        "pdf_link": "http://arxiv.org/pdf/1611.07856v1"
    },
    {
        "title": "Visualizing dispersive features in 2D image via minimum gradient method",
        "authors": [
            "Yu He",
            "Yan Wang",
            "Zhi-Xun Shen"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We developed a minimum gradient based method to track ridge features in 2D\nimage plot, which is a typical data representation in many momentum resolved\nspectroscopy experiments. Through both analytic formulation and numerical\nsimulation, we compare this new method with existing DC (distribution curve)\nbased and higher order derivative based analyses. We find that the new method\nhas good noise resilience and enhanced contrast especially for weak intensity\nfeatures, meanwhile preserves the quantitative local maxima information from\nthe raw image. An algorithm is proposed to extract 1D ridge dispersion from the\n2D image plot, whose quantitative application to angle-resolved photoemission\nspectroscopy measurements on high temperature superconductors is demonstrated.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07880v1"
    },
    {
        "title": "Support Vector Machines and generalisation in HEP",
        "authors": [
            "Adrian Bevan",
            "Rodrigo Gamboa Goñi",
            "Jon Hays",
            "Tom Stevenson"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We review the concept of Support Vector Machines (SVMs) and discuss examples\nof their use in a number of scenarios. Several SVM implementations have been\nused in HEP and we exemplify this algorithm using the Toolkit for Multivariate\nAnalysis (TMVA) implementation. We discuss examples relevant to HEP including\nbackground suppression for $H\\to\\tau^+\\tau^-$ at the LHC with several different\nkernel functions. Performance benchmarking leads to the issue of generalisation\nof hyper-parameter selection. The avoidance of fine tuning (over training or\nover fitting) in MVA hyper-parameter optimisation, i.e. the ability to ensure\ngeneralised performance of an MVA that is independent of the training,\nvalidation and test samples, is of utmost importance. We discuss this issue and\ncompare and contrast performance of hold-out and k-fold cross-validation. We\nhave extended the SVM functionality and introduced tools to facilitate cross\nvalidation in TMVA and present results based on these improvements.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.04686v1"
    },
    {
        "title": "Image Analysis Using a Dual-Tree $M$-Band Wavelet Transform",
        "authors": [
            "Caroline Chaux",
            "Laurent Duval",
            "Jean-Christophe Pesquet"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We propose a 2D generalization to the $M$-band case of the dual-tree\ndecomposition structure (initially proposed by N. Kingsbury and further\ninvestigated by I. Selesnick) based on a Hilbert pair of wavelets. We\nparticularly address (\\textit{i}) the construction of the dual basis and\n(\\textit{ii}) the resulting directional analysis. We also revisit the necessary\npre-processing stage in the $M$-band case. While several reconstructions are\npossible because of the redundancy of the representation, we propose a new\noptimal signal reconstruction technique, which minimizes potential estimation\nerrors. The effectiveness of the proposed $M$-band decomposition is\ndemonstrated via denoising comparisons on several image types (natural,\ntexture, seismics), with various $M$-band wavelets and thresholding strategies.\nSignificant improvements in terms of both overall noise reduction and direction\npreservation are observed.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.08534v1"
    },
    {
        "title": "Single-lead f-wave extraction using diffusion geometry",
        "authors": [
            "John Malik",
            "Neil Reed",
            "Chun-Li Wang",
            "Hautieng Wu"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  A novel single-lead f-wave extraction algorithm based on the modern diffusion\ngeometry data analysis framework is proposed. The algorithm is essentially an\naveraged beat subtraction algorithm, where the ventricular activity template is\nestimated by combining a newly designed metric, the \"diffusion distance,\" and\nthe non-local Euclidean median based on the non-linear manifold setup. We\ncoined the algorithm DD-NLEM. Two simulation schemes are considered, and the\nnew algorithm DD-NLEM outperforms traditional algorithms, including the average\nbeat subtraction, principal component analysis, and adaptive singular value\ncancellation, in different evaluation metrics with statistical significance.\nThe clinical potential is shown in the real Holter signal, and we introduce a\nnew score to evaluate the performance of the algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.08638v2"
    },
    {
        "title": "Statistical properties of interaction parameter estimates in direct\n  coupling analysis",
        "authors": [
            "Yingying Xu",
            "Erik Aurell",
            "Jukka Corander",
            "Yoshiyuki Kabashima"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We consider the statistical properties of interaction parameter estimates\nobtained by the direct coupling analysis (DCA) approach to learning\ninteractions from large data sets. Assuming that the data are generated from a\nrandom background distribution, we determine the distribution of inferred\ninteractions. Two inference methods are considered: the L2 regularized naive\nmean-field inference procedure (regularized least squares, RLS), and the\npseudo-likelihood maximization (plmDCA). For RLS we also study a model where\nthe data matrix elements are real numbers, identically and independently\ngenerated from a Gaussian distribution; in this setting we analytically find\nthat the distribution of the inferred interactions is Gaussian. For data of\nBoolean type, more realistic in practice, the inferred interactions do not\ngenerally follow a Gaussian. However, extensive numerical simulations indicate\nthat their distribution can be characterized by a single function determined by\na few system parameters after normalization by the standard deviation. This\nproperty holds for both RLS and plmDCA and may be exploitable for inferring the\ndistribution of extremely large interactions from simulations for smaller\nsystem sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.01459v1"
    },
    {
        "title": "PAFit: an R Package for the Non-Parametric Estimation of Preferential\n  Attachment and Node Fitness in Temporal Complex Networks",
        "authors": [
            "Thong Pham",
            "Paul Sheridan",
            "Hidetoshi Shimodaira"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Many real-world systems are profitably described as complex networks that\ngrow over time. Preferential attachment and node fitness are two simple growth\nmechanisms that not only explain certain structural properties commonly\nobserved in real-world systems, but are also tied to a number of applications\nin modeling and inference. While there are statistical packages for estimating\nvarious parametric forms of the preferential attachment function, there is no\nsuch package implementing non-parametric estimation procedures. The\nnon-parametric approach to the estimation of the preferential attachment\nfunction allows for comparatively finer-grained investigations of the\n`rich-get-richer' phenomenon that could lead to novel insights in the search to\nexplain certain nonstandard structural properties observed in real-world\nnetworks. This paper introduces the R package PAFit, which implements\nnon-parametric procedures for estimating the preferential attachment function\nand node fitnesses in a growing network, as well as a number of functions for\ngenerating complex networks from these two mechanisms. The main computational\npart of the package is implemented in C++ with OpenMP to ensure scalability to\nlarge-scale networks. We first introduce the main functionalities of PAFit\nthrough simulated examples, and then use the package to analyze a collaboration\nnetwork between scientists in the field of complex networks. The results\nindicate the joint presence of `rich-get-richer' and `fit-get-richer' phenomena\nin the collaboration network. The estimated attachment function is observed to\nbe near-linear, which we interpret as meaning that the chance an author gets a\nnew collaborator is proportional to their current number of collaborators.\nFurthermore, the estimated author fitnesses reveal a host of familiar faces\nfrom the complex networks community among the field's topmost fittest network\nscientists.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.06017v5"
    },
    {
        "title": "Regression Driven F--Transform and Application to Smoothing of Financial\n  Time Series",
        "authors": [
            "Luigi Troiano",
            "Pravesh Kriplani",
            "Irene Diaz"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  In this paper we propose to extend the definition of fuzzy transform in order\nto consider an interpolation of models that are richer than the standard fuzzy\ntransform. We focus on polynomial models, linear in particular, although the\napproach can be easily applied to other classes of models. As an example of\napplication, we consider the smoothing of time series in finance. A comparison\nwith moving averages is performed using NIFTY 50 stock market index.\nExperimental results show that a regression driven fuzzy transform (RDFT)\nprovides a smoothing approximation of time series, similar to moving average,\nbut with a smaller delay. This is an important feature for finance and other\napplication, where time plays a key role.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01941v1"
    },
    {
        "title": "Generalization of Effective Conductance Centrality for Egonetworks",
        "authors": [
            "Heman Shakeri",
            "Behnaz Moradi-Jamei",
            "Pietro Poggi-Corradini",
            "Nathan Albin",
            "Caterina Scoglio"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We study the popular centrality measure known as effective conductance or in\nsome circles as information centrality. This is an important notion of\ncentrality for undirected networks, with many applications, e.g., for random\nwalks, electrical resistor networks, epidemic spreading, etc. In this paper, we\nfirst reinterpret this measure in terms of modulus (energy) of families of\nwalks on the network. This modulus centrality measure coincides with the\neffective conductance measure on simple undirected networks, and extends it to\nmuch more general situations, e.g., directed networks as well. Secondly, we\nstudy a variation of this modulus approach in the egocentric network paradigm.\nEgonetworks are networks formed around a focal node (ego) with a specific order\nof neighborhoods. We propose efficient analytical and approximate methods for\ncomputing these measures on both undirected and directed networks. Finally, we\ndescribe a simple method inspired by the modulus point-of-view, called shell\ndegree, which proved to be a useful tool for network science.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.02703v2"
    },
    {
        "title": "A method and tool for combining differential or inclusive measurements\n  obtained with simultaneously constrained uncertainties",
        "authors": [
            "Jan Kieseler"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  A method is discussed that allows combining sets of differential or inclusive\nmeasurements. It is assumed that at least one measurement was obtained with\nsimultaneously fitting a set of nuisance parameters, representing sources of\nsystematic uncertainties. As a result of beneficial constraints from the data\nall such fitted parameters are correlated among each other. The best approach\nfor a combination of these measurements would be the maximisation of a combined\nlikelihood, for which the full fit model of each measurement and the original\ndata are required. However, only in rare cases this information is publicly\navailable. In absence of this information most commonly used combination\nmethods are not able to account for these correlations between uncertainties,\nwhich can lead to severe biases as shown in this article. The method discussed\nhere provides a solution for this problem. It relies on the public result and\nits covariance or Hessian, only, and is validated against the\ncombined-likelihood approach. A dedicated software package implementing this\nmethod is also presented. It provides a text-based user interface alongside a\nC++ interface. The latter also interfaces to ROOT classes for simple\ncombination of binned measurements such as differential cross sections.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01681v3"
    },
    {
        "title": "Maximum-entropy from the probability calculus: exchangeability,\n  sufficiency",
        "authors": [
            "P. G. L. Porta Mana"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Maximum-entropy distributions are shown to appear in the probability calculus\nas approximations of a model by exchangeability or a model by sufficiency, the\nformer model being preferable. The implications of this fact are discussed,\ntogether with other questions: Prediction or retrodiction? How good is the\nmaximum-entropy approximation? Is this a \"derivation\" of maximum-entropy?\n",
        "pdf_link": "http://arxiv.org/pdf/1706.02561v2"
    },
    {
        "title": "Charged particle tracking without magnetic field: optimal measurement of\n  track momentum by a Bayesian analysis of the multiple measurements of\n  deflections due to multiple scattering",
        "authors": [
            "Mikael Frosini",
            "Denis Bernard"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We revisit the precision of the measurement of track parameters (position,\nangle) with optimal methods in the presence of detector resolution, multiple\nscattering and zero magnetic field. We then obtain an optimal estimator of the\ntrack momentum by a Bayesian analysis of the filtering innovations of a series\nof Kalman filters applied to the track.\n  This work could pave the way to the development of autonomous\nhigh-performance gas time-projection chambers (TPC) or silicon wafer gamma-ray\nspace telescopes and be a powerful guide in the optimisation of the design of\nthe multi-kilo-ton liquid argon TPCs that are under development for neutrino\nstudies.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.05863v2"
    },
    {
        "title": "Complexity of eye fixation duration time series in reading of Persian\n  texts: A multifractal detrended fluctuation analysis",
        "authors": [
            "Mohammad Sharifi",
            "Hamed Farahani",
            "Farhad Shahbazi",
            "Masood Sharifi",
            "Christofer T. Kello",
            "Marzieh Zare"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  There is growing evidence that cognitive processes may have fractal\nstructures as a signature of complexity. It is an an ongoing topic of research\nto study the class of complexity and how it may differ as a function of\ncognitive variables. Here, we explore the eye movement trajectories generated\nduring reading different Persian texts. Features of eye movement trajectories\nwere recorded during reading Persian texts using an eye tracker. We show that\nfixation durations, as the main components of eye movements reflecting\ncognitive processing, exhibits multifractal behavior. This indicates that\nmultiple exponents are needed to capture the neural and cognitive processes\ninvolved in decoding symbols to derive meaning. We test whether multifractal\nbehavior varies as a function of two different fonts, familiarity of the text\nfor readers, and reading silently or aloud, and goal-oriented versus\nnon-goal-oriented reading. We find that, while mean fixation duration is\naffected by some of these factors, the multifractal pattern in time series of\neye fixation durations did not change significantly. Our results suggest that\nmultifractal dynamics may be intrinsic to the reading process.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.02932v1"
    },
    {
        "title": "Laser beam imaging from the speckle pattern of the off-axis scattered\n  intensity",
        "authors": [
            "Liliana Borcea",
            "Josselin Garnier"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We study the inverse problem of localization (imaging) of a laser beam from\nmeasurements of the intensity of light scattered off-axis by a Poisson cloud of\nsmall particles. Starting from the wave equation, we analyze the microscopic\ncoherence of the scattered intensity and show that it is possible to determine\nthe laser beam from the speckle pattern captured by a group of CCD cameras. Two\ngroups of cameras are sufficient when the particles are either small or large\nwith respect to the wavelength. For general particle sizes the accuracy of the\nlaser localization with two groups of cameras is subject to knowing the\nscattering properties of the cloud. However, three or more groups of cameras\nallow accurate localization that is robust to uncertainty of the type, size,\nshape and concentration of the particles in the cloud. We introduce a novel\nlaser beam localization algorithm and give some numerical illustrations in a\nregime relevant to the application of imaging high energy lasers in a maritime\natmosphere.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.08452v1"
    },
    {
        "title": "Improved Pseudolikelihood Regularization and Decimation methods on\n  Non-linearly Interacting Systems with Continuous Variables",
        "authors": [
            "Alessia Marruzzo",
            "Payal Tyagi",
            "Fabrizio Antenucci",
            "Andrea Pagnani",
            "Luca Leuzzi"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We propose and test improvements to state-of-the-art techniques of Bayeasian\nstatistical inference based on pseudolikelihood maximization with $\\ell_1$\nregularization and with decimation. In particular, we present a method to\ndetermine the best value of the regularizer parameter starting from a\nhypothesis testing technique. Concerning the decimation, we also analyze the\nworst case scenario in which there is no sharp peak in the\ntilded-pseudolikelihood function, firstly defined as a criterion to stop the\ndecimation. Techniques are applied to noisy systems with non-linear dynamics,\nmapped onto multi-variable interacting Hamiltonian effective models for waves\nand phasors. Results are analyzed varying the number of available samples and\nthe externally tunable temperature-like parameter mimicing real data noise.\nEventually the behavior of inference procedures described are tested against a\nwrong hypothesis: non-linearly generated data are analyzed with a pairwise\ninteracting hypothesis. Our analysis shows that, looking at the behavior of the\ninverse graphical problem as data size increases, the methods exposed allow to\nrule out a wrong hypothesis.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.00787v4"
    },
    {
        "title": "Quantifying multivariate redundancy with maximum entropy decompositions\n  of mutual information",
        "authors": [
            "Daniel Chicharro"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Williams and Beer (2010) proposed a nonnegative mutual information\ndecomposition, based on the construction of redundancy lattices, which allows\nseparating the information that a set of variables contains about a target\nvariable into nonnegative components interpretable as the unique information of\nsome variables not provided by others as well as redundant and synergistic\ncomponents. However, the definition of multivariate measures of redundancy that\ncomply with nonnegativity and conform to certain axioms that capture\nconceptually desirable properties of redundancy has proven to be elusive. We\nhere present a procedure to determine nonnegative multivariate redundancy\nmeasures, within the maximum entropy framework. In particular, we generalize\nexisting bivariate maximum entropy measures of redundancy and unique\ninformation, defining measures of the redundant information that a group of\nvariables has about a target, and of the unique redundant information that a\ngroup of variables has about a target that is not redundant with information\nfrom another group. The two key ingredients for this approach are: First, the\nidentification of a type of constraints on entropy maximization that allows\nisolating components of redundancy and unique redundancy by mirroring them to\nsynergy components. Second, the construction of rooted tree-based\ndecompositions of the mutual information, which conform to the axioms of the\nredundancy lattice by the local implementation at each tree node of binary\nunfoldings of the information using hierarchically related maximum entropy\nconstraints. Altogether, the proposed measures quantify the different\nmultivariate redundancy contributions of a nonnegative mutual information\ndecomposition consistent with the redundancy lattice.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.03845v2"
    },
    {
        "title": "AMORPH: A statistical program for characterizing amorphous materials by\n  X-ray diffraction",
        "authors": [
            "Michael C. Rowe",
            "Brendon J. Brewer"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  AMORPH utilizes a new Bayesian statistical approach to interpreting X-ray\ndiffraction results of samples with both crystalline and amorphous components.\nAMORPH fits X-ray diffraction patterns with a mixture of narrow and wide\ncomponents, simultaneously inferring all of the model parameters and\nquantifying their uncertainties. The program simulates background patterns\npreviously applied manually, providing reproducible results, and significantly\nreducing inter- and intra-user biases. This approach allows for the\nquantification of amorphous and crystalline materials and for the\ncharacterization of the amorphous component, including properties such as the\ncentre of mass, width, skewness, and nongaussianity of the amorphous component.\nResults demonstrate the applicability of this program for calculating amorphous\ncontents of volcanic materials and independently modeling their properties in\ncompositionally variable materials.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.04556v2"
    },
    {
        "title": "Modeling Smooth Backgrounds and Generic Localized Signals with Gaussian\n  Processes",
        "authors": [
            "Meghan Frate",
            "Kyle Cranmer",
            "Saarik Kalia",
            "Alexander Vandenberg-Rodes",
            "Daniel Whiteson"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We describe a procedure for constructing a model of a smooth data spectrum\nusing Gaussian processes rather than the historical parametric description.\nThis approach considers a fuller space of possible functions, is robust at\nincreasing luminosity, and allows us to incorporate our understanding of the\nunderlying physics. We demonstrate the application of this approach to modeling\nthe background to searches for dijet resonances at the Large Hadron Collider\nand describe how the approach can be used in the search for generic localized\nsignals.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.05681v1"
    },
    {
        "title": "Towards automation of data quality system for CERN CMS experiment",
        "authors": [
            "Maxim Borisyak",
            "Fedor Ratnikov",
            "Denis Derkach",
            "Andrey Ustyuzhanin"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Daily operation of a large-scale experiment is a challenging task,\nparticularly from perspectives of routine monitoring of quality for data being\ntaken. We describe an approach that uses Machine Learning for the automated\nsystem to monitor data quality, which is based on partial use of data qualified\nmanually by detector experts. The system automatically classifies marginal\ncases: both of good an bad data, and use human expert decision to classify\nremaining \"grey area\" cases.\n  This study uses collision data collected by the CMS experiment at LHC in\n2010. We demonstrate that proposed workflow is able to automatically process at\nleast 20\\% of samples without noticeable degradation of the result.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08607v1"
    },
    {
        "title": "From time-series to complex networks: Application to the cerebrovascular\n  flow patterns in atrial fibrillation",
        "authors": [
            "Stefania Scarsoglio",
            "Fabio Cazzato",
            "Luca Ridolfi"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  A network-based approach is presented to investigate the cerebrovascular flow\npatterns during atrial fibrillation (AF) with respect to normal sinus rhythm\n(NSR). AF, the most common cardiac arrhythmia with faster and irregular\nbeating, has been recently and independently associated with the increased risk\nof dementia. However, the underlying hemodynamic mechanisms relating the two\npathologies remain mainly undetermined so far; thus the contribution of\nmodeling and refined statistical tools is valuable. Pressure and flow rate\ntemporal series in NSR and AF are here evaluated along representative cerebral\nsites (from carotid arteries to capillary brain circulation), exploiting\nreliable artificially built signals recently obtained from an in silico\napproach. The complex network analysis evidences, in a synthetic and original\nway, a dramatic signal variation towards the distal/capillary cerebral regions\nduring AF, which has no counterpart in NSR conditions. At the large artery\nlevel, networks obtained from both AF and NSR hemodynamic signals exhibit\nelongated and chained features, which are typical of pseudo-periodic series.\nThese aspects are almost completely lost towards the microcirculation during\nAF, where the networks are topologically more circular and present random-like\ncharacteristics. As a consequence, all the physiological phenomena at\nmicrocerebral level ruled by periodicity - such as regular perfusion, mean\npressure per beat, and average nutrient supply at cellular level - can be\nstrongly compromised, since the AF hemodynamic signals assume irregular\nbehaviour and random-like features. Through a powerful approach which is\ncomplementary to the classical statistical tools, the present findings further\nstrengthen the potential link between AF hemodynamic and cognitive decline.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.09087v1"
    },
    {
        "title": "Fluorescence decay data analysis correcting for detector pulse pile-up\n  at very high count rates",
        "authors": [
            "Matthias Patting",
            "Paja Reisch",
            "Marcus Sackrow",
            "Rhys Dowler",
            "Marcelle Koenig",
            "Michael Wahl"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Using Time-Correlated Single Photon Counting (TCSPC) for the purpose of\nfluorescence lifetime measurements is usually limited in speed due to pile-up.\nWith modern instrumentation this limitation can be lifted significantly but\nsome artefacts due to frequent merging of closely spaced detector pulses\n(detector pulse pile-up) remains an issue to be addressed. We propose here a\ndata analysis method correcting for this type of artefact and the resulting\nsystematic errors. It physically models the photon losses due to detector pulse\npile-up and incorporates the loss in the decay fit model employed to obtain\nfluorescence lifetimes and relative amplitudes of the decay components.\nComparison of results with and without this correction show a significant\nreduction of systematic errors at count rates approaching the excitation rate.\nThis allows quantitatively accurate fluorescense lifetime imaging (FLIM) at\nvery high frame rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.01137v3"
    },
    {
        "title": "Deep learning for inferring cause of data anomalies",
        "authors": [
            "V. Azzolini",
            "M. Borisyak",
            "G. Cerminara",
            "D. Derkach",
            "G. Franzoni",
            "F. De Guio",
            "O. Koval",
            "M. Pierini",
            "A. Pol",
            "F. Ratnikov",
            "F. Siroky",
            "A. Ustyuzhanin",
            "J-R. Vlimant"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Daily operation of a large-scale experiment is a resource consuming task,\nparticularly from perspectives of routine data quality monitoring. Typically,\ndata comes from different sub-detectors and the global quality of data depends\non the combinatorial performance of each of them. In this paper, the problem of\nidentifying channels in which anomalies occurred is considered. We introduce a\ngeneric deep learning model and prove that, under reasonable assumptions, the\nmodel learns to identify 'channels' which are affected by an anomaly. Such\nmodel could be used for data quality manager cross-check and assistance and\nidentifying good channels in anomalous data samples. The main novelty of the\nmethod is that the model does not require ground truth labels for each channel,\nonly global flag is used. This effectively distinguishes the model from\nclassical classification methods. Being applied to CMS data collected in the\nyear 2010, this approach proves its ability to decompose anomaly by separate\nchannels.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.07051v1"
    },
    {
        "title": "Two- and Multi-dimensional Curve Fitting using Bayesian Inference",
        "authors": [
            "Andrew W. Steiner"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Fitting models to data using Bayesian inference is quite common, but when\neach point in parameter space gives a curve, fitting the curve to a data set\nrequires new nuisance parameters, which specify the metric embedding the\none-dimensional curve into the higher-dimensional space occupied by the data. A\ngeneric formalism for curve fitting in the context of Bayesian inference is\ndeveloped which shows how the aforementioned metric arises. The result is a\nnatural generalization of previous works, and is compared to oft-used\nfrequentist approaches and similar Bayesian techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.05339v2"
    },
    {
        "title": "Computational Techniques for the Analysis of Small Signals in\n  High-Statistics Neutrino Oscillation Experiments",
        "authors": [
            " IceCube Collaboration",
            "M. G. Aartsen",
            "M. Ackermann",
            "J. Adams",
            "J. A. Aguilar",
            "M. Ahlers",
            "M. Ahrens",
            "I. Al Samarai",
            "D. Altmann",
            "K. Andeen",
            "T. Anderson",
            "I. Ansseau",
            "G. Anton",
            "C. Argüelles",
            "T. C. Arlen",
            "J. Auffenberg",
            "S. Axani",
            "H. Bagherpour",
            "X. Bai",
            "A. Balagopal V.",
            "J. P. Barron",
            "I. Bartos",
            "S. W. Barwick",
            "V. Baum",
            "R. Bay",
            "J. J. Beatty",
            "J. Becker Tjus",
            "K. -H. Becker",
            "S. BenZvi",
            "D. Berley",
            "E. Bernardini",
            "D. Z. Besson",
            "G. Binder",
            "D. Bindig",
            "E. Blaufuss",
            "S. Blot",
            "C. Bohm",
            "M. Bohmer",
            "M. Börner",
            "F. Bos",
            "S. Böser",
            "O. Botner",
            "E. Bourbeau",
            "J. Bourbeau",
            "F. Bradascio",
            "J. Braun",
            "M. Brenzke",
            "H. -P. Bretz",
            "S. Bron",
            "J. Brostean-Kaiser",
            "A. Burgman",
            "R. S. Busse",
            "T. Carver",
            "E. Cheung",
            "D. Chirkin",
            "A. Christov",
            "K. Clark",
            "L. Classen",
            "G. H. Collin",
            "J. M. Conrad",
            "P. Coppin",
            "P. Correa",
            "D. F. Cowen",
            "R. Cross",
            "P. Dave",
            "M. Day",
            "J. P. A. M. de André",
            "C. De Clercq",
            "J. J. DeLaunay",
            "H. Dembinski",
            "S. De Ridder",
            "P. Desiati",
            "K. D. de Vries",
            "G. de Wasseige",
            "M. de With",
            "T. DeYoung",
            "J. C. Díaz-Vélez",
            "V. di Lorenzo",
            "H. Dujmovic",
            "J. P. Dumm",
            "M. Dunkman",
            "M. A. DuVernois",
            "E. Dvorak",
            "B. Eberhardt",
            "T. Ehrhardt",
            "B. Eichmann",
            "P. Eller",
            "R. Engel",
            "J. J. Evans",
            "P. A. Evenson",
            "S. Fahey",
            "A. R. Fazely",
            "J. Felde",
            "K. Filimonov",
            "C. Finley",
            "S. Flis",
            "A. Franckowiak",
            "E. Friedman",
            "A. Fritz",
            "T. K. Gaisser",
            "J. Gallagher",
            "A. Gartner",
            "L. Gerhardt",
            "R. Gernhaeuser",
            "K. Ghorbani",
            "W. Giang",
            "T. Glauch",
            "T. Glüsenkamp",
            "A. Goldschmidt",
            "J. G. Gonzalez",
            "D. Grant",
            "Z. Griffith",
            "C. Haack",
            "A. Hallgren",
            "F. Halzen",
            "K. Hanson",
            "J. Haugen",
            "A. Haungs",
            "D. Hebecker",
            "D. Heereman",
            "K. Helbing",
            "R. Hellauer",
            "F. Henningsen",
            "S. Hickford",
            "J. Hignight",
            "G. C. Hill",
            "K. D. Hoffman",
            "B. Hoffmann",
            "R. Hoffmann",
            "T. Hoinka",
            "B. Hokanson-Fasig",
            "K. Holzapfel",
            "K. Hoshina",
            "F. Huang",
            "M. Huber",
            "T. Huber",
            "T. Huege",
            "K. Hultqvist",
            "M. Hünnefeld",
            "R. Hussain",
            "S. In",
            "N. Iovine",
            "A. Ishihara",
            "E. Jacobi",
            "G. S. Japaridze",
            "M. Jeong",
            "K. Jero",
            "B. J. P. Jones",
            "P. Kalaczynski",
            "O. Kalekin",
            "W. Kang",
            "D. Kang",
            "A. Kappes",
            "D. Kappesser",
            "T. Karg",
            "A. Karle",
            "T. Katori",
            "U. Katz",
            "M. Kauer",
            "A. Keivani",
            "J. L. Kelley",
            "A. Kheirandish",
            "J. Kim",
            "M. Kim",
            "T. Kintscher",
            "J. Kiryluk",
            "T. Kittler",
            "S. R. Klein",
            "R. Koirala",
            "H. Kolanoski",
            "L. Köpke",
            "C. Kopper",
            "S. Kopper",
            "J. P. Koschinsky",
            "D. J. Koskinen",
            "M. Kowalski",
            "C. B. Krauss",
            "K. Krings",
            "M. Kroll",
            "G. Krückl",
            "S. Kunwar",
            "N. Kurahashi",
            "T. Kuwabara",
            "A. Kyriacou",
            "M. Labare",
            "J. L. Lanfranchi",
            "M. J. Larson",
            "F. Lauber",
            "D. Lennarz",
            "K. Leonard",
            "M. Lesiak-Bzdak",
            "A. Leszczynska",
            "M. Leuermann",
            "Q. R. Liu",
            "E. Lohfink",
            "J. LoSecco",
            "C. J. Lozano Mariscal",
            "L. Lu",
            "J. Lünemann",
            "W. Luszczak",
            "J. Madsen",
            "G. Maggi",
            "K. B. M. Mahn",
            "S. Mancina",
            "S. Mandalia",
            "S. Marka",
            "Z. Marka",
            "R. Maruyama",
            "K. Mase",
            "R. Maunu",
            "K. Meagher",
            "M. Medici",
            "M. Meier",
            "T. Menne",
            "G. Merino",
            "T. Meures",
            "S. Miarecki",
            "J. Micallef",
            "G. Momenté",
            "T. Montaruli",
            "R. W. Moore",
            "M. Moulai",
            "R. Nahnhauer",
            "P. Nakarmi",
            "U. Naumann",
            "G. Neer",
            "H. Niederhausen",
            "S. C. Nowicki",
            "D. R. Nygren",
            "A. Obertacke Pollmann",
            "M. Oehler",
            "A. Olivas",
            "A. O'Murchadha",
            "E. O'Sullivan",
            "A. Palazzo",
            "T. Palczewski",
            "H. Pandya",
            "D. V. Pankova",
            "L. Papp",
            "P. Peiffer",
            "J. A. Pepper",
            "C. Pérez de los Heros",
            "T. C. Petersen",
            "D. Pieloth",
            "E. Pinat",
            "J. L. Pinfold",
            "M. Plum",
            "P. B. Price",
            "G. T. Przybylski",
            "C. Raab",
            "L. Rädel",
            "M. Rameez",
            "L. Rauch",
            "K. Rawlins",
            "I. C. Rea",
            "R. Reimann",
            "B. Relethford",
            "M. Relich",
            "M. Renschler",
            "E. Resconi",
            "W. Rhode",
            "M. Richman",
            "M. Riegel",
            "S. Robertson",
            "M. Rongen",
            "C. Rott",
            "T. Ruhe",
            "D. Ryckbosch",
            "D. Rysewyk",
            "I. Safa",
            "T. Sälzer",
            "S. E. Sanchez Herrera",
            "A. Sandrock",
            "J. Sandroos",
            "P. Sandstrom",
            "M. Santander",
            "S. Sarkar",
            "S. Sarkar",
            "K. Satalecka",
            "H. Schieler",
            "P. Schlunder",
            "T. Schmidt",
            "A. Schneider",
            "S. Schoenen",
            "S. Schöneberg",
            "F. G. Schröder",
            "L. Schumacher",
            "S. Sclafani",
            "D. Seckel",
            "S. Seunarine",
            "M. H. Shaevitz",
            "J. Soedingrekso",
            "D. Soldin",
            "S. Söldner-Rembold",
            "M. Song",
            "G. M. Spiczak",
            "C. Spiering",
            "J. Stachurska",
            "M. Stamatikos",
            "T. Stanev",
            "A. Stasik",
            "R. Stein",
            "J. Stettner",
            "A. Steuer",
            "T. Stezelberger",
            "R. G. Stokstad",
            "A. Stößl",
            "N. L. Strotjohann",
            "T. Stuttard",
            "G. W. Sullivan",
            "M. Sutherland",
            "I. Taboada",
            "A. Taketa",
            "H. K. M. Tanaka",
            "J. Tatar",
            "F. Tenholt",
            "S. Ter-Antonyan",
            "A. Terliuk",
            "S. Tilav",
            "P. A. Toale",
            "M. N. Tobin",
            "C. Tönnis",
            "S. Toscano",
            "D. Tosi",
            "M. Tselengidou",
            "C. F. Tung",
            "A. Turcati",
            "C. F. Turley",
            "B. Ty",
            "E. Unger",
            "M. Usner",
            "J. Vandenbroucke",
            "W. Van Driessche",
            "D. van Eijk",
            "N. van Eijndhoven",
            "S. Vanheule",
            "J. van Santen",
            "D. Veberic",
            "E. Vogel",
            "M. Vraeghe",
            "C. Walck",
            "A. Wallace",
            "M. Wallraff",
            "F. D. Wandler",
            "N. Wandkowsky",
            "A. Waza",
            "C. Weaver",
            "A. Weindl",
            "M. J. Weiss",
            "C. Wendt",
            "J. Werthebach",
            "S. Westerhoff",
            "B. J. Whelan",
            "K. Wiebe",
            "C. H. Wiebusch",
            "L. Wille",
            "D. R. Williams",
            "L. Wills",
            "M. Wolf",
            "J. Wood",
            "T. R. Wood",
            "E. Woolsey",
            "K. Woschnagg",
            "G. Wrede",
            "S. Wren",
            "D. L. Xu",
            "X. W. Xu",
            "Y. Xu",
            "J. P. Yanez",
            "G. Yodh",
            "S. Yoshida",
            "T. Yuan"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The current and upcoming generation of Very Large Volume Neutrino\nTelescopes---collecting unprecedented quantities of neutrino events---can be\nused to explore subtle effects in oscillation physics, such as (but not\nrestricted to) the neutrino mass ordering. The sensitivity of an experiment to\nthese effects can be estimated from Monte Carlo simulations. With the high\nnumber of events that will be collected, there is a trade-off between the\ncomputational expense of running such simulations and the inherent statistical\nuncertainty in the determined values. In such a scenario, it becomes\nimpractical to produce and use adequately-sized sets of simulated events with\ntraditional methods, such as Monte Carlo weighting. In this work we present a\nstaged approach to the generation of binned event distributions in order to\novercome these challenges. By combining multiple integration and smoothing\ntechniques which address limited statistics from simulation it arrives at\nreliable analysis results using modest computational resources.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.05390v2"
    },
    {
        "title": "Adaptive Decision Making via Entropy Minimization",
        "authors": [
            "Armen E. Allahverdyan",
            "Aram Galstyan",
            "Ali E. Abbas",
            "Zbigniew R. Struzik"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  An agent choosing between various actions tends to take the one with the\nlowest cost. But this choice is arguably too rigid (not adaptive) to be useful\nin complex situations, e.g., where exploration-exploitation trade-off is\nrelevant in creative task solving or when stated preferences differ from\nrevealed ones. Here we study an agent who is willing to sacrifice a fixed\namount of expected utility for adaptation. How can/ought our agent choose an\noptimal (in a technical sense) mixed action? We explore consequences of making\nthis choice via entropy minimization, which is argued to be a specific example\nof risk-aversion. This recovers the $\\epsilon$-greedy probabilities known in\nreinforcement learning. We show that the entropy minimization leads to\nrudimentary forms of intelligent behavior: (i) the agent assigns a\nnon-negligible probability to costly events; but (ii) chooses with a sizable\nprobability the action related to less cost (lesser of two evils) when\nconfronted with two actions with comparable costs; (iii) the agent is subject\nto effects similar to cognitive dissonance and frustration. Neither of these\nfeatures are shown by entropy maximization.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.06638v2"
    },
    {
        "title": "Statistical test for fractional Brownian motion based on detrending\n  moving average algorithm",
        "authors": [
            "Grzegorz Sikora"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Motivated by contemporary and rich applications of anomalous diffusion\nprocesses we propose a new statistical test for fractional Brownian motion,\nwhich is one of the most popular models for anomalous diffusion systems. The\ntest is based on detrending moving average statistic and its probability\ndistribution. Using the theory of Gaussian quadratic forms we determined it as\na generalized chi-squared distribution. The proposed test could be generalized\nfor statistical testing of any centered non-degenerate Gaussian process.\nFinally, we examine the test via Monte Carlo simulations for two exemplary\nscenarios of subdiffusive and superdiffusive dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08553v1"
    },
    {
        "title": "Causal network discovery by iterative conditioning: comparison of\n  algorithms",
        "authors": [
            "Jakub Kořenek",
            "Jaroslav Hlinka"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Estimating causal interactions in complex dynamical systems is an important\nproblem encountered in many fields of current science. While a theoretical\nsolution for detecting the causal interactions has been previously formulated\nin the framework of prediction improvement, it generally requires the\ncomputation of high-dimensional information functionals -- a situation invoking\nthe curse of dimensionality with increasing network size. Recently, several\nmethods have been proposed to alleviate this problem, based on iterative\nprocedures for assessment of conditional (in)dependences. In the current work,\nwe bring a comparison of several such prominent approaches. This is done both\nby theoretical comparison of the algorithms using a formulation in a common\nframework, and by numerical simulations including realistic complex coupling\npatterns. The theoretical analysis highlights the key similarities and\ndifferences between the algorithms, hinting on their comparative strengths and\nweaknesses. The method assumptions and specific properties such as false\npositive control and order dependence are discussed. Numerical simulations\nsuggest that while the accuracy of most of the algorithms is almost\nindistinguishable, there are substantial differences in their computational\ndemands, ranging theoretically from polynomial to exponential complexity, and\nleading to substantial differences in computation time in realistic scenarios\ndepending on the density and size of networks. Based on analysis of the\nalgorithms and numerical simulations, we propose a hybrid approach providing\ncompetitive accuracy with improved computational efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.08173v2"
    },
    {
        "title": "BlurRing",
        "authors": [
            "Lydia Brenner",
            "Carsten Burgard"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  A code package, BlurRing, is developed as a method to allow for\nmulti-dimensional likelihood visualisation. From the BlurRing visualisation\nadditional information about the likelihood can be extracted. The spread in any\ndirection of the overlaid likelihood curves gives information about the\nuncertainty on the confidence intervals presented in the two-dimensional\nlikelihood plots.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.07213v1"
    },
    {
        "title": "Unfolding with Generative Adversarial Networks",
        "authors": [
            "Kaustuv Datta",
            "Deepak Kar",
            "Debarati Roy"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Correcting measured detector-level distributions to particle-level is\nessential to make data usable outside the experimental collaborations. The term\nunfolding is used to describe this procedure. A new method of unfolding data\nusing a modified Generative Adversarial Network (MSGAN) is presented here.\nApplied to various distributions with widely different shapes, it performs\nroughly at par with currently used methods. This is a proof-of-principle\ndemonstration of a state-of-the-art machine learning method that can be used to\nmodel detector effects well.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.00433v2"
    },
    {
        "title": "PyUnfold: A Python Package for Iterative Unfolding",
        "authors": [
            "James Bourbeau",
            "Zigfried Hampel-Arias"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  PyUnfold is a Python package for incorporating imperfections of the\nmeasurement process into a data analysis pipeline. In an ideal world, we would\nhave access to the perfect detector: an apparatus that makes no error in\nmeasuring a desired quantity. However, in real life, detectors have finite\nresolutions, characteristic biases that cannot be eliminated, less than full\ndetection efficiencies, and statistical and systematic uncertainties. By\nbuilding a matrix that encodes a detector's smearing of the desired true\nquantity into the measured observable(s), a deconvolution can be performed that\nprovides an estimate of the true variable. This deconvolution process is known\nas unfolding. The unfolding method implemented in PyUnfold accomplishes this\ndeconvolution via an iterative procedure, providing results based on physical\nexpectations of the desired quantity. Furthermore, tedious book-keeping for\nboth statistical and systematic errors produces precise final uncertainty\nestimates.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.03350v1"
    },
    {
        "title": "Tutorial: Maximum likelihood estimation in the context of an optical\n  measurement",
        "authors": [
            "Anthony Vella"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The method of maximum likelihood estimation (MLE) is a widely used\nstatistical approach for estimating the values of one or more unknown\nparameters of a probabilistic model based on observed data. In this tutorial, I\nbriefly review the mathematical foundations of MLE, then reformulate the\nproblem for the measurement of a spatially-varying optical intensity\ndistribution. In this context, the detection of each individual photon is\ntreated as a random event, the outcome being the photon's location. A typical\nmeasurement consists of many detected photons, which accumulate to form a\nspatial intensity profile. Here, I show a straightforward derivation for the\nlikelihood function and Fisher information matrix (FIM) associated with a\nmeasurement of multiple photons incident on a detector comprised of a discrete\narray of pixels. An estimate for the parameter(s) of interest may then be\nobtained by maximizing the likelihood function, while the FIM determines the\nuncertainty of the estimate. To illustrate these concepts, several simple\nexamples are presented for the one- and two-parameter cases, revealing many\ninteresting properties of the MLE formalism, as well as some practical\nconsiderations for optical experiments. Throughout these examples, connections\nare also drawn to optical applications of quantum weak measurements, including\noff-null ellipsometry and scatterometry.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.04503v3"
    },
    {
        "title": "Analytical approach to network inference: Investigating degree\n  distribution",
        "authors": [
            "Gloria Cecchini",
            "Bjoern Schelter"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  When the network is reconstructed, two types of errors can occur: false\npositive and false negative errors about the presence or absence of links. In\nthis paper, the influence of these two errors on the vertex degree distribution\nis analytically analysed. Moreover, an analytic formula of the density of the\nbiased vertex degree distribution is found. In the inverse problem, we find a\nreliable procedure to reconstruct analytically the density of the vertex degree\ndistribution of any network based on the inferred network and estimates for the\nfalse positive and false negative errors based on, e.g., simulation studies.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.06312v1"
    },
    {
        "title": "Application of the Iterated Weighted Least-Squares Fit to counting\n  experiments",
        "authors": [
            "Hans Dembinski",
            "Michael Schmelling",
            "Roland Waldi"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Least-squares fits are an important tool in many data analysis applications.\nIn this paper, we review theoretical results, which are relevant for their\napplication to data from counting experiments. Using a simple example, we\nillustrate the well known fact that commonly used variants of the least-squares\nfit applied to Poisson-distributed data produce biased estimates. The bias can\nbe overcome with an iterated weighted least-squares method, which produces\nresults identical to the maximum-likelihood method. For linear models, the\niterated weighted least-squares method converges faster than the equivalent\nmaximum-likelihood method, and does not require problem-specific starting\nvalues, which may be a practical advantage. The equivalence of both methods\nalso holds for binomially distributed data. We further show that the unbinned\nmaximum-likelihood method can be derived as a limiting case of the iterated\nleast-squares fit when the bin width goes to zero, which demonstrates a deep\nconnection between the two methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07911v9"
    },
    {
        "title": "Scaling in the eigenvalue fluctuations of the empirical correlation\n  matrices",
        "authors": [
            "Udaysinh T. Bhosale",
            "S. Harshini Tekur",
            "M. S. Santhanam"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The spectra of empirical correlation matrices, constructed from multivariate\ndata, are widely used in many areas of sciences, engineering and social\nsciences as a tool to understand the information contained in typically large\ndatasets. In the last two decades, random matrix theory-based tools such as the\nnearest neighbour eigenvalue spacing and eigenvector distributions have been\nemployed to extract the significant modes of variability present in such\nempirical correlations. In this work, we present an alternative analysis in\nterms of the recently introduced spacing ratios, which does not require the\ncumbersome unfolding process. It is shown that the higher order spacing ratio\ndistributions for the Wishart ensemble of random matrices, characterized by the\nDyson index $\\beta$, is related to the first order spacing ratio distribution\nwith a modified value of co-dimension $\\beta'$. This scaling is demonstrated\nfor Wishart ensemble and also for the spectra of empirical correlation matrices\ndrawn from the observed stock market and atmospheric pressure data. Using a\ncombination of analytical and numerics, such scalings in spacing distributions\nare also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07968v2"
    },
    {
        "title": "End-to-End Physics Event Classification with CMS Open Data: Applying\n  Image-Based Deep Learning to Detector Data for the Direct Classification of\n  Collision Events at the LHC",
        "authors": [
            "Michael Andrews",
            "Manfred Paulini",
            "Sergei Gleyzer",
            "Barnabas Poczos"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  This paper describes the construction of novel end-to-end image-based\nclassifiers that directly leverage low-level simulated detector data to\ndiscriminate signal and background processes in pp collision events at the\nLarge Hadron Collider at CERN. To better understand what end-to-end classifiers\nare capable of learning from the data and to address a number of associated\nchallenges, we distinguish the decay of the standard model Higgs boson into two\nphotons from its leading background sources using high-fidelity simulated CMS\nOpen Data. We demonstrate the ability of end-to-end classifiers to learn from\nthe angular distribution of the photons recorded as electromagnetic showers,\ntheir intrinsic shapes, and the energy of their constituent hits, even when the\nunderlying particles are not fully resolved, delivering a clear advantage in\nsuch cases over purely kinematics-based classifiers.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.11916v3"
    },
    {
        "title": "Detector monitoring with artificial neural networks at the CMS\n  experiment at the CERN Large Hadron Collider",
        "authors": [
            "Adrian Alan Pol",
            "Gianluca Cerminara",
            "Cecile Germain",
            "Maurizio Pierini",
            "Agrima Seth"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Reliable data quality monitoring is a key asset in delivering collision data\nsuitable for physics analysis in any modern large-scale High Energy Physics\nexperiment. This paper focuses on the use of artificial neural networks for\nsupervised and semi-supervised problems related to the identification of\nanomalies in the data collected by the CMS muon detectors. We use deep neural\nnetworks to analyze LHC collision data, represented as images organized\ngeographically. We train a classifier capable of detecting the known anomalous\nbehaviors with unprecedented efficiency and explore the usage of convolutional\nautoencoders to extend anomaly detection capabilities to unforeseen failure\nmodes. A generalization of this strategy could pave the way to the automation\nof the data quality assessment process for present and future high-energy\nphysics experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.00911v1"
    },
    {
        "title": "Integration with an Adaptive Harmonic Mean Algorithm",
        "authors": [
            "Allen Caldwell",
            "Philipp Eller",
            "Vasyl Hafych",
            "Rafael C. Schick",
            "Oliver Schulz",
            "Marco Szalay"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Numerically estimating the integral of functions in high dimensional spaces\nis a non-trivial task. A oft-encountered example is the calculation of the\nmarginal likelihood in Bayesian inference, in a context where a sampling\nalgorithm such as a Markov Chain Monte Carlo provides samples of the function.\nWe present an Adaptive Harmonic Mean Integration (AHMI) algorithm. Given\nsamples drawn according to a probability distribution proportional to the\nfunction, the algorithm will estimate the integral of the function and the\nuncertainty of the estimate by applying a harmonic mean estimator to adaptively\nchosen regions of the parameter space. We describe the algorithm and its\nmathematical properties, and report the results using it on multiple test\ncases.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.08051v2"
    },
    {
        "title": "Linear classifier, least-squares cost function, and outliers",
        "authors": [
            "Babatunde M. Ayeni"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  A set of introductory notes on the subject of data classification using a\nlinear classifier and least-squares cost function, and the negative effect of\nthe presence of outliers on the decision boundary of the linear discriminant.\nWe also show how a simple scaling could make the outlier less significant,\nthereby obtaining a much better decision boundary. We present some numerical\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.09222v2"
    },
    {
        "title": "Harmonizing discovery thresholds and reporting two-sided confidence\n  intervals: a modified Feldman & Cousins method",
        "authors": [
            "Knut Dundas Morå"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  When searching for new physics effects, collaborations will often wish to\npublish upper limits and intervals with a lower confidence level than the\nthreshold they would set to claim an excess or a discovery. However, confidence\nintervals are typically constructed to provide constant coverage, or\nprobability to contain the true value, with possible overcoverage if the random\nparameter is discrete. In particular, that means that the confidence interval\nwill contain the $0$-signal case with the same frequency as the confidence\nlevel. This paper details a modification to the Feldman-Cousins method to allow\na different, higher excess reporting significance than the interval confidence\nlevel.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.02024v2"
    },
    {
        "title": "DeepEfficiency - optimal efficiency inversion in higher dimensions at\n  the LHC",
        "authors": [
            "Mikael Mieskolainen"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We introduce a new high dimensional algorithm for efficiency corrected,\nmaximally Monte Carlo event generator independent fiducial measurements at the\nLHC and beyond. The approach is driven probabilistically using a Deep Neural\nNetwork on an event-by-event basis, trained using detector simulation and even\nonly pure phase space distributed events. This approach gives also a glimpse\ninto the future of high energy physics, where experiments publish new type of\nmeasurements in a radically multidimensional way.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.06101v1"
    },
    {
        "title": "ReSyst: a novel technique to Reduce the Systematic uncertainty for\n  precision measurements",
        "authors": [
            "Petra Van Mulders"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We are in an era of precision measurements at the Large Hadron Collider. The\nprecision that can be achieved on some of those is limited however due to large\nsystematic uncertainties. This paper introduces a new technique to reduce the\ntotal systematic uncertainty by quantifying the systematic impact of single\nevents and correlating it with event observables to identify classes of events\nthat are more sensitive to systematic effects. A proof of concept is presented\nby means of a simplified top quark mass estimator applied on simulated events.\nEven without a thorough optimization, it is shown that the total systematic\nuncertainty can be reduced by at least 30%.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.07700v2"
    },
    {
        "title": "The stepping-stone sampling algorithm for calculating the evidence of\n  gravitational wave models",
        "authors": [
            "Patricio Maturana Russel",
            "Renate Meyer",
            "John Veitch",
            "Nelson Christensen"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Bayesian statistical inference has become increasingly important for the\nanalysis of observations from the Advanced LIGO and Advanced Virgo\ngravitational-wave detectors. To this end, iterative simulation techniques, in\nparticular nested sampling and parallel tempering, have been implemented in the\nsoftware library LALInference to sample from the posterior distribution of\nwaveform parameters of compact binary coalescence events. Nested sampling was\nmainly developed to calculate the marginal likelihood of a model but can\nproduce posterior samples as a by-product. Thermodynamic integration is\nemployed to calculate the evidence using samples generated by parallel\ntempering but has been found to be computationally demanding. Here we propose\nthe stepping-stone sampling algorithm, originally proposed by Xie et al. (2011)\nin phylogenetics and a special case of path sampling, as an alternative to\nthermodynamic integration. The stepping-stone sampling algorithm is also based\non samples from the power posteriors of parallel tempering but has superior\nperformance as fewer temperature steps and thus computational resources are\nneeded to achieve the same accuracy. We demonstrate its performance and\ncomputational costs in comparison to thermodynamic integration and nested\nsampling in a simulation study and a case study of computing the marginal\nlikelihood of a binary black hole signal model applied to simulated data from\nthe Advanced LIGO and Advanced Virgo gravitational wave detectors. To deal with\nthe inadequate methods currently employed to estimate the standard errors of\nevidence estimates based on power posterior techniques, we propose a novel\nblock bootstrap approach and show its potential in our simulation study and\nLIGO application.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.04488v1"
    },
    {
        "title": "Discovering state-parameter mappings in subsurface models using\n  generative adversarial networks",
        "authors": [
            "Alexander Y. Sun"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  A fundamental problem in geophysical modeling is related to the\nidentification and approximation of causal structures among physical processes.\nHowever, resolving the bidirectional mappings between physical parameters and\nmodel state variables (i.e., solving the forward and inverse problems) is\nchallenging, especially when parameter dimensionality is high. Deep learning\nhas opened a new door toward knowledge representation and complex pattern\nidentification. In particular, the recently introduced generative adversarial\nnetworks (GANs) hold strong promises in learning cross-domain mappings for\nimage translation. This study presents a state-parameter identification GAN\n(SPID-GAN) for simultaneously learning bidirectional mappings between a\nhigh-dimensional parameter space and the corresponding model state space.\nSPID-GAN is demonstrated using a series of representative problems from\nsubsurface flow modeling. Results show that SPID-GAN achieves satisfactory\nperformance in identifying the bidirectional state-parameter mappings,\nproviding a new deep-learning-based, knowledge representation paradigm for a\nwide array of complex geophysical problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.12856v1"
    },
    {
        "title": "A binned likelihood for stochastic models",
        "authors": [
            "Carlos A. Argüelles",
            "Austin Schneider",
            "Tianlu Yuan"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Metrics of model goodness-of-fit, model comparison, and model parameter\nestimation are the main categories of statistical problems in science. Bayesian\nand frequentist methods that address these questions often rely on a likelihood\nfunction, which is the key ingredient in order to assess the plausibility of\nmodel parameters given observed data. In some complex systems or experimental\nsetups, predicting the outcome of a model cannot be done analytically, and\nMonte Carlo techniques are used. In this paper, we present a new analytic\nlikelihood that takes into account Monte Carlo uncertainties, appropriate for\nuse in the large and small sample size limits. Our formulation performs better\nthan semi-analytic methods, prevents strong claims on biased statements, and\nprovides improved coverage properties compared to available methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04645v2"
    },
    {
        "title": "Field dynamics inference for local and causal interactions",
        "authors": [
            "Philipp Frank",
            "Reimar Leike",
            "Torsten A. Enßlin"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Inference of fields defined in space and time from observational data is a\ncore discipline in many scientific areas. This work approaches the problem in a\nBayesian framework. The proposed method is based on statistically homogeneous\nrandom fields defined in space and time and demonstrates how to reconstruct the\nfield together with its prior correlation structure from data. The prior model\nof the correlation structure is described in a non-parametric fashion and\nsolely builds on fundamental physical assumptions such as space-time\nhomogeneity, locality, and causality. These assumptions are sufficient to\nsuccessfully infer the field and its prior correlation structure from noisy and\nincomplete data of a single realization of the process as demonstrated via\nmultiple numerical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.02624v3"
    },
    {
        "title": "The maximum a posteriori probability rule for atom column detection from\n  HAADF STEM images",
        "authors": [
            "J. Fatermans",
            "S. Van Aert",
            "A. J. den Dekker"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Recently, the maximum a posteriori (MAP) probability rule has been proposed\nas an objective and quantitative method to detect atom columns and even single\natoms from high-resolution high-angle annular dark-field (HAADF) scanning\ntransmission electron microscopy (STEM) images. The method combines statistical\nparameter estimation and model-order selection using a Bayesian framework and\nhas been shown to be especially useful for the analysis of the structure of\nbeam-sensitive nanomaterials. In order to avoid beam damage, images of such\nmaterials are usually acquired using a limited incoming electron dose resulting\nin a low contrast-to-noise ratio (CNR) which makes visual inspection\nunreliable. This creates a need for an objective and quantitative approach. The\npresent paper describes the methodology of the MAP probability rule, gives its\nstep-by-step derivation and discusses its algorithmic implementation for atom\ncolumn detection. In addition, simulation results are presented showing that\nthe performance of the MAP probability rule to detect the correct number of\natomic columns from HAADF STEM images is superior to that of other model-order\nselection criteria, including the Akaike Information Criterion (AIC) and the\nBayesian Information Criterion (BIC). Moreover, the MAP probability rule is\nused as a tool to evaluate the relation between STEM image quality measures and\natom detectability resulting in the introduction of the so-called integrated\nCNR (ICNR) as a new image quality measure that better correlates with atom\ndetectability than conventional measures such as signal-to-noise ratio (SNR)\nand CNR.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.05809v1"
    },
    {
        "title": "Determination of the quark-gluon string parameters from the data on pp,\n  pA and AA collisions at wide energy range using Bayesian Gaussian Process\n  Optimization",
        "authors": [
            "Vladimir Kovalenko"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Bayesian Gaussian Process Optimization can be considered as a method of the\ndetermination of the model parameters, based on the experimental data. In the\nrange of soft QCD physics, the processes of hadron and nuclear interactions\nrequire using phenomenological models containing many parameters. In order to\nminimize the computation time, the model predictions can be parameterized using\nGaussian Process regression, and then provide the input to the Bayesian\nOptimization. In this paper, the Bayesian Gaussian Process Optimization has\nbeen applied to the Monte Carlo model with string fusion. The parameters of the\nmodel are determined using experimental data on multiplicity and cross section\nof pp, pA and AA collisions at wide energy range. The results provide important\nconstraints on the transverse radius of the quark-gluon string ($r_{str}$) and\nthe mean multiplicity per rapidity from one string ($\\mu_0$).\n",
        "pdf_link": "http://arxiv.org/pdf/1902.11082v1"
    },
    {
        "title": "Accurate reconstruction of EBSD datasets by a multimodal data approach\n  using an evolutionary algorithm",
        "authors": [
            "Marie-Agathe Charpagne",
            "Florian Strub",
            "Tresa M. Pollock"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  A new method has been developed for the correction of the distortions and/or\nenhanced phase differentiation in Electron Backscatter Diffraction (EBSD) data.\nUsing a multi-modal data approach, the method uses segmented images of the\nphase of interest (laths, precipitates, voids, inclusions) on images gathered\nby backscattered or secondary electrons of the same area as the EBSD map. The\nproposed approach then search for the best transformation to correct their\nrelative distortions and recombines the data in a new EBSD file. Speckles of\nthe features of interest are first segmented in both the EBSD and image data\nmodes. The speckle extracted from the EBSD data is then meshed, and the\nCovariance Matrix Adaptation Evolution Strategy (CMA-ES) is implemented to\ndistort the mesh until the speckles superimpose. The quality of the matching is\nquantified via a score that is linked to the number of overlapping pixels in\nthe speckles. The locations of the points of the distorted mesh are compared to\nthose of the initial positions to create pairs of matching points that are used\nto calculate the polynomial function that describes the distortion the best.\nThis function is then applied to un-distort the EBSD data, and the phase\ninformation is inferred using the data of the segmented speckle. Fast and\nversatile, this method does not require any human annotation and can be applied\nto large datasets and wide areas. Besides, this method requires very few\nassumptions concerning the shape of the distortion function. It can be used for\nthe single compensation of the distortions or combined with the phase\ndifferentiation. The accuracy of this method is of the order of the pixel size.\nSome application examples in multiphase materials with feature sizes down to 1\n$\\mu$m are presented, including Ti-6Al-4V Titanium alloy, Rene 65 and additive\nmanufactured Inconel 718 Nickel-base superalloys.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.02988v2"
    },
    {
        "title": "Combined Neyman-Pearson Chi-square: An Improved Approximation to the\n  Poisson-likelihood Chi-square",
        "authors": [
            "Xiangpan Ji",
            "Wenqiang Gu",
            "Xin Qian",
            "Hanyu Wei",
            "Chao Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We describe an approximation to the widely-used Poisson-likelihood chi-square\nusing a linear combination of Neyman's and Pearson's chi-squares, namely\n\"combined Neyman-Pearson chi-square\" ($\\chi^2_{\\mathrm{CNP}}$). Through\nanalytical derivations and toy model simulations, we show that\n$\\chi^2_\\mathrm{CNP}$ leads to a significantly smaller bias on the best-fit\nmodel parameters compared to those using either Neyman's or Pearson's\nchi-square. When the computational cost of using the Poisson-likelihood\nchi-square is high, $\\chi^2_\\mathrm{CNP}$ provides a good alternative given its\nnatural connection to the covariance matrix formalism.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.07185v3"
    },
    {
        "title": "The peculiar statistical mechanics of Optimal Learning Machines",
        "authors": [
            "Matteo Marsili"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Optimal Learning Machines (OLM) are systems that extract maximally\ninformative representation of the environment they are in contact with, or of\nthe data they are presented. It has recently been suggested that these systems\nare characterised by an exponential distribution of energy levels. In order to\nunderstand the peculiar properties of OLM within a broader framework, I\nconsider an ensemble of optimisation problems over functions of many variables,\npart of which describe a sub-system and the rest account for its interaction\nwith a random environment. The number of states of the sub-system with a given\nvalue of the objective function obeys a stretched exponential distribution,\nwith exponent $\\gamma$, and the interaction part is drawn at random from the\nsame distribution, independently for each configuration of the whole system.\nSystems with $\\gamma=1$ then correspond to OLM, and we find that they sit at\nthe boundary between two regions with markedly different properties. For all\n$\\gamma>0$ the system exhibits a freezing phase transition. The transition is\ndiscontinuous for $\\gamma<1$ and it is continuous for $\\gamma>1$. The region\n$\\gamma>1$ corresponds to learnable energy landscapes and the behaviour of the\nsub-system becomes predictable as the size of the environment exceeds a\ncritical threshold. For $\\gamma<1$, instead, the energy landscape is\nunlearnable and the behaviour of the system becomes more and more unpredictable\nas the size of the environment increases. Sub-systems with $\\gamma=1$ (OLM)\nfeature a behaviour which is independent of the relative size of the\nenvironment. This is consistent with the expectation that efficient\nrepresentations should be largely independent of the level of detail of the\ndescription of the environment.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.09144v2"
    },
    {
        "title": "Estimating the Mutual Information between two Discrete, Asymmetric\n  Variables with Limited Samples",
        "authors": [
            "Damián G. Hernández",
            "Inés Samengo"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Determining the strength of non-linear statistical dependencies between two\nvariables is a crucial matter in many research fields. The established measure\nfor quantifying such relations is the mutual information. However, estimating\nmutual information from limited samples is a challenging task. Since the mutual\ninformation is the difference of two entropies, the existing Bayesian\nestimators of entropy may be used to estimate information. This procedure,\nhowever, is still biased in the severely under-sampled regime. Here we propose\nan alternative estimator that is applicable to those cases in which the\nmarginal distribution of one of the two variables---the one with minimal\nentropy---is well sampled. The other variable, as well as the joint and\nconditional distributions, can be severely undersampled. We obtain an estimator\nthat presents very low bias, outperforming previous methods even when the\nsampled data contain few coincidences. As with other Bayesian estimators, our\nproposal focuses on the strength of the interaction between two discrete\nvariables, without seeking to model the specific way in which the variables are\nrelated. A distinctive property of our method is that the main data statistics\ndetermining the amount of mutual information is the inhomogeneity of the\nconditional distribution of the low-entropy variable in those states (typically\nfew) in which the large-entropy variable registers coincidences.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.02034v1"
    },
    {
        "title": "A class of randomized Subset Selection Methods for large complex\n  networks",
        "authors": [
            "Amit Reza",
            "Richa Tripathi"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Most of the real world complex networks such as the Internet, World Wide Web\nand collaboration networks are huge; and to infer their structure and dynamics\none requires handling large connectivity (adjacency) matrices. Also, to find\nout the spectra of these networks, one needs to perform the EigenValue\nDecomposition(or Singular Value Decomposition for bipartite networks) of these\nlarge adjacency matrices or their Laplacian matrices. In the present work, we\nproposed randomized versions of the existing heuristics to infer the norm and\nthe spectrum of the adjacency matrices. In an earlier work [1], we used Subset\nSelection (SS) procedure to obtain the critical network structure which is\nsmaller in size and retains the properties of original networks in terms of its\nPrincipal Singular Vector and eigenvalue spectra. We now present a few\nrandomized versions of SS (RSS) and their time and space complexity calculation\non various benchmark and real-world networks. We find that the RSS based on\nusing QR decomposition instead of SVD in deterministic SS is the fastest. We\nevaluate the correctness and the performance speed after running these\nrandomized SS heuristics on test networks and comparing the results with\ndeterministic counterpart reported earlier. We find the proposed methods can be\nused effectively in large and sparse networks; they can be extended to analyse\nimportant network structure in dynamically evolving networks owing to their\nreduced time complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.04452v1"
    },
    {
        "title": "Iterative subtraction method for Feature Ranking",
        "authors": [
            "Paul Glaysher",
            "Judith M. Katzy",
            "Sitong An"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Training features used to analyse physical processes are often highly\ncorrelated and determining which ones are most important for the classification\nis a non-trivial tasks. For the use case of a search for a top-quark pair\nproduced in association with a Higgs boson decaying to bottom-quarks at the\nLHC, we compare feature ranking methods for a classification BDT. Ranking\nmethods, such as the BDT Selection Frequency commonly used in High Energy\nPhysics and the Permutational Performance, are compared with the\ncomputationally expense Iterative Addition and Iterative Removal procedures,\nwhile the latter was found to be the most performant.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05718v1"
    },
    {
        "title": "Information-theoretic measures for non-linear causality detection:\n  application to social media sentiment and cryptocurrency prices",
        "authors": [
            "Z. Keskin",
            "T. Aste"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Information transfer between time series is calculated by using the\nasymmetric information-theoretic measure known as transfer entropy. Geweke's\nautoregressive formulation of Granger causality is used to find linear transfer\nentropy, and Schreiber's general, non-parametric, information-theoretic\nformulation is used to detect non-linear transfer entropy.\n  We first validate these measures against synthetic data. Then we apply these\nmeasures to detect causality between social sentiment and cryptocurrency\nprices. We perform significance tests by comparing the information transfer\nagainst a null hypothesis, determined via shuffled time series, and calculate\nthe Z-score. We also investigate different approaches for partitioning in\nnonparametric density estimation which can improve the significance of results.\n  Using these techniques on sentiment and price data over a 48-month period to\nAugust 2018, for four major cryptocurrencies, namely bitcoin (BTC), ripple\n(XRP), litecoin (LTC) and ethereum (ETH), we detect significant information\ntransfer, on hourly timescales, in directions of both sentiment to price and of\nprice to sentiment. We report the scale of non-linear causality to be an order\nof magnitude greater than linear causality.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05740v2"
    },
    {
        "title": "Uncertainty in the Predictive Capability of Detectors that Process\n  Waveforms from Explosions",
        "authors": [
            "Joshua D Carmichael",
            "Robert J Nemzek"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Explosions near ground generate multiple geophysical waveforms in the\nradiation-dominated range of their signature fields. Multi-phenomological\nexplosion monitoring (MultiPEM) at these ranges requires the predictive\ncapability to forecast trigger rates of digital detectors that process such\nwaveform data, and thereby accurately anticipate the probability that\nhypothetical explosions can be identified in operations. To confront this\nchallenge, we derive and compare the predicted and observed performance of\nthree digital detectors that process radio, acoustic and seismic waveform data\nthat record a small, aboveground explosion. We measure this comparison with the\npeak range in magnitude (magnitude discrepancy) over which different\nperformance curves report the same probability of detection, within an interval\nof moderate detection probability, and thereby quantify solutions to three\ntopical monitoring questions. In particular, our solutions (1) demonstrate how\nempirically parameterized detectors that operate in a variable noisy\nenvironments provide fair-to-very good forecasting capability to detect small\nexplosions, (2) show that the observed performance of a particular waveform\ndetector can better forecast performance curves constructed from different\nobservations, when compared to theoretical performance curves, and (3) provide\nan upper bound on detection uncertainty, in terms of a physical source\nattribute (magnitude)\n",
        "pdf_link": "http://arxiv.org/pdf/1906.09350v1"
    },
    {
        "title": "Data-driven prediction of vortex-induced vibration response of marine\n  risers subjected to three-dimensional current",
        "authors": [
            "Signe Riemer-Sørensen",
            "Jie Wu",
            "Halvor Lie",
            "Svein Sævik",
            "Sang-Woo Kim"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Slender marine structures such as deep-water marine risers are subjected to\ncurrents and will normally experience Vortex Induced Vibrations (VIV), which\ncan cause fast accumulation of fatigue damage. The ocean current is often\nthree-dimensional (3D), i.e., the direction and magnitude of the current vary\nthroughout the water column. Today, semi-empirical tools are used by the\nindustry to predict VIV induced fatigue on risers. The load model and\nhydrodynamic parameters in present VIV prediction tools are developed based on\ntwo-dimensional (2D) flow conditions, as it is challenging to consider the\neffect of 3D flow along the risers. Accordingly, the current profiles must be\npurposely made 2D during the design process, which leads to significant\nuncertainty in the prediction results. Further, due to the limitations in the\nlaboratory, VIV model tests are mostly carried out under 2D flow conditions and\nthus little experimental data exist to document VIV response of riser subjected\nto varying directions of the current. However, a few experiments have been\nconducted with 3D current. We have used results from one of these experiments\nto investigate how well 1) traditional and 2) an alternative method based on a\ndata driven prediction can describe VIV in 3D currents. Data driven modelling\nis particularly suited for complicated problems with many parameters and\nnon-linear relationships. We have applied a data clustering algorithm to the\nexperimental 3D flow data in order to identify measurable parameters that can\ninfluence responses. The riser responses are grouped based on their statistical\ncharacteristics, which relate to the direction of the flow. Furthermore we fit\na random forest regression model to the measured VIV response and compare its\nperformance with the predictions of existing VIV prediction tools (VIVANA-FD).\n",
        "pdf_link": "http://arxiv.org/pdf/1906.11177v1"
    },
    {
        "title": "A geometric approach to the transport of discontinuous densities",
        "authors": [
            "Caroline Moosmüller",
            "Felix Dietrich",
            "Ioannis G. Kevrekidis"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Different observations of a relation between inputs (\"sources\") and outputs\n(\"targets\") are often reported in terms of histograms (discretizations of the\nsource and the target densities). Transporting these densities to each other\nprovides insight regarding the underlying relation. In (forward) uncertainty\nquantification, one typically studies how the distribution of inputs to a\nsystem affects the distribution of the system responses. Here, we focus on the\nidentification of the system (the transport map) itself, once the input and\noutput distributions are determined, and suggest a modification of current\npractice by including data from what we call \"an observation process\". We\nhypothesize that there exists a smooth manifold underlying the relation; the\nsources and the targets are then partial observations (possibly projections) of\nthis manifold. Knowledge of such a manifold implies knowledge of the relation,\nand thus of \"the right\" transport between source and target observations. When\nthe source-target observations are not bijective (when the manifold is not the\ngraph of a function over both observation spaces, either because folds over\nthem give rise to density singularities, or because it marginalizes over\nseveral observables), recovery of the manifold is obscured. Using ideas from\nattractor reconstruction in dynamical systems, we demonstrate how additional\ninformation in the form of short histories of an observation process can help\nus recover the underlying manifold. The types of additional information\nemployed and the relation to optimal transport based solely on density\nobservations is illustrated and discussed, along with limitations in the\nrecovery of the true underlying relation.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.08260v2"
    },
    {
        "title": "Spectral data analysis methods for the two-dimensional imaging\n  diagnostics",
        "authors": [
            "Minjun J. Choi"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Some spectral data analysis methods that are useful for the two-dimensional\nimaging diagnostics data are introduced. It is shown that the frequency\nspectrum, the local dispersion relation, the flow shear, and the nonlinear\nenergy transfer rates can be estimated using the proper analysis methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.09184v3"
    },
    {
        "title": "The permutation entropy and its applications on fire tests data",
        "authors": [
            "Flavia-Corina Mitroi-Symeonidis",
            "Ion Anghel",
            "Octavian Lalu",
            "Constantin Popa"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Based on the data gained from a full-scale experiment, the order/disorder\ncharacteristics of the compartment fire temperatures are analyzed. Among the\nknown permutation/encoding type entropies used to analyze time series, we look\nfor those that fit better in the fire phenomena. The literature in its major\npart does not focus on time series with data collected during full-scale fire\nexperiments, therefore we do not only perform our analysis and report the\nresults, but also discuss methods, algorithms, the novelty of our entropic\napproach and details behind the scene. The embedding dimension selection in the\ncomplexity evaluation is also discussed. Finally, more research directions are\nproposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.04274v1"
    },
    {
        "title": "Unfolding as Quantum Annealing",
        "authors": [
            "Kyle Cormier",
            "Riccardo Di Sipio",
            "Peter Wittek"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  High-energy physics is replete with hard computational problems and it is one\nof the areas where quantum computing could be used to speed up calculations. We\npresent an implementation of likelihood-based regularized unfolding on a\nquantum computer. The inverse problem is recast in terms of quadratic\nunconstrained binary optimization (QUBO), which has the same form of the Ising\nhamiltonian and hence it is solvable on a programmable quantum annealer. We\ntested the method using a model that captures the essence of the problem, and\ncompared the results with a baseline method commonly used in precision\nmeasurements at the Large Hadron Collider (LHC) at CERN. The unfolded\ndistribution is in very good agreement with the original one. We also show how\nthe method can be extended to include the effect of nuisance parameters\nrepresenting sources of systematic uncertainties affecting the measurement.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.08519v2"
    },
    {
        "title": "libGroomRL: Reinforcement Learning for Jets",
        "authors": [
            "Stefano Carrazza",
            "Frédéric A. Dreyer"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  In these proceedings, we present a library allowing for straightforward calls\nin C++ to jet grooming algorithms trained with deep reinforcement learning. The\nRL agent is trained with a reward function constructed to optimize the groomed\njet properties, using both signal and background samples in a simultaneous\nmulti-level training. We show that the grooming algorithm derived from the deep\nRL agent can match state-of-the-art techniques used at the Large Hadron\nCollider, resulting in improved mass resolution for boosted objects. Given a\nsuitable reward function, the agent learns how to train a policy which\noptimally removes soft wide-angle radiation, allowing for a modular grooming\ntechnique that can be applied in a wide range of contexts. The neural network\ntrained with GroomRL can be used in a FastJet analysis through the libGroomRL\nC++ library.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.00410v1"
    },
    {
        "title": "Parametrizing the Detector Response with Neural Networks",
        "authors": [
            "Sanha Cheong",
            "Aviv Cukierman",
            "Benjamin Nachman",
            "Murtaza Safdari",
            "Ariel Schwartzman"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  In high energy physics, characterizing the response of a detector to\nradiation is one of the most important and basic experimental tasks. In many\ncases, this task is accomplished by parameterizing summary statistics of the\nfull detector response probability density. The parameterized detector response\ncan then be used for calibration as well as for directly improving physics\nanalysis sensitivity. This paper discusses how to parameterize summary\nstatistics of the detector response using neural networks. In particular,\nneural networks are powerful tools for incorporating multidimensional data and\nthe loss function used during training determines which summary statistic is\nlearned. One common summary statistic that has not been combined with deep\nlearning (as far as the authors are aware) is the mode. A neural network-based\napproach to mode learning is proposed and empirically demonstrated in the\ncontext of high energy jet calibrations. Altogether, the neural network-based\ntoolkit for detector response parameterization can enhance the utility of data\ncollected at high energy physics experiments and beyond.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.03773v3"
    },
    {
        "title": "Iterative procedure for network inference",
        "authors": [
            "Gloria Cecchini",
            "Bjoern Schelter"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  When a network is reconstructed from data, two types of errors can occur:\nfalse positive and false negative errors about the presence or absence of\nlinks. In this paper, the vertex degree distribution of the true underlying\nnetwork is analytically reconstructed using an iterative procedure. Such\nprocedure is based on the inferred network and estimates for the probabilities\n$\\alpha$ and $\\beta$ of type I and type II errors, respectively. The iteration\nprocedure consists of choosing various values for $\\alpha$ to perform the\niteration steps of the network reconstruction. For the first step, the standard\nvalue for $\\alpha$ of 0.05 can be chosen as an example. The result of this\nfirst step gives a first estimate of the network topology of interest. For the\nsecond iteration step the value for $\\alpha$ is adjusted according to the\nfindings of the first step. This procedure is iterated, ultimately leading to a\nreconstruction of the vertex degree distribution tailored to its previously\nunknown network topology.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.06593v2"
    },
    {
        "title": "A 5D, polarised, Bethe-Heitler event generator for $γ\\to\n  μ^+μ^-$ conversion",
        "authors": [
            "Denis Bernard"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  I describe a five-dimensional, polarised, Bethe-Heitler event generator of\n$\\gamma$-ray conversions to $\\mu^+\\mu^-$, based on a generator for conversion\nto $e^+e^-$ developed in the past. Verifications are performed from\nclose-to-threshold to high energies.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.12501v1"
    },
    {
        "title": "Unrecognized Sources of Uncertainties (USU) in Experimental Nuclear Data",
        "authors": [
            "R. Capote",
            "S. Badikov",
            "A. Carlson",
            "I. Duran",
            "F. Gunsing",
            "D. Neudecker",
            "V. G. Pronyaev",
            "P. Schillebeeckx",
            "G. Schnabel",
            "D. L. Smith",
            "A. Wallner"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Evaluated nuclear data uncertainties are often perceived as unrealistic, most\noften because they are thought to be too small. The impact of this issue in\napplied nuclear science has been discussed widely in recent years. Commonly\nsuggested causes are: poor estimates of specific error components, neglect of\nuncertainty correlations, and overlooked known error sources. However,\ninstances have been reported where very careful, objective assessments of all\nknown error sources have been made with realistic error magnitudes and\ncorrelations provided, yet the resulting evaluated uncertainties still appear\nto be inconsistent with observed scatter of predicted mean values. These\ndiscrepancies might be attributed to significant unrecognized sources of\nuncertainty (USU) that limit the accuracy to which these physical quantities\ncan be determined. The objective of our work has been to develop procedures for\nrevealing and including USU estimates in nuclear data evaluations involving\nexperimental input data. We conclude that the presence of USU may be revealed,\nand estimates of magnitudes made, through quantitative analyses. This paper\nidentifies several specific clues that can be explored by evaluators in\nidentifying the existence of USU. It then describes numerical procedures to\ngenerate quantitative estimates of USU magnitudes. Key requirements for these\nprocedures to be viable are that sufficient numbers of data points be\navailable, for statistical reasons, and that additional supporting information\nabout the measurements be provided by the experimenters. Realistic examples are\ndescribed to illustrate these procedures and demonstrate their outcomes as well\nas limitations. Our work strongly supports the view that USU is an important\nissue in nuclear data evaluation, with significant consequences for\napplications, and that this topic warrants further investigation by the nuclear\nscience community.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.01825v1"
    },
    {
        "title": "Dynamical Heart Beat Correlations during Running",
        "authors": [
            "Matti Molkkari",
            "Giorgio Angelotti",
            "Thorsten Emig",
            "Esa Räsänen"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Fluctuations of the human heart beat constitute a complex system that has\nbeen studied mostly under resting conditions using conventional time series\nanalysis methods. During physical exercise, the variability of the fluctuations\nis reduced, and the time series of beat-to-beat RR intervals (RRIs) become\nhighly non-stationary. Here we develop a dynamical approach to analyze the time\nevolution of RRI correlations in running across various training and racing\nevents under real-world conditions. In particular, we introduce dynamical\ndetrended fluctuation analysis and dynamical partial autocorrelation functions,\nwhich are able to detect real-time changes in the scaling and correlations of\nthe RRIs as functions of the scale and the lag. We relate these changes to the\nexercise intensity quantified by the heart rate (HR). Beyond subject-specific\nHR thresholds the RRIs show multiscale anticorrelations with both universal and\nindividual scale-dependent structure that is potentially affected by the stride\nfrequency. These preliminary results are encouraging for future applications of\nthe dynamical statistical analysis in exercise physiology and cardiology, and\nthe presented methodology is also applicable across various disciplines.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.04218v3"
    },
    {
        "title": "Intrinsic regularization effect in Bayesian nonlinear regression scaled\n  by observed data",
        "authors": [
            "Satoru Tokuda",
            "Kenji Nagata",
            "Masato Okada"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Occam's razor is a guiding principle that models should be simple enough to\ndescribe observed data. While Bayesian model selection (BMS) embodies it by the\nintrinsic regularization effect (IRE), how observed data scale the IRE has not\nbeen fully understood. In the nonlinear regression with conditionally\nindependent observations, we show that the IRE is scaled by observations'\nfineness, defined by the amount and quality of observed data. We introduce an\nobservable that quantifies the IRE, referred to as the Bayes specific heat,\ninspired by the correspondence between statistical inference and statistical\nphysics. We derive its scaling relation to observations' fineness. We\ndemonstrate that the optimal model chosen by the BMS changes at critical values\nof observations' fineness, accompanying the IRE's variation. The changes are\nfrom choosing a coarse-grained model to a fine-grained one as observations'\nfineness increases. Our findings expand an understanding of BMS's typicality\nwhen observed data are insufficient.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.01335v2"
    },
    {
        "title": "Machine learning technique to improve anti-neutrino detection efficiency\n  for the ISMRAN experiment",
        "authors": [
            "D. Mulmule",
            "P. K. Netrakanti",
            "L. M. Pant",
            "B. K. Nayak"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The Indian Scintillator Matrix for Reactor Anti-Neutrino detection - ISMRAN\nexperiment aims to detect electron anti-neutrinos ($\\bar\\nu_e$) emitted from a\nreactor via inverse beta decay reaction (IBD). The setup, consisting of 1 ton\nsegmented Gadolinium foil wrapped plastic scintillator array, is planned for\nremote reactor monitoring and sterile neutrino search. The detection of prompt\npositron and delayed neutron from IBD will provide the signature of $\\bar\\nu_e$\nevent in ISMRAN. The number of segments with energy deposit\n($\\mathrm{N_{bars}}$) and sum total of these deposited energies are used as\ndiscriminants for identifying prompt positron event and delayed neutron capture\nevent. However, a simple cut based selection of above variables leads to a low\n$\\bar\\nu_e$ signal detection efficiency due to overlapping region of\n$\\mathrm{N_{bars}}$ and sum energy for the prompt and delayed events.\nMultivariate analysis (MVA) tools, employing variables suitably tuned for\ndiscrimination, can be useful in such scenarios. In this work we report the\nresults from an application of artificial neural network -- the multilayer\nperceptron (MLP), particularly the Bayesian extension -- MLPBNN, to the\nsimulated signal and background events in ISMRAN. The results from application\nof MLP to classify prompt positron events from delayed neutron capture events\non Hydrogen, Gadolinium nuclei and also from the typical reactor $\\gamma$-ray\nand fast neutron backgrounds is reported. An enhanced efficiency of\n$\\sim$91$\\%$ with a background rejection of $\\sim$73$\\%$ for prompt selection\nand an efficiency of $\\sim$89$\\%$ with a background rejection of $\\sim$71$\\%$\nfor the delayed capture event, is achieved using the MLPBNN classifier for the\nISMRAN experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.01628v2"
    },
    {
        "title": "Fully Bayesian Unfolding with Regularization",
        "authors": [
            "Petr Baron"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Fully Bayesian Unfolding differs from other unfolding methods by providing\nthe full posterior probability of unfolded spectra for each bin. We extended\nthe method for the feature of regularization which could be helpful for\nunfolding non-smooth, over-binned or generally non-standard shaped spectra. To\ndecrease the computation time, the iteration process is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.05877v1"
    },
    {
        "title": "On a curious bias arising when the $\\sqrt{χ^2/ν}$ scaling\n  prescription is first applied to a sub-sample of the individual results",
        "authors": [
            "Giulio D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  As it is well known, the standard deviation of a weighted average depends\nonly on the individual standard deviations, but not on the dispersion of the\nvalues around the mean. This property leads sometimes to the embarrassing\nsituation in which the combined result 'looks' somehow at odds with the\nindividual ones. A practical way to cure the problem is to enlarge the\nresulting standard deviation by the $\\sqrt{\\chi^2/\\nu}$ scaling, a prescription\nemployed with arbitrary criteria on when to apply it and which individual\nresults to use in the combination. But the `apparent' discrepancy between the\ncombined result and the individual ones often remains. Moreover this rule does\nnot affect the resulting `best value', even if the pattern of the individual\nresults is highly skewed. In addition to these reasons of dissatisfaction,\nshared by many practitioners, the method causes another issue, recently noted\non the published measurements of the charged kaon mass. It happens in fact\nthat, if the prescription is applied twice, i.e. first to a sub-sample of the\nindividual results and subsequently to the entire sample, then a bias on the\nresult of the overall combination is introduced. The reason is that the\nprescription does not guaranty statistical sufficiency, whose importance is\nreminded in this script, written with a didactic spirit, with some historical\nnotes and with a language to which most physicists are accustomed. The\nconclusion contains general remarks on the effective presentation of the\nexperimental findings and a pertinent puzzle is proposed in the Appendix.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.07562v1"
    },
    {
        "title": "SquidLab -- a user-friendly program for background subtraction and\n  fitting of magnetization data",
        "authors": [
            "Matthew J. Coak",
            "Cheng Liu",
            "David M. Jarvis",
            "Seunghyun Park",
            "Matthew J. Cliffe",
            "Paul A. Goddard"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We present an open-source program free to download for academic use with full\nuser-friendly graphical interface for performing flexible and robust background\nsubtraction and dipole fitting on magnetization data. For magnetic samples with\nsmall moment sizes or sample environments with large or asymmetric magnetic\nbackgrounds, it can become necessary to separate background and sample\ncontributions to each measured raw voltage measurement before fitting the\ndipole signal to extract magnetic moments. Originally designed for use with\npressure cells on a Quantum Design MPMS3 SQUID magnetometer, SquidLab is a\nmodular object-oriented platform implemented in Matlab with a range of\nimporters for different widely-available magnetometer systems (including MPMS,\nMPMS-XL, MPMS-IQuantum, MPMS3 and S700X models), and has been tested with a\nbroad variety of background and signal types. The software allows background\nsubtraction of baseline signals, signal preprocessing, and performing fits to\ndipole data using Levenberg-Marquadt non-linear least squares, or a singular\nvalue decomposition linear algebra algorithm which excels at picking out noisy\nor weak dipole signals. A plugin system allows users to easily extend the\nbuilt-in functionality with their own importers, processes or fitting\nalgorithms. SquidLab can be downloaded, under Academic License, from the\nUniversity of Warwick depository (wrap.warwick.ac.uk/129665).\n",
        "pdf_link": "http://arxiv.org/pdf/2001.09037v2"
    },
    {
        "title": "BLUE: combining correlated estimates of physics observables within ROOT\n  using the Best Linear Unbiased Estimate method",
        "authors": [
            "Richard Nisius"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  This software performs the combination of $m$ correlated estimates of $n$\nphysics observables ($m\\ge n$) using the Best Linear Unbiased Estimate (BLUE)\nmethod. It is implemented as a C++ class, to be used within the ROOT analysis\npackage. It features easy disabling of specific estimates or uncertainty\nsources, the investigation of different correlation assumptions, and allows\nperforming combinations according to the importance of the estimates. This\nenables systematic investigations of the combination on details of the\nmeasurements from within the software, without touching the input.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.10310v2"
    },
    {
        "title": "Super Resolution Convolutional Neural Network for Feature Extraction in\n  Spectroscopic Data",
        "authors": [
            "Han Peng",
            "Xiang Gao",
            "Yu He",
            "Yiwei Li",
            "Yuchen Ji",
            "Chuhang Liu",
            "Sandy A. Ekahana",
            "Ding Pei",
            "Zhongkai Liu",
            "Zhixun Shen",
            "Yulin Chen"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Two dimensional (2D) peak finding is a common practice in data analysis for\nphysics experiments, which is typically achieved by computing the local\nderivatives. However, this method is inherently unstable when the local\nlandscape is complicated, or the signal-to-noise ratio of the data is low. In\nthis work, we propose a new method in which the peak tracking task is\nformalized as an inverse problem, thus can be solved with a convolutional\nneural network (CNN). In addition, we show that the underlying physics\nprinciple of the experiments can be used to generate the training data. By\ngeneralizing the trained neural network on real experimental data, we show that\nthe CNN method can achieve comparable or better results than traditional\nderivative based methods. This approach can be further generalized in different\nphysics experiments when the physical process is known.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.10908v1"
    },
    {
        "title": "On the impact of selected modern deep-learning techniques to the\n  performance and celerity of classification models in an experimental\n  high-energy physics use case",
        "authors": [
            "Giles Chatham Strong"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Beginning from a basic neural-network architecture, we test the potential\nbenefits offered by a range of advanced techniques for machine learning, in\nparticular deep learning, in the context of a typical classification problem\nencountered in the domain of high-energy physics, using a well-studied dataset:\nthe 2014 Higgs ML Kaggle dataset. The advantages are evaluated in terms of both\nperformance metrics and the time required to train and apply the resulting\nmodels. Techniques examined include domain-specific data-augmentation, learning\nrate and momentum scheduling, (advanced) ensembling in both model-space and\nweight-space, and alternative architectures and connection methods. Following\nthe investigation, we arrive at a model which achieves equal performance to the\nwinning solution of the original Kaggle challenge, whilst being significantly\nquicker to train and apply, and being suitable for use with both GPU and CPU\nhardware setups. These reductions in timing and hardware requirements\npotentially allow the use of more powerful algorithms in HEP analyses, where\nmodels must be retrained frequently, sometimes at short notice, by small groups\nof researchers with limited hardware resources. Additionally, a new wrapper\nlibrary for PyTorch called LUMIN is presented, which incorporates all of the\ntechniques studied.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.01427v4"
    },
    {
        "title": "Object condensation: one-stage grid-free multi-object reconstruction in\n  physics detectors, graph and image data",
        "authors": [
            "Jan Kieseler"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  High-energy physics detectors, images, and point clouds share many\nsimilarities in terms of object detection. However, while detecting an unknown\nnumber of objects in an image is well established in computer vision, even\nmachine learning assisted object reconstruction algorithms in particle physics\nalmost exclusively predict properties on an object-by-object basis. Traditional\napproaches from computer vision either impose implicit constraints on the\nobject size or density and are not well suited for sparse detector data or rely\non objects being dense and solid. The object condensation method proposed here\nis independent of assumptions on object size, sorting or object density, and\nfurther generalises to non-image-like data structures, such as graphs and point\nclouds, which are more suitable to represent detector signals. The pixels or\nvertices themselves serve as representations of the entire object, and a\ncombination of learnable local clustering in a latent space and confidence\nassignment allows one to collect condensates of the predicted object properties\nwith a simple algorithm. As proof of concept, the object condensation method is\napplied to a simple object classification problem in images and used to\nreconstruct multiple particles from detector signals. The latter results are\nalso compared to a classic particle flow approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.03605v3"
    },
    {
        "title": "Critical Temperature Prediction for a Superconductor: A Variational\n  Bayesian Neural Network Approach",
        "authors": [
            "Thanh Dung Le",
            "Rita Noumeir",
            "Huu Luong Quach",
            "Ji Hyung Kim",
            "Jung Ho Kim",
            "Ho Min Kim"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Much research in recent years has focused on using empirical machine learning\napproaches to extract useful insights on the structure-property relationships\nof superconductor material. Notably, these approaches are bringing extreme\nbenefits when superconductivity data often come from costly and arduously\nexperimental work. However, this assessment cannot be based solely on an open\nblack-box machine learning, which is not fully interpretable, because it can be\ncounter-intuitive to understand why the model may give an appropriate response\nto a set of input data for superconductivity characteristic analyses, e.g.,\ncritical temperature. The purpose of this study is to describe and examine an\nalternative approach for predicting the superconducting transition temperature\n$T_c$ from SuperCon database obtained by Japan's National Institute for\nMaterials Science. We address a generative machine-learning framework called\nVariational Bayesian Neural Network using superconductors chemical elements and\nformula to predict $T_c$. In such a context, the importance of the paper in\nfocus is twofold. First, to improve the interpretability, we adopt a\nvariational inference to approximate the distribution in latent parameter space\nfor the generative model. It statistically captures the mutual correlation of\nsuperconductor compounds and; then, gives the estimation for the $T_c$. Second,\na stochastic optimization algorithm, which embraces a statistical inference\nnamed Monte Carlo sampler, is utilized to optimally approximate the proposed\ninference model, ultimately determine and evaluate the predictive performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.04977v1"
    },
    {
        "title": "Uncertainty Quantification of Mode Shape Variation Utilizing Multi-Level\n  Multi-Response Gaussian Process",
        "authors": [
            "Kai Zhou",
            "Jiong Tang"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Mode shape information play the essential role in deciding the spatial\npattern of vibratory response of a structure. The uncertainty quantification of\nmode shape, i.e., predicting mode shape variation when the structure is\nsubjected to uncertainty, can provide guidance for robust design and control.\nNevertheless, computational efficiency is a challenging issue. Direct Monte\nCarlo simulation is unlikely to be feasible especially for a complex structure\nwith large number of degrees of freedom. In this research, we develop a new\nprobabilistic framework built upon Gaussian process meta-modeling architecture\nto analyze mode shape variation. To expedite the generation of input dataset\nfor meta-model establishment, a multi-level strategy is adopted which can blend\na large amount of low-fidelity data acquired from order-reduced analysis with a\nsmall amount of high-fidelity data produced by high-dimensional full finite\nelement analysis. To take advantage of the intrinsic relation of spatial\ndistribution of mode shape, a multi-response strategy is incorporated to\npredict mode shape variation at different locations simultaneously. These yield\na multi-level, multi-response Gaussian process that can efficiently and\naccurately quantify the effect of structural uncertainty to mode shape\nvariation. Comprehensive case studies are carried out for demonstration and\nvalidation.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.09287v1"
    },
    {
        "title": "Reconstructing particle number distributions with convoluting volume\n  fluctuations",
        "authors": [
            "ShinIchi Esumi",
            "Kana Nakagawa",
            "Toshihiro Nonaka"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We propose methods to reconstruct particle distributions with and without\nconsidering initial volume fluctuations. This approach enables us to correct\nfor detector efficiencies and initial volume fluctuations simultaneously. Our\nstudy suggests such a tool could investigate the possible bimodal structure of\nnet-proton distribution in Au+Au collisions at $\\sqrt{s_{\\rm NN}}=$7.7 GeV a\nsignature of first-order phase transition and critical point\n[arXiv:1804.04463,arXiv:1811.04456].\n",
        "pdf_link": "http://arxiv.org/pdf/2002.11253v4"
    },
    {
        "title": "Compressive Sensing for Dynamic XRF Scanning",
        "authors": [
            "George Kourousias",
            "Fulvio Billè",
            "Roberto Borghes",
            "Antonio Alborini",
            "Simone Sala",
            "Roberto Alberti",
            "Alessandra Gianoncelli"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  X-Ray Fluorescence (XRF) scanning is a widespread technique of high\nimportance and impact since it provides chemical composition maps crucial for\nseveral scientific investigations. There are continuous requirements for\nlarger, faster and highly resolved acquisitions in order to study complex\nstructures. Among the scientific applications that benefit from it, some of\nthem, such as wide scale brain imaging, are prohibitively difficult due to time\nconstraints. However, typically the overall XRF imaging performance is\nimproving through technological progress on XRF detectors and X-ray sources.\nThis paper suggests an additional approach where XRF scanning is performed in a\nsparse way by skipping specific points or by varying dynamically acquisition\ntime or other scan settings in a conditional manner. This paves the way for\nCompressive Sensing in XRF scans where data are acquired in a reduced manner\nallowing for challenging experiments, currently not feasible with the\ntraditional scanning strategies. A series of different compressive sensing\nstrategies for dynamic scans are presented here. A proof of principle\nexperiment was performed at the TwinMic beamline of Elettra synchrotron. The\noutcome demonstrates the potential of Compressive Sensing for dynamic scans,\nsuggesting its use in challenging scientific experiments while proposing a\ntechnical solution for beamline acquisition software.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.04198v2"
    },
    {
        "title": "A Comprehensive Monte Carlo Framework for Jet-Quenching",
        "authors": [
            "R. A. Soltz"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  This article presents the motivation for developing a comprehensive modeling\nframework in which different models and parameter inputs can be compared and\nevaluated for a large range of jet-quenching observables measured in\nrelativistic heavy-ion collisions at RHIC and the LHC. The concept of a\nframework us discussed within the context of recent efforts by the JET\nCollaboration, the authors of JEWEL, and the JETSCAPE collaborations. The\nframework ingredients for each of these approaches is presented with a sample\nof important results from each. The role of advanced statistical tools in\ncomparing models to data is also discussed, along with the need for a more\ndetailed accounting of correlated errors in experimental results.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.11728v1"
    },
    {
        "title": "Optimising HEP parameter fits via Monte Carlo weight derivative\n  regression",
        "authors": [
            "Andrea Valassi"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  HEP event selection is traditionally considered a binary classification\nproblem, involving the dichotomous categories of signal and background. In\ndistribution fits for particle masses or couplings, however, signal events are\nnot all equivalent, as the signal differential cross section has different\nsensitivities to the measured parameter in different regions of phase space. In\nthis paper, I describe a mathematical framework for the evaluation and\noptimization of HEP parameter fits, where this sensitivity is defined on an\nevent-by-event basis, and for MC events it is modeled in terms of their MC\nweight derivatives with respect to the measured parameter. Minimising the\nstatistical error on a measurement implies the need to resolve (i.e. separate)\nevents with different sensitivities, which ultimately represents a\nnon-dichotomous classification problem. Since MC weight derivatives are not\navailable for real data, the practical strategy I suggest consists in training\na regressor of weight derivatives against MC events, and then using it as an\noptimal partitioning variable for 1-dimensional fits of data events. This\nCHEP2019 paper is an extension of the study presented at CHEP2018: in\nparticular, event-by-event sensitivities allow the exact computation of the\n\"FIP\" ratio between the Fisher information obtained from an analysis and the\nmaximum information that could possibly be obtained with an ideal detector.\nUsing this expression, I discuss the relationship between FIP and two metrics\ncommonly used in Meteorology (Brier score and MSE), and the importance of\n\"sharpness\" both in HEP and in that domain. I finally point out that HEP\ndistribution fits should be optimized and evaluated using probabilistic metrics\n(like FIP or MSE), whereas ranking metrics (like AUC) or threshold metrics\n(like accuracy) are of limited relevance for these specific problems.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.12853v2"
    },
    {
        "title": "Response and Uncertainty of the Parabolic Variance PVAR to Non-Integer\n  Exponents of the Power Law",
        "authors": [
            "François Vernotte",
            "Siyuan Chen",
            "Enrico Rubiola"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Oscillator fluctuations are described as the phase or frequency noise\nspectrum, or in terms of a wavelet variance as a function of the measurement\ntime. The spectrum is generally approximated by the `power law,' i.e., a\nLaurent polynomial with integer exponents of the frequency. This article\nextends the domain of application of PVAR, a wavelet variance which uses the\nlinear regression on phase data to estimate the frequency, and called\n`parabolic' because such regression is equivalent to a parabolic-shaped weight\nfunction applied to frequency fluctuations. In turn, PVAR is relevant in that\nit improves on the widely-used Modified Allan variance (MVAR) enabling the\ndetection of the same noise processes at the same confidence level in a shorter\nmeasurement time. More specifically, we provide (i) the analytical expression\nof the response of the PVAR to the frequency-noise spectrum in the general case\nof non-integer exponents of the frequency, and (ii) a useful approximate\nexpression of the statistical uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.13631v3"
    },
    {
        "title": "Pileup corrections on higher-order cumulants",
        "authors": [
            "Toshihiro Nonaka",
            "Masakiyo Kitazawa",
            "ShinIchi Esumi"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We propose a method to remove the contributions of pileup events from\nhigher-order cumulants and moments of event-by-event particle distributions.\nAssuming that the pileup events are given by the superposition of two\nindependent single-collision events, we show that the true moments in each\nmultiplicity bin can be obtained recursively from lower multiplicity events. In\nthe correction procedure the necessary information are only the probabilities\nof pileup events. Other terms are extracted from the experimental data. We\ndemonstrate that the true cumulants can be reconstructed successfully by this\nmethod in simple models. Systematics on trigger inefficiencies and correction\nparameters are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.15809v1"
    },
    {
        "title": "Active Importance Sampling for Variational Objectives Dominated by Rare\n  Events: Consequences for Optimization and Generalization",
        "authors": [
            "Grant M. Rotskoff",
            "Andrew R. Mitchell",
            "Eric Vanden-Eijnden"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Deep neural networks, when optimized with sufficient data, provide accurate\nrepresentations of high-dimensional functions; in contrast, function\napproximation techniques that have predominated in scientific computing do not\nscale well with dimensionality. As a result, many high-dimensional sampling and\napproximation problems once thought intractable are being revisited through the\nlens of machine learning. While the promise of unparalleled accuracy may\nsuggest a renaissance for applications that require parameterizing\nrepresentations of complex systems, in many applications gathering sufficient\ndata to develop such a representation remains a significant challenge. Here we\nintroduce an approach that combines rare events sampling techniques with neural\nnetwork optimization to optimize objective functions that are dominated by rare\nevents. We show that importance sampling reduces the asymptotic variance of the\nsolution to a learning problem, suggesting benefits for generalization. We\nstudy our algorithm in the context of learning dynamical transition pathways\nbetween two states of a system, a problem with applications in statistical\nphysics and implications in machine learning theory. Our numerical experiments\ndemonstrate that we can successfully learn even with the compounding\ndifficulties of high-dimension and rare data.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.06334v2"
    },
    {
        "title": "Practical Guide of Using Kendall's τ in the Context of Forecasting\n  Critical Transitions",
        "authors": [
            "Shiyang Chen",
            "Amin Ghadami",
            "Bogdan I. Epureanu"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Recent studies demonstrate that trends in indicators extracted from measured\ntime series can indicate approaching to an impending transition. Kendall's\n{\\tau} coefficient is often used to study the trend of statistics related to\nthe critical slowing down phenomenon and other methods to forecast critical\ntransitions. Because statistics are estimated from time series, the values of\nKendall's {\\tau} are affected by parameters such as window size, sample rate\nand length of the time series, resulting in challenges and uncertainties in\ninterpreting results. In this study, we examine the effects of different\nparameters on the distribution of the trend obtained from Kendall's {\\tau}, and\nprovide insights into how to choose these parameters. We also suggest the use\nof the non-parametric Mann-Kendall test to evaluate the significance of a\nKendall's {\\tau} value. The non-parametric test is computationally much faster\ncompared to the traditional parametric ARMA test.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.02478v1"
    },
    {
        "title": "Pulse Shape Discrimination in CUPID-Mo using Principal Component\n  Analysis",
        "authors": [
            "R. Huang",
            "E. Armengaud",
            "C. Augier",
            "A. S. Barabash",
            "F. Bellini",
            "G. Benato",
            "A. Benoît",
            "M. Beretta",
            "L. Bergé",
            "J. Billard",
            "Yu. A. Borovlev",
            "Ch. Bourgeois",
            "V. B. Brudanin",
            "P. Camus",
            "L. Cardani",
            "N. Casali",
            "A. Cazes",
            "M. Chapellier",
            "F. Charlieux",
            "M. de Combarieu",
            "I. Dafinei",
            "F. A. Danevich",
            "M. De Jesus",
            "T. Dixon",
            "L. Dumoulin",
            "K. Eitel",
            "F. Ferri",
            "B. K. Fujikawa",
            "J. Gascon",
            "L. Gironi",
            "A. Giuliani",
            "V. D. Grigorieva",
            "M. Gros",
            "E. Guerard",
            "D. L. Helis",
            "H. Z. Huang",
            "J. Johnston",
            "A. Juillard",
            "H. Khalife",
            "M. Kleifges",
            "V. V. Kobychev",
            "Yu. G. Kolomensky",
            "S. I. Konovalov",
            "P. Loaiza",
            "L. Ma",
            "E. P. Makarov",
            "P. de Marcillac",
            "R. Mariam",
            "L. Marini",
            "S. Marnieros",
            "D. Misiak",
            "X. -F. Navick",
            "C. Nones",
            "E. Olivieri",
            "J. L. Ouellet",
            "L. Pagnanini",
            "P. Pari",
            "L. Pattavina",
            "B. Paul",
            "M. Pavan",
            "H. Peng",
            "G. Pessina",
            "S. Pirro",
            "D. V. Poda",
            "O. G. Polischuk",
            "E. Previtali",
            "Th. Redon",
            "S. Rozov",
            "C. Rusconi",
            "V. Sanglard",
            "J. A. Scarpaci",
            "K. Schäffner",
            "B. Schmidt",
            "Y. Shen",
            "V. N. Shlegel",
            "B. Siebenborn",
            "V. Singh",
            "C. Tomei",
            "V. I. Tretyak",
            "V. I. Umatov",
            "L. Vagneron",
            "M. Velázquez",
            "M. Weber",
            "B. Welliver",
            "L. Winslow",
            "M. Xue",
            "E. Yakushev",
            "M. M. Zarytskyy",
            "A. S. Zolotarova"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  CUPID-Mo is a cryogenic detector array designed to search for neutrinoless\ndouble-beta decay ($0\\nu\\beta\\beta$) of $^{100}$Mo. It uses 20 scintillating\n$^{100}$Mo-enriched Li$_2$MoO$_4$ bolometers instrumented with Ge light\ndetectors to perform active suppression of $\\alpha$ backgrounds, drastically\nreducing the expected background in the $0\\nu\\beta\\beta$ signal region. As a\nresult, pileup events and small detector instabilities that mimic normal\nsignals become non-negligible potential backgrounds. These types of events can\nin principle be eliminated based on their signal shapes, which are different\nfrom those of regular bolometric pulses. We show that a purely data-driven\nprincipal component analysis based approach is able to filter out these\nanomalous events, without the aid of detector response simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.04033v2"
    },
    {
        "title": "A surrogate-based optimal likelihood function for the Bayesian\n  calibration of catalytic recombination in atmospheric entry protection\n  materials",
        "authors": [
            "Anabel del Val",
            "Olivier P. Le Maître",
            "Olivier Chazot",
            "Thierry E. Magin",
            "Pietro M. Congedo"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  This work deals with the inference of catalytic recombination parameters from\nplasma wind tunnel experiments for reusable thermal protection materials. One\nof the critical factors affecting the performance of such materials is the\ncontribution to the heat flux of the exothermic recombination reactions at the\nvehicle surface. The main objective of this work is to develop a dedicated\nBayesian framework that allows us to compare uncertain measurements with model\npredictions which depend on the catalytic parameter values. Our framework\naccounts for uncertainties involved in the model definition and incorporates\nall measured variables with their respective uncertainties. The physical model\nused for the estimation consists of a 1D boundary layer solver along the\nstagnation line. The chemical production term included in the surface mass\nbalance depends on the catalytic recombination efficiency. As not all the\ndifferent quantities needed to simulate a reacting boundary layer can be\nmeasured or known (such as the flow enthalpy at the inlet boundary), we propose\nan optimization procedure built on the construction of the likelihood function\nto determine their most likely values based on the available experimental data.\nThis procedure avoids the need to introduce any a priori estimates on the\nnuisance quantities, namely, the boundary layer edge enthalpy, wall\ntemperatures, static and dynamic pressures, which would entail the use of very\nwide priors. We substitute the optimal likelihood of the experimental data with\na surrogate model to make the inference procedure both faster and more robust.\nWe show that the resulting Bayesian formulation yields meaningful and accurate\nposterior distributions of the catalytic parameters with a reduction of more\nthan 20% of the standard deviation with respect to previous works. We also\nstudy the implications of an extension of the experimental procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.04649v1"
    },
    {
        "title": "Independent Normalization for $γ$-ray Strength Functions: The Shape\n  Method",
        "authors": [
            "M. Wiedeking",
            "M. Guttormsen",
            "A. C. Larsen",
            "F. Zeiser",
            "A. Görgen",
            "S. N. Liddick",
            "D. Mücher",
            "S. Siem",
            "A. Spyrou"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The Shape method, a novel approach to obtain the functional form of the\n$\\gamma$-ray strength function ($\\gamma$SF) in the absence of neutron resonance\nspacing data, is introduced. When used in connection with the Oslo method the\nslope of the Nuclear Level Density (NLD) is obtained simultaneously. The\nfoundation of the Shape method lies in the primary $\\gamma$-ray transitions\nwhich preserve information on the functional form of the $\\gamma$SF. The Shape\nmethod has been applied to $^{56}$Fe, $^{92}$Zr, $^{164}$Dy, and $^{240}$Pu,\nwhich are representative cases for the variety of situations encountered in\ntypical NLD and $\\gamma$SF studies. The comparisons of results from the Shape\nmethod to those from the Oslo method demonstrate that the functional form of\nthe $\\gamma$SF is retained regardless of nuclear structure details or $J^\\pi$\nvalues of the states fed by the primary transitions.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.15696v1"
    },
    {
        "title": "Accuracy and precision of the estimation of the number of missing levels\n  in chaotic spectra using long-range correlations",
        "authors": [
            "I. Casal",
            "L. Muñoz",
            "R. A. Molina"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We study the accuracy and precision for estimating the fraction of observed\nlevels $\\varphi$ in quantum chaotic spectra through long-range correlations. We\nfocus on the main statistics where theoretical formulas for the fraction of\nmissing levels have been derived, the $\\Delta_3$ of Dyson and Mehta and the\npower spectrum of the $\\delta_n$ statistic. We use Monte Carlo simulations of\nthe spectra from the diagonalization of Gaussian Orthogonal Ensemble matrices\nwith a definite number of levels randomly taken out to fit the formulas and\ncalculate the distribution of the estimators for different sizes of the\nspectrum and values of $\\varphi$. A proper averaging of the power spectrum of\nthe $\\delta_n$ statistic needs to be performed for avoiding systematic errors\nin the estimation. Once the proper averaging is made the estimation of the\nfraction of observed levels has quite good accuracy for the two methods even\nfor the lowest dimensions we consider $d=100$. However, the precision is\ngenerally better for the estimation using the power spectrum of the $\\delta_n$\nas compared to the estimation using the $\\Delta_3$ statistic. This difference\nis clearly bigger for larger dimensions. Our results show that a careful\nanalysis of the value of the fit in view of the ensemble distribution of the\nestimations is mandatory for understanding its actual significance and give a\nrealistic error interval.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.01667v1"
    },
    {
        "title": "Learning to Identify Electrons",
        "authors": [
            "Julian Collado",
            "Jessica N. Howard",
            "Taylor Faucett",
            "Tony Tong",
            "Pierre Baldi",
            "Daniel Whiteson"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We investigate whether state-of-the-art classification features commonly used\nto distinguish electrons from jet backgrounds in collider experiments are\noverlooking valuable information. A deep convolutional neural network analysis\nof electromagnetic and hadronic calorimeter deposits is compared to the\nperformance of typical features, revealing a $\\approx 5\\%$ gap which indicates\nthat these lower-level data do contain untapped classification power. To reveal\nthe nature of this unused information, we use a recently developed technique to\nmap the deep network into a space of physically interpretable observables. We\nidentify two simple calorimeter observables which are not typically used for\nelectron identification, but which mimic the decisions of the convolutional\nnetwork and nearly close the performance gap.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.01984v2"
    },
    {
        "title": "Mode hunting through active information",
        "authors": [
            "Daniel Andrés Díaz-Pachón",
            "Juan Pablo Sáenz",
            "J. Sunil Rao",
            "Jean-Eudes Dazard"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We propose a new method to find modes based on active information. We develop\nan algorithm that, when applied to the whole space, will say whether there are\nany modes present \\textit{and} where they are; this algorithm will reduce the\ndimensionality without resorting to Principal Components; and more importantly,\npopulation-wise, will not detect modes when they are not present.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.05794v1"
    },
    {
        "title": "Sensitivity optimization of multichannel searches for new signals",
        "authors": [
            "Giovanni Punzi"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The frequentist definition of sensitivity of a search for new phenomena\nproposed in arXiv:0308063 has been utilized in a number of published\nexperimental searches. In most cases, the simple approximate formula for the\ncommon problem of Poisson counts with background has been deemed adequate for\nthe purpose. There are however many problems nowadays in which more complex\nanalysis is required, involving multiple channels. In this article, the same\napproach of arXiv:0308063 is applied to a multichannel Poisson problem, and a\nconvenient formula is derived in closed form, generalizing the known result for\nthe simple counting experiment. An explicit solution is also derived for the\ncommon case of a search for a Gaussian signal superimposed over a flat\nbackground.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.11770v1"
    },
    {
        "title": "Explainable AI for ML jet taggers using expert variables and layerwise\n  relevance propagation",
        "authors": [
            "Garvita Agarwal",
            "Lauren Hay",
            "Ia Iashvili",
            "Benjamin Mannix",
            "Christine McLean",
            "Margaret Morris",
            "Salvatore Rappoccio",
            "Ulrich Schubert"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  A framework is presented to extract and understand decision-making\ninformation from a deep neural network (DNN) classifier of jet substructure\ntagging techniques. The general method studied is to provide expert variables\nthat augment inputs (\"eXpert AUGmented\" variables, or XAUG variables), then\napply layerwise relevance propagation (LRP) to networks both with and without\nXAUG variables. The XAUG variables are concatenated with the intermediate\nlayers after network-specific operations (such as convolution or recurrence),\nand used in the final layers of the network. The results of comparing networks\nwith and without the addition of XAUG variables show that XAUG variables can be\nused to interpret classifier behavior, increase discrimination ability when\ncombined with low-level features, and in some cases capture the behavior of the\nclassifier completely. The LRP technique can be used to find relevant\ninformation the network is using, and when combined with the XAUG variables,\ncan be used to rank features, allowing one to find a reduced set of features\nthat capture part of the network performance. In the studies presented, adding\nXAUG variables to low-level DNNs increased the efficiency of classifiers by as\nmuch as 30-40\\%. In addition to performance improvements, an approach to\nquantify numerical uncertainties in the training of these DNNs is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.13466v3"
    },
    {
        "title": "What the new RooFit can do for your analysis",
        "authors": [
            "Stephan Hageboeck"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  RooFit is a toolkit for statistical modelling and fitting, and together with\nRooStats it is used for measurements and statistical tests by most experiments\nin particle physics. Since one year, RooFit is being modernised. In this talk,\nimprovements already released with ROOT will be discussed, such as faster data\nloading, vectorised computations and more standard-like interfaces. These allow\nfor speeding up unbinned fits by several factors, and make RooFit easier to use\nfrom both C++ and Python.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.02746v2"
    },
    {
        "title": "Maximum Entropy competes with Maximum Likelihood",
        "authors": [
            "A. E. Allahverdyan",
            "N. H. Martirosyan"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Maximum entropy (MAXENT) method has a large number of applications in\ntheoretical and applied machine learning, since it provides a convenient\nnon-parametric tool for estimating unknown probabilities. The method is a major\ncontribution of statistical physics to probabilistic inference. However, a\nsystematic approach towards its validity limits is currently missing. Here we\nstudy MAXENT in a Bayesian decision theory set-up, i.e. assuming that there\nexists a well-defined prior Dirichlet density for unknown probabilities, and\nthat the average Kullback-Leibler (KL) distance can be employed for deciding on\nthe quality and applicability of various estimators. These allow to evaluate\nthe relevance of various MAXENT constraints, check its general applicability,\nand compare MAXENT with estimators having various degrees of dependence on the\nprior, viz. the regularized maximum likelihood (ML) and the Bayesian\nestimators. We show that MAXENT applies in sparse data regimes, but needs\nspecific types of prior information. In particular, MAXENT can outperform the\noptimally regularized ML provided that there are prior rank correlations\nbetween the estimated random quantity and its probabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.09430v1"
    },
    {
        "title": "Bayesian Analysis of a Future Beta Decay Experiment's Sensitivity to\n  Neutrino Mass Scale and Ordering",
        "authors": [
            "A. Ashtari Esfahani",
            "M. Betancourt",
            "Z. Bogorad",
            "S. Böser",
            "N. Buzinsky",
            "R. Cervantes",
            "C. Claessens",
            "L. de Viveiros",
            "M. Fertl",
            "J. A. Formaggio",
            "L. Gladstone",
            "M. Grando",
            "M. Guigue",
            "J. Hartse",
            "K. M. Heeger",
            "X. Huyan",
            "J. Johnston",
            "A. M. Jones",
            "K. Kazkaz",
            "B. H. LaRoque",
            "A. Lindman",
            "R. Mohiuddin",
            "B. Monreal",
            "J. A. Nikkel",
            "E. Novitski",
            "N. S. Oblath",
            "M. Ottiger",
            "W. Pettus",
            "R. G. H. Robertson",
            "G. Rybka",
            "L. Saldaña",
            "M. Schram",
            "V. Sibille",
            "P. L. Slocum",
            "Y. -H. Sun",
            "P. T. Surukuchi",
            "J. R. Tedeschi",
            "A. B. Telles",
            "M. Thomas",
            "T. Thümmler",
            "L. Tvrznikova",
            "B. A. VanDevender",
            "T. E. Weiss",
            "T. Wendler",
            "E. Zayas",
            "A. Ziegler"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Bayesian modeling techniques enable sensitivity analyses that incorporate\ndetailed expectations regarding future experiments. A model-based approach also\nallows one to evaluate inferences and predicted outcomes, by calibrating (or\nmeasuring) the consequences incurred when certain results are reported. We\npresent procedures for calibrating predictions of an experiment's sensitivity\nto both continuous and discrete parameters. Using these procedures and a new\nBayesian model of the $\\beta$-decay spectrum, we assess a high-precision\n$\\beta$-decay experiment's sensitivity to the neutrino mass scale and ordering,\nfor one assumed design scenario. We find that such an experiment could measure\nthe electron-weighted neutrino mass within $\\sim40\\,$meV after 1 year (90$\\%$\ncredibility). Neutrino masses $>500\\,$meV could be measured within\n$\\approx5\\,$meV. Using only $\\beta$-decay and external reactor neutrino data,\nwe find that next-generation $\\beta$-decay experiments could potentially\nconstrain the mass ordering using a two-neutrino spectral model analysis. By\ncalibrating mass ordering results, we identify reporting criteria that can be\ntuned to suppress false ordering claims. In some cases, a two-neutrino analysis\ncan reveal that the mass ordering is inverted, an unobtainable result for the\ntraditional one-neutrino analysis approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.14341v2"
    },
    {
        "title": "End-to-End Jet Classification of Boosted Top Quarks with the CMS Open\n  Data",
        "authors": [
            "Michael Andrews",
            "Bjorn Burkle",
            "Yi-fan Chen",
            "Davide DiCroce",
            "Sergei Gleyzer",
            "Ulrich Heintz",
            "Meenakshi Narain",
            "Manfred Paulini",
            "Nikolas Pervan",
            "Yusef Shafi",
            "Wei Sun",
            "Emanuele Usai",
            "Kun Yang"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We describe a novel application of the end-to-end deep learning technique to\nthe task of discriminating top quark-initiated jets from those originating from\nthe hadronization of a light quark or a gluon. The end-to-end deep learning\ntechnique combines deep learning algorithms and low-level detector\nrepresentation of the high-energy collision event. In this study, we use\nlow-level detector information from the simulated CMS Open Data samples to\nconstruct the top jet classifiers. To optimize classifier performance we\nprogressively add low-level information from the CMS tracking detector,\nincluding pixel detector reconstructed hits and impact parameters, and\ndemonstrate the value of additional tracking information even when no new\nspatial structures are added. Relying only on calorimeter energy deposits and\nreconstructed pixel detector hits, the end-to-end classifier achieves an AUC\nscore of 0.975$\\pm$0.002 for the task of classifying boosted top quark jets.\nAfter adding derived track quantities, the classifier AUC score increases to\n0.9824$\\pm$0.0013, serving as the first performance benchmark for these CMS\nOpen Data samples. We additionally provide a timing performance comparison of\ndifferent processor unit architectures for training the network.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14659v3"
    },
    {
        "title": "Nuclear data evaluation with Bayesian networks",
        "authors": [
            "Georg Schnabel",
            "Roberto Capote",
            "Arjan Koning",
            "David Brown"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Bayesian networks are graphical models to represent the probabilistic\nrelationships between variables in the Bayesian framework. The knowledge of all\nvariables can be updated using new information about some of the variables. We\nshow that relying on the Bayesian network interpretation enables large scale\ninference and gives flexibility in incorporating prior assumptions and\nconstraints into the nuclear data evaluation process, such as sum rules and the\nnon-negativity of cross sections. The latter constraint is accounted for by a\nnon-linear transformation and therefore we also discuss inference in Bayesian\nnetworks with non-linear relationships. Using Bayesian networks, the evaluation\nprocess yields detailed information, such as posterior estimates and\nuncertainties of all statistical and systematic errors. We also elaborate on a\nsparse Gaussian process construction compatible with the Bayesian network\nframework that can for instance be used as prior on energy-dependent model\nparameters, model deficiencies and energy-dependent systematic errors of\nexperiments. We present three proof-of-concept examples that emerged in the\ncontext of the neutron data standards project and in the ongoing international\nevaluation efforts of $^{56}$Fe. In the first example we demonstrate the\nmodelization and explicit estimation of relative energy-dependent error\ncomponents of experimental datasets. Then we show an example evaluation using\nthe outlined Gaussian process construction in an evaluation of $^{56}$Fe in the\nenergy range between one and two MeV, where R-Matrix and nuclear model fits are\ndifficult. Finally, we present a model-based evaluation of $^{56}$Fe between 5\nMeV and 30 MeV with a sound treatment of model deficiencies. The R scripts to\nreproduce the Bayesian network examples and the nucdataBaynet package for\nBayesian network modeling and inference have been made publicly available.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.10322v1"
    },
    {
        "title": "Explaining machine-learned particle-flow reconstruction",
        "authors": [
            "Farouk Mokhtar",
            "Raghav Kansal",
            "Daniel Diaz",
            "Javier Duarte",
            "Joosep Pata",
            "Maurizio Pierini",
            "Jean-Roch Vlimant"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The particle-flow (PF) algorithm is used in general-purpose particle\ndetectors to reconstruct a comprehensive particle-level view of the collision\nby combining information from different subdetectors. A graph neural network\n(GNN) model, known as the machine-learned particle-flow (MLPF) algorithm, has\nbeen developed to substitute the rule-based PF algorithm. However,\nunderstanding the model's decision making is not straightforward, especially\ngiven the complexity of the set-to-set prediction task, dynamic graph building,\nand message-passing steps. In this paper, we adapt the layerwise-relevance\npropagation technique for GNNs and apply it to the MLPF algorithm to gauge the\nrelevant nodes and features for its predictions. Through this process, we gain\ninsight into the model's decision-making.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.12840v1"
    },
    {
        "title": "Particle Graph Autoencoders and Differentiable, Learned Energy Mover's\n  Distance",
        "authors": [
            "Steven Tsan",
            "Raghav Kansal",
            "Anthony Aportela",
            "Daniel Diaz",
            "Javier Duarte",
            "Sukanya Krishna",
            "Farouk Mokhtar",
            "Jean-Roch Vlimant",
            "Maurizio Pierini"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Autoencoders have useful applications in high energy physics in anomaly\ndetection, particularly for jets - collimated showers of particles produced in\ncollisions such as those at the CERN Large Hadron Collider. We explore the use\nof graph-based autoencoders, which operate on jets in their \"particle cloud\"\nrepresentations and can leverage the interdependencies among the particles\nwithin a jet, for such tasks. Additionally, we develop a differentiable\napproximation to the energy mover's distance via a graph neural network, which\nmay subsequently be used as a reconstruction loss function for autoencoders.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.12849v1"
    },
    {
        "title": "Detecting structured sources in noisy images via Minkowski maps",
        "authors": [
            "Michael Andreas Klatt",
            "Klaus Mecke"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Astronomy, biophysics, and material science often depend on the possibility\nto extract information out of faint spatial signals. Here we present a\nmorphometric analysis technique to quantify the shape of structural deviations\nin greyscale images. It identifies important features in noisy spatial data,\nespecially for short observation times and low statistics. Without assuming any\nprior knowledge about potential sources, the additional shape information can\nincrease the sensitivity by 14 orders of magnitude compared to previous\nmethods. Rejection rates can increase by an order of magnitude. As a key\ningredient to such a dramatic increase, we accurately describe the distribution\nof the homogeneous background noise in terms of the density of states\n$\\Omega(A,P,\\chi)$ for the area $A$, perimeter $P$, and Euler characteristic\n$\\chi$ of random black-and-white images. The technique is successfully applied\nto data of the H.E.S.S. experiment for the detection of faint extended sources.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.13348v1"
    },
    {
        "title": "Inferring a property of a large system from a small number of samples",
        "authors": [
            "Damián G. Hernández",
            "Inés Samengo"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Inferring the value of a property of a large stochastic system is a difficult\ntask when the number of samples is insufficient to reliably estimate the\nprobability distribution. The Bayesian estimator of the property of interest\nrequires the knowledge of the prior distribution, and in many situations, it is\nnot clear which prior should be used. Several estimators have been developed so\nfar, in which the proposed prior was individually tailored for each property of\ninterest; such is the case, for example, for the entropy, the amount of mutual\ninformation, or the correlation between pairs of variables. In this paper we\npropose a general framework to select priors, valid for arbitrary properties.\nWe first demonstrate that only certain aspects of the prior distribution\nactually affect the inference process. We then expand the sought prior as a\nlinear combination of a one-dimensional family of indexed priors, each of which\nis obtained through a maximum entropy approach with constrained mean value of\nthe property under study. In many cases of interest, only one or very few\ncomponents of the expansion turn out to contribute to the Bayesian estimator,\nso it is often valid to only keep a single component. The relevant component is\nselected by the data, so no handcrafted priors are required. We test the\nperformance of this approximation with a few paradigmatic examples, and show\nthat it performs well in comparison to the ad-hoc methods previously proposed\nin the literature. Our method highlights the connection between Bayesian\ninference and equilibrium statistical mechanics, since the most relevant\ncomponent of the expansion can be argued to be the one with the right\ntemperature.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.00671v1"
    },
    {
        "title": "Quantitative assessment of fitting errors associated with streak camera\n  noise in Thomson scattering data analysis",
        "authors": [
            "G. F. Swadling",
            "C. Bruulsema",
            "W. Rozmus",
            "J. Katz"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Thomson scattering measurements in High Energy Density experiments are often\nrecorded using optical streak cameras. In the low-signal regime, noise\nintroduced by the streak camera can become an important and sometimes the\ndominate source of measurement uncertainty. In this paper we present a formal\nmethod of accounting for the presence of streak camera noise in our\nmeasurements. We present a phenomenological description of the noise generation\nmechanisms and present a statistical model that may be used to construct the\ncovariance matrix associated with a given measurement. This model is\nbenchmarked against simulations of streak camera images. We demonstrate how\nthis covariance may then be used to weight fitting of the data and provide\nquantitative assessments of the uncertainty in the fitting parameters\ndetermined by the best fit to the data. These methods will have general\napplicability to other measurements made using optical streak cameras.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.12260v4"
    },
    {
        "title": "RAMANMETRIX: a delightful way to analyze Raman spectra",
        "authors": [
            "Darina Storozhuk",
            "Oleg Ryabchykov",
            "Juergen Popp",
            "Thomas Bocklitz"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Although Raman spectroscopy is widely used for the investigation of\nbiomedical samples and has a high potential for use in clinical applications,\nit is not common in clinical routines. One of the factors that obstruct the\nintegration of Raman spectroscopic tools into clinical routines is the\ncomplexity of the data processing workflow. Software tools that simplify\nspectroscopic data handling may facilitate such integration by familiarizing\nclinical experts with the advantages of Raman spectroscopy.\n  Here, RAMANMETRIX is introduced as a user-friendly software with an intuitive\nweb-based graphical user interface (GUI) that incorporates a complete workflow\nfor chemometric analysis of Raman spectra, from raw data pretreatment to a\nrobust validation of machine learning models. The software can be used both for\nmodel training and for the application of the pretrained models onto new data\nsets. Users have full control of the parameters during model training, but the\ntesting data flow is frozen and does not require additional user input.\nRAMANMETRIX is available in two versions: as standalone software and web\napplication. Due to the modern software architecture, the computational backend\npart can be executed separately from the GUI and accessed through an\napplication programming interface (API) for applying a preconstructed model to\nthe measured data. This opens up possibilities for using the software as a data\nprocessing backend for the measurement devices in real-time.\n  The models preconstructed by more experienced users can be exported and\nreused for easy one-click data preprocessing and prediction, which requires\nminimal interaction between the user and the software. The results of such\nprediction and graphical outputs of the different data processing steps can be\nexported and saved.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.07586v1"
    },
    {
        "title": "Accelerating Laue Depth Reconstruction Algorithm with CUDA",
        "authors": [
            "Ke Yue",
            "Schwarz Nicholas",
            "Tischler Jonathan Z"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The Laue diffraction microscopy experiment uses the polychromatic Laue\nmicro-diffraction technique to examine the structure of materials with\nsub-micron spatial resolution in all three dimensions. During this experiment,\nlocal crystallographic orientations, orientation gradients and strains are\nmeasured as properties which will be recorded in HDF5 image format. The\nrecorded images will be processed with a depth reconstruction algorithm for\nfuture data analysis. But the current depth reconstruction algorithm consumes\nconsiderable processing time and might take up to 2 weeks for reconstructing\ndata collected from one single experiment. To improve the depth reconstruction\ncomputation speed, we propose a scalable GPU program solution on the depth\nreconstruction problem in this paper. The test result shows that the running\ntime would be 10 to 20 times faster than the prior CPU design for various size\nof input data.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.13309v1"
    },
    {
        "title": "Zipf's law and log-normal distributions in measures of scientific output\n  across fields and institutions: 40 years of Slovenia's research as an example",
        "authors": [
            "Matjaz Perc"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Slovenia's Current Research Information System (SICRIS) currently hosts\n86,443 publications with citation data from 8,359 researchers working on the\nwhole plethora of social and natural sciences from 1970 till present. Using\nthese data, we show that the citation distributions derived from individual\npublications have Zipfian properties in that they can be fitted by a power law\n$P(x) \\sim x^{-\\alpha}$, with $\\alpha$ between 2.4 and 3.1 depending on the\ninstitution and field of research. Distributions of indexes that quantify the\nsuccess of researchers rather than individual publications, on the other hand,\ncannot be associated with a power law. We find that for Egghe's g-index and\nHirsch's h-index the log-normal form $P(x) \\sim \\exp[-a\\ln x -b(\\ln x)^2]$\napplies best, with $a$ and $b$ depending moderately on the underlying set of\nresearchers. In special cases, particularly for institutions with a strongly\nhierarchical constitution and research fields with high self-citation rates,\nexponential distributions can be observed as well. Both indexes yield\ndistributions with equivalent statistical properties, which is a strong\nindicator for their consistency and logical connectedness. At the same time,\ndifferences in the assessment of citation histories of individual researchers\nstrengthen their importance for properly evaluating the quality and impact of\nscientific output.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.1018v1"
    },
    {
        "title": "The physical limit of logical compare operation",
        "authors": [
            "Feng Pan",
            "Heng-Liang Zhang",
            "Jie Qi"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  In this paper two connected Szilard single molecule engines (with different\ntemperature) model of Maxwell's demon are used to demonstrate and analysis the\nlogical compare operation. The logical and physical complexity of compare\noperations are both showed to be kTln2. Then this limit was used to prove the\ntime complexity lower bound of sorting problem. It confirmed the proposed way\nto measure the complexity of a problem, provided another evidence of the\nequivalence between information theoretical and thermodynamic entropies.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.0445v1"
    },
    {
        "title": "Latent common manifold learning with alternating diffusion: analysis and\n  applications",
        "authors": [
            "Ronen Talmon",
            "Hau-tieng Wu"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The analysis of data sets arising from multiple sensors has drawn significant\nresearch attention over the years. Traditional methods, including kernel-based\nmethods, are typically incapable of capturing nonlinear geometric structures.\nWe introduce a latent common manifold model underlying multiple sensor\nobservations for the purpose of multimodal data fusion. A method based on\nalternating diffusion is presented and analyzed; we provide theoretical\nanalysis of the method under the latent common manifold model. To exemplify the\npower of the proposed framework, experimental results in several applications\nare reported.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.00078v2"
    },
    {
        "title": "Particle identification in ALICE: a Bayesian approach",
        "authors": [
            " ALICE Collaboration"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We present a Bayesian approach to particle identification (PID) within the\nALICE experiment. The aim is to more effectively combine the particle\nidentification capabilities of its various detectors. After a brief explanation\nof the adopted methodology and formalism, the performance of the Bayesian PID\napproach for charged pions, kaons and protons in the central barrel of ALICE is\nstudied. PID is performed via measurements of specific energy loss\n($\\mathrm{d}E/\\mathrm{d}x$) and time-of-flight. PID efficiencies and\nmisidentification probabilities are extracted and compared with Monte Carlo\nsimulations using high-purity samples of identified particles in the decay\nchannels ${\\rm K}^0_S \\rightarrow \\pi^-\\pi^+$, $\\phi \\rightarrow {\\rm K}^-{\\rm\nK}^+$, and $\\Lambda \\rightarrow {\\rm p}\\pi^-$ in p-Pb collisions at\n$\\sqrt{s_{\\rm NN}}=5.02$ TeV. In order to thoroughly assess the validity of the\nBayesian approach, this methodology was used to obtain corrected $p_{\\rm T}$\nspectra of pions, kaons, protons, and D$^0$ mesons in pp collisions at\n$\\sqrt{s}=7$ TeV. In all cases, the results using Bayesian PID were found to be\nconsistent with previous measurements performed by ALICE using a standard PID\napproach. For the measurement of D$^0 \\rightarrow {\\rm K}^-\\pi^+$, it was found\nthat a Bayesian PID approach gave a higher signal-to-background ratio and a\nsimilar or larger statistical significance when compared with standard PID\nselections, despite a reduced identification efficiency. Finally, we present an\nexploratory study of the measurement of $\\Lambda_{\\rm c}^{+}\\rightarrow {\\rm p}\n{\\rm K}^-\\pi^+$ in pp collisions at $\\sqrt{s}=7$ TeV, using the Bayesian\napproach for the identification of its decay products.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.01392v2"
    },
    {
        "title": "SHARP: a distributed, GPU-based ptychographic solver",
        "authors": [
            "Stefano Marchesini",
            "Hari Krishnan",
            "Benedikt J. Daurer",
            "David A. Shapiro",
            "Talita Perciano",
            "James A. Sethian",
            "Filipe R. N. C. Maia"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Ever brighter light sources, fast parallel detectors, and advances in phase\nretrieval methods, have made ptychography a practical and popular imaging\ntechnique. Compared to previous techniques, ptychography provides superior\nrobustness and resolution at the expense of more advanced and time consuming\ndata analysis. By taking advantage of massively parallel architectures,\nhigh-throughput processing can expedite this analysis and provide microscopists\nwith immediate feedback. These advances allow real-time imaging at wavelength\nlimited resolution, coupled with a large field of view. Here, we introduce a\nset of algorithmic and computational methodologies used at the Advanced Light\nSource, and DOE light sources packaged as a CUDA based software environment\nnamed SHARP (http://camera.lbl.gov/sharp), aimed at providing state-of-the-art\nhigh-throughput ptychography reconstructions for the coming era of diffraction\nlimited light sources.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.01448v2"
    },
    {
        "title": "On the potential of multivariate techniques for the determination of\n  multidimensional efficiencies",
        "authors": [
            "Benoit Viaud"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Differential measurements of particle collisions or decays can provide\nstringent constraints on physics beyond the Standard Model of particle physics.\nIn particular, the distributions of the kinematical and angular variables that\ncharacterise heavy me- son multibody decays are non trivial and can sign the\nunderlying interaction physics. In the era of high luminosity opened by the\nadvent of the Large Hadron Collider and of Flavor Factories, differential\nmeasurements are less and less dominated by statistical precision and require a\nprecise determination of efficiencies that depend simultaneously on several\nvariables and do not factorise in these variables. This docu- ment is a\nreflection on the potential of multivariate techniques for the determination of\nsuch multidimensional efficiencies. We carried out two case studies that show\nthat multilayer perceptron neural networks can determine and correct for the\ndistortions introduced by reconstruction and selection criteria in the\nmultidimensional phase space of the decays $B^{0}\\rightarrow K^{*0}(\\rightarrow\nK^{+}\\pi^{-}) \\mu^{+}\\mu^{-}$ and $D^{0}\\rightarrow\nK^{-}\\pi^{+}\\pi^{+}\\pi^{-}$, at the price of a minimal analysis effort. We\nconclude that this method can already be used for measurements which\nstatistical precision does not yet reach the percent level and that with more\nsophisticated machine learning methods, the aforementioned potential is very\npromising.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.03758v1"
    },
    {
        "title": "Practical Introduction to Clustering Data",
        "authors": [
            "Alexander K. Hartmann"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Data clustering is an approach to seek for structure in sets of complex data,\ni.e., sets of \"objects\". The main objective is to identify groups of objects\nwhich are similar to each other, e.g., for classification. Here, an\nintroduction to clustering is given and three basic approaches are introduced:\nthe k-means algorithm, neighbour-based clustering, and an agglomerative\nclustering method. For all cases, C source code examples are given, allowing\nfor an easy implementation.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05124v1"
    },
    {
        "title": "The scaling of the minimum sum of edge lengths in uniformly random trees",
        "authors": [
            "Juan Luis Esteban",
            "Ramon Ferrer-i-Cancho",
            "Carlos Gómez-Rodríguez"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The minimum linear arrangement problem on a network consists of finding the\nminimum sum of edge lengths that can be achieved when the vertices are arranged\nlinearly. Although there are algorithms to solve this problem on trees in\npolynomial time, they have remained theoretical and have not been implemented\nin practical contexts to our knowledge. Here we use one of those algorithms to\ninvestigate the growth of this sum as a function of the size of the tree in\nuniformly random trees. We show that this sum is bounded above by its value in\na star tree. We also show that the mean edge length grows logarithmically in\noptimal linear arrangements, in stark contrast to the linear growth that is\nexpected on optimal arrangements of star trees or on random linear\narrangements.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.07940v2"
    },
    {
        "title": "Estimating the resolution of real images",
        "authors": [
            "Ryuta Mizutani",
            "Rino Saiga",
            "Susumu Takekoshi",
            "Chie Inomoto",
            "Naoya Nakamura",
            "Makoto Arai",
            "Kenichi Oshima",
            "Masanari Itokawa",
            "Akihisa Takeuchi",
            "Kentaro Uesugi",
            "Yasuko Terada",
            "Yoshio Suzuki"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Image resolvability is the primary concern in imaging. This paper reports an\nestimation of the full width at half maximum of the point spread function from\na Fourier domain plot of real sample images by neither using test objects, nor\ndefining a threshold criterion. We suggest that this method can be applied to\nany type of image, independently of the imaging modality.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00992v1"
    },
    {
        "title": "Robust Wrapping-free Phase Retrieval Method Based on Weighted\n  Least-square Method",
        "authors": [
            "Minmin Wang",
            "Canlin Zhou",
            "Shuchun Si",
            "XiaoLei Li",
            "Zhenkun Lei",
            "YanJie Li"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  For many profilometry techniques, phase unwrapping is one of the most\nchallenging process. In order to sidestep the phase unwrapping process,\nPerciante et. al [Appl Opt 2015; 54(10):3018-23] proposed a wrapping-free\nmethod based on the direct integration of the spatial derivatives of the\npatterns to retrieve the phase. But it is only applicable for the case of the\nphase continuity for the tested object, which means it may fail to handle\nfringe patterns containing complicated singularities, such as noise, shadow,\nshears and surface discontinuity. In view of this problems, a robust\nwrapping-free phase retrieval method is proposed in this paper, which is based\non combined Perciante's method and weighted least-squares method. Two partial\nderivatives of the desired phase is obtained from the fringe patterns,\nmeanwhile the carrier is eliminated using direct phase difference method. The\nphase singularities are determined using derivative variance correlation map\n(DVCM), and the weighting coefficient is obtained from the binary mask of the\nreverse DVCM. Simulations and experiments are conducted to prove the validity\nof the proposed method. Results are analyzed and compared with those of\nPerciante's method, demonstrating that the proposed method can be available for\nmeasuring objects with some kinds of singularities sources.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.01435v1"
    },
    {
        "title": "Decomposition of conditional probability for high-order symbolic Markov\n  chains",
        "authors": [
            "S. S. Melnik",
            "O. V. Usatenko"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The main goal of the paper is to develop an estimate for the conditional\nprobability function of random stationary ergodic symbolic sequences with\nelements belonging to a finite alphabet. We elaborate a decomposition procedure\nfor the conditional probability function of sequences considered as the\nhigh-order Markov chains. We represent the conditional probability function as\nthe sum of multi-linear memory function monomials of different orders (from\nzero up to the chain order). This allows us to construct artificial sequences\nby method of successive iterations taking into account at each step of\niterations increasingly more high correlations among random elements. At weak\ncorrelations, the memory functions are uniquely expressed in terms of the\nhigh-order symbolic correlation functions. The proposed method fills up the gap\nbetween two approaches: the likelihood estimation and the additive Markov\nchains. The obtained results might be used for sequential approximation of\nartificial neural networks training.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.07764v1"
    },
    {
        "title": "A New Correlator to Detect and Characterize the Chiral Magnetic Effect",
        "authors": [
            "Niseem Magdy",
            "Shuzhe Shi",
            "Jinfeng Liao",
            "N. Ajitanand",
            "Roy A. Lacey"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  A charge-sensitive in-event correlator is proposed and tested for its\nefficacy to detect and characterize charge separation associated with the\nChiral Magnetic Effect (CME) in heavy ion collisions. Tests, performed with the\naid of two reaction models, indicate discernible responses for background- and\nCME-driven charge separation, relative to the second- ($\\Psi_{2}$) and\nthird-order ($\\Psi_{3}$) event planes, which could serve to identify the CME.\nThe tests also indicate a degree of sensitivity which would enable robust\ncharacterization of the CME via Anomalous Viscous Fluid Dynamics (AVFD) model\ncomparisons.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.01717v4"
    },
    {
        "title": "A combinatorial framework to quantify peak/pit asymmetries in complex\n  dynamics",
        "authors": [
            "Uri Hasson",
            "Jacopo Iacovacci",
            "Ben Davis",
            "Ryan Flanagan",
            "Enzo Tagliazucchi",
            "Helmut Laufs",
            "Lucas Lacasa"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We explore a combinatorial framework which efficiently quantifies the\nasymmetries between minima and maxima in local fluctuations of time series. We\nfirstly showcase its performance by applying it to a battery of synthetic\ncases. We find rigorous results on some canonical dynamical models (stochastic\nprocesses with and without correlations, chaotic processes) complemented by\nextensive numerical simulations for a range of processes which indicate that\nthe methodology correctly distinguishes different complex dynamics and\noutperforms state of the art metrics in several cases. Subsequently, we apply\nthis methodology to real-world problems emerging across several disciplines\nincluding cases in neurobiology, finance and climate science. We conclude that\ndifferences between the statistics of local maxima and local minima in time\nseries are highly informative of the complex underlying dynamics and a\ngraph-theoretic extraction procedure allows to use these features for\nstatistical learning purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.04947v1"
    },
    {
        "title": "Web interface for reflectivity fitting",
        "authors": [
            "Mathieu Doucet",
            "Ricardo Miguel Ferraz Leal",
            "Tanner C. Hobson"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  The Liquids Reflectometer at Oak Ridge National Laboratory provides neutron\nreflectivity capability for an average of about 30 experiments each year. In\nrecent years, there has been a large effort to streamline the data processing\nand analysis for the instrument. While much of the data reduction can be\nautomated, data analysis remains something that needs to be done by scientists.\nFor this purpose, we present a reflectivity fitting web interface that captures\nthe process of setting up and executing fits while reducing the need for\ninstalling software or writing Python scripts.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.06767v1"
    },
    {
        "title": "Significance of an excess in a counting experiment: assessing the impact\n  of systematic uncertainties and the case with Gaussian background",
        "authors": [
            "G. Vianello"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Several experiments in high-energy physics and astrophysics can be treated as\non/off measurements, where an observation potentially containing a new source\nor effect (\"on\" measurement) is contrasted with a background-only observation\nfree of the effect (\"off\" measurement). In counting experiments, the\nsignificance of the new source or effect can be estimated with a widely-used\nformula from [LiMa], which assumes that both measurements are Poisson random\nvariables. In this paper we study three other cases: i) the ideal case where\nthe background measurement has no uncertainty, which can be used to study the\nmaximum sensitivity that an instrument can achieve, ii) the case where the\nbackground estimate $b$ in the off measurement has an additional systematic\nuncertainty, and iii) the case where $b$ is a Gaussian random variable instead\nof a Poisson random variable. The latter case applies when $b$ comes from a\nmodel fitted on archival or ancillary data, or from the interpolation of a\nfunction fitted on data surrounding the candidate new source/effect.\nPractitioners typically use in this case a formula which is only valid when $b$\nis large and when its uncertainty is very small, while we derive a general\nformula that can be applied in all regimes. We also develop simple methods that\ncan be used to assess how much an estimate of significance is sensitive to\nsystematic uncertainties on the efficiency or on the background. Examples of\napplications include the detection of short Gamma-Ray Bursts and of new X-ray\nor $\\gamma$-ray sources.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.00118v2"
    },
    {
        "title": "Machine learning as an instrument for data unfolding",
        "authors": [
            "Alexander Glazov"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  A method for correcting for detector smearing effects using machine learning\ntechniques is presented. Compared to the standard approaches the method can use\nmore than one reconstructed variable to infere the value of the unsmeared\nquantity on event by event basis. The method is implemented using a sequential\nneural network with a categorical cross entropy as the loss function. It is\ntested on a toy example and is shown to satisfy basic closure tests. Possible\napplication of the method for analysis of the data from high energy physics\nexperiments is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.01814v1"
    },
    {
        "title": "Information Perspective to Probabilistic Modeling: Boltzmann Machines\n  versus Born Machines",
        "authors": [
            "Song Cheng",
            "Jing Chen",
            "Lei Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We compare and contrast the statistical physics and quantum physics inspired\napproaches for unsupervised generative modeling of classical data. The two\napproaches represent probabilities of observed data using energy-based models\nand quantum states respectively.Classical and quantum information patterns of\nthe target datasets therefore provide principled guidelines for structural\ndesign and learning in these two approaches. Taking the restricted Boltzmann\nmachines (RBM) as an example, we analyze the information theoretical bounds of\nthe two approaches. We verify our reasonings by comparing the performance of\nRBMs of various architectures on the standard MNIST datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.04144v1"
    },
    {
        "title": "On the inherent competition between valid and spurious inductive\n  inferences in Boolean data",
        "authors": [
            "M. Andrecut"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Inductive inference is the process of extracting general rules from specific\nobservations. This problem also arises in the analysis of biological networks,\nsuch as genetic regulatory networks, where the interactions are complex and the\nobservations are incomplete. A typical task in these problems is to extract\ngeneral interaction rules as combinations of Boolean covariates, that explain a\nmeasured response variable. The inductive inference process can be considered\nas an incompletely specified Boolean function synthesis problem. This\nincompleteness of the problem will also generate spurious inferences, which are\na serious threat to valid inductive inference rules. Using random Boolean data\nas a null model, here we attempt to measure the competition between valid and\nspurious inductive inference rules from a given data set. We formulate two\ngreedy search algorithms, which synthesize a given Boolean response variable in\na sparse disjunct normal form, and respectively a sparse generalized algebraic\nnormal form of the variables from the observation data, and we evaluate\nnumerically their performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02068v1"
    },
    {
        "title": "Characterization of Time Series Via Rényi Complexity-Entropy Curves",
        "authors": [
            "Max Jauregui",
            "Luciano Zunino",
            "Ervin K. Lenzi",
            "Renio S. Mendes",
            "Haroldo V. Ribeiro"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  One of the most useful tools for distinguishing between chaotic and\nstochastic time series is the so-called complexity-entropy causality plane.\nThis diagram involves two complexity measures: the Shannon entropy and the\nstatistical complexity. Recently, this idea has been generalized by considering\nthe Tsallis monoparametric generalization of the Shannon entropy, yielding\ncomplexity-entropy curves. These curves have proven to enhance the\ndiscrimination among different time series related to stochastic and chaotic\nprocesses of numerical and experimental nature. Here we further explore these\ncomplexity-entropy curves in the context of the R\\'enyi entropy, which is\nanother monoparametric generalization of the Shannon entropy. By combining the\nR\\'enyi entropy with the proper generalization of the statistical complexity,\nwe associate a parametric curve (the R\\'enyi complexity-entropy curve) with a\ngiven time series. We explore this approach in a series of numerical and\nexperimental applications, demonstrating the usefulness of this new technique\nfor time series analysis. We show that the R\\'enyi complexity-entropy curves\nenable the differentiation among time series of chaotic, stochastic, and\nperiodic nature. In particular, time series of stochastic nature are associated\nwith curves displaying positive curvature in a neighborhood of their initial\npoints, whereas curves related to chaotic phenomena have a negative curvature;\nfinally, periodic time series are represented by vertical straight lines.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.05738v1"
    },
    {
        "title": "Multivariate analysis of short time series in terms of ensembles of\n  correlation matrices",
        "authors": [
            "Manan Vyas",
            "T. Guhr",
            "T. H. Seligman"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  When dealing with non-stationary systems, for which many time series are\navailable, it is common to divide time in epochs, i.e. smaller time intervals\nand deal with short time series in the hope to have some form of approximate\nstationarity on that time scale. We can then study time evolution by looking at\nproperties as a function of the epochs. This leads to singular correlation\nmatrices and thus poor statistics. In the present paper, we propose an ensemble\ntechnique to deal with a large set of short time series without any\nconsideration of non-stationarity. Given a singular data matrix, we randomly\nselect subsets of time series and thus create an ensemble of non-singular\ncorrelation matrices. As the selection possibilities are binomially large, we\nwill obtain good statistics for eigenvalues of correlation matrices, which are\ntypically not independent. Once we defined the ensemble, we analyze its\nbehavior for constant and block-diagonal correlations and compare numerics with\nanalytic results for the corresponding correlated Wishart ensembles. We discuss\ndifferences resulting from spurious correlations due to repetitive use of\ntime-series. The usefulness of this technique should extend beyond the\nstationary case if, on the time scale of the epochs, we have quasi-stationarity\nat least for most epochs.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.07790v2"
    },
    {
        "title": "Beyond single-threshold searches: the Event Stacking Test",
        "authors": [
            "Ryan Lynch",
            "Salvatore Vitale",
            "Erik Katsavounidis"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We present a new statistical test that examines the consistency of the tails\nof two empirical distributions at multiple thresholds. Such distributions are\noften encountered in counting experiments, in physics and elsewhere, where the\nsignificance of populations of events is evaluated. This multi-threshold\napproach has the effect of \"stacking\" multiple events into the tail bin of the\ndistribution, and thus we call it the Event Stacking Test. This test has the\nability to confidently detect inconsistencies composed of multiple events, even\nif these events are low-significance outliers in isolation. We derive the Event\nStacking Test from first principles and show that the p-value it reports is a\nwell-calibrated representation of noise fluctuations. When applying this test\nto the detection of gravitational-wave transients in LIGO-Virgo data, we find\nthat it performs better than or comparably to other statistical tests\nhistorically used within the gravitational-wave community. This test is\nparticularly well-suited for detecting classes of gravitational-wave transients\nthat are minimally-modeled, i.e., gravitational-wave bursts. We show that the\nEvent Stacking Test allows us to set upper limits on the astrophysical\nrate-density of gravitational-wave bursts that are stricter than those set\nusing other statistical tests by factors of up to 2 - 3.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.01297v1"
    },
    {
        "title": "Fast and interpretable classification of small X-ray diffraction\n  datasets using data augmentation and deep neural networks",
        "authors": [
            "Felipe Oviedo",
            "Zekun Ren",
            "Shijing Sun",
            "Charlie Settens",
            "Zhe Liu",
            "Noor Titan Putri Hartono",
            "Ramasamy Savitha",
            "Brian L. DeCost",
            "Siyu I. P. Tian",
            "Giuseppe Romano",
            "Aaron Gilad Kusne",
            "Tonio Buonassisi"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  X-ray diffraction (XRD) data acquisition and analysis is among the most\ntime-consuming steps in the development cycle of novel thin-film materials. We\npropose a machine-learning-enabled approach to predict crystallographic\ndimensionality and space group from a limited number of thin-film XRD patterns.\nWe overcome the scarce-data problem intrinsic to novel materials development by\ncoupling a supervised machine learning approach with a model agnostic,\nphysics-informed data augmentation strategy using simulated data from the\nInorganic Crystal Structure Database (ICSD) and experimental data. As a test\ncase, 115 thin-film metal halides spanning 3 dimensionalities and 7\nspace-groups are synthesized and classified. After testing various algorithms,\nwe develop and implement an all convolutional neural network, with cross\nvalidated accuracies for dimensionality and space-group classification of 93%\nand 89%, respectively. We propose average class activation maps, computed from\na global average pooling layer, to allow high model interpretability by human\nexperimentalists, elucidating the root causes of misclassification. Finally, we\nsystematically evaluate the maximum XRD pattern step size (data acquisition\nrate) before loss of predictive accuracy occurs, and determine it to be\n0.16{\\deg}, which enables an XRD pattern to be obtained and classified in 5.5\nminutes or less.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.08425v2"
    },
    {
        "title": "Recycling cardiogenic artifacts in impedance pneumography",
        "authors": [
            "Yao Lu",
            "Hau-tieng Wu",
            "John Malik"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Purpose: Biomedical sensors often exhibit cardiogenic artifacts which, while\ndistorting the signal of interest, carry useful hemodynamic information. We\npropose an algorithm to remove and extract hemodynamic information from these\ncardiogenic artifacts. Methods: We apply a nonlinear time-frequency analysis\ntechnique, the de-shape synchrosqueezing transform (dsSST), to adaptively\nisolate the high- and low-frequency components of a single-channel signal. We\ndemonstrate this technique's effectiveness by removing and deriving hemodynamic\ninformation from the cardiogenic artifact in an impedance pneumography (IP).\nResults: The instantaneous heart rate is extracted, and the cardiac and\nrespiratory signals are reconstructed. Conclusions: The dsSST is suitable for\ngenerating useful hemodynamic information from the cardiogenic artifact in a\nsingle-channel IP. We propose that the usefulness of the dsSST as a recycling\ntool extends to other biomedical sensors exhibiting cardiogenic artifacts.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.11074v2"
    },
    {
        "title": "Bayesian parameter estimation of miss-specified models",
        "authors": [
            "Johannes Oberpriller",
            "T. A. Enßlin"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Fitting a simplifying model with several parameters to real data of complex\nobjects is a highly nontrivial task, but enables the possibility to get\ninsights into the objects physics. Here, we present a method to infer the\nparameters of the model, the model error as well as the statistics of the model\nerror. This method relies on the usage of many data sets in a simultaneous\nanalysis in order to overcome the problems caused by the degeneracy between\nmodel parameters and model error. Errors in the modeling of the measurement\ninstrument can be absorbed in the model error allowing for applications with\ncomplex instruments.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.08194v1"
    },
    {
        "title": "Event-shape engineering and heavy-flavour observables in relativistic\n  heavy-ion collisions",
        "authors": [
            "A. Beraudo",
            "A. De Pace",
            "M. Monteno",
            "M. Nardi",
            "F. Prino"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Traditionally, events collected at relativistic heavy-ion colliders are\nclassified according to some centrality estimator (e.g. the number of produced\ncharged particles) related to the initial energy density and volume of the\nsystem. In a naive picture the latter are directly related to the impact\nparameter of the two nuclei, which sets also the initial eccentricity of the\nsystem: zero in the case of the most central events and getting larger for more\nperipheral collisions. A more realistic modelling requires to take into account\nevent-by-event fluctuations, in particular in the nucleon positions within the\ncolliding nuclei: collisions belonging to the same centrality class can give\nrise to systems with different initial eccentricity and hence different flow\nharmonics for the final hadron distributions. This issue can be addressed by an\nevent-shape-engineering analysis, consisting in selecting events with the same\ncentrality but different magnitude of the average bulk anisotropic flow and\ntherefore of the initial-state eccentricity. In this paper we present the\nimplementation of this analysis in the POWLANG transport model, providing\npredictions for the transverse-momentum and angular distributions of charm and\nbeauty hadrons for event-shape selected collisions. In this way it is possible\nto get information on how the heavy quarks propagating (and hadronizing) in a\nhot environment respond both to its energy density and to its geometric\nasymmetry, breaking the perfect correlation between eccentricity and impact\nparameter which characterizes a modelling of the medium based on smooth average\ninitial conditions\n",
        "pdf_link": "http://arxiv.org/pdf/1812.08337v1"
    },
    {
        "title": "Efficiency correction for cumulants of multiplicity distributions based\n  on track-by-track efficiency",
        "authors": [
            "Xiaofeng Luo",
            "Toshihiro Nonaka"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We propose a simplified procedure for the experimental application of the\nefficiency correction on higher order cumulants in heavy-ion collisions. By\nusing the track-by-track efficiency, we can eliminate possible bias arising\nfrom the average efficiencies calculated within the arbitrary binning of the\nphase space. Furthermore, the corrected particle spectra is no longer necessary\nfor the average efficiency estimation and the time cost for the calculation of\nbootstrap statistical error can be significantly reduced.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.10303v2"
    },
    {
        "title": "Data blinding for the nEDM experiment at PSI",
        "authors": [
            "N. J. Ayres",
            "G. Ban",
            "G. Bison",
            "K. Bodek",
            "V. Bondar",
            "E. Chanel",
            "P. -J. Chiu",
            "C. Crawford",
            "M. Daum",
            "S. Emmenegger",
            "L. Ferraris-Bouchez",
            "P. Flaux",
            "P. G Harris",
            "Z. Grujić",
            "N. Hild",
            "J. Hommet",
            "B. Lauss",
            "T. Lefort",
            "Y. Lemiere",
            "M. Kasprzak",
            "Y. Kermaidic",
            "K. Kirch",
            "S. Komposch",
            "A. Kozela",
            "J. Krempel",
            "A. Leredde",
            "P. Mohanmurthy",
            "A. Mtchedlishvili",
            "O. Naviliat-Cuncic",
            "D. Pais",
            "G. Pignol",
            "F. M. Piegsa",
            "M. Rawlik",
            "D. Rebreyend",
            "I. Rienäcker",
            "D. Ries",
            "S. Roccia",
            "D. Rozpedzik",
            "P. Schmidt-Wellenburg",
            "A. Schnabel",
            "R. Virot",
            "A. Weis",
            "E. Wursten",
            "J. Zejma",
            "G. Zsigmond"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Psychological bias towards, or away from, a prior measurement or a theory\nprediction is an intrinsic threat to any data analysis. While various methods\ncan be used to avoid the bias, e.g. actively not looking at the result, only\ndata blinding is a traceable and thus trustworthy method to circumvent the bias\nand to convince a public audience that there is not even an accidental\npsychological bias.\n  Data blinding is nowadays a standard practice in particle physics, but it is\nparticularly difficult for experiments searching for the neutron electric\ndipole moment, as several cross measurements, in particular of the magnetic\nfield, create a self-consistent network into which it is hard to inject a fake\nsignal.\n  We present an algorithm that modifies the data without influencing the\nexperiment. Results of an automated analysis of the data are used to change the\nrecorded spin state of a few neutrons of each measurement cycle.\n  The flexible algorithm is applied twice to the data, to provide different\ndata to various analysis teams. This gives us the option to sequentially apply\nvarious blinding offsets for separate analysis steps with independent teams.\nThe subtle modification of the data allows us to modify the algorithm and to\nproduce a re-blinded data set without revealing the blinding secret. The method\nwas designed for the 2015/2016 measurement campaign of the nEDM experiment at\nthe Paul Scherrer Institute. However, it can be re-used with minor modification\nfor the follow-up experiment n2EDM, and may be suitable for comparable efforts.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.09244v2"
    },
    {
        "title": "MODULO: A software for Multiscale Proper Orthogonal Decomposition of\n  data",
        "authors": [
            "Davide Ninni",
            "Miguel A. Mendez"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  In the era of the Big Data revolution, methods for the automatic discovery of\nregularities in large datasets are becoming essential tools in applied\nsciences. This article presents an open software package, named MODULO (MODal\nmULtiscale pOd), to perform the Multiscale Proper Orthogonal Decomposition\n(mPOD) of numerical and experimental data. This novel decomposition combines\nMulti-resolution Analysis (MRA) and standard Proper Orthogonal Decomposition\n(POD) to allow for the optimal compromise between decomposition convergence and\nspectral purity of its modes. The software is equipped with a Graphical User\nInterface (GUI) and enriched by numerous examples and video tutorials (see\nYoutube channel MODULO mPOD). The MATLAB source codes and an executable for\nWindows users can be downloaded at\n\\url{https://github.com/mendezVKI/MODULO/releases}; a collection of exercises\nin Matlab and Python are provided in \\url{https://github.com/mendezVKI/MODULO}\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12123v2"
    },
    {
        "title": "Bayesian machine scientist to compare data collapses for the Nikuradse\n  dataset",
        "authors": [
            "Ignasi Reichardt",
            "Jordi Pallares Marta Sales-Pardo",
            "Roger Guimera"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Ever since Nikuradse's experiments on turbulent friction in 1933, there have\nbeen theoretical attempts to describe his measurements by collapsing the data\ninto single-variable functions. However, this approach, which is common in\nother areas of physics and in other fields, is limited by the lack of rigorous\nquantitative methods to compare alternative data collapses. Here, we address\nthis limitation by using an unsupervised method to find analytic functions that\noptimally describe each of the data collapses for the Nikuradse dataset. By\ndescaling these analytic functions, we show that a low dispersion of the scaled\ndata does not guarantee that a data collapse is a good description of the\noriginal data. In fact, we find that, out of all the proposed data collapses,\nthe original one proposed by Prandtl and Nikuradse over 80 years ago provides\nthe best description of the data so far, and that it also agrees well with\nrecent experimental data, provided that some model parameters are allowed to\nvary across experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12163v1"
    },
    {
        "title": "An encryption-decryption framework for validating single-particle\n  imaging",
        "authors": [
            "Zhou Shen",
            "Colin Zhi Wei Teo",
            "Kartik Ayyer",
            "N. Duane Loh"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We propose an encryption-decryption framework for validating diffraction\nintensity volumes reconstructed using single-particle imaging (SPI) with x-ray\nfree-electron lasers (XFELs) when the ground truth volume is absent. This\nframework exploits each reconstructed volumes' ability to decipher latent\nvariables (e.g. orientations) of unseen sentinel diffraction patterns. Using\nthis framework, we quantify novel measures of orientation disconcurrence,\ninconsistency, and disagreement between the decryptions by two independently\nreconstructed volumes. We also study how these measures can be used to define\ndata sufficiency and its relation to spatial resolution, and the practical\nconsequences of focusing XFEL pulses to smaller foci. This framework overcomes\ncritical ambiguities in using Fourier Shell Correlation (FSC) as a validation\nmeasure for SPI. Finally, we show how this encryption-decryption framework\nnaturally leads to an information-theoretic reformulation of the resolving\npower of XFEL-SPI, which we hope will lead to principled frameworks for\nexperiment and instrument design.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.03054v2"
    },
    {
        "title": "A GPU based multidimensional amplitude analysis to search for tetraquark\n  candidates",
        "authors": [
            "Nairit Sur",
            "Leonardo Cristella",
            "Adriano Di Florio",
            "Vincenzo Mastrapasqua"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The demand for computational resources is steadily increasing in experimental\nhigh energy physics as the current collider experiments continue to accumulate\nhuge amounts of data and physicists indulge in more complex and ambitious\nanalysis strategies. This is especially true in the fields of hadron\nspectroscopy and flavour physics where the analyses often depend on complex\nmultidimensional unbinned maximum-likelihood fits, with several dozens of free\nparameters, with an aim to study the internal structure of hadrons.\n  Graphics processing units (GPUs) represent one of the most sophisticated and\nversatile parallel computing architectures that are becoming popular toolkits\nfor high energy physicists to meet their computational demands. GooFit is an\nupcoming open-source tool interfacing ROOT/RooFit to the CUDA platform on\nNVIDIA GPUs that acts as a bridge between the MINUIT minimization algorithm and\na parallel processor, allowing probability density functions to be estimated on\nmultiple cores simultaneously.\n  In this article, a full-fledged amplitude analysis framework developed using\nGooFit is tested for its speed and reliability. The four-dimensional fitter\nframework, one of the firsts of its kind to be built on GooFit, is geared\ntowards the search for exotic tetraquark states in the $B^0 \\rightarrow J/\\psi\nK \\pi$ decays and can also be seamlessly adapted for other similar analyses.\nThe GooFit fitter, running on GPUs, shows a remarkable improvement in the\ncomputing speed compared to a ROOT/RooFit implementation of the same analysis\nrunning on multi-core CPU clusters. Furthermore, it shows sensitivity to\ncomponents with small contributions to the overall fit. It has the potential to\nbe a powerful tool for sensitive and computationally intensive physics\nanalyses.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.14931v3"
    },
    {
        "title": "Graph neural network for 3D classification of ambiguities and optical\n  crosstalk in scintillator-based neutrino detectors",
        "authors": [
            "Saúl Alonso-Monsalve",
            "Dana Douqa",
            "César Jesús-Valls",
            "Thorsten Lux",
            "Sebastian Pina-Otey",
            "Federico Sánchez",
            "Davide Sgalaberna",
            "Leigh H. Whitehead"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Deep learning tools are being used extensively in high energy physics and are\nbecoming central in the reconstruction of neutrino interactions in particle\ndetectors. In this work, we report on the performance of a graph neural network\nin assisting with particle flow event reconstruction. The three-dimensional\nreconstruction of particle tracks produced in neutrino interactions can be\nsubject to ambiguities due to high multiplicity signatures in the detector or\nleakage of signal between neighboring active detector volumes. Graph neural\nnetworks potentially have the capability of identifying all these features to\nboost the reconstruction performance. As an example case study, we tested a\ngraph neural network, inspired by the GraphSAGE algorithm, on a novel\n3D-granular plastic-scintillator detector, that will be used to upgrade the\nnear detector of the T2K experiment. The developed neural network has been\ntrained and tested on diverse neutrino interaction samples, showing very\npromising results: the classification of particle track voxels produced in the\ndetector can be done with efficiencies and purities of 94-96% per event and\nmost of the ambiguities can be identified and rejected, while being robust\nagainst systematic effects.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.00688v2"
    },
    {
        "title": "Criteria for projected discovery and exclusion sensitivities of counting\n  experiments",
        "authors": [
            "Prudhvi N. Bhattiprolu",
            "Stephen P. Martin",
            "James D. Wells"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The projected discovery and exclusion capabilities of particle physics and\nastrophysics/cosmology experiments are often quantified using the median\nexpected $p$-value or its corresponding significance. We argue that this\ncriterion leads to flawed results, which for example can counterintuitively\nproject lessened sensitivities if the experiment takes more data or reduces its\nbackground. We discuss the merits of several alternatives to the median\nexpected significance, both when the background is known and when it is subject\nto some uncertainty. We advocate for standard use of the \"exact Asimov\nsignificance\" $Z^{\\rm A}$ detailed in this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.07249v2"
    },
    {
        "title": "Estimating Experimental Dispersion Curves from Steady-State Frequency\n  Response Measurements",
        "authors": [
            "V. V. N. Sriram Malladi",
            "Mohammad I. Albakri",
            "Manu Krishnan",
            "Serkan Gugercin",
            "Pablo A. Tarazaga"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Dispersion curves characterize the frequency dependence of the phase and the\ngroup velocities of propagating elastic waves. Many analytical and numerical\ntechniques produce dispersion curves from physics-based models. However, it is\noften challenging to accurately model engineering structures with intricate\ngeometric features and inhomogeneous material properties. For such cases, this\npaper proposes a novel method to estimate group velocities from experimental\ndata-driven models. Experimental frequency response functions (FRFs) are used\nto develop data-driven models, {which are then used to estimate dispersion\ncurves}. The advantages of this approach over other traditionally used\ntransient techniques stem from the need to conduct only steady-state\nexperiments. In comparison, transient experiments often need a higher-sampling\nrate for wave-propagation applications and are more susceptible to noise.\n  The vector-fitting (VF) algorithm is adopted to develop data-driven models\nfrom experimental in-plane and out-of-plane FRFs of a one-dimensional\nstructure. The quality of the corresponding data-driven estimates is evaluated\nusing an analytical Timoshenko beam as a baseline. The data-driven model (using\nthe out-of-plane FRFs) estimates the anti-symmetric ($A_0$) group velocity with\na maximum error of $4\\%$ over a 40~kHz frequency band. In contrast, group\nvelocities estimated from transient experiments resulted in a maximum error of\n$6\\%$ over the same frequency band.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.00155v1"
    },
    {
        "title": "Persistent Homology of Fractional Gaussian Noise",
        "authors": [
            "H. Masoomy",
            "B. Askari",
            "M. N. Najafi",
            "S. M. S. Movahed"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  In this paper, we employ the persistent homology (PH) technique to examine\nthe topological properties of fractional Gaussian noise (fGn). We develop the\nweighted natural visibility graph algorithm, and the associated simplicial\ncomplexes through the filtration process are quantified by PH. The evolution of\nthe homology group dimension represented by Betti numbers demonstrates a strong\ndependency on the Hurst exponent ($H$). The coefficients of the birth and death\ncurve of the $k$-dimensional topological holes ($k$-holes) at a given threshold\ndepend on $H$ which is almost not affected by finite sample size. We show that\nthe distribution function of a lifetime for $k$-holes decays exponentially and\nthe corresponding slope is an increasing function versus $H$, and more\ninterestingly, the sample size effect completely disappears in this quantity.\nThe persistence entropy logarithmically grows with the size of the visibility\ngraph of a system with almost $H$-dependent prefactors. On the contrary, the\nlocal statistical features are not able to determine the corresponding Hurst\nexponent of fGn data, while the moments of eigenvalue distribution ($M_{n}$)\nfor $n\\ge1$ reveal a dependency on $H$, containing the sample size effect.\nFinally, the PH shows the correlated behavior of electroencephalography for\nboth healthy and schizophrenic samples.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03328v2"
    },
    {
        "title": "Random Conical Tilt Reconstruction without Particle Picking in\n  Cryo-electron Microscopy",
        "authors": [
            "Ti-Yen Lan",
            "Nicolas Boumal",
            "Amit Singer"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We propose a method to reconstruct the 3-D molecular structure from\nmicrographs collected at just one sample tilt angle in the random conical tilt\nscheme in cryo-electron microscopy. Our method uses autocorrelation analysis on\nthe micrographs to estimate features of the molecule which are invariant under\ncertain nuisance parameters such as the positions of molecular projections in\nthe micrographs. This enables us to reconstruct the molecular structure\ndirectly from micrographs, completely circumventing the need for particle\npicking. We demonstrate reconstructions with simulated data and investigate the\neffect of the missing-cone region. These results show promise to reduce the\nsize limit for single particle reconstruction in cryo-electron microscopy.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03500v1"
    },
    {
        "title": "Multivariate cumulants in flow analyses: The Next Generation",
        "authors": [
            "Ante Bilandzic",
            "Marcel Lesch",
            "Cindy Mordasini",
            "Seyed Farid Taghavi"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We reconcile for the first time the strict mathematical formalism of\nmultivariate cumulants with the usage of cumulants in anisotropic flow analyses\nin high-energy nuclear collisions. This reconciliation yields to the next\ngeneration of estimators to be used in flow analyses. We review all fundamental\nproperties of multivariate cumulants and use them as a foundation to establish\ntwo simple necessary conditions to determine whether some multivariate random\nvariable is a multivariate cumulant in the basis they are expressed in. We\nargue that properties of cumulants are preserved only for the stochastic\nvariables on which the cumulant expansion has been performed directly, and if\nthere are no underlying symmetries due to which some terms in the cumulant\nexpansion are identically zero. We illustrate one possibility of new\nmultivariate cumulants of azimuthal angles by defining them event-by-event and\nby keeping all non-isotropic terms in the cumulant expansion. Further, we\nintroduce new cumulants of flow amplitudes named Asymmetric Cumulants, which\ngeneralize recently introduced Symmetric Cumulants for the case when flow\namplitudes are raised to different powers. Finally, we present the new concept\nof Cumulants of Symmetry Plane Correlations and provide the first realisation\nfor the lowest orders. The new estimators can be used directly to constrain the\nmultivariate probability density function of flow fluctuations, since its\nfunctional form can be reconstructed only from its true moments or cumulants.\nThe new definition for cumulants of azimuthal angles enables separation of\nnonflow and flow contributions, and offers first insights into how the\ncombinatorial background contributes for small multiplicities to flow\nmeasurements with correlation techniques. All the presented results are\nsupported by Monte Carlo studies using state-of-the-art models.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.05619v4"
    },
    {
        "title": "Fitting very flexible models: Linear regression with large numbers of\n  parameters",
        "authors": [
            "David W. Hogg",
            "Soledad Villar"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  There are many uses for linear fitting; the context here is interpolation and\ndenoising of data, as when you have calibration data and you want to fit a\nsmooth, flexible function to those data. Or you want to fit a flexible function\nto de-trend a time series or normalize a spectrum. In these contexts,\ninvestigators often choose a polynomial basis, or a Fourier basis, or wavelets,\nor something equally general. They also choose an order, or number of basis\nfunctions to fit, and (often) some kind of regularization. We discuss how this\nbasis-function fitting is done, with ordinary least squares and extensions\nthereof. We emphasize that it is often valuable to choose far more parameters\nthan data points, despite folk rules to the contrary: Suitably regularized\nmodels with enormous numbers of parameters generalize well and make good\npredictions for held-out data; over-fitting is not (mainly) a problem of having\ntoo many parameters. It is even possible to take the limit of infinite\nparameters, at which, if the basis and regularization are chosen correctly, the\nleast-squares fit becomes the mean of a Gaussian process. We recommend\ncross-validation as a good empirical method for model selection (for example,\nsetting the number of parameters and the form of the regularization), and\njackknife resampling as a good empirical method for estimating the\nuncertainties of the predictions made by the model. We also give advice for\nbuilding stable computational implementations.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.07256v1"
    },
    {
        "title": "E Pluribus Unum Ex Machina: Learning from Many Collider Events at Once",
        "authors": [
            "Benjamin Nachman",
            "Jesse Thaler"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  There have been a number of recent proposals to enhance the performance of\nmachine learning strategies for collider physics by combining many distinct\nevents into a single ensemble feature. To evaluate the efficacy of these\nproposals, we study the connection between single-event classifiers and\nmulti-event classifiers under the assumption that collider events are\nindependent and identically distributed (IID). We show how one can build\noptimal multi-event classifiers from single-event classifiers, and we also show\nhow to construct multi-event classifiers such that they produce optimal\nsingle-event classifiers. This is illustrated for a Gaussian example as well as\nfor classification tasks relevant for searches and measurements at the Large\nHadron Collider. We extend our discussion to regression tasks by showing how\nthey can be phrased in terms of parametrized classifiers. Empirically, we find\nthat training a single-event (per-instance) classifier is more effective than\ntraining a multi-event (per-ensemble) classifier, as least for the cases we\nstudied, and we relate this fact to properties of the loss function gradient in\nthe two cases. While we did not identify a clear benefit from using multi-event\nclassifiers in the collider context, we speculate on the potential value of\nthese methods in cases involving only approximate independence, as relevant for\njet substructure studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.07263v3"
    },
    {
        "title": "Pattern Ensembling for Spatial Trajectory Reconstruction",
        "authors": [
            "Shivam Pathak",
            "Mingyi He",
            "Sergey Malinchik",
            "Stanislav Sobolevsky"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Digital sensing provides an unprecedented opportunity to assess and\nunderstand mobility. However, incompleteness, missing information, possible\ninaccuracies, and temporal heterogeneity in the geolocation data can undermine\nits applicability. As mobility patterns are often repeated, we propose a method\nto use similar trajectory patterns from the local vicinity and\nprobabilistically ensemble them to robustly reconstruct missing or unreliable\nobservations. We evaluate the proposed approach in comparison with traditional\nfunctional trajectory interpolation using a case of sea vessel trajectory data\nprovided by The Automatic Identification System (AIS). By effectively\nleveraging the similarities in real-world trajectories, our pattern ensembling\nmethod helps to reconstruct missing trajectory segments of extended length and\ncomplex geometry. It can be used for locating mobile objects when temporary\nunobserved as well as for creating an evenly sampled trajectory interpolation\nuseful for further trajectory mining.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.09844v1"
    },
    {
        "title": "Development of a Vertex Finding Algorithm using Recurrent Neural Network",
        "authors": [
            "Kiichi Goto",
            "Taikan Suehara",
            "Tamaki Yoshioka",
            "Masakazu Kurata",
            "Hajime Nagahara",
            "Yuta Nakashima",
            "Noriko Takemura",
            "Masako Iwasaki"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Deep learning is a rapidly-evolving technology with possibility to\nsignificantly improve physics reach of collider experiments. In this study we\ndeveloped a novel algorithm of vertex finding for future lepton colliders such\nas the International Linear Collider. We deploy two networks; one is simple\nfully-connected layers to look for vertex seeds from track pairs, and the other\nis a customized Recurrent Neural Network with an attention mechanism and an\nencoder-decoder structure to associate tracks to the vertex seeds. The\nperformance of the vertex finder is compared with the standard ILC\nreconstruction algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.11906v5"
    },
    {
        "title": "Learning to Isolate Muons",
        "authors": [
            "Julian Collado",
            "Kevin Bauer",
            "Edmund Witkowski",
            "Taylor Faucett",
            "Daniel Whiteson",
            "Pierre Baldi"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Distinguishing between prompt muons produced in heavy boson decay and muons\nproduced in association with heavy-flavor jet production is an important task\nin analysis of collider physics data. We explore whether there is information\navailable in calorimeter deposits that is not captured by the standard approach\nof isolation cones. We find that convolutional networks and particle-flow\nnetworks accessing the calorimeter cells surpass the performance of isolation\ncones, suggesting that the radial energy distribution and the angular structure\nof the calorimeter deposits surrounding the muon contain unused discrimination\npower. We assemble a small set of high-level observables which summarize the\ncalorimeter information and close the performance gap with networks which\nanalyze the calorimeter cells directly. These observables are theoretically\nwell-defined and can be studied with collider data.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.02278v2"
    },
    {
        "title": "Point Cloud Transformers applied to Collider Physics",
        "authors": [
            "Vinicius Mikuni",
            "Florencia Canelli"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Methods for processing point cloud information have seen a great success in\ncollider physics applications. One recent breakthrough in machine learning is\nthe usage of Transformer networks to learn semantic relationships between\nsequences in language processing. In this work, we apply a modified Transformer\nnetwork called Point Cloud Transformer as a method to incorporate the\nadvantages of the Transformer architecture to an unordered set of particles\nresulting from collision events. To compare the performance with other\nstrategies, we study jet-tagging applications for highly-boosted particles.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.05073v2"
    },
    {
        "title": "Sequence-based Machine Learning Models in Jet Physics",
        "authors": [
            "Rafael Teixeira de Lima"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Sequence-based modeling broadly refers to algorithms that act on data that is\nrepresented as an ordered set of input elements. In particular, Machine\nLearning algorithms with sequences as inputs have seen successfull applications\nto important problems, such as Natural Language Processing (NLP) and speech\nsignal modeling. The usage this class of models in collider physics leverages\ntheir ability to act on data with variable sequence lengths, such as\nconstituents inside a jet. In this document, we explore the application of\nRecurrent Neural Networks (RNNs) and other sequence-based neural network\narchitectures to classify jets, regress jet-related quantities and to build a\nphysics-inspired jet representation, in connection to jet clustering\nalgorithms. In addition, alternatives to sequential data representations are\nbriefly discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.06128v1"
    },
    {
        "title": "Data-driven formulation of natural laws by recursive-LASSO-based\n  symbolic regression",
        "authors": [
            "Yuma Iwasaki",
            "Masahiko Ishida"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Discovery of new natural laws has for a long time relied on the inspiration\nof some genius. Recently, however, machine learning technologies, which analyze\nbig data without human prejudice and bias, are expected to find novel natural\nlaws. Here we demonstrate that our proposed machine learning,\nrecursive-LASSO-based symbolic (RLS) regression, enables data-driven\nformulation of natural laws from noisy data. The RLS regression recurrently\nrepeats feature generation and feature selection, eventually constructing a\ndata-driven model with highly nonlinear features. This data-driven formulation\nmethod is quite general and thus can discover new laws in various scientific\nfields.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.09210v1"
    },
    {
        "title": "Performance of a Geometric Deep Learning Pipeline for HL-LHC Particle\n  Tracking",
        "authors": [
            "Xiangyang Ju",
            "Daniel Murnane",
            "Paolo Calafiura",
            "Nicholas Choma",
            "Sean Conlon",
            "Steve Farrell",
            "Yaoyuan Xu",
            "Maria Spiropulu",
            "Jean-Roch Vlimant",
            "Adam Aurisano",
            "V Hewes",
            "Giuseppe Cerati",
            "Lindsey Gray",
            "Thomas Klijnsma",
            "Jim Kowalkowski",
            "Markus Atkinson",
            "Mark Neubauer",
            "Gage DeZoort",
            "Savannah Thais",
            "Aditi Chauhan",
            "Alex Schuy",
            "Shih-Chieh Hsu",
            "Alex Ballow",
            "and Alina Lazar"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The Exa.TrkX project has applied geometric learning concepts such as metric\nlearning and graph neural networks to HEP particle tracking. Exa.TrkX's\ntracking pipeline groups detector measurements to form track candidates and\nfilters them. The pipeline, originally developed using the TrackML dataset (a\nsimulation of an LHC-inspired tracking detector), has been demonstrated on\nother detectors, including DUNE Liquid Argon TPC and CMS High-Granularity\nCalorimeter. This paper documents new developments needed to study the physics\nand computing performance of the Exa.TrkX pipeline on the full TrackML dataset,\na first step towards validating the pipeline using ATLAS and CMS data. The\npipeline achieves tracking efficiency and purity similar to production tracking\nalgorithms. Crucially for future HEP applications, the pipeline benefits\nsignificantly from GPU acceleration, and its computational requirements scale\nclose to linearly with the number of particles in the event.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.06995v2"
    },
    {
        "title": "Pandemonium: a clustering tool to partition parameter space --\n  application to the B anomalies",
        "authors": [
            "Ursula Laa",
            "German Valencia"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We introduce the interactive tool pandemonium to cluster model predictions\nthat depend on a set of parameters. The model predictions are used to define\nthe coordinates in observable space which go into the clustering. The results\nof this partitioning are then visualized in both observable and parameter space\nto study correlations between them. The tool offers multiple choices for\ncoordinates, distance functions and linkage methods within hierarchical\nclustering. It provides a set of diagnostic statistics and visualization\nmethods to study the clustering results in order to interpret the outcome. The\nmethods are most useful in an interactive environment that enables exploration,\nand we have implemented them with a graphical user interface in R. We\ndemonstrate the concepts with an application to phenomenological studies in\nflavor physics in the context of the so-called B anomalies.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.07937v2"
    },
    {
        "title": "SymPKF: a symbolic and computational toolbox for the design of\n  parametric Kalman filter dynamics",
        "authors": [
            "Olivier Pannekoucke",
            "Philippe Arbogast"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Recent researches in data assimilation lead to the introduction of the\nparametric Kalman filter (PKF): an implementation of the Kalman filter, where\nthe covariance matrices are approximated by a parameterized covariance model.\nIn the PKF, the dynamics of the covariance during the forecast step relies on\nthe prediction of the covariance parameters. Hence, the design of the parameter\ndynamics is crucial while it can be tedious to do this by hand. This\ncontribution introduces a python package, SymPKF, able to compute PKF dynamics\nfor univariate statistics and when the covariance model is parameterized from\nthe variance and the local anisotropy of the correlations. The ability of\nSymPKF to produce the PKF dynamics is shown on a non-linear diffusive advection\n(Burgers equation) over a 1D domain and the linear advection over a 2D domain.\nThe computation of the PKF dynamics is performed at a symbolic level, but an\nautomatic code generator is also introduced to perform numerical simulations. A\nfinal multivariate example illustrates the potential of SymPKF to go beyond the\nunivariate case.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.09226v2"
    },
    {
        "title": "Reduced Precision Strategies for Deep Learning: A High Energy Physics\n  Generative Adversarial Network Use Case",
        "authors": [
            "Florian Rehm",
            "Sofia Vallecorsa",
            "Vikram Saletore",
            "Hans Pabst",
            "Adel Chaibi",
            "Valeriu Codreanu",
            "Kerstin Borras",
            "Dirk Krücker"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Deep learning is finding its way into high energy physics by replacing\ntraditional Monte Carlo simulations. However, deep learning still requires an\nexcessive amount of computational resources. A promising approach to make deep\nlearning more efficient is to quantize the parameters of the neural networks to\nreduced precision. Reduced precision computing is extensively used in modern\ndeep learning and results to lower execution inference time, smaller memory\nfootprint and less memory bandwidth. In this paper we analyse the effects of\nlow precision inference on a complex deep generative adversarial network model.\nThe use case which we are addressing is calorimeter detector simulations of\nsubatomic particle interactions in accelerator based high energy physics. We\nemploy the novel Intel low precision optimization tool (iLoT) for quantization\nand compare the results to the quantized model from TensorFlow Lite. In the\nperformance benchmark we gain a speed-up of 1.73x on Intel hardware for the\nquantized iLoT model compared to the initial, not quantized, model. With\ndifferent physics-inspired self-developed metrics, we validate that the\nquantized iLoT model shows a lower loss of physical accuracy in comparison to\nthe TensorFlow Lite model.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.10142v1"
    },
    {
        "title": "Mixture Density Network Estimation of Continuous Variable Maximum\n  Likelihood Using Discrete Training Samples",
        "authors": [
            "Charles Burton",
            "Spencer Stubbs",
            "Peter Onyisi"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Mixture Density Networks (MDNs) can be used to generate probability density\nfunctions of model parameters $\\boldsymbol{\\theta}$ given a set of observables\n$\\mathbf{x}$. In some applications, training data are available only for\ndiscrete values of a continuous parameter $\\boldsymbol{\\theta}$. In such\nsituations a number of performance-limiting issues arise which can result in\nbiased estimates. We demonstrate the usage of MDNs for parameter estimation,\ndiscuss the origins of the biases, and propose a corrective method for each\nissue.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.13416v2"
    },
    {
        "title": "Shower Identification in Calorimeter using Deep Learning",
        "authors": [
            "Yogesh Verma",
            "Satyajit Jena"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Pions constitute nearly $70\\%$ of final state particles in ultra high energy\ncollisions. They act as a probe to understand the statistical properties of\nQuantum Chromodynamics (QCD) matter i.e. Quark Gluon Plasma (QGP) created in\nsuch relativistic heavy ion collisions (HIC). Apart from this, direct photons\nare the most versatile tools to study relativistic HIC. They are produced, by\nvarious mechanisms, during the entire space-time history of the strongly\ninteracting system. Direct photons provide measure of jet-quenching when\ncompared with other quark or gluon jets. The $\\pi^{0}$ decay into two photons\nmake the identification of non-correlated gamma coming from another process\ncumbersome in the Electromagnetic Calorimeter. We investigate the use of deep\nlearning architecture for reconstruction and identification of single as well\nas multi particles showers produced in calorimeter by particles created in high\nenergy collisions. We utilize the data of electromagnetic shower at calorimeter\ncell-level to train the network and show improvements for identification and\ncharacterization. These networks are fast and computationally inexpensive for\nparticle shower identification and reconstruction for current and future\nexperiments at particle colliders.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.16247v1"
    },
    {
        "title": "Comment on \"Reproducibility and Replication of Experimental Particle\n  Physics Results\"",
        "authors": [
            "Andrew Fowlie"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  I would like to thank Junk and Lyons (arXiv:2009.06864) for beginning a\ndiscussion about replication in high-energy physics (HEP). Junk and Lyons\nultimately argue that HEP learned its lessons the hard way through past\nfailures and that other fields could learn from our procedures. They emphasize\nthat experimental collaborations would risk their legacies were they to make a\ntype-1 error in a search for new physics and outline the vigilance taken to\navoid one, such as data blinding and a strict $5\\sigma$ threshold. The\ndiscussion, however, ignores an elephant in the room: there are regularly\nanomalies in searches for new physics that result in substantial scientific\nactivity but don't replicate with more data.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.03082v1"
    },
    {
        "title": "AtomAI: A Deep Learning Framework for Analysis of Image and Spectroscopy\n  Data in (Scanning) Transmission Electron Microscopy and Beyond",
        "authors": [
            "Maxim Ziatdinov",
            "Ayana Ghosh",
            "Tommy Wong",
            "Sergei V. Kalinin"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  AtomAI is an open-source software package bridging instrument-specific Python\nlibraries, deep learning, and simulation tools into a single ecosystem. AtomAI\nallows direct applications of the deep convolutional neural networks for atomic\nand mesoscopic image segmentation converting image and spectroscopy data into\nclass-based local descriptors for downstream tasks such as statistical and\ngraph analysis. For atomically-resolved imaging data, the output is types and\npositions of atomic species, with an option for subsequent refinement. AtomAI\nfurther allows the implementation of a broad range of image and spectrum\nanalysis functions, including invariant variational autoencoders (VAEs). The\nlatter consists of VAEs with rotational and (optionally) translational\ninvariance for unsupervised and class-conditioned disentanglement of\ncategorical and continuous data representations. In addition, AtomAI provides\nutilities for mapping structure-property relationships via im2spec and spec2im\ntype of encoder-decoder models. Finally, AtomAI allows seamless connection to\nthe first principles modeling with a Python interface, including molecular\ndynamics and density functional theory calculations on the inferred atomic\nposition. While the majority of applications to date were based on atomically\nresolved electron microscopy, the flexibility of AtomAI allows straightforward\nextension towards the analysis of mesoscopic imaging data once the labels and\nfeature identification workflows are established/available. The source code and\nexample notebooks are available at https://github.com/pycroscopy/atomai.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.07485v1"
    },
    {
        "title": "Uncertainty Aware Learning for High Energy Physics",
        "authors": [
            "Aishik Ghosh",
            "Benjamin Nachman",
            "Daniel Whiteson"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Machine learning techniques are becoming an integral component of data\nanalysis in High Energy Physics (HEP). These tools provide a significant\nimprovement in sensitivity over traditional analyses by exploiting subtle\npatterns in high-dimensional feature spaces. These subtle patterns may not be\nwell-modeled by the simulations used for training machine learning methods,\nresulting in an enhanced sensitivity to systematic uncertainties.\n  Contrary to the traditional wisdom of constructing an analysis strategy that\nis invariant to systematic uncertainties, we study the use of a classifier that\nis fully aware of uncertainties and their corresponding nuisance parameters. We\nshow that this dependence can actually enhance the sensitivity to parameters of\ninterest. Studies are performed using a synthetic Gaussian dataset as well as a\nmore realistic HEP dataset based on Higgs boson decays to tau leptons. For both\ncases, we show that the uncertainty aware approach can achieve a better\nsensitivity than alternative machine learning strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.08742v2"
    },
    {
        "title": "Adaptive Monte Carlo augmented with normalizing flows",
        "authors": [
            "Marylou Gabrié",
            "Grant M. Rotskoff",
            "Eric Vanden-Eijnden"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Many problems in the physical sciences, machine learning, and statistical\ninference necessitate sampling from a high-dimensional, multi-modal probability\ndistribution. Markov Chain Monte Carlo (MCMC) algorithms, the ubiquitous tool\nfor this task, typically rely on random local updates to propagate\nconfigurations of a given system in a way that ensures that generated\nconfigurations will be distributed according to a target probability\ndistribution asymptotically. In high-dimensional settings with multiple\nrelevant metastable basins, local approaches require either immense\ncomputational effort or intricately designed importance sampling strategies to\ncapture information about, for example, the relative populations of such\nbasins. Here we analyze an adaptive MCMC which augments MCMC sampling with\nnonlocal transition kernels parameterized with generative models known as\nnormalizing flows. We focus on a setting where there is no preexisting data, as\nis commonly the case for problems in which MCMC is used. Our method uses: (i) a\nMCMC strategy that blends local moves obtained from any standard transition\nkernel with those from a generative model to accelerate the sampling and (ii)\nthe data generated this way to adapt the generative model and improve its\nefficacy in the MCMC algorithm. We provide a theoretical analysis of the\nconvergence properties of this algorithm, and investigate numerically its\nefficiency, in particular in terms of its propensity to equilibrate fast\nbetween metastable modes whose rough location is known \\textit{a~priori} but\nrespective probability weight is not. We show that our algorithm can sample\neffectively across large free energy barriers, providing dramatic accelerations\nrelative to traditional MCMC algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.12603v3"
    },
    {
        "title": "Effect of non-uniform efficiency on higher-order cumulants in heavy-ion\n  collisions",
        "authors": [
            "Fan Si",
            "Yifei Zhang",
            "Xiaofeng Luo"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We perform a systematic study on the effect of non-uniform track efficiency\ncorrection in higher-order cumulant analysis in heavy-ion collisions. Through\nanalytical derivation, we find that the true values of cumulants can be\nsuccessfully reproduced by the efficiency correction with an average of the\nrealistic detector efficiency for particles with the same charges within each\nsingle phase space. The theoretical conclusions are supported by a toy model\nsimulation by tuning the non-uniformity of the efficiency employed in the\ntrack-by-track efficiency correction method. The valid averaged efficiency is\nfound to suppress the statistical uncertainties of the reproduced cumulants\ndramatically. Thus, usage of the averaged efficiency requires a careful study\nof phase space dependence. This study is important for carrying out precision\nmeasurements of higher-order cumulants in heavy-ion collision experiments at\npresent and in future.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.13128v3"
    },
    {
        "title": "Using Machine Learning to Select High-Quality Measurements",
        "authors": [
            "Andrew Edmonds",
            "David Brown",
            "Luciano Vinas",
            "Samantha Pagan"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We describe the use of machine learning algorithms to select high-quality\nmeasurements for the Mu2e experiment. This technique is important for\nexperiments with backgrounds that arise due to measurement errors. The\nalgorithms use multiple pieces of ancillary information that are sensitive to\nmeasurement quality to separate high-quality and low-quality measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.08891v1"
    },
    {
        "title": "Interactive Causal Structure Discovery in Earth System Sciences",
        "authors": [
            "Laila Melkas",
            "Rafael Savvides",
            "Suyog Chandramouli",
            "Jarmo Mäkelä",
            "Tuomo Nieminen",
            "Ivan Mammarella",
            "Kai Puolamäki"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Causal structure discovery (CSD) models are making inroads into several\ndomains, including Earth system sciences. Their widespread adaptation is\nhowever hampered by the fact that the resulting models often do not take into\naccount the domain knowledge of the experts and that it is often necessary to\nmodify the resulting models iteratively. We present a workflow that is required\nto take this knowledge into account and to apply CSD algorithms in Earth system\nsciences. At the same time, we describe open research questions that still need\nto be addressed. We present a way to interactively modify the outputs of the\nCSD algorithms and argue that the user interaction can be modelled as a greedy\nfinding of the local maximum-a-posteriori solution of the likelihood function,\nwhich is composed of the likelihood of the causal model and the prior\ndistribution representing the knowledge of the expert user. We use a real-world\ndata set for examples constructed in collaboration with our co-authors, who are\nthe domain area experts. We show that finding maximally usable causal models in\nthe Earth system sciences or other similar domains is a difficult task which\ncontains many interesting open research questions. We argue that taking the\ndomain knowledge into account has a substantial effect on the final causal\nmodels discovered.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.01126v1"
    },
    {
        "title": "Entropy, Information, and the Updating of Probabilities",
        "authors": [
            "Ariel Caticha"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  This paper is a review of a particular approach to the method of maximum\nentropy as a general framework for inference. The discussion emphasizes the\npragmatic elements in the derivation. An epistemic notion of information is\ndefined in terms of its relation to the Bayesian beliefs of ideally rational\nagents. The method of updating from a prior to a posterior probability\ndistribution is designed through an eliminative induction process. The\nlogarithmic relative entropy is singled out as the unique tool for updating\nthat (a) is of universal applicability; (b) that recognizes the value of prior\ninformation; and (c) that recognizes the privileged role played by the notion\nof independence in science. The resulting framework -- the ME method -- can\nhandle arbitrary priors and arbitrary constraints. It includes MaxEnt and\nBayes' rule as special cases and, therefore, it unifies entropic and Bayesian\nmethods into a single general inference scheme. The ME method goes beyond the\nmere selection of a single posterior, but also addresses the question of how\nmuch less probable other distributions might be, which provides a direct bridge\nto the theories of fluctuations and large deviations.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.04529v1"
    },
    {
        "title": "Transport away your problems: Calibrating stochastic simulations with\n  optimal transport",
        "authors": [
            "Chris Pollard",
            "Philipp Windischhofer"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Stochastic simulators are an indispensable tool in many branches of science.\nOften based on first principles, they deliver a series of samples whose\ndistribution implicitly defines a probability measure to describe the phenomena\nof interest. However, the fidelity of these simulators is not always sufficient\nfor all scientific purposes, necessitating the construction of ad-hoc\ncorrections to \"calibrate\" the simulation and ensure that its output is a\nfaithful representation of reality. In this paper, we leverage methods from\ntransportation theory to construct such corrections in a systematic way. We use\na neural network to compute minimal modifications to the individual samples\nproduced by the simulator such that the resulting distribution becomes properly\ncalibrated. We illustrate the method and its benefits in the context of\nexperimental particle physics, where the need for calibrated stochastic\nsimulators is particularly pronounced.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.08648v2"
    },
    {
        "title": "Neural Conditional Reweighting",
        "authors": [
            "Benjamin Nachman",
            "Jesse Thaler"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  There is a growing use of neural network classifiers as unbinned,\nhigh-dimensional (and variable-dimensional) reweighting functions. To date, the\nfocus has been on marginal reweighting, where a subset of features are used for\nreweighting while all other features are integrated over. There are some\nsituations, though, where it is preferable to condition on auxiliary features\ninstead of marginalizing over them. In this paper, we introduce neural\nconditional reweighting, which extends neural marginal reweighting to the\nconditional case. This approach is particularly relevant in high-energy physics\nexperiments for reweighting detector effects conditioned on particle-level\ntruth information. We leverage a custom loss function that not only allows us\nto achieve neural conditional reweighting through a single training procedure,\nbut also yields sensible interpolation even in the presence of phase space\nholes. As a specific example, we apply neural conditional reweighting to the\nenergy response of high-energy jets, which could be used to improve the\nmodeling of physics objects in parametrized fast simulation packages.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.08979v2"
    },
    {
        "title": "Learning to discover: expressive Gaussian mixture models for\n  multi-dimensional simulation and parameter inference in the physical sciences",
        "authors": [
            "Stephen B. Menary",
            "Darren D. Price"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We show that density models describing multiple observables with (i) hard\nboundaries and (ii) dependence on external parameters may be created using an\nauto-regressive Gaussian mixture model. The model is designed to capture how\nobservable spectra are deformed by hypothesis variations, and is made more\nexpressive by projecting data onto a configurable latent space. It may be used\nas a statistical model for scientific discovery in interpreting experimental\nobservations, for example when constraining the parameters of a physical model\nor tuning simulation parameters according to calibration data. The model may\nalso be sampled for use within a Monte Carlo simulation chain, or used to\nestimate likelihood ratios for event classification. The method is demonstrated\non simulated high-energy particle physics data considering the anomalous\nelectroweak production of a $Z$ boson in association with a dijet system at the\nLarge Hadron Collider, and the accuracy of inference is tested using a\nrealistic toy example. The developed methods are domain agnostic; they may be\nused within any field to perform simulation or inference where a dataset\nconsisting of many real-valued observables has conditional dependence on\nexternal parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.11481v2"
    },
    {
        "title": "Unravelling the origins of anomalous diffusion: from molecules to\n  migrating storks",
        "authors": [
            "Ohad Vilk",
            "Erez Aghion",
            "Tal Avgar",
            "Carsten Beta",
            "Oliver Nagel",
            "Adal Sabri",
            "Raphael Sarfati",
            "Daniel K. Schwartz",
            "Matthias Weiss",
            "Diego Krapf",
            "Ran Nathan",
            "Ralf Metzler",
            "Michael Assaf"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Anomalous diffusion or, more generally, anomalous transport, with nonlinear\ndependence of the mean-squared displacement on the measurement time, is\nubiquitous in nature. It has been observed in processes ranging from\nmicroscopic movement of molecules to macroscopic, large-scale paths of\nmigrating birds. Using data from multiple empirical systems, spanning 12 orders\nof magnitude in length and 8 orders of magnitude in time, we employ a method to\ndetect the individual underlying origins of anomalous diffusion and transport\nin the data. This method decomposes anomalous transport into three primary\neffects: long-range correlations (\"Joseph effect\"), fat-tailed probability\ndensity of increments (\"Noah effect\"), and non-stationarity (\"Moses effect\").\nWe show that such a decomposition of real-life data allows to infer nontrivial\nbehavioral predictions, and to resolve open questions in the fields of single\nparticle tracking in living cells and movement ecology.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.04309v3"
    },
    {
        "title": "Application of Deep Learning Technique to an Analysis of Hard Scattering\n  Processes at Colliders",
        "authors": [
            "Lev Dudko",
            "Petr Volkov",
            "Georgii Vorotnikov",
            "Andrei Zaborenko"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Deep neural networks have rightfully won the place of one of the most\naccurate analysis tools in high energy physics. In this paper we will cover\nseveral methods of improving the performance of a deep neural network in a\nclassification task in an instance of top quark analysis. The approaches and\nrecommendations will cover hyperparameter tuning, boosting on errors and AutoML\nalgorithms applied to collider physics.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.08520v1"
    },
    {
        "title": "Grassmannian diffusion maps based surrogate modeling via geometric\n  harmonics",
        "authors": [
            "Ketson R. M. dos Santos",
            "Dimitrios G. Giovanis",
            "Katiana Kontolati",
            "Dimitrios Loukrezis",
            "Michael D. Shields"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  In this paper, a novel surrogate model based on the Grassmannian diffusion\nmaps (GDMaps) and utilizing geometric harmonics is developed for predicting the\nresponse of engineering systems and complex physical phenomena. The method\nutilizes the GDMaps to obtain a low-dimensional representation of the\nunderlying behavior of physical/mathematical systems with respect to\nuncertainties in the input parameters. Using this representation, geometric\nharmonics, an out-of-sample function extension technique, is employed to create\na global map from the space of input parameters to a Grassmannian diffusion\nmanifold. Geometric harmonics is also employed to locally map points on the\ndiffusion manifold onto the tangent space of a Grassmann manifold. The\nexponential map is then used to project the points in the tangent space onto\nthe Grassmann manifold, where reconstruction of the full solution is performed.\nThe performance of the proposed surrogate modeling is verified with three\nexamples. The first problem is a toy example used to illustrate the development\nof the technique. In the second example, errors associated with the various\nmappings employed in the technique are assessed by studying response\npredictions of the electric potential of a dielectric cylinder in a homogeneous\nelectric field. The last example applies the method for uncertainty prediction\nin the strain field evolution in a model amorphous material using the shear\ntransformation zone (STZ) theory of plasticity. In all examples, accurate\npredictions are obtained, showing that the present technique is a strong\ncandidate for the application of uncertainty quantification in large-scale\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.13805v1"
    },
    {
        "title": "Effective Lagrangian Morphing",
        "authors": [
            "Rahul Balasubramanian",
            "Lydia Brenner",
            "Carsten Burgard",
            "Wouter Verkerke"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  With the LHC entering the precision era, focus on interpreting the\nmeasurements performed in an effective field theory holds key to testing the\nStandard Model. An effective field theory provides a well-defined theoretical\nformalism which extends the Standard Model by introduce new terms with free\ncoefficients that can be measured with experimental data. Constructing models\nparametric in these new coefficients is achieved by virtue of combining\npredictions from Monte Carlo generators for different interactions. This paper\nbuilds upon earlier works and describes a state-of-the-art approach to build\nmultidimensional parametric models for new physics using effective Lagrangians.\nDocumentation and tutorials for an associated toolkit that have been\ncontributed to the ROOT data analysis software framework is included.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.13612v3"
    },
    {
        "title": "neos: End-to-End-Optimised Summary Statistics for High Energy Physics",
        "authors": [
            "Nathan Simpson",
            "Lukas Heinrich"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The advent of deep learning has yielded powerful tools to automatically\ncompute gradients of computations. This is because training a neural network\nequates to iteratively updating its parameters using gradient descent to find\nthe minimum of a loss function. Deep learning is then a subset of a broader\nparadigm; a workflow with free parameters that is end-to-end optimisable,\nprovided one can keep track of the gradients all the way through. This work\nintroduces neos: an example implementation following this paradigm of a fully\ndifferentiable high-energy physics workflow, capable of optimising a learnable\nsummary statistic with respect to the expected sensitivity of an analysis.\nDoing this results in an optimisation process that is aware of the modelling\nand treatment of systematic uncertainties.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.05570v1"
    },
    {
        "title": "Quantum computing for data analysis in high energy physics",
        "authors": [
            "Andrea Delgado",
            "Kathleen E. Hamilton",
            "Prasanna Date",
            "Jean-Roch Vlimant",
            "Duarte Magano",
            "Yasser Omar",
            "Pedrame Bargassa",
            "Anthony Francis",
            "Alessio Gianelle",
            "Lorenzo Sestini",
            "Donatella Lucchesi",
            "Davide Zuliani",
            "Davide Nicotra",
            "Jacco de Vries",
            "Dominica Dibenedetto",
            "Miriam Lucio Martinez",
            "Eduardo Rodrigues",
            "Carlos Vazquez Sierra",
            "Sofia Vallecorsa",
            "Jesse Thaler",
            "Carlos Bravo-Prieto",
            "su Yeon Chang",
            "Jeffrey Lazar",
            "Carlos A. Argüelles",
            "Jorge J. Martinez de Lejarza"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Some of the biggest achievements of the modern era of particle physics, such\nas the discovery of the Higgs boson, have been made possible by the tremendous\neffort in building and operating large-scale experiments like the Large Hadron\nCollider or the Tevatron. In these facilities, the ultimate theory to describe\nmatter at the most fundamental level is constantly probed and verified. These\nexperiments often produce large amounts of data that require storing,\nprocessing, and analysis techniques that often push the limits of traditional\ninformation processing schemes. Thus, the High-Energy Physics (HEP) field has\nbenefited from advancements in information processing and the development of\nalgorithms and tools for large datasets. More recently, quantum computing\napplications have been investigated in an effort to understand how the\ncommunity can benefit from the advantages of quantum information science. In\nthis manuscript, we provide an overview of the state-of-the-art applications of\nquantum computing to data analysis in HEP, discuss the challenges and\nopportunities in integrating these novel analysis techniques into a day-to-day\nanalysis workflow, and whether there is potential for a quantum advantage.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08805v2"
    },
    {
        "title": "Collaborative Computing Support for Analysis Facilities Exploiting\n  Software as Infrastructure Techniques",
        "authors": [
            "Maria Acosta Flechas",
            "Garhan Attebury",
            "Kenneth Bloom",
            "Brian Bockelman",
            "Lindsey Gray",
            "Burt Holzman",
            "Carl Lundstedt",
            "Oksana Shadura",
            "Nicholas Smith",
            "John Thiltges"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Prior to the public release of Kubernetes it was difficult to conduct joint\ndevelopment of elaborate analysis facilities due to the highly non-homogeneous\nnature of hardware and network topology across compute facilities. However,\nsince the advent of systems like Kubernetes and OpenShift, which provide\ndeclarative interfaces for building fault-tolerant and self-healing deployments\nof networked software, it is possible for multiple institutes to collaborate\nmore effectively since resource details are abstracted away through various\nforms of hardware and software virtualization. In this whitepaper we will\noutline the development of two analysis facilities: \"Coffea-casa\" at University\nof Nebraska Lincoln and the \"Elastic Analysis Facility\" at Fermilab, and how\nutilizing platform abstraction has improved the development of common software\nfor each of these facilities, and future development plans made possible by\nthis methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.10161v2"
    },
    {
        "title": "Improving Robustness of Jet Tagging Algorithms with Adversarial Training",
        "authors": [
            "Annika Stein",
            "Xavier Coubez",
            "Spandan Mondal",
            "Andrzej Novak",
            "Alexander Schmidt"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Deep learning is a standard tool in the field of high-energy physics,\nfacilitating considerable sensitivity enhancements for numerous analysis\nstrategies. In particular, in identification of physics objects, such as jet\nflavor tagging, complex neural network architectures play a major role.\nHowever, these methods are reliant on accurate simulations. Mismodeling can\nlead to non-negligible differences in performance in data that need to be\nmeasured and calibrated against. We investigate the classifier response to\ninput data with injected mismodelings and probe the vulnerability of flavor\ntagging algorithms via application of adversarial attacks. Subsequently, we\npresent an adversarial training strategy that mitigates the impact of such\nsimulated attacks and improves the classifier robustness. We examine the\nrelationship between performance and vulnerability and show that this method\nconstitutes a promising approach to reduce the vulnerability to poor modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.13890v2"
    },
    {
        "title": "Deficit hawks: robust new physics searches with unknown backgrounds",
        "authors": [
            "Jelle Aalbers"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Searches for new physics often face unknown backgrounds, causing false\ndetections or weakened upper limits. This paper introduces the deficit hawk\ntechnique, which mitigates unknown backgrounds by testing multiple options for\ndata cuts, such as fiducial volumes or energy thresholds. Combining the power\nof likelihood ratios with the robustness of the interval-searching techniques,\ndeficit hawks could improve mean upper limits on new physics by a factor two\nfor experiments with partial or speculative background knowledge. Deficit hawks\nare well-suited to analyses that use machine learning or other multidimensional\ndiscrimination techniques, and can be extended to permit discoveries in regions\nwithout unknown background.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.03264v2"
    },
    {
        "title": "Machine learning based event classification for the energy-differential\n  measurement of the $^\\text{nat}$C(n,p) and $^\\text{nat}$C(n,d) reactions",
        "authors": [
            "P. Žugec",
            "M. Barbagallo",
            "J. Andrzejewski",
            "J. Perkowski",
            "N. Colonna",
            "D. Bosnar",
            "A. Gawlik",
            "M. Sabate-Gilarte",
            "M. Bacak",
            "F. Mingrone",
            "E. Chiaveri"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The paper explores the feasibility of using machine learning techniques, in\nparticular neural networks, for classification of the experimental data from\nthe joint $^\\text{nat}$C(n,p) and $^\\text{nat}$C(n,d) reaction cross section\nmeasurement from the neutron time of flight facility n_TOF at CERN. Each\nrelevant $\\Delta E$-$E$ pair of strips from two segmented silicon telescopes is\ntreated separately and afforded its own dedicated neural network. An important\npart of the procedure is a careful preparation of training datasets, based on\nthe raw data from Geant4 simulations. Instead of using these raw data for the\ntraining of neural networks, we divide a relevant 3-parameter space into\ndiscrete voxels, classify each voxel according to a particle/reaction type and\nsubmit these voxels to a training procedure. The classification capabilities of\nthe structurally optimized and trained neural networks are found to be superior\nto those of the manually selected cuts.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.04955v1"
    },
    {
        "title": "20 years of ordinal patterns: Perspectives and challenges",
        "authors": [
            "Inmaculada Leyva",
            "Johann Martinez",
            "Cristina Masoller",
            "Osvaldo A. Rosso",
            "Massimiliano Zanin"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  In 2002, in a seminal article, Christoph Bandt and Bernd Pompe proposed a new\nmethodology for the analysis of complex time series, now known as Ordinal\nAnalysis. The ordinal methodology is based on the computation of symbols (known\nas ordinal patterns) which are defined in terms of the temporal ordering of\ndata points in a time series, and whose probabilities are known as ordinal\nprobabilities. With the ordinal probabilities, the Shannon entropy can be\ncalculated, which is the permutation entropy. Since it was proposed, the\nordinal method has found applications in fields as diverse as biomedicine and\nclimatology. However, some properties of ordinal probabilities are still not\nfully understood, and how to combine the ordinal approach of feature extraction\nwith machine learning techniques for model identification, time series\nclassification or forecasting remains a challenge. The objective of this\nperspective article is to present some recent advances and to discuss some open\nproblems.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12883v1"
    },
    {
        "title": "Quantifying rare events in spotting: How far do wildfires spread?",
        "authors": [
            "Alex Mendez",
            "Mohammad Farazmand"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Spotting refers to the transport of burning pieces of firebrand by wind\nwhich, at the time of landing, may ignite new fires beyond the direct ignition\nzone of the main fire. Spot fires that occur far from the original burn unit\nare rare but have consequential ramifications since their prediction and\ncontrol remains challenging. To facilitate their prediction, we examine three\nmethods for quantifying the landing distribution of firebrands: crude Monte\nCarlo simulations, importance sampling, and large deviation theory (LDT). In\nparticular, we propose an LDT method that accurately and parsimoniously\nquantifies the low probability events at the tail of the landing distribution.\nIn contrast, Monte Carlo and importance sampling methods are most efficient in\nquantifying the high probability landing distances near the mode of the\ndistribution. However, they become computationally intractable for quantifying\nthe tail of the distribution due to the large sample size required. We also\nshow that the most probable landing distance grows linearly with the mean\ncharacteristic velocity of the wind field. Furthermore, defining the relative\nlanded mass as the proportion of mass landed at a given distance from the main\nfire, we derive an explicit formula which allows computing this quantity as a\nfunction of the landing distribution at a negligible computational cost. We\nnumerically demonstrate our findings on two prescribed wind fields.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.02838v1"
    },
    {
        "title": "Approaches to the classification of complex systems: Words, texts, and\n  more",
        "authors": [
            "Andrij Rovenchak"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The Chapter starts with introductory information about quantitative\nlinguistics notions, like rank--frequency dependence, Zipf's law, frequency\nspectra, etc. Similarities in distributions of words in texts with level\noccupation in quantum ensembles hint at a superficial analogy with statistical\nphysics. This enables one to define various parameters for texts based on this\nphysical analogy, including \"temperature\", \"chemical potential\", entropy, and\nsome others. Such parameters provide a set of variables to classify texts\nserving as an example of complex systems. Moreover, texts are perhaps the\neasiest complex systems to collect and analyze.\n  Similar approaches can be developed to study, for instance, genomes due to\nwell-known linguistic analogies. We consider a couple of approaches to define\nnucleotide sequences in mitochondrial DNAs and viral RNAs and demonstrate their\npossible application as an auxiliary tool for comparative analysis of genomes.\n  Finally, we discuss entropy as one of the parameters, which can be easily\ncomputed from rank--frequency dependences. Being a discriminating parameter in\nsome problems of classification of complex systems, entropy can be given a\nproper interpretation only in a limited class of problems. Its overall role and\nsignificance remain an open issue so far.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04060v1"
    },
    {
        "title": "Reduction of detection limit and quantification uncertainty due to\n  interferent by neural classification with abstention",
        "authors": [
            "Alex Hagen",
            "Ken Jarman",
            "Jesse Ward",
            "Greg Eiden",
            "Charles Barinaga",
            "Emily Mace",
            "Craig Aalseth",
            "Anthony Carado"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Many measurements in the physical sciences can be cast as counting\nexperiments, where the number of occurrences of a physical phenomenon informs\nthe prevalence of the phenomenon's source. Often, detection of the physical\nphenomenon (termed signal) is difficult to distinguish from naturally occurring\nphenomena (termed background). In this case, the discrimination of signal\nevents from background can be performed using classifiers, and they may range\nfrom simple, threshold-based classifiers to sophisticated neural networks.\nThese classifiers are often trained and validated to obtain optimal accuracy,\nhowever we show that the optimal accuracy classifier does not generally\ncoincide with a classifier that provides the lowest detection limit, nor the\nlowest quantification uncertainty. We present a derivation of the detection\nlimit and quantification uncertainty in the classifier-based counting\nexperiment case. We also present a novel abstention mechanism to minimize the\ndetection limit or quantification uncertainty \\emph{a posteriori}. We\nillustrate the method on two data sets from the physical sciences,\ndiscriminating Ar-37 and Ar-39 radioactive decay from non-radioactive events in\na gas proportional counter, and discriminating neutrons from photons in an\ninorganic scintillator and report results therefrom.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.07609v1"
    },
    {
        "title": "Gaussian Processes and Bayesian Optimization for High Precision\n  Experiments",
        "authors": [
            "Max Lamparth",
            "Mattis Bestehorn",
            "Bastian Märkisch"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  High-precision measurements require optimal setups and analysis tools to\nachieve continuous improvements. Systematic corrections need to be modeled with\nhigh accuracy and known uncertainty to reconstruct underlying physical\nphenomena. To this end, we present Gaussian processes for modeling experiments\nand usage with Bayesian optimization, on the example of an electron energy\ndetector, achieving optimal performance. We demonstrate the method's strengths\nand outline stochastic variational Gaussian processes for physics applications\nwith large data sets, enabling new solutions for current problems.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.07625v2"
    },
    {
        "title": "Neural network model for imprecise regression with interval dependent\n  variables",
        "authors": [
            "Krasymyr Tretiak",
            "Georg Schollmeyer",
            "Scott Ferson"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  This paper presents a computationally feasible method to compute rigorous\nbounds on the interval-generalisation of regression analysis to account for\nepistemic uncertainty in the output variables. The new iterative method uses\nmachine learning algorithms to fit an imprecise regression model to data that\nconsist of intervals rather than point values. The method is based on a\nsingle-layer interval neural network which can be trained to produce an\ninterval prediction. It seeks parameters for the optimal model that minimizes\nthe mean squared error between the actual and predicted interval values of the\ndependent variable using a first-order gradient-based optimization and interval\nanalysis computations to model the measurement imprecision of the data. An\nadditional extension to a multi-layer neural network is also presented. We\nconsider the explanatory variables to be precise point values, but the measured\ndependent values are characterized by interval bounds without any probabilistic\ninformation. The proposed iterative method estimates the lower and upper bounds\nof the expectation region, which is an envelope of all possible precise\nregression lines obtained by ordinary regression analysis based on any\nconfiguration of real-valued points from the respective y-intervals and their\nx-values.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.02467v2"
    },
    {
        "title": "Constraining Model Uncertainty in Plasma Equation-of-State Models with a\n  Physics-Constrained Gaussian Process",
        "authors": [
            "Jim A Gaffney",
            "Lin Yang",
            "Suzanne Ali"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Equation-of-state (EOS) models underpin numerical simulations at the core of\nresearch in high energy density physics, inertial confinement fusion,\nlaboratory astrophysics, and elsewhere. In these applications EOS models are\nneeded that span ranges of thermodynamic variables that far exceed the ranges\nwhere data are available, making uncertainty quantification (UQ) of EOS models\na significant concern. Model uncertainty, arising from the choice of functional\nform assumed for the EOS, is a major challenge to UQ studies for EOS that is\nusually neglected in favor of parameteric and data uncertainties which are\neasier to capture without violating the physical constraints on EOSs. In this\nwork we introduce a new statistical EOS construction that naturally captures\nmodel uncertainty while automatically obeying the thermodynamic consistency\nconstraint. We apply the model to existing data for $B_4C$\\ to place an upper\nbound on the uncertainty in the EOS and Hugoniot, and show that the neglect of\nthermodynamic constraints overestimates the uncertainty by factors of several\nwhen data are available and underestimates when extrapolating to regions where\nthey are not. We discuss extensions to this approach, and the role of GP-based\nmodels in accelerating simulation and experimental studies, defining portable\nuncertainty-aware EOS tables, and enabling uncertainty-aware downstream tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.00668v2"
    },
    {
        "title": "Cover Your Bases: Asymptotic Distributions of the Profile Likelihood\n  Ratio When Constraining Effective Field Theories in High-Energy Physics",
        "authors": [
            "Florian U. Bernlochner",
            "Daniel C. Fry",
            "Stephen B. Menary",
            "Eric Persson"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We investigate the asymptotic distribution of the profile likelihood ratio\n(PLR) when constraining effective field theories (EFTs) and show that Wilks'\ntheorem is often violated, meaning that we should not assume the PLR to follow\na $\\chi^2$-distribution. We derive the correct asymptotic distributions when\neither one or two real EFT couplings modulate observable cross sections with a\npurely linear or quadratic dependence. We then discover that when both the\nlinear and quadratic terms contribute, the PLR distribution does not have a\nsimple form. In this case we provide a partly-numerical solution for the\none-parameter case. Using a novel approach, we find that the constants which\ndefine our asymptotic distributions may be obtained experimentally using a\nprofile of the Asimov likelihood contour. Our results may be immediately used\nto obtain the correct coverage when deriving real-world EFT constraints using\nthe PLR as a test-statistic.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.01350v2"
    },
    {
        "title": "Interpretable Boosted Decision Tree Analysis for the Majorana\n  Demonstrator",
        "authors": [
            "I. J. Arnquist",
            "F. T. Avignone III",
            "A. S. Barabash",
            "C. J. Barton",
            "K. H. Bhimani",
            "E. Blalock",
            "B. Bos",
            "M. Busch",
            "M. Buuck",
            "T. S. Caldwell",
            "Y -D. Chan",
            "C. D. Christofferson",
            "P. -H. Chu",
            "M. L. Clark",
            "C. Cuesta",
            "J. A. Detwiler",
            "Yu. Efremenko",
            "S. R. Elliott",
            "G. K. Giovanetti",
            "M. P. Green",
            "J. Gruszko",
            "I. S. Guinn",
            "V. E. Guiseppe",
            "C. R. Haufe",
            "R. Henning",
            "D. Hervas Aguilar",
            "E. W. Hoppe",
            "A. Hostiuc",
            "M. F. Kidd",
            "I. Kim",
            "R. T. Kouzes",
            "T. E. Lannen V",
            "A. Li",
            "J. M. Lopez-Castano",
            "E. L. Martin",
            "R. D. Martin",
            "R. Massarczyk",
            "S. J. Meijer",
            "T. K. Oli",
            "G. Othman",
            "L. S. Paudel",
            "W. Pettus",
            "A. W. P. Poon",
            "D. C. Radford",
            "A. L. Reine",
            "K. Rielage",
            "N. W. Ruof",
            "D. C. Schaper",
            "D. Tedeschi",
            "R. L. Varner",
            "S. Vasilyev",
            "J. F. Wilkerson",
            "C. Wiseman",
            "W. Xu",
            "C. -H. Yu"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The Majorana Demonstrator is a leading experiment searching for neutrinoless\ndouble-beta decay with high purity germanium detectors (HPGe). Machine learning\nprovides a new way to maximize the amount of information provided by these\ndetectors, but the data-driven nature makes it less interpretable compared to\ntraditional analysis. An interpretability study reveals the machine's\ndecision-making logic, allowing us to learn from the machine to feedback to the\ntraditional analysis. In this work, we have presented the first machine\nlearning analysis of the data from the Majorana Demonstrator; this is also the\nfirst interpretable machine learning analysis of any germanium detector\nexperiment. Two gradient boosted decision tree models are trained to learn from\nthe data, and a game-theory-based model interpretability study is conducted to\nunderstand the origin of the classification power. By learning from data, this\nanalysis recognizes the correlations among reconstruction parameters to further\nenhance the background rejection performance. By learning from the machine,\nthis analysis reveals the importance of new background categories to\nreciprocally benefit the standard Majorana analysis. This model is highly\ncompatible with next-generation germanium detector experiments like LEGEND\nsince it can be simultaneously trained on a large number of detectors.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.10710v5"
    },
    {
        "title": "Machine Learned Particle Detector Simulations",
        "authors": [
            "D. Darulis",
            "R. Tyson",
            "D. G. Ireland",
            "D. I. Glazier",
            "B. McKinnon",
            "P. Pauli"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The use of machine learning algorithms is an attractive way to produce very\nfast detector simulations for scattering reactions that can otherwise be\ncomputationally expensive. Here we develop a factorised approach where we deal\nwith each particle produced in a reaction individually: first determine if it\nwas detected (acceptance) and second determine its reconstructed variables such\nas four momentum (reconstruction). For the acceptance we propose using a\nprobability classification density ratio technique to determine the probability\nthe particle was detected as a function of many variables. Neural Network and\nBoosted Decision Tree classifiers were tested for this purpose and we found\nusing a combination of both, through a reweighting stage, provided the most\nreliable results. For reconstruction a simple method of synthetic data\ngeneration, based on nearest neighbour or decision trees was developed. Using a\ntoy parameterised detector we demonstrate that such a method can reliably and\naccurately reproduce kinematic distributions from a physics reaction. The\nrelatively simple algorithms allow for small training overheads whilst\nproducing reliable results. Possible applications for such fast simulated data\ninclude Toy-MC studies of parameter extraction, preprocessing expensive\nsimulations or generating templates for background distributions shapes.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.11254v1"
    },
    {
        "title": "Active learning-assisted neutron spectroscopy with log-Gaussian\n  processes",
        "authors": [
            "Mario Teixeira Parente",
            "Georg Brandl",
            "Christian Franz",
            "Uwe Stuhr",
            "Marina Ganeva",
            "Astrid Schneidewind"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Neutron scattering experiments at three-axes spectrometers (TAS) investigate\nmagnetic and lattice excitations by measuring intensity distributions to\nunderstand the origins of materials properties. The high demand and limited\navailability of beam time for TAS experiments however raise the natural\nquestion whether we can improve their efficiency and make better use of the\nexperimenter's time. In fact, there are a number of scientific problems that\nrequire searching for signals, which may be time consuming and inefficient if\ndone manually due to measurements in uninformative regions. Here, we describe a\nprobabilistic active learning approach that not only runs autonomously, i.e.,\nwithout human interference, but can also directly provide locations for\ninformative measurements in a mathematically sound and methodologically robust\nway by exploiting log-Gaussian processes. Ultimately, the resulting benefits\ncan be demonstrated on a real TAS experiment and a benchmark including numerous\ndifferent excitations.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.00980v3"
    },
    {
        "title": "Bézier interpolation improves the inference of dynamical models from\n  data",
        "authors": [
            "Kai Shimagaki",
            "John P. Barton"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Many dynamical systems, from quantum many-body systems to evolving\npopulations to financial markets, are described by stochastic processes.\nParameters characterizing such processes can often be inferred using\ninformation integrated over stochastic paths. However, estimating\ntime-integrated quantities from real data with limited time resolution is\nchallenging. Here, we propose a framework for accurately estimating\ntime-integrated quantities using B\\'ezier interpolation. We applied our\napproach to two dynamical inference problems: determining fitness parameters\nfor evolving populations and inferring forces driving Ornstein-Uhlenbeck\nprocesses. We found that B\\'ezier interpolation reduces the estimation bias for\nboth dynamical inference problems. This improvement was especially noticeable\nfor data sets with limited time resolution. Our method could be broadly applied\nto improve accuracy for other dynamical inference problems using finitely\nsampled data.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.11285v2"
    },
    {
        "title": "The Shannon Entropy of a Histogram",
        "authors": [
            "Stephen Watts",
            "Lisa Crow"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The histogram is a key method for visualizing data and estimating the\nunderlying probability distribution. Incorrect conclusions about the data\nresult from over or under-binning. A new method based on the Shannon entropy of\nthe histogram uses a simple formula based on the differential entropy estimated\nfrom nearest-neighbour distances. Links are made between the new method and\nother algorithms such as Scott's formula, and cost and risk function methods. A\nparameter is found that predicts over and under-binning, which can be estimated\nfor any histogram. The new algorithm is shown to be robust by application to\nreal data.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.02848v1"
    },
    {
        "title": "Inference in conditioned dynamics through causality restoration",
        "authors": [
            "Alfredo Braunstein",
            "Giovanni Catania",
            "Luca Dall'Asta",
            "Matteo Mariani",
            "Anna Paola Muntoni"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Computing observables from conditioned dynamics is typically computationally\nhard, because, although obtaining independent samples efficiently from the\nunconditioned dynamics is usually feasible, generally most of the samples must\nbe discarded (in a form of importance sampling) because they do not satisfy the\nimposed conditions. Sampling directly from the conditioned distribution is\nnon-trivial, as conditioning breaks the causal properties of the dynamics which\nultimately renders the sampling procedure efficient. One standard way of\nachieving it is through a Metropolis Monte-Carlo procedure, but this procedure\nis normally slow and a very large number of Monte-Carlo steps is needed to\nobtain a small number of statistically independent samples. In this work, we\npropose an alternative method to produce independent samples from a conditioned\ndistribution. The method learns the parameters of a generalized dynamical model\nthat optimally describe the conditioned distribution in a variational sense.\nThe outcome is an effective, unconditioned, dynamical model, from which one can\ntrivially obtain independent samples, effectively restoring causality of the\nconditioned distribution. The consequences are twofold: on the one hand, it\nallows us to efficiently compute observables from the conditioned dynamics by\nsimply averaging over independent samples. On the other hand, the method gives\nan effective unconditioned distribution which is easier to interpret. The\nmethod is flexible and can be applied virtually to any dynamics. We discuss an\nimportant application of the method, namely the problem of epidemic risk\nassessment from (imperfect) clinical tests, for a large family of\ntime-continuous epidemic models endowed with a Gillespie-like sampler. We show\nthat the method compares favorably against the state of the art, including the\nsoft-margin approach and mean-field methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.10179v2"
    },
    {
        "title": "DUNE Offline Computing Conceptual Design Report",
        "authors": [
            " The DUNE collaboration"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  This document describes the conceptual design for the Offline Software and\nComputing for the Deep Underground Neutrino Experiment (DUNE). The goals of the\nexperiment include 1) studying neutrino oscillations using a beam of neutrinos\nsent from Fermilab in Illinois to the Sanford Underground Research Facility\n(SURF) in Lead, South Dakota, 2) studying astrophysical neutrino sources and\nrare processes and 3) understanding the physics of neutrino interactions in\nmatter. We describe the development of the computing infrastructure needed to\nachieve the physics goals of the experiment by storing, cataloging,\nreconstructing, simulating, and analyzing $\\sim$ 30 PB of data/year from DUNE\nand its prototypes. Rather than prescribing particular algorithms, our goal is\nto provide resources that are flexible and accessible enough to support\ncreative software solutions and advanced algorithms as HEP computing evolves.\nWe describe the physics objectives, organization, use cases, and proposed\ntechnical solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.15665v1"
    },
    {
        "title": "Sequential hypothesis testing for Axion Haloscopes",
        "authors": [
            "Andrea Gallo Rosso",
            "Sara Algeri",
            "Jan Conrad"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  The goal of this paper is to introduce a novel likelihood-based inferential\nframework for axion haloscopes which is valid under the commonly applied\n\"rescanning\" protocol. The proposed method enjoys short data acquisition times\nand a simple tuning of the detector configuration. Local statistical\nsignificance and power are computed analytically, avoiding the need of\nburdensome simulations. Adequate corrections for the look-elsewhere effect are\nalso discussed. The performance of our inferential strategy is compared with\nthat of a simple method which exploits the geometric probability of rescan.\nFinally, we exemplify the method with an application to a HAYSTAC type axion\nhaloscope.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.16095v1"
    },
    {
        "title": "A robust estimator of mutual information for deep learning\n  interpretability",
        "authors": [
            "Davide Piras",
            "Hiranya V. Peiris",
            "Andrew Pontzen",
            "Luisa Lucie-Smith",
            "Ningyuan Guo",
            "Brian Nord"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We develop the use of mutual information (MI), a well-established metric in\ninformation theory, to interpret the inner workings of deep learning models. To\naccurately estimate MI from a finite number of samples, we present GMM-MI\n(pronounced $``$Jimmie$\"$), an algorithm based on Gaussian mixture models that\ncan be applied to both discrete and continuous settings. GMM-MI is\ncomputationally efficient, robust to the choice of hyperparameters and provides\nthe uncertainty on the MI estimate due to the finite sample size. We\nextensively validate GMM-MI on toy data for which the ground truth MI is known,\ncomparing its performance against established mutual information estimators. We\nthen demonstrate the use of our MI estimator in the context of representation\nlearning, working with synthetic data and physical datasets describing highly\nnon-linear processes. We train deep learning models to encode high-dimensional\ndata within a meaningful compressed (latent) representation, and use GMM-MI to\nquantify both the level of disentanglement between the latent variables, and\ntheir association with relevant physical quantities, thus unlocking the\ninterpretability of the latent representation. We make GMM-MI publicly\navailable.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.00024v2"
    },
    {
        "title": "Genuine multifractality in time series is due to temporal correlations",
        "authors": [
            "Jarosław Kwapień",
            "Pawel Blasiak",
            "Stanisław Drożdż",
            "Paweł Oświęcimka"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Based on the mathematical arguments formulated within the Multifractal\nDetrended Fluctuation Analysis (MFDFA) approach it is shown that in the\nuncorrelated time series from the Gaussian basin of attraction the effects\nresembling multifractality asymptotically disappear for positive moments when\nthe length of time series increases. A hint is given that this applies to the\nnegative moments as well and extends to the L\\'evy stable regime of\nfluctuations. The related effects are also illustrated and confirmed by\nnumerical simulations. This documents that the genuine multifractality in time\nseries may only result from the long-range temporal correlations and the fatter\ndistribution tails of fluctuations may broaden the width of singularity\nspectrum only when such correlations are present. The frequently asked question\nof what makes multifractality in time series - temporal correlations or broad\ndistribution tails - is thus ill posed. In the absence of correlations only the\nbifractal or monofractal cases are possible. The former corresponds to the\nL\\'evy stable regime of fluctuations while the latter to the ones belonging to\nthe Gaussian basin of attraction in the sense of the Central Limit Theorem.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.00728v2"
    },
    {
        "title": "Artificial intelligence for improved fitting of trajectories of\n  elementary particles in inhomogeneous dense materials immersed in a magnetic\n  field",
        "authors": [
            "Saúl Alonso-Monsalve",
            "Davide Sgalaberna",
            "Xingyu Zhao",
            "Clark McGrew",
            "André Rubbia"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  In this article, we use artificial intelligence algorithms to show how to\nenhance the resolution of the elementary particle track fitting in\ninhomogeneous dense detectors, such as plastic scintillators. We use deep\nlearning to replace more traditional Bayesian filtering methods, drastically\nimproving the reconstruction of the interacting particle kinematics. We show\nthat a specific form of neural network, inherited from the field of natural\nlanguage processing, is very close to the concept of a Bayesian filter that\nadopts a hyper-informative prior. Such a paradigm change can influence the\ndesign of future particle physics experiments and their data exploitation.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.04890v1"
    },
    {
        "title": "Hunting for bumps in the margins",
        "authors": [
            "David Yallup",
            "Will Handley"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Data driven modelling is vital to many analyses at collider experiments,\nhowever the derived inference of physical properties becomes subject to details\nof the model fitting procedure. This work brings a principled Bayesian picture,\nbased on the marginal likelihood, of both data modelling and signal extraction\nto a common collider physics scenario. First the marginal likelihood based\nmethod is used to propose a more principled construction of the background\nprocess, systematically exploring a variety of candidate shapes. Second the\npicture is extended to propose the marginal likelihood as a useful tool for\nanomaly detection challenges in particle physics. This proposal offers insight\ninto both precise background model determination and demonstrates a flexible\nmethod to extend signal determination beyond a simple bump hunt.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.10391v2"
    },
    {
        "title": "MFV approach to robust estimate of neutron lifetime",
        "authors": [
            "Jiang Zhang",
            "Sen Zhang",
            "Zhen-Rong Zhang",
            "Pu Zhang",
            "Wen-Bin Li",
            "Yan Hong"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  Aiming at evaluating the lifetime of the neutron, we introduce a novel\nstatistical method to analyse the updated compilation of precise measurements\nincluding the 2022 dataset of Particle Data Group (PDG). Based on the\nminimization for the information loss principle, unlike the median statistics\nmethod, we apply the most frequent value (MFV) procedure to estimate the\nneutron lifetime, irrespective of the Gaussian or non-Gaussian distributions.\nProviding a more robust way, the calculated result of the MFV is\n$\\tau_n=881.16^{+2.25}_{-2.35}$ s with statistical bootstrap errors, while the\nresult of median statistics is $\\tau_n=881.5^{+5.5}_{-3}$ s according to the\nbinomial distribution. Using the different central estimates, we also construct\nthe error distributions of neutron lifetime measurements and find the\nnon-Gaussianity, which is still meaningful.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.05890v1"
    },
    {
        "title": "Parameterizing Network Graph Heterogeneity using a Modified Weibull\n  Distribution",
        "authors": [
            "Sinan A. Ozbay",
            "Maximilian M. Nguyen"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We present a simple method to quantitatively capture the heterogeneity in the\ndegree distribution of a network graph using a single parameter $\\sigma$. Using\nan exponential transformation of the shape parameter of the Weibull\ndistribution, this control parameter allows the degree distribution to be\neasily interpolated between highly symmetric and highly heterogeneous\ndistributions on the unit interval. This parameterization of heterogeneity also\nrecovers several other canonical distributions as intermediate special cases,\nincluding the Gaussian, Rayleigh, and exponential distributions. We then\noutline a general graph generation algorithm to produce graphs with a desired\namount of heterogeneity. The utility of this formulation of a heterogeneity\nparameter is demonstrated with examples relating to epidemiological modeling\nand spectral analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.06994v2"
    },
    {
        "title": "Rivet and the analysis preservation in heavy-ion collisions experiments",
        "authors": [
            "Antonio Carlos Oliveira da Silva"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The comparison of experimental data and theoretical predictions is important\nfor our understanding of the mechanisms for interactions and particle\nproduction in hadron collisions, both at the Large Hadron Collider and at the\nRelativistic Heavy-Ion Collider experiments. Several tools were ideated to help\nwith that. Rivet (Robust Independent Validation of Experiment and Theory) is a\nframework that facilitates the comparison between measurements from high-energy\nphysics experiments and Monte Carlo event generators able to produce outputs\nusing the HepMC package. Rivet contains a repository with analysis algorithms\ndeveloped by experiments, providing analysis documentation and preservation.\n  The recent development of features for the implementation of heavy-ion\ncollision analyses, such as centrality determination, will be presented in this\ncontribution, together with some of the open points still to be addressed.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.02704v1"
    },
    {
        "title": "Non-linear, bivariate stochastic modelling of power-grid frequency\n  applied to islands",
        "authors": [
            "Ulrich Oberhofer",
            "Leonardo Rydin Gorjão",
            "G. Cigdem Yalcin",
            "Oliver Kamps",
            "Veit Hagenmeyer",
            "Benjamin Schäfer"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Mitigating climate change requires a transition away from fossil fuels\ntowards renewable energy. As a result, power generation becomes more volatile\nand options for microgrids and islanded power-grid operation are being broadly\ndiscussed. Therefore, studying the power grids of physical islands, as a model\nfor islanded microgrids, is of particular interest when it comes to enhancing\nour understanding of power-grid stability. In the present paper, we investigate\nthe statistical properties of the power-grid frequency of three island systems:\nIceland, Ireland, and the Balearic Islands. We utilise a Fokker-Planck approach\nto construct stochastic differential equations that describe market activities,\ncontrol, and noise acting on power-grid dynamics. Using the obtained parameters\nwe create synthetic time series of the frequency dynamics. Our main\ncontribution is to propose two extensions of stochastic power-grid frequency\nmodels and showcase the applicability of these new models to non-Gaussian\nstatistics, as encountered in islands.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.04551v2"
    },
    {
        "title": "Learn your entropy from informative data: an axiom ensuring the\n  consistent identification of generalized entropies",
        "authors": [
            "Andrea Somazzi",
            "Diego Garlaschelli"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Shannon entropy, a cornerstone of information theory, statistical physics and\ninference methods, is uniquely identified by the Shannon-Khinchin or\nShore-Johnson axioms. Generalizations of Shannon entropy, motivated by the\nstudy of non-extensive or non-ergodic systems, relax some of these axioms and\nlead to entropy families indexed by certain `entropic' parameters. In general,\nthe selection of these parameters requires pre-knowledge of the system or\nencounters inconsistencies. Here we introduce a simple axiom for any entropy\nfamily: namely, that no entropic parameter can be inferred from a completely\nuninformative (uniform) probability distribution. When applied to the\nUffink-Jizba-Korbel and Hanel-Thurner entropies, the axiom selects only R\\'enyi\nentropy as viable. It also extends consistency with the Maximum Likelihood\nprinciple, which can then be generalized to estimate the entropic parameter\npurely from data, as we confirm numerically. Remarkably, in a generalized\nmaximum-entropy framework the axiom implies that the maximized log-likelihood\nalways equals minus Shannon entropy, even if the inferred probability\ndistribution maximizes a generalized entropy and not Shannon's, solving a\nseries of problems encountered in previous approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.05660v1"
    },
    {
        "title": "Information loss from dimensionality reduction in 5D-Gaussian spectral\n  data",
        "authors": [
            "A. Schelle",
            "H. Lüling"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Understanding the loss of information in spectral analytics is a crucial\nfirst step towards finding root causes for failures and uncertainties using\nspectral data in artificial intelligence models built from modern complex data\nscience applications. Here, we show from an elementary Shannon entropy model\nanalysis with quantum statistics of Gaussian distributed spectral data, that\nthe relative loss of information from dimensionality reduction due to the\nprojection of an initial five-dimensional dataset onto two-dimensional diagrams\nis less than one percent in the parameter range of small data sets with sample\nsizes on the order of few hundred data samples. From our analysis, we also\nconclude that the density and expectation value of the entropy probability\ndistribution increases with the sample number and sample size using artificial\ndata models derived from random sampling Monte Carlo simulation methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.11923v2"
    },
    {
        "title": "Bayesian estimation of information-theoretic metrics for sparsely\n  sampled distributions",
        "authors": [
            "Angelo Piga",
            "Lluc Font-Pomarol",
            "Marta Sales-Pardo",
            "Roger Guimerà"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Estimating the Shannon entropy of a discrete distribution from which we have\nonly observed a small sample is challenging. Estimating other\ninformation-theoretic metrics, such as the Kullback-Leibler divergence between\ntwo sparsely sampled discrete distributions, is even harder. Existing\napproaches to address these problems have shortcomings: they are biased,\nheuristic, work only for some distributions, and/or cannot be applied to all\ninformation-theoretic metrics. Here, we propose a fast, semi-analytical\nestimator for sparsely sampled distributions that is efficient, precise, and\ngeneral. Its derivation is grounded in probabilistic considerations and uses a\nhierarchical Bayesian approach to extract as much information as possible from\nthe few observations available. Our approach provides estimates of the Shannon\nentropy with precision at least comparable to the state of the art, and most\noften better. It can also be used to obtain accurate estimates of any other\ninformation-theoretic metric, including the notoriously challenging\nKullback-Leibler divergence. Here, again, our approach performs consistently\nbetter than existing estimators.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.13647v2"
    },
    {
        "title": "The Classification of Short and Long-term Driving Behavior for an\n  Advanced Driver Assistance System by Analyzing Bidirectional Driving Features",
        "authors": [
            "Mudasser Seraj"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Insight into individual driving behavior and habits is essential in traffic\noperation, safety, and energy management. With Connected Vehicle (CV)\ntechnology aiming to address all three of these, the identification of driving\npatterns is a necessary component in the design of personalized Advanced Driver\nAssistance Systems (ADAS) for CVs. Our study aims to address this need by\ntaking a unique approach to analyzing bidirectional (i.e. longitudinal and\nlateral) control features of drivers, using a simple rule-based classification\nprocess to group their driving behaviors and habits. We have analyzed high\nresolution driving data from the real-world CV-testbed, Safety Pilot Model\nDeployment, in Ann Arbor, Michigan, to identify diverse driving behavior on\nfreeway, arterial, and ramp road types. Using three vehicular features known as\njerk, leading headway, and yaw rate, driving characteristics are classified\ninto two groups (Safe Driving and Hostile Driving) on short-term\nclassification, and drivers habits are categorized into three classes (Calm\nDriver, Rational Driver, and Aggressive Driver). Proposed classification models\nare tested on unclassified datasets to validate the model conviction regarding\nspeeding and steep acceleration. Through the proposed method, behavior\nclassification has been successfully identified about 90 percent of speeding\nand similar level of acute acceleration instances. In addition, our study\nadvances an ADAS interface that interacts with drivers in real-time in order to\ntransform information about driving behaviors and habits into feedback to\nindividual drivers. We propose an adaptive and flexible classification approach\nto identify both short-term and long-term driving behavior from naturalistic\ndriving data to identify and, eventually, communicate adverse driving\nbehavioral patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.14743v1"
    },
    {
        "title": "Continuous-Time Modeling and Analysis of Particle Beam Metrology",
        "authors": [
            "Akshay Agarwal",
            "Minxu Peng",
            "Vivek K. Goyal"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Particle beam microscopy (PBM) performs nanoscale imaging by pixelwise\ncapture of scalar values representing noisy measurements of the response from\nsecondary electrons (SEs) integrated over a dwell time. Extended to metrology,\ngoals include estimating SE yield at each pixel and detecting differences in SE\nyield across pixels; obstacles include shot noise in the particle source as\nwell as lack of knowledge of and variability in the instrument response to\nsingle SEs. A recently introduced time-resolved measurement paradigm promises\nmitigation of source shot noise, but its analysis and development have been\nlargely limited to estimation problems under an idealization in which SE bursts\nare directly and perfectly counted. Here, analyses are extended to error\nexponents in feature detection problems and to degraded measurements that are\nrepresentative of actual instrument behavior for estimation problems. For\nestimation from idealized SE counts, insights on existing estimators and a\nsuperior estimator are also provided. For estimation in a realistic PBM imaging\nscenario, extensions to the idealized model are introduced, methods for model\nparameter extraction are discussed, and large improvements from time-resolved\ndata are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.04100v1"
    },
    {
        "title": "Suppression of accidental backgrounds with deep neural networks in the\n  PandaX-II experiment",
        "authors": [
            "Nasir Shaheed",
            "Xun Chen",
            "Meng Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The PandaX dark matter detection project searches for dark matter particles\nusing the technology of dual phase xenon time projection chamber. The low\nexpected rate of the signal events makes the control of backgrounds crucial for\nthe experiment success. In addition to reducing external and internal\nbackgrounds during the construction and operation of the detector, special\ntechniques are employed to suppress the background events during the data\nanalysis. In this article, we demonstrate the use of deep neural\nnetworks(DNNs)for suppressing the accidental backgrounds, as an alternative to\nthe boosted-decision-tree method used in previous analysis of PandaX-II. A new\ndata preparation approach is proposed to enhance the stability of the machine\nlearning algorithms to be run and ultimately the sensitivity of the final data\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.05088v2"
    },
    {
        "title": "Limit setting using spacings in the presence of unknown backgrounds",
        "authors": [
            "Lolian Shtembari",
            "Allen Caldwell"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Finding upper limits on the rate of events from a proposed process in the\npresence of unknown backgrounds is an often encountered problem in the search\nfor rare processes. Methods based on unusually large \"gaps\", or spacings, in\nthe event distribution allow to set limits on the rate of the proposed signal\ndistribution. In this paper, we present two novel spacings-based methods: the\n\"Sum of sorted spacings\" and the \"Product of complementary spacings\" as tests\nand compare these to existing tests on synthetic data as well as on a published\ndata set.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.09520v1"
    },
    {
        "title": "Multiscale Relevance of Natural Images",
        "authors": [
            "Samy Lakhal",
            "Alexandre Darmon",
            "Iacopo Mastromatteo",
            "Matteo Marsili",
            "Michael Benzaquen"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  We use an agnostic information-theoretic approach to investigate the\nstatistical properties of natural images. We introduce the Multiscale Relevance\n(MSR) measure to assess the robustness of images to compression at all scales.\nStarting in a controlled environment, we characterize the MSR of synthetic\nrandom textures as function of image roughness H and other relevant parameters.\nWe then extend the analysis to natural images and find striking similarities\nwith critical (H = 0) random textures. We show that the MSR is more robust and\ninformative of image content than classical methods such as power spectrum\nanalysis. Finally, we confront the MSR to classical measures for the\ncalibration of common procedures such as color mapping and denoising. Overall,\nthe MSR approach appears to be a good candidate for advanced image analysis and\nimage processing, while providing a good level of physical interpretability.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.12717v1"
    },
    {
        "title": "Progress towards an improved particle flow algorithm at CMS with machine\n  learning",
        "authors": [
            "Farouk Mokhtar",
            "Joosep Pata",
            "Javier Duarte",
            "Eric Wulff",
            "Maurizio Pierini",
            "Jean-Roch Vlimant"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The particle-flow (PF) algorithm, which infers particles based on tracks and\ncalorimeter clusters, is of central importance to event reconstruction in the\nCMS experiment at the CERN LHC, and has been a focus of development in light of\nplanned Phase-2 running conditions with an increased pileup and detector\ngranularity. In recent years, the machine learned particle-flow (MLPF)\nalgorithm, a graph neural network that performs PF reconstruction, has been\nexplored in CMS, with the possible advantages of directly optimizing for the\nphysical quantities of interest, being highly reconfigurable to new conditions,\nand being a natural fit for deployment to heterogeneous accelerators. We\ndiscuss progress in CMS towards an improved implementation of the MLPF\nreconstruction, now optimized using generator/simulation-level particle\ninformation as the target for the first time. This paves the way to potentially\nimproving the detector response in terms of physical quantities of interest. We\ndescribe the simulation-based training target, progress and studies on\nevent-based loss terms, details on the model hyperparameter tuning, as well as\nphysics validation with respect to the current PF algorithm in terms of\nhigh-level physical quantities such as the jet and missing transverse momentum\nresolutions. We find that the MLPF algorithm, trained on a generator/simulator\nlevel particle information for the first time, results in broadly compatible\nparticle and jet reconstruction performance with the baseline PF, setting the\nstage for improving the physics performance by additional training statistics\nand model tuning.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.17657v1"
    },
    {
        "title": "Clustering and visualization tools to study high dimensional parameter\n  spaces: B anomalies example",
        "authors": [
            "Ursula Laa",
            "German Valencia"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  We describe the applications of clustering and visualization tools using the\nso-called neutral B anomalies as an example. Clustering permits parameter space\npartitioning into regions that can be separated with some given measurements.\nIt provides a visualization of the collective dependence of all the observables\non the parameters of the problem. These methods highlight the relative\nimportance of different observables, and the effect of correlations, and help\nto understand tensions in global fits. The tools we describe also permit a\nvisual inspection of high dimensional observable and parameter spaces through\nboth linear projections and slicing.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.00151v1"
    },
    {
        "title": "A universal model for the Lorenz curve with novel applications for\n  datasets containing zeros and/or exhibiting extreme inequality",
        "authors": [
            "Thitithep Sitthiyot",
            "Kanyarat Holasut"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Given that the existing parametric functional forms for the Lorenz curve do\nnot fit all possible size distributions, a universal parametric functional form\nis introduced. By using the empirical data from different scientific\ndisciplines and also the hypothetical data, this study shows that, the proposed\nmodel fits not only the data whose actual Lorenz plots have a typical convex\nsegment but also the data whose actual Lorenz plots have both horizontal and\nconvex segments practically well. It also perfectly fits the data whose\nobservation is larger in size while the rest of observations are smaller and\nequal in size as characterized by 2 positive-slope linear segments. In\naddition, the proposed model has a closed-form expression for the Gini index,\nmaking it computationally convenient to calculate. Considering that the Lorenz\ncurve and the Gini index are widely used in various disciplines of sciences,\nthe proposed model and the closed-form expression for the Gini index could be\nused as alternative tools to analyze size distributions of non-negative\nquantities and examine their inequalities or unevennesses.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.13934v1"
    },
    {
        "title": "Inferring Local Structure from Pairwise Correlations",
        "authors": [
            "Mahajabin Rahman",
            "Ilya Nemenman"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  To construct models of large, multivariate complex systems, such as those in\nbiology, one needs to constrain which variables are allowed to interact. This\ncan be viewed as detecting \"local\" structures among the variables. In the\ncontext of a simple toy model of 2D natural and synthetic images, we show that\npairwise correlations between the variables -- even when severely undersampled\n-- provide enough information to recover local relations, including the\ndimensionality of the data, and to reconstruct arrangement of pixels in fully\nscrambled images. This proves to be successful even though higher order\ninteraction structures are present in our data. We build intuition behind the\nsuccess, which we hope might contribute to modeling complex, multivariate\nsystems and to explaining the success of modern attention-based machine\nlearning approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04386v2"
    },
    {
        "title": "Discovering Causal Relations and Equations from Data",
        "authors": [
            "Gustau Camps-Valls",
            "Andreas Gerhardus",
            "Urmi Ninad",
            "Gherardo Varando",
            "Georg Martius",
            "Emili Balaguer-Ballester",
            "Ricardo Vinuesa",
            "Emiliano Diaz",
            "Laure Zanna",
            "Jakob Runge"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Physics is a field of science that has traditionally used the scientific\nmethod to answer questions about why natural phenomena occur and to make\ntestable models that explain the phenomena. Discovering equations, laws and\nprinciples that are invariant, robust and causal explanations of the world has\nbeen fundamental in physical sciences throughout the centuries. Discoveries\nemerge from observing the world and, when possible, performing interventional\nstudies in the system under study. With the advent of big data and the use of\ndata-driven methods, causal and equation discovery fields have grown and made\nprogress in computer science, physics, statistics, philosophy, and many applied\nfields. All these domains are intertwined and can be used to discover causal\nrelations, physical laws, and equations from observational data. This paper\nreviews the concepts, methods, and relevant works on causal and equation\ndiscovery in the broad field of Physics and outlines the most important\nchallenges and promising future lines of research. We also provide a taxonomy\nfor observational causal and equation discovery, point out connections, and\nshowcase a complete set of case studies in Earth and climate sciences, fluid\ndynamics and mechanics, and the neurosciences. This review demonstrates that\ndiscovering fundamental laws and causal relations by observing natural\nphenomena is being revolutionised with the efficient exploitation of\nobservational data, modern machine learning algorithms and the interaction with\ndomain knowledge. Exciting times are ahead with many challenges and\nopportunities to improve our understanding of complex systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.13341v1"
    },
    {
        "title": "Review of the some specific features of the detecting of heavy recoils",
        "authors": [
            "Yu. S. Tsyganov",
            "D. Ibadullayev",
            "A. N. Polyakov",
            "A. A. Voinov",
            "M. V. Shumeiko",
            "V. A. Shubin",
            "V. B. Zlokazov",
            "D. A. Kuznetsov"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  In this paper, we present the results of the first beam tests of the\ndetection system at the focal plane of the Dubna Gas-Filled Recoil Separator-2\n(DGFRS-2), which receives beams from the DC-280 FLNR cyclotron. The high beam\nintensity of $^{48}$Ca$^{+10}$ heavy ions from the cyclotron enables us to\nobtain a number of superheavy recoils sufficient to compare both the measured\nand calculated spectra of superheavy recoils implanted into a silicon detector.\nA real-time algorithm to search for an Evaporation Residue (ER) -- $\\alpha$\ncorrelated sequences is described in brief. It should be noted that the DGFRS-2\nspectrometer operates in conjunction with the 48x128 strip DSSD (Double-sided\nSilicon Strip Detector; 48x226 mm2) detector and a low-pressure pentane-filled\ngaseous detector (1.2 Torr; 80x230 mm$^2$). A block-diagram of the spectrometer\nand the event format are also presented. Special attention is paid to the\nresponse of a low-pressure pentane-filled DeltaE multiwire proportional chamber\nfor recoils of Fl, synthesized in the $^{242}$Pu+$^{48}$Ca $\\to$ $^{287}$Fl +3n\ncomplete fusion nuclear reaction. Some actual parameters of the detection\nsystem have also been extracted from $^{\\rm nat}$Yb + $^{48}$Ca, $^{232}$Th +\n$^{48}$Ca, $^{243}$Am + $^{48}$Ca, $^{238}$U + $^{48}$Ca reactions. The effect\nof neighbor strip charge sharing for the ohmic side of the DSSD detector is\nalso under consideration.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.13344v1"
    },
    {
        "title": "Transporting Densities Across Dimensions",
        "authors": [
            "Michael Plainer",
            "Felix Dietrich",
            "Ioannis G. Kevrekidis"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Even the best scientific equipment can only partially observe reality.\nRecorded data is often lower-dimensional, e.g., two-dimensional pictures of the\nthree-dimensional world. Combining data from multiple experiments then results\nin a marginal density. This work shows how to transport such lower-dimensional\nmarginal densities into a more informative, higher-dimensional joint space by\nleveraging time-delayed measurements from an observation process. This can\naugment the information from scientific equipment to construct a more coherent\nview. Classical transportation algorithms can be used when the source and\ntarget dimensions match. Our approach allows the transport of samples between\nspaces of different dimensions by exploiting information from the sample\ncollection process. We reconstruct the surface of an implant from partial\nrecordings of bacteria moving on it and construct a joint space for satellites\norbiting the Earth by combining one-dimensional, time-delayed altitude\nmeasurements.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.16227v1"
    },
    {
        "title": "PyPWA: A Software Toolkit for Parameter Optimization and Amplitude\n  Analysis",
        "authors": [
            "Mark Jones",
            "Peter Hurck",
            "William Phelps",
            "Carlos W. Salgado"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  PyPWA is a toolkit designed to optimize parametric models describing data and\ngenerate simulated distributions according to a model. Its software has been\nwritten within the python ecosystem with the goal of performing Amplitude or\nPartial Wave Analysis (PWA) in nuclear and particle physics experiments. We\nbriefly describe the general features of amplitude analysis and the PyPWA\nsoftware design and usage. We provide benchmarks of the scaling and an example\nof its application.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.01192v1"
    },
    {
        "title": "Validating an algebraic approach to characterizing resonator networks",
        "authors": [
            "Viva R. Horowitz",
            "Brittany Carter",
            "Uriel Hernandez",
            "Trevor Scheuing",
            "Benjamín J. Alemán"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Resonator networks are ubiquitous in natural and engineered systems, such as\nsolid-state materials, neural tissue, and electrical circuits. To understand\nand manipulate these networks, it is essential to characterize their building\nblocks, which include the mechanical analogs of mass, elasticity, damping, and\ncoupling of each resonator element. While these mechanical parameters are\ntypically obtained from response spectra using least-squares fitting, this\napproach requires a priori knowledge of all parameters and is susceptible to\nlarge error due to convergence to local minima. Here we validate an alternative\nalgebraic means to characterize resonator networks with no or minimal a priori\nknowledge. Our approach recasts the equations of motion of the network into a\nlinear homogeneous algebraic equation and solves the equation with a set of\ndiscrete measured network response vectors. For validation, we employ our\napproach on noisy simulated data from a single resonator and a coupled\nresonator pair, and we characterize the accuracy of the recovered parameters\nusing high-dimension factorial simulations. Generally, we find that the error\nis inversely proportional to the signal-to-noise ratio, that measurements at\ntwo frequencies are sufficient to recover all parameters, and that sampling\nnear the resonant peaks is optimal. Our simple, powerful tool will enable\nfuture efforts to ascertain network properties and control resonator networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.01556v1"
    },
    {
        "title": "Scattering Spectra Models for Physics",
        "authors": [
            "Sihao Cheng",
            "Rudy Morel",
            "Erwan Allys",
            "Brice Ménard",
            "Stéphane Mallat"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Physicists routinely need probabilistic models for a number of tasks such as\nparameter inference or the generation of new realizations of a field.\nEstablishing such models for highly non-Gaussian fields is a challenge,\nespecially when the number of samples is limited. In this paper, we introduce\nscattering spectra models for stationary fields and we show that they provide\naccurate and robust statistical descriptions of a wide range of fields\nencountered in physics. These models are based on covariances of scattering\ncoefficients, i.e. wavelet decomposition of a field coupled with a point-wise\nmodulus. After introducing useful dimension reductions taking advantage of the\nregularity of a field under rotation and scaling, we validate these models on\nvarious multi-scale physical fields and demonstrate that they reproduce\nstandard statistics, including spatial moments up to 4th order. These\nscattering spectra provide us with a low-dimensional structured representation\nthat captures key properties encountered in a wide range of physical fields.\nThese generic models can be used for data exploration, classification,\nparameter inference, symmetry detection, and component separation.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.17210v2"
    },
    {
        "title": "The most likely common cause",
        "authors": [
            "A. Hovhannisyan",
            "A. E. Allahverdyan"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The common cause principle for two random variables $A$ and $B$ is examined\nin the case of causal insufficiency, when their common cause $C$ is known to\nexist, but only the joint probability of $A$ and $B$ is observed. As a result,\n$C$ cannot be uniquely identified (the latent confounder problem). We show that\nthe generalized maximum likelihood method can be applied to this situation and\nallows identification of $C$ that is consistent with the common cause\nprinciple. It closely relates to the maximum entropy principle. Investigation\nof the two binary symmetric variables reveals a non-analytic behavior of\nconditional probabilities reminiscent of a second-order phase transition. This\noccurs during the transition from correlation to anti-correlation in the\nobserved probability distribution. The relation between the generalized\nlikelihood approach and alternative methods, such as predictive likelihood and\nthe minimum common cause entropy, is discussed. The consideration of the common\ncause for three observed variables (and one hidden cause) uncovers causal\nstructures that defy representation through directed acyclic graphs with the\nMarkov condition.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.17557v2"
    },
    {
        "title": "Exploring Model Misspecification in Statistical Finite Elements via\n  Shallow Water Equations",
        "authors": [
            "Connor Duffin",
            "Paul Branson",
            "Matt Rayson",
            "Mark Girolami",
            "Edward Cripps",
            "Thomas Stemler"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The abundance of observed data in recent years has increased the number of\nstatistical augmentations to complex models across science and engineering. By\naugmentation we mean coherent statistical methods that incorporate measurements\nupon arrival and adjust the model accordingly. However, in this research area\nmethodological developments tend to be central, with important assessments of\nmodel fidelity often taking second place. Recently, the statistical finite\nelement method (statFEM) has been posited as a potential solution to the\nproblem of model misspecification when the data are believed to be generated\nfrom an underlying partial differential equation system. Bayes nonlinear\nfiltering permits data driven finite element discretised solutions that are\nupdated to give a posterior distribution which quantifies the uncertainty over\nmodel solutions. The statFEM has shown great promise in systems subject to mild\nmisspecification but its ability to handle scenarios of severe model\nmisspecification has not yet been presented. In this paper we fill this gap,\nstudying statFEM in the context of shallow water equations chosen for their\noceanographic relevance. By deliberately misspecifying the governing equations,\nvia linearisation, viscosity, and bathymetry, we systematically analyse\nmisspecification through studying how the resultant approximate posterior\ndistribution is affected, under additional regimes of decreasing spatiotemporal\nobservational frequency. Results show that statFEM performs well with\nreasonable accuracy, as measured by theoretically sound proper scoring rules.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.05334v1"
    },
    {
        "title": "Bounds on the rates of statistical divergences and mutual information\n  via stochastic thermodynamics",
        "authors": [
            "Jan Karbowski"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Statistical divergences are important tools in data analysis, information\ntheory, and statistical physics, and there exist well known inequalities on\ntheir bounds. However, in many circumstances involving temporal evolution, one\nneeds limitations on the rates of such quantities, instead. Here, several\ngeneral upper bounds on the rates of some f-divergences are derived, valid for\nany type of stochastic dynamics (both Markovian and non-Markovian), in terms of\ninformation-like and/or thermodynamic observables. As special cases, the\nanalytical bounds on the rate of mutual information are obtained. The major\nrole in all those limitations is played by temporal Fisher information,\ncharacterizing the speed of global system dynamics, and some of them contain\nentropy production, suggesting a link with stochastic thermodynamics. Indeed,\nthe derived inequalities can be used for estimation of minimal dissipation and\nglobal speed in thermodynamic stochastic systems. Specific applications of\nthese inequalities in physics and neuroscience are given, which include the\nbounds on the rates of free energy and work in nonequilibrium systems, limits\non the speed of information gain in learning synapses, as well as the bounds on\nthe speed of predictive inference and learning rate. Overall, the derived\nbounds can be applied to any complex network of interacting elements, where\npredictability and thermodynamics of network dynamics are of prime concern.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.05597v2"
    },
    {
        "title": "Efficient set-theoretic algorithms for computing high-order Forman-Ricci\n  curvature on abstract simplicial complexes",
        "authors": [
            "Danillo Barros de Souza",
            "Jonatas T. S. da Cunha",
            "Fernando A. N. Santos",
            "Jürgen Jost",
            "Serafim Rodrigues"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Forman-Ricci curvature (FRC) is a potent and powerful tool for analysing\nempirical networks, as the distribution of the curvature values can identify\nstructural information that is not readily detected by other geometrical\nmethods. Crucially, FRC captures higher-order structural information of clique\ncomplexes of a graph or Vietoris-Rips complexes, which is not readily\naccessible to alternative methods. However, existing FRC platforms are\nprohibitively computationally expensive. Therefore, herein we develop an\nefficient set-theoretic formulation for computing such high-order FRC in\nsimplicial complexes. Significantly, our set theory representation reveals\nprevious computational bottlenecks and also accelerates the computation of FRC.\nFinally, We provide a pseudo-code, a software implementation coined FastForman,\nas well as a benchmark comparison with alternative implementations. We envisage\nthat FastForman will be used in Topological and Geometrical Data analysis for\nhigh-dimensional complex data sets. Moreover, our development paves the way for\nfuture generalisations towards efficient computations of FRC on cell complexes.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.11763v5"
    },
    {
        "title": "Untangling The Relationship Between Power Outage and Population Activity\n  Recovery in Disasters",
        "authors": [
            "Chia-Wei Hsu",
            "Ali Mostafavi"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Despite recognition of the relationship between infrastructure resilience and\ncommunity recovery, very limited empirical evidence exists regarding the extent\nto which the disruptions in and restoration of infrastructure services\ncontribute to the speed of community recovery. To address this gap, this study\ninvestigates the relationship between community and infrastructure systems in\nthe context of hurricane impacts, focusing on the recovery dynamics of\npopulation activity and power infrastructure restoration. Empirical\nobservational data were utilized to analyze the extent of impact, recovery\nduration, and recovery types of both systems in the aftermath of Hurricane Ida.\nThe study reveals three key findings. First, power outage duration positively\ncorrelates with outage extent until a certain impact threshold is reached.\nBeyond this threshold, restoration time remains relatively stable regardless of\noutage magnitude. This finding underscores the need to strengthen power\ninfrastructure, particularly in extreme weather conditions, to minimize outage\nrestoration time. Second, power was fully restored in 70\\% of affected areas\nbefore population activity levels normalized. This finding suggests the role\ninfrastructure functionality plays in post-disaster community recovery.\nInterestingly, quicker power restoration did not equate to rapid population\nactivity recovery due to other possible factors such as transportation, housing\ndamage, and business interruptions. Finally, if power outages last beyond two\nweeks, community activity resumes before complete power restoration, indicating\nadaptability in prolonged outage scenarios. This implies the capacity of\ncommunities to adapt to ongoing power outages and continue daily life\nactivities...\n",
        "pdf_link": "http://arxiv.org/pdf/2309.10118v1"
    },
    {
        "title": "Designing Observables for Measurements with Deep Learning",
        "authors": [
            "Owen Long",
            "Benjamin Nachman"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Many analyses in particle and nuclear physics use simulations to infer\nfundamental, effective, or phenomenological parameters of the underlying\nphysics models. When the inference is performed with unfolded cross sections,\nthe observables are designed using physics intuition and heuristics. We propose\nto design targeted observables with machine learning. Unfolded, differential\ncross sections in a neural network output contain the most information about\nparameters of interest and can be well-measured by construction. The networks\nare trained using a custom loss function that rewards outputs that are\nsensitive to the parameter(s) of interest while simultaneously penalizing\noutputs that are different between particle-level and detector-level (to\nminimize detector distortions). We demonstrate this idea in simulation using\ntwo physics models for inclusive measurements in deep inelastic scattering. We\nfind that the new approach is more sensitive than classical observables at\ndistinguishing the two models and also has a reduced unfolding uncertainty due\nto the reduced detector distortions.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.08717v2"
    },
    {
        "title": "Event-by-event Comparison between Machine-Learning- and\n  Transfer-Matrix-based Unfolding Methods",
        "authors": [
            "Mathias Backes",
            "Anja Butter",
            "Monica Dunford",
            "Bogdan Malaescu"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The unfolding of detector effects is a key aspect of comparing experimental\ndata with theoretical predictions. In recent years, different Machine-Learning\nmethods have been developed to provide novel features, e.g. high dimensionality\nor a probabilistic single-event unfolding based on generative neural networks.\nTraditionally, many analyses unfold detector effects using\ntransfer-matrix--based algorithms, which are well established in\nlow-dimensional unfolding. They yield an unfolded distribution of the total\nspectrum, together with its covariance matrix. This paper proposes a method to\nobtain probabilistic single-event unfolded distributions, together with their\nuncertainties and correlations, for the transfer-matrix--based unfolding. The\nalgorithm is first validated on a toy model and then applied to pseudo-data for\nthe $pp\\rightarrow Z\\gamma \\gamma$ process. In both examples the performance is\ncompared to the Machine-Learning--based single-event unfolding using an\niterative approach with conditional invertible neural networks (IcINN).\n",
        "pdf_link": "http://arxiv.org/pdf/2310.17037v2"
    },
    {
        "title": "Jackpot statistics, a physicist's approach",
        "authors": [
            "István Gere",
            "Szabolcs Kelemen",
            "Zoltán Néda",
            "Tamás S. Biró"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  At a first glance lottery is a form of gambling, a game in which the chances\nof winning is extremely small. But upon a deeper look, considering that the\nJackpot prize of lotteries is a result of the active participation of millions\nof players, we come to the conclusion that the interaction of the simple rules\nwith the high number of players creates an emergent complex system. Such a\nsystem is characterized by its time-series that presents some interesting\nproperties. Given the inherent stochastic nature of this game, it can be\ndescribed within a mean-field type approach, such as the one implemented in the\nLocal Growth and Global Reset (LGGR) model. We argue that the Jackpot\ntime-series behaves ergodic for six lotteries with diverse formats and player\npools. Specifying this consideration in the framework of the LGGR model, we\nmodel the lotteries with growth rates confirmed by the time-series. The reset\nrate is deduced mathematically and confirmed by data. Given these parameters we\ncalculate the probability density of the Jackpot prizes, that fits well the\nempirically observed ones. We propose to use a single w parameter, as the\nproduct of the player pools found under the jurisdiction of the lottery and the\nchance that a single lottery ticket wins.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.04826v2"
    },
    {
        "title": "GPGPU Acceleration of Incoherent Scatter Radar Plasma Line Analysis",
        "authors": [
            "Natalie Hilliard",
            "Juha Vierinen",
            "Philip J. Erickson"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The incoherent scatter radar (ISR) technique is a powerful remote sensing\ntool for ionosphere and thermosphere dynamics in the near-Earth space\nenvironment. Weak ISR scatter from naturally occurring Langmuir oscillations,\nor plasma lines, contain high precision information on the altitude-dependent\nthermal ionospheric electron density. However, analyzing this\nfrequency-dependent scatter over a large number of radar ranges requires large\ncomputational power, especially when the goal is realtime analysis. General\npurpose computing on graphics processing units (GPGPU) offers immense\ncomputational speedup when compared to traditional central processing unit\n(CPU) calculations for highly parallelizable tasks, and it is well suited for\nISR analysis applications. This paper extends a single graphics processing unit\n(GPU) algorithmic solution in a GPGPU framework, and discusses the algorithm\ndeveloped, including GPU hardware considerations. Results indicate an\norder-of-magnitude improvement over CPU analysis and suggest that GPGPU can\nachieve realtime speed for plasma line applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.18737v1"
    },
    {
        "title": "DeepTreeGANv2: Iterative Pooling of Point Clouds",
        "authors": [
            "Moritz Alfons Wilhelm Scham",
            "Dirk Krücker",
            "Kerstin Borras"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  In High Energy Physics, detailed and time-consuming simulations are used for\nparticle interactions with detectors. To bypass these simulations with a\ngenerative model, the generation of large point clouds in a short time is\nrequired, while the complex dependencies between the particles must be\ncorrectly modelled. Particle showers are inherently tree-based processes, as\neach particle is produced by the decay or detector interaction of a particle of\nthe previous generation. In this work, we present a significant extension to\nDeepTreeGAN, featuring a critic, that is able to aggregate such point clouds\niteratively in a tree-based manner. We show that this model can reproduce\ncomplex distributions, and we evaluate its performance on the public JetNet 150\ndataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.00042v2"
    },
    {
        "title": "High Pileup Particle Tracking with Object Condensation",
        "authors": [
            "Kilian Lieret",
            "Gage DeZoort",
            "Devdoot Chatterjee",
            "Jian Park",
            "Siqi Miao",
            "Pan Li"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Recent work has demonstrated that graph neural networks (GNNs) can match the\nperformance of traditional algorithms for charged particle tracking while\nimproving scalability to meet the computing challenges posed by the HL-LHC.\nMost GNN tracking algorithms are based on edge classification and identify\ntracks as connected components from an initial graph containing spurious\nconnections. In this talk, we consider an alternative based on object\ncondensation (OC), a multi-objective learning framework designed to cluster\npoints (hits) belonging to an arbitrary number of objects (tracks) and regress\nthe properties of each object. Building on our previous results, we present a\nstreamlined model and show progress toward a one-shot OC tracking algorithm in\na high-pileup environment.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.03823v1"
    },
    {
        "title": "Improving new physics searches with diffusion models for event\n  observables and jet constituents",
        "authors": [
            "Debajyoti Sengupta",
            "Matthew Leigh",
            "John Andrew Raine",
            "Samuel Klein",
            "Tobias Golling"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  We introduce a new technique called Drapes to enhance the sensitivity in\nsearches for new physics at the LHC. By training diffusion models on side-band\ndata, we show how background templates for the signal region can be generated\neither directly from noise, or by partially applying the diffusion process to\nexisting data. In the partial diffusion case, data can be drawn from side-band\nregions, with the inverse diffusion performed for new target conditional\nvalues, or from the signal region, preserving the distribution over the\nconditional property that defines the signal region. We apply this technique to\nthe hunt for resonances using the LHCO di-jet dataset, and achieve\nstate-of-the-art performance for background template generation using high\nlevel input features. We also show how Drapes can be applied to low level\ninputs with jet constituents, reducing the model dependence on the choice of\ninput observables. Using jet constituents we can further improve sensitivity to\nthe signal process, but observe a loss in performance where the signal\nsignificance before applying any selection is below 4$\\sigma$.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.10130v2"
    },
    {
        "title": "Machine Learning for Anomaly Detection in Particle Physics",
        "authors": [
            "Vasilis Belis",
            "Patrick Odagiu",
            "Thea Klæboe Årrestad"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The detection of out-of-distribution data points is a common task in particle\nphysics. It is used for monitoring complex particle detectors or for\nidentifying rare and unexpected events that may be indicative of new phenomena\nor physics beyond the Standard Model. Recent advances in Machine Learning for\nanomaly detection have encouraged the utilization of such techniques on\nparticle physics problems. This review article provides an overview of the\nstate-of-the-art techniques for anomaly detection in particle physics using\nmachine learning. We discuss the challenges associated with anomaly detection\nin large and complex data sets, such as those produced by high-energy particle\ncolliders, and highlight some of the successful applications of anomaly\ndetection in particle physics experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.14190v1"
    },
    {
        "title": "Information Flow Rate for Cross-Correlated Stochastic Processes",
        "authors": [
            "Dionissios T. Hristopulos"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Causal inference seeks to identify cause-and-effect interactions in coupled\nsystems. A recently proposed method by Liang detects causal relations by\nquantifying the direction and magnitude of information flow between time\nseries. The theoretical formulation of information flow for stochastic\ndynamical systems provides a general expression and a data-driven statistic for\nthe rate of entropy transfer between different system units. To advance\nunderstanding of information flow rate in terms of intuitive concepts and\nphysically meaningful parameters, we investigate statistical properties of the\ndata-driven information flow rate between coupled stochastic processes. We\nderive relations between the expectation of the information flow rate statistic\nand properties of the auto- and cross-correlation functions. Thus, we elucidate\nthe dependence of the information flow rate on the analytical properties and\ncharacteristic times of the correlation functions. Our analysis provides\ninsight into the influence of the sampling step, the strength of\ncross-correlations, and the temporal delay of correlations on information flow\nrate. We support the theoretical results with numerical simulations of\ncorrelated Gaussian processes.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.04950v1"
    },
    {
        "title": "Mini-jet Clustering Algorithm Using Transverse-momentum Seeds in\n  High-energy Nuclear Collisions",
        "authors": [
            "Hanpu Jiang",
            "Nanxi Yao",
            "Cheuk-Yin Wong",
            "Gang Wang",
            "Huan Zhong Huang"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  We propose an algorithm to detect mini-jet clusters in high-energy nuclear\ncollisions, by selecting a high-transverse-momentum ($p_T$) particle as a seed\nand assigning a clustering radius ($R$) in the pseudorapidity and\nazimuthal-angle space. Our PYTHIA simulations for $p$+$p$ collisions show that\na scheme with a seeding $p_T$ of around 0.5 GeV/$c$ and $R$ of approximately\n0.6 satisfactorily identifies mini-jet clusters. The correlation between\nclusters obtained in PYTHIA calculations using the algorithm exhibits the\nproper behavior of hard-scattering-like processes, suggesting its usefulness in\nisolating mini-jet-like clusters from non-hard-scattering soft processes when\napplied to actual nuclear-collision data, thereby allowing a closer examination\nof both the mini-jet and the soft mechanisms.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.06895v2"
    },
    {
        "title": "Physics-informed Meta-instrument for eXperiments (PiMiX) with\n  applications to fusion energy",
        "authors": [
            "Zhehui Wang",
            "Shanny Lin",
            "Miles Teng-Levy",
            "Pinghan Chu",
            "Bradley T. Wolfe",
            "Chun-Shang Wong",
            "Christopher S. Campbell",
            "Xin Yue",
            "Liyuan Zhang",
            "Derek Aberle",
            "Mariana Alvarado Alvarez",
            "David Broughton",
            "Ray T. Chen",
            "Baolian Cheng",
            "Feng Chu",
            "Eric R. Fossum",
            "Mark A. Foster",
            "Chengkun Huang",
            "Velat Kilic",
            "Karl Krushelnick",
            "Wenting Li",
            "Eric Loomis",
            "Thomas Schmidt Jr.",
            "Sky K. Sjue",
            "Chris Tomkins",
            "Dmitry A. Yarotski",
            "Renyuan Zhu"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Data-driven methods (DDMs), such as deep neural networks, offer a generic\napproach to integrated data analysis (IDA), integrated diagnostic-to-control\n(IDC) workflows through data fusion (DF), which includes multi-instrument data\nfusion (MIDF), multi-experiment data fusion (MXDF), and simulation-experiment\ndata fusion (SXDF). These features make DDMs attractive to nuclear fusion\nenergy and power plant applications, leveraging accelerated workflows through\nmachine learning and artificial intelligence. Here we describe Physics-informed\nMeta-instrument for eXperiments (PiMiX) that integrates X-ray (including\nhigh-energy photons such as $\\gamma$-rays from nuclear fusion), neutron and\nothers (such as proton radiography) measurements for nuclear fusion. PiMiX\nsolves multi-domain high-dimensional optimization problems and integrates\nmulti-modal measurements with multiphysics modeling through neural networks.\nSuper-resolution for neutron detection and energy resolved X-ray detection have\nbeen demonstrated. Multi-modal measurements through MIDF can extract more\ninformation than individual or uni-modal measurements alone. Further\noptimization schemes through DF are possible towards empirical fusion scaling\nlaws discovery and new fusion reactor designs.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.08390v1"
    },
    {
        "title": "Data Unfolding with Mean Integrated Square Error Optimization",
        "authors": [
            "Nikolay D. Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Experimental data in Particle and Nuclear physics, Particle Astrophysics and\nRadiation Protection Dosimetry are obtained from experimental facilities\ncomprising a complex array of sensors, electronics and software. Computer\nsimulation is used to study the measurement process. Probability Density\nFunctions (PDFs) of measured physical parameters deviate from true PDFs due to\nresolution, bias, and efficiency effects. Good estimates of the true PDF are\nnecessary for testing theoretical models, comparing results from different\nexperiments, and combining results from various research endeavors. In the\narticle, the histogram method is employed to estimate both the measured and\ntrue PDFs. The binning of histograms is determined using the K-means clustering\nalgorithm. The true PDF is estimated through the maximization of the likelihood\nfunction with entropy regularization, utilizing a non-linear optimization\nalgorithm specially designed for this purpose. The accuracy of the results is\nassessed using the Mean Integrated Square Error. To determine the optimal value\nfor the regularization parameter, a bootstrap method is applied. Additionally,\na mathematical model of the measurement system is formulated using system\nidentification methods. This approach enhances the robustness and precision of\nthe estimation process, providing a more reliable analysis of the system's\ncharacteristics.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.12990v3"
    },
    {
        "title": "NuGraph2: A Graph Neural Network for Neutrino Physics Event\n  Reconstruction",
        "authors": [
            "V Hewes",
            "Adam Aurisano",
            "Giuseppe Cerati",
            "Jim Kowalkowski",
            "Claire Lee",
            "Wei-keng Liao",
            "Daniel Grzenda",
            "Kaushal Gumpula",
            "Xiaohe Zhang"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a\nwealth of high-resolution information on particle interactions, and leveraging\nthat information to its full potential requires sophisticated automated\nreconstruction techniques. This article describes NuGraph2, a Graph Neural\nNetwork (GNN) for low-level reconstruction of simulated neutrino interactions\nin a LArTPC detector. Simulated neutrino interactions in the MicroBooNE\ndetector geometry are described as heterogeneous graphs, with energy\ndepositions on each detector plane forming nodes on planar subgraphs. The\nnetwork utilizes a multi-head attention message-passing mechanism to perform\nbackground filtering and semantic labelling on these graph nodes, identifying\nthose associated with the primary physics interaction with 98.0\\% efficiency\nand labelling them according to particle type with 94.9\\% efficiency. The\nnetwork operates directly on detector observables across multiple 2D\nrepresentations, but utilizes a 3D-context-aware mechanism to encourage\nconsistency between these representations. Model inference takes 0.12 s/event\non a CPU, and 0.005 s/event batched on a GPU. This architecture is designed to\nbe a general-purpose solution for particle reconstruction in neutrino physics,\nwith the potential for deployment across a broad range of detector\ntechnologies, and offers a core convolution engine that can be leveraged for a\nvariety of tasks beyond the two described in this article.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.11872v1"
    },
    {
        "title": "Physics Event Classification Using Large Language Models",
        "authors": [
            "Cristiano Fanelli",
            "James Giroux",
            "Patrick Moran",
            "Hemalata Nayak",
            "Karthik Suresh",
            "Eric Walter"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The 2023 AI4EIC hackathon was the culmination of the third annual AI4EIC\nworkshop at The Catholic University of America. This workshop brought together\nresearchers from physics, data science and computer science to discuss the\nlatest developments in Artificial Intelligence (AI) and Machine Learning (ML)\nfor the Electron Ion Collider (EIC), including applications for detectors,\naccelerators, and experimental control. The hackathon, held on the final day of\nthe workshop, involved using a chatbot powered by a Large Language Model,\nChatGPT-3.5, to train a binary classifier neutrons and photons in simulated\ndata from the \\textsc{GlueX} Barrel Calorimeter. In total, six teams of up to\nfour participants from all over the world took part in this intense educational\nand research event. This article highlights the hackathon challenge, the\nresources and methodology used, and the results and insights gained from\nanalyzing physics data using the most cutting-edge tools in AI/ML.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.05752v1"
    },
    {
        "title": "A feature-based information-theoretic approach for detecting\n  interpretable, long-timescale pairwise interactions from time series",
        "authors": [
            "Aria Nguyen",
            "Oscar McMullin",
            "Joseph T. Lizier",
            "Ben D. Fulcher"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Quantifying relationships between components of a complex system is critical\nto understanding the rich network of interactions that characterize the\nbehavior of the system. Traditional methods for detecting pairwise dependence\nof time series, such as Pearson correlation, Granger causality, and mutual\ninformation, are computed directly in the space of measured time-series values.\nBut for systems in which interactions are mediated by statistical properties of\nthe time series (`time-series features') over longer timescales, this approach\ncan fail to capture the underlying dependence from limited and noisy\ntime-series data, and can be challenging to interpret. Addressing these issues,\nhere we introduce an information-theoretic method for detecting dependence\nbetween time series mediated by time-series features that provides\ninterpretable insights into the nature of the interactions. Our method extracts\na candidate set of time-series features from sliding windows of the source time\nseries and assesses their role in mediating a relationship to values of the\ntarget process. Across simulations of three different generative processes, we\ndemonstrate that our feature-based approach can outperform a traditional\ninference approach based on raw time-series values, especially in challenging\nscenarios characterized by short time-series lengths, high noise levels, and\nlong interaction timescales. Our work introduces a new tool for inferring and\ninterpreting feature-mediated interactions from time-series data, contributing\nto the broader landscape of quantitative analysis in complex systems research,\nwith potential applications in various domains including but not limited to\nneuroscience, finance, climate science, and engineering.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.05929v1"
    },
    {
        "title": "Trajectory analysis through entropy characterization over coded\n  representation",
        "authors": [
            "Roxana Peña-Mendieta",
            "Ania Mesa-Rodríguez",
            "Ernesto Estevez-Rams",
            "Daniel Estevez-Moya",
            "Danays Kunka"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Any continuous curve in a higher dimensional space can be considered a\ntrajectory that can be parameterized by a single variable, usually taken as\ntime. It is well known that a continuous curve can have a fractional\ndimensionality, which can be estimated using already standard algorithms.\nHowever, characterizing a trajectory from an entropic perspective is far less\ndeveloped. The search for such characterization leads us to use chain coding to\ndiscretize the description of a curve. Calculating the entropy density and\nentropy-related magnitudes from the resulting finite alphabet code becomes\nstraightforward. In such a way, the entropy of a trajectory can be defined and\nused as an effective tool to assert creativity and pattern formation from a\nShannon perspective. Applying the procedure to actual experimental\nphysiological data and modelled trajectories of astronomical dynamics proved\nthe robustness of the entropic characterization in a wealth of trajectories of\ndifferent origins and the insight that can be gained from its use.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.03693v1"
    },
    {
        "title": "Lorentz-Equivariant Geometric Algebra Transformers for High-Energy\n  Physics",
        "authors": [
            "Jonas Spinner",
            "Victor Bresó",
            "Pim de Haan",
            "Tilman Plehn",
            "Jesse Thaler",
            "Johann Brehmer"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Extracting scientific understanding from particle-physics experiments\nrequires solving diverse learning problems with high precision and good data\nefficiency. We propose the Lorentz Geometric Algebra Transformer (L-GATr), a\nnew multi-purpose architecture for high-energy physics. L-GATr represents\nhigh-energy data in a geometric algebra over four-dimensional space-time and is\nequivariant under Lorentz transformations, the symmetry group of relativistic\nkinematics. At the same time, the architecture is a Transformer, which makes it\nversatile and scalable to large systems. L-GATr is first demonstrated on\nregression and classification tasks from particle physics. We then construct\nthe first Lorentz-equivariant generative model: a continuous normalizing flow\nbased on an L-GATr network, trained with Riemannian flow matching. Across our\nexperiments, L-GATr is on par with or outperforms strong domain-specific\nbaselines.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.14806v3"
    },
    {
        "title": "Towards Universal Unfolding of Detector Effects in High-Energy Physics\n  using Denoising Diffusion Probabilistic Models",
        "authors": [
            "Camila Pazos",
            "Shuchin Aeron",
            "Pierre-Hugues Beauchemin",
            "Vincent Croft",
            "Zhengyan Huan",
            "Martin Klassen",
            "Taritree Wongjirad"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Correcting for detector effects in experimental data, particularly through\nunfolding, is critical for enabling precision measurements in high-energy\nphysics. However, traditional unfolding methods face challenges in scalability,\nflexibility, and dependence on simulations. We introduce a novel approach to\nmultidimensional object-wise unfolding using conditional Denoising Diffusion\nProbabilistic Models (cDDPM). Our method utilizes the cDDPM for a\nnon-iterative, flexible posterior sampling approach, incorporating distribution\nmoments as conditioning information, which exhibits a strong inductive bias\nthat allows it to generalize to unseen physics processes without explicitly\nassuming the underlying distribution. Our results highlight the potential of\nthis method as a step towards a \"universal\" unfolding tool that reduces\ndependence on truth-level assumptions, while enabling the unfolding of a wide\nrange of measured distributions with improved adaptability and accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.01507v3"
    },
    {
        "title": "Effectiveness of denoising diffusion probabilistic models for fast and\n  high-fidelity whole-event simulation in high-energy heavy-ion experiments",
        "authors": [
            "Yeonju Go",
            "Dmitrii Torbunov",
            "Timothy Rinn",
            "Yi Huang",
            "Haiwang Yu",
            "Brett Viren",
            "Meifeng Lin",
            "Yihui Ren",
            "Jin Huang"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Artificial intelligence (AI) generative models, such as generative\nadversarial networks (GANs), variational auto-encoders, and normalizing flows,\nhave been widely used and studied as efficient alternatives for traditional\nscientific simulations. However, they have several drawbacks, including\ntraining instability and inability to cover the entire data distribution,\nespecially for regions where data are rare. This is particularly challenging\nfor whole-event, full-detector simulations in high-energy heavy-ion\nexperiments, such as sPHENIX at the Relativistic Heavy Ion Collider and Large\nHadron Collider experiments, where thousands of particles are produced per\nevent and interact with the detector. This work investigates the effectiveness\nof Denoising Diffusion Probabilistic Models (DDPMs) as an AI-based generative\nsurrogate model for the sPHENIX experiment that includes the heavy-ion event\ngeneration and response of the entire calorimeter stack. DDPM performance in\nsPHENIX simulation data is compared with a popular rival, GANs. Results show\nthat both DDPMs and GANs can reproduce the data distribution where the examples\nare abundant (low-to-medium calorimeter energies). Nonetheless, DDPMs\nsignificantly outperform GANs, especially in high-energy regions where data are\nrare. Additionally, DDPMs exhibit superior stability compared to GANs. The\nresults are consistent between both central and peripheral centrality heavy-ion\ncollision events. Moreover, DDPMs offer a substantial speedup of approximately\na factor of 100 compared to the traditional Geant4 simulation method.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.01602v1"
    },
    {
        "title": "Parnassus: An Automated Approach to Accurate, Precise, and Fast Detector\n  Simulation and Reconstruction",
        "authors": [
            "Etienne Dreyer",
            "Eilam Gross",
            "Dmitrii Kobylianskii",
            "Vinicius Mikuni",
            "Benjamin Nachman",
            "Nathalie Soybelman"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Detector simulation and reconstruction are a significant computational\nbottleneck in particle physics. We develop Particle-flow Neural Assisted\nSimulations (Parnassus) to address this challenge. Our deep learning model\ntakes as input a point cloud (particles impinging on a detector) and produces a\npoint cloud (reconstructed particles). By combining detector simulations and\nreconstruction into one step, we aim to minimize resource utilization and\nenable fast surrogate models suitable for application both inside and outside\nlarge collaborations. We demonstrate this approach using a publicly available\ndataset of jets passed through the full simulation and reconstruction pipeline\nof the CMS experiment. We show that Parnassus accurately mimics the CMS\nparticle flow algorithm on the (statistically) same events it was trained on\nand can generalize to jet momentum and type outside of the training\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.01620v1"
    },
    {
        "title": "Mesoscopic Bayesian Inference by Solvable Models",
        "authors": [
            "Shun Katakami",
            "Shuhei Kashiwamura",
            "Kenji Nagata",
            "Masaichiro Mizumaki",
            "Masato Okada"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The rapid advancement of data science and artificial intelligence has\naffected physics in numerous ways, including the application of Bayesian\ninference, setting the stage for a revolution in research methodology. Our\ngroup has proposed Bayesian measurement, a framework that applies Bayesian\ninference to measurement science with broad applicability across various\nnatural sciences. This framework enables the determination of posterior\nprobability distributions of system parameters, model selection, and the\nintegration of multiple measurement datasets. However, applying Bayesian\nmeasurement to real data analysis requires a more sophisticated approach than\ntraditional statistical methods like Akaike information criterion (AIC) and\nBayesian information criterion (BIC), which are designed for an infinite number\nof measurements $N$. Therefore, in this paper, we propose an analytical theory\nthat explicitly addresses the case where $N$ is finite in the linear regression\nmodel. We introduce $O(1)$ mesoscopic variables for $N$ observation noises.\nUsing this mesoscopic theory, we analyze the three core principles of Bayesian\nmeasurement: parameter estimation, model selection, and measurement\nintegration. Furthermore, by introducing these mesoscopic variables, we\ndemonstrate that the difference in free energies, critical for both model\nselection and measurement integration, can be analytically reduced by two\nmesoscopic variables of $N$ observation noises. This provides a deeper\nqualitative understanding of model selection and measurement integration and\nfurther provides deeper insights into actual measurements for nonlinear models.\nOur framework presents a novel approach to understanding Bayesian measurement\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.02869v2"
    },
    {
        "title": "Generative Diffusion Models for Fast Simulations of Particle Collisions\n  at CERN",
        "authors": [
            "Mikołaj Kita",
            "Jan Dubiński",
            "Przemysław Rokita",
            "Kamil Deja"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  In High Energy Physics simulations play a crucial role in unraveling the\ncomplexities of particle collision experiments within CERN's Large Hadron\nCollider. Machine learning simulation methods have garnered attention as\npromising alternatives to traditional approaches. While existing methods mainly\nemploy Variational Autoencoders (VAEs) or Generative Adversarial Networks\n(GANs), recent advancements highlight the efficacy of diffusion models as\nstate-of-the-art generative machine learning methods. We present the first\nsimulation for Zero Degree Calorimeter (ZDC) at the ALICE experiment based on\ndiffusion models, achieving the highest fidelity compared to existing\nbaselines. We perform an analysis of trade-offs between generation times and\nthe simulation quality. The results indicate a significant potential of latent\ndiffusion model due to its rapid generation time.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.03233v1"
    },
    {
        "title": "XENONnT WIMP Search: Signal & Background Modeling and Statistical\n  Inference",
        "authors": [
            " XENON Collaboration",
            "E. Aprile",
            "J. Aalbers",
            "K. Abe",
            "S. Ahmed Maouloud",
            "L. Althueser",
            "B. Andrieu",
            "E. Angelino",
            "D. Antón Martin",
            "F. Arneodo",
            "L. Baudis",
            "M. Bazyk",
            "L. Bellagamba",
            "R. Biondi",
            "A. Bismark",
            "K. Boese",
            "A. Brown",
            "G. Bruno",
            "R. Budnik",
            "J. M. R. Cardoso",
            "A. P. Cimental Chávez",
            "A. P. Colijn",
            "J. Conrad",
            "J. J. Cuenca-García",
            "V. D'Andrea",
            "L. C. Daniel Garcia",
            "M. P. Decowski",
            "C. Di Donato",
            "P. Di Gangi",
            "S. Diglio",
            "K. Eitel",
            "A. Elykov",
            "A. D. Ferella",
            "C. Ferrari",
            "H. Fischer",
            "T. Flehmke",
            "M. Flierman",
            "W. Fulgione",
            "C. Fuselli",
            "P. Gaemers",
            "R. Gaior",
            "M. Galloway",
            "F. Gao",
            "S. Ghosh",
            "R. Giacomobono",
            "R. Glade-Beucke",
            "L. Grandi",
            "J. Grigat",
            "H. Guan",
            "M. Guida",
            "P. Gyoergy",
            "R. Hammann",
            "A. Higuera",
            "C. Hils",
            "L. Hoetzsch",
            "N. F. Hood",
            "M. Iacovacci",
            "Y. Itow",
            "J. Jakob",
            "F. Joerg",
            "Y. Kaminaga",
            "M. Kara",
            "P. Kavrigin",
            "S. Kazama",
            "M. Kobayashi",
            "A. Kopec",
            "F. Kuger",
            "H. Landsman",
            "R. F. Lang",
            "L. Levinson",
            "I. Li",
            "S. Li",
            "S. Liang",
            "Y. -T. Lin",
            "S. Lindemann",
            "M. Lindner",
            "K. Liu",
            "J. Loizeau",
            "F. Lombardi",
            "J. Long",
            "J. A. M. Lopes",
            "T. Luce",
            "Y. Ma",
            "C. Macolino",
            "J. Mahlstedt",
            "A. Mancuso",
            "L. Manenti",
            "F. Marignetti",
            "T. Marrodán Undagoitia",
            "K. Martens",
            "J. Masbou",
            "E. Masson",
            "S. Mastroianni",
            "A. Melchiorre",
            "M. Messina",
            "A. Michael",
            "K. Miuchi",
            "A. Molinario",
            "S. Moriyama",
            "K. Morå",
            "Y. Mosbacher",
            "M. Murra",
            "J. Müller",
            "K. Ni",
            "U. Oberlack",
            "B. Paetsch",
            "Y. Pan",
            "Q. Pellegrini",
            "R. Peres",
            "C. Peters",
            "J. Pienaar",
            "M. Pierre",
            "G. Plante",
            "T. R. Pollmann",
            "L. Principe",
            "J. Qi",
            "J. Qin",
            "D. Ramírez García",
            "M. Rajado",
            "R. Singh",
            "L. Sanchez",
            "J. M. F. dos Santos",
            "I. Sarnoff",
            "G. Sartorelli",
            "J. Schreiner",
            "D. Schulte",
            "P. Schulte",
            "H. Schulze Eißing",
            "M. Schumann",
            "L. Scotto Lavina",
            "M. Selvi",
            "F. Semeria",
            "P. Shagin",
            "S. Shi",
            "J. Shi",
            "M. Silva",
            "H. Simgen",
            "A. Takeda",
            "P. -L. Tan",
            "A. Terliuk",
            "D. Thers",
            "F. Toschi",
            "G. Trinchero",
            "C. D. Tunnell",
            "F. Tönnies",
            "K. Valerius",
            "S. Vecchi",
            "S. Vetter",
            "F. I. Villazon Solar",
            "G. Volta",
            "C. Weinheimer",
            "M. Weiss",
            "D. Wenz",
            "C. Wittweg",
            "V. H. S. Wu",
            "Y. Xing",
            "D. Xu",
            "Z. Xu",
            "M. Yamashita",
            "L. Yang",
            "J. Ye",
            "L. Yuan",
            "G. Zavattini",
            "M. Zhong"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The XENONnT experiment searches for weakly-interacting massive particle\n(WIMP) dark matter scattering off a xenon nucleus. In particular, XENONnT uses\na dual-phase time projection chamber with a 5.9-tonne liquid xenon target,\ndetecting both scintillation and ionization signals to reconstruct the energy,\nposition, and type of recoil. A blind search for nuclear recoil WIMPs with an\nexposure of 1.1 tonne-years yielded no signal excess over background\nexpectations, from which competitive exclusion limits were derived on\nWIMP-nucleon elastic scatter cross sections, for WIMP masses ranging from 6\nGeV/$c^2$ up to the TeV/$c^2$ scale. This work details the modeling and\nstatistical methods employed in this search. By means of calibration data, we\nmodel the detector response, which is then used to derive background and signal\nmodels. The construction and validation of these models is discussed, alongside\nadditional purely data-driven backgrounds. We also describe the statistical\ninference framework, including the definition of the likelihood function and\nthe construction of confidence intervals.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.13638v1"
    },
    {
        "title": "Data Reduction for Low Energy Nuclear Physics Experiments Using Data\n  Frames",
        "authors": [
            "Caleb Marshall"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Low energy nuclear physics experiments are transitioning towards fully\ndigital data acquisition systems. Realizing the gains in flexibility afforded\nby these systems relies on equally flexible data reduction techniques. In this\npaper, methods utilizing data frames and in-memory techniques to work with\ndata, including data from self-triggering, digital data acquisition systems,\nare discussed within the context of a Python package, \\texttt{sauce}. It is\nshown that data frame operations can encompass common analysis needs and allow\ninteractive data analysis. Two event building techniques, dubbed referenced and\nreferenceless event building, are shown to provide a means to transform raw\nlist mode data into correlated multi-detector events. These techniques are\ndemonstrated in the analysis of two example data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.15404v2"
    },
    {
        "title": "Exploring the Statistical Properties of Outputs from a Process Inspired\n  by Geometrical Interpretation of Newton's Method",
        "authors": [
            "Taki Kirouani"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  In this paper, the statistical properties of Newton s method algorithm output\nin a specific case have been studied. The relative frequency density of this\nsample converges to a well-defined function, prompting us to explore its\ndistribution. Through rigorous mathematical proof, we demonstrate that the\nprobability density function follows a Cauchy distribution. Additionally, a new\nmethod to generate a uniform distribution is proposed. To further confirm our\nfindings, we employed statistical tests, including the Kolmogorov-Smirnov test\nand Anderson-Darling test, which showed high p-values. Furthermore, we show\nthat the distribution of the distance between two successive outputs can be\nobtained through a transformation method applied to the Cauchy distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.09583v1"
    },
    {
        "title": "EggNet: An Evolving Graph-based Graph Attention Network for Particle\n  Track Reconstruction",
        "authors": [
            "Paolo Calafiura",
            "Jay Chan",
            "Loic Delabrouille",
            "Brandon Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Track reconstruction is a crucial task in particle experiments and is\ntraditionally very computationally expensive due to its combinatorial nature.\nRecently, graph neural networks (GNNs) have emerged as a promising approach\nthat can improve scalability. Most of these GNN-based methods, including the\nedge classification (EC) and the object condensation (OC) approach, require an\ninput graph that needs to be constructed beforehand. In this work, we consider\na one-shot OC approach that reconstructs particle tracks directly from a set of\nhits (point cloud) by recursively applying graph attention networks with an\nevolving graph structure. This approach iteratively updates the graphs and can\nbetter facilitate the message passing across each graph. Preliminary studies on\nthe TrackML dataset show better track performance compared to the methods that\nrequire a fixed input graph.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.13925v1"
    },
    {
        "title": "Investigating the Event-Shape Methods in Search for the Chiral Magnetic\n  Effect in Relativistic Heavy Ion Collisions",
        "authors": [
            "Han-Sheng Li",
            "Yicheng Feng",
            "Fuqiang Wang"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Chiral Magnetic Effect (CME) is a phenomenon in which electric charge is\nseparated by a strong magnetic field from local domains of chirality imbalance\nand parity violation in quantum chromodynamics (QCD). The CME-sensitive\nobservable, charge-dependent three-point azimuthal correlator $\\Delta\\gamma$,\nis contaminated by a major physics background proportional to the particle's\nelliptic anisotropy $v_2$. Event-shape engineering (ESE) binning events in\ndynamical fluctuations of $v_2$ and event-shape selection (ESS) binning events\nin statistical fluctuations of $v_2$ are two methods to search for the CME by\nprojecting $\\Delta\\gamma$ to the $v_2=0$ intercept. We conduct a systematic\nstudy of these two methods using physics models as well as toy model\nsimulations. It is observed that the ESE method requires significantly more\nstatistics than the ESS method to achieve the same statistical precision of the\nintercept. It is found that the intercept from the ESS method depends on the\ndetails of the event content, such as the mixtures of background contributing\nsources, and thus is not a clean measure of the CME.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.14489v2"
    },
    {
        "title": "Unraveling Complexity: Singular Value Decomposition in Complex\n  Experimental Data Analysis",
        "authors": [
            "Judith F. Stein",
            "Aviad Frydman",
            "Richard Berkovits"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Analyzing complex experimental data with multiple parameters is challenging.\nWe propose using Singular Value Decomposition (SVD) as an effective solution.\nThis method, demonstrated through real experimental data analysis, surpasses\nconventional approaches in understanding complex physics data. Singular values\nand vectors distinguish and highlight various physical mechanisms and scales,\nrevealing previously challenging elements. SVD emerges as a powerful tool for\nnavigating complex experimental landscapes, showing promise for diverse\nexperimental measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.16267v1"
    },
    {
        "title": "A short introduction to Neural Networks and their application to Earth\n  and Materials Science Science",
        "authors": [
            "Duccio Fanelli",
            "Luca Bindi",
            "Lorenzo Chicchi",
            "Claudio Pereti",
            "Roberta Sessoli",
            "Simone Tommasini"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Neural networks are gaining widespread relevance for their versatility,\nholding the promise to yield a significant methodological shift in different\ndomain of applied research. Here, we provide a simple pedagogical account of\nthe basic functioning of a feedforward neural network. Then we move forward to\nreviewing two recent applications of machine learning to Earth and Materials\nScience. We will in particular begin by discussing a neural network based\ngeothermobarometer, which returns reliable predictions of the\npressure/temperature conditions of magma storage. Further, we will turn to\nillustrate how machine learning tools, tested on the list of minerals from the\nInternational Mineralogical Association, can help in the search for novel\nsuperconducting materials.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.11395v1"
    },
    {
        "title": "SeeBand: A highly efficient, interactive tool for analyzing electronic\n  transport data",
        "authors": [
            "Michael Parzer",
            "Alexander Riss",
            "Fabian Garmroudi",
            "Johannes de Boor",
            "Takao Mori",
            "Ernst Bauer"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Linking the fundamental physics of band structure and scattering theory with\nmacroscopic features such as measurable bulk thermoelectric transport\nproperties is indispensable to a thorough understanding of transport phenomena\nand ensures more targeted and efficient experimental research. Here, we\nintroduce SeeBand, a highly efficient and interactive fitting tool based on\nBoltzmann transport theory. A fully integrated user interface and visualization\ntool enable real-time comparison and connection between the electronic band\nstructure (EBS) and microscopic transport properties. It allows simultaneous\nanalysis of data for the Seebeck coefficient $S$, resistivity $\\rho$ and Hall\ncoefficient $R_\\text{H}$ to identify suitable EBS models and extract the\nunderlying microscopic material parameters and additional information from the\nmodel. Crucially, the EBS can be obtained by directly fitting the\ntemperature-dependent properties of a single sample, which goes beyond previous\napproaches that look into doping dependencies. Finally, the combination of\nneural-network-assisted initial guesses and an efficient subsequent fitting\nroutine allows for a rapid processing of big datasets, facilitating\nhigh-throughput analyses to identify underlying, yet undiscovered dependencies,\nthereby guiding material design.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.06261v2"
    },
    {
        "title": "Converting sWeights to Probabilities with Density Ratios",
        "authors": [
            "D. I. Glazier",
            "R. Tyson"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The use of machine learning approaches continues to have many benefits in\nexperimental nuclear and particle physics. One common issue is generating\ntraining data which is sufficiently realistic to give reliable results. Here we\nadvocate using real experimental data as the source of training data and\ndemonstrate how one might subtract background contributions through the use of\nprobabilistic weights which can be readily applied to training data. The sPlot\nformalism is a common tool used to isolate distributions from different\nsources. However, negative sWeights produced by the sPlot technique can lead to\nissues in training and poor predictive power. This article demonstrates how\ndensity ratio estimation can be applied to convert sWeights to event\nprobabilities, which we call drWeights. The drWeights can then be applied to\nproduce the distributions of interest and are consistent with direct use of the\nsWeights. This article will also show how decision trees are particular well\nsuited to converting sWeights, with the benefit of fast prediction rates and\nadaptability to aspects of the experimental data such as data sample size and\nproportions of different event sources. We also show that a double density\nratio approach where the initial drWeights are reweighted by an additional\nclassifier gives substantially better results.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08183v1"
    },
    {
        "title": "A new approach to handling factorial moment correlations through\n  principal component analysis",
        "authors": [
            "Nikolaos Davis"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Intermittency analysis of factorial moments is a promising method used for\nthe detection of power-law scaling in high-energy collision data. In\nparticular, it has been employed in the search of fluctuations characteristic\nof the critical point (CP) of strongly interacting matter. However,\nintermittency analysis has been hindered by the fact that factorial moments\nmeasurements corresponding to different scales are correlated, since the same\ndata are conventionally used to calculate them. This invalidates many\nassumptions involved in fitting data sets and determining the best fit values\nof power-law exponents. We present a novel approach to intermittency analysis,\nemploying the well-established statistical and data science tool of Principal\nComponent Analysis (PCA). This technique allows for the proper handling of\ncorrelations between scales without the need for subdividing the data sets\navailable.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.14185v1"
    },
    {
        "title": "Machine Learning-Powered Data Cleaning for LEGEND",
        "authors": [
            "E. León",
            "A. Li",
            "M. A. Bahena Schott",
            "B. Bos",
            "M. Busch",
            "J. R. Chapman",
            "G. L. Duran",
            "J. Gruszko",
            "R. Henning",
            "E. L. Martin",
            "J. F. Wilkerson"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Neutrinoless double-beta decay ($0\\nu\\beta\\beta$) is a rare nuclear process\nthat, if observed, will provide insight into the nature of neutrinos and help\nexplain the matter-antimatter asymmetry in the universe. The Large Enriched\nGermanium Experiment for Neutrinoless Double-Beta Decay (LEGEND) will operate\nin two phases to search for $0\\nu\\beta\\beta$. The first (second) stage will\nemploy 200 (1000) kg of High-Purity Germanium (HPGe) enriched in $^{76}$Ge to\nachieve a half-life sensitivity of 10$^{27}$ (10$^{28}$) years. In this study,\nwe present a semi-supervised data-driven approach to remove non-physical events\ncaptured by HPGe detectors powered by a novel artificial intelligence model. We\nutilize Affinity Propagation to cluster waveform signals based on their shape\nand a Support Vector Machine to classify them into different categories. We\ntrain, optimize, test our model on data taken from a natural abundance HPGe\ndetector installed in the Full Chain Test experimental stand at the University\nof North Carolina at Chapel Hill. We demonstrate that our model yields a\nmaximum physics event sacrifice of $0.024 ^{+0.004}_{-0.003} \\%$ when\nperforming data cleaning cuts. Our model is being used to accelerate data\ncleaning development for LEGEND-200.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.14701v1"
    },
    {
        "title": "A totally empirical basis of science",
        "authors": [
            "Orestis Loukas",
            "Ho-Ryun Chung"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Statistical hypothesis testing is the central method to demarcate scientific\ntheories in both exploratory and inferential analyses. However, whether this\nmethod befits such purpose remains a matter of debate. Established approaches\nto hypothesis testing make several assumptions on the data generation process\nbeyond the scientific theory. Most of these assumptions not only remain unmet\nin realistic datasets, but often introduce unwarranted bias in the analysis.\nHere, we depart from such restrictive assumptions to propose an alternative\nframework of total empiricism. We derive the Information-test ($I$-test) which\nallows for testing versatile hypotheses including non-null effects. To\nexemplify the adaptability of the $I$-test to application and study design, we\nrevisit the hypothesis of interspecific metabolic scaling in mammals,\nultimately rejecting both competing theories of pure allometry.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.19866v1"
    },
    {
        "title": "Uncertainty Propagation within Chained Models for Machine Learning\n  Reconstruction of Neutrino-LAr Interactions",
        "authors": [
            "Daniel Douglas",
            "Aashwin Mishra",
            "Daniel Ratner",
            "Felix Petersen",
            "Kazuhiro Terao"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Sequential or chained models are increasingly prevalent in machine learning\nfor scientific applications, due to their flexibility and ease of development.\nChained models are particularly useful when a task is separable into distinct\nsteps with a hierarchy of meaningful intermediate representations. In\nreliability-critical tasks, it is important to quantify the confidence of model\ninferences. However, chained models pose an additional challenge for\nuncertainty quantification, especially when input uncertainties need to be\npropagated. In such cases, a fully uncertainty-aware chain of models is\nrequired, where each step accepts a probability distribution over the input\nspace, and produces a probability distribution over the output space. In this\nwork, we present a case study for adapting a single model within an existing\nchain, designed for reconstruction within neutrino-Argon interactions,\ndeveloped for neutrino oscillation experiments such as MicroBooNE, ICARUS, and\nthe future DUNE experiment. We test the performance of an input\nuncertainty-enabled model against an uncertainty-blinded model using a method\nfor generating synthetic noise. By comparing these two, we assess the increase\nin inference quality achieved by exposing models to upstream uncertainty\nestimates.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.09864v2"
    },
    {
        "title": "Neural network biased corrections: Cautionary study in background\n  corrections for quenched jets",
        "authors": [
            "David Stewart",
            "Joern Putschke"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Jets clustered from heavy ion collision measurements combine a dense\nbackground of particles with those actually resulting from a hard partonic\nscattering. The background contribution to jet transverse momentum ($p_{T}$)\nmay be corrected by subtracting the collision average background; however, the\nbackground inhomogeneity limits the resolution of this correction. Many recent\nstudies have embedded jets into heavy ion backgrounds and demonstrated a\nmarkedly improved background correction is achievable by using neural networks\n(NNs) trained with aspects of jet substructure which are used to map measured\njet $p_\\mathrm{T}$ to the embedded truth jet $p_\\mathrm{T}$. However, jet\nquenching in heavy ion collisions modifies jet substructure, and\ncorrespondingly biases the NNs' background corrections. This study investigates\nthose biases by using simulations of jet quenching in central Au+Au collisions\nat $\\sqrt{s_\\mathrm{NN}}=200\\;\\mathrm{GeV}/c$ with hydrodynamically modeled\nquark-gluon plasma (QGP) evolution. To demonstrate the magnitude of the effect\nof such biases in measurement, a leading jet nuclear modification factor\n($R_\\mathrm{AA}$) is calculated and reported using the NN background correction\non jets quenched utilizing a brick of QGP.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.15440v2"
    },
    {
        "title": "New method of image processing via statistical analysis for application\n  in intelligent systems",
        "authors": [
            "Monalisa Cavalcante",
            "José Araújo",
            "José Holanda"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Image processing has always been a topic of significant importance to\nsociety. Recently, this field has gained considerable prominence due to the\ndevelopment of intelligent systems. In this work, we present a new method of\nimage processing that utilizes statistical analysis, specifically designed for\napplications in intelligent systems. We tested our method on a large collection\nof images to assess its effectiveness.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.18575v1"
    },
    {
        "title": "Automatizing the search for mass resonances using BumpNet",
        "authors": [
            "Jean-Francois Arguin",
            "Georges Azuelos",
            "Émile Baril",
            "Ilan Bessudo",
            "Fannie Bilodeau",
            "Maryna Borysova",
            "Shikma Bressler",
            "Samuel Calvet",
            "Julien Donini",
            "Etienne Dreyer",
            "Michael Kwok Lam Chu",
            "Eva Mayer",
            "Ethan Meszaros",
            "Nilotpal Kakati",
            "Bruna Pascual Dias",
            "Joséphine Potdevin",
            "Amit Shkuri",
            "Muhammad Usman"
        ],
        "category": "physics.data-an",
        "published_year": "2025",
        "summary": "  The search for resonant mass bumps in invariant-mass distributions remains a\ncornerstone strategy for uncovering Beyond the Standard Model (BSM) physics at\nthe Large Hadron Collider (LHC). Traditional methods often rely on predefined\nfunctional forms and exhaustive computational and human resources, limiting the\nscope of tested final states and selections. This work presents BumpNet, a\nmachine learning-based approach leveraging advanced neural network\narchitectures to generalize and enhance the Data-Directed Paradigm (DDP) for\nresonance searches. Trained on a diverse dataset of smoothly-falling analytical\nfunctions and realistic simulated data, BumpNet efficiently predicts\nstatistical significance distributions across varying histogram configurations,\nincluding those derived from LHC-like conditions. The network's performance is\nvalidated against idealized likelihood ratio-based tests, showing minimal bias\nand strong sensitivity in detecting mass bumps across a range of scenarios.\nAdditionally, BumpNet's application to realistic BSM scenarios highlights its\ncapability to identify subtle signals while managing the look-elsewhere effect.\nThese results underscore BumpNet's potential to expand the reach of resonance\nsearches, paving the way for more comprehensive explorations of LHC data in\nfuture analyses.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.05603v1"
    },
    {
        "title": "Temporal and topological partitioning in real-world growing networks for\n  scale-free properties study",
        "authors": [
            "Guillaume Rousseau"
        ],
        "category": "physics.data-an",
        "published_year": "2025",
        "summary": "  We introduce a method to study evolution rules and scale-free hypothesis of\nreal-world growing networks using natural partitions of nodes and edges based\non temporal and topological attributes, and analyzing degree distributions.\n  We apply this method to the Software Heritage dataset, which collects\nsoftware releases and revisions from open-source communities.Nodes with native\ntemporal information does not fully capture the overall network dynamics, and\ndegree distributions show greater regularity with fewer outliers, suggesting a\nmore likely scale-free regime when examining networks derived from temporal and\ntopological partitions.\n  However, underlying aging, fitness, and inheritance mechanisms, along with\nchosen partitioning, hinder definitive conclusions and suggest that the very\ncommon ``pure parametric power-law'' hypothesis for the tail of degree\ndistributions is too strong. Node's type derived from partions and changes in\nevolution rules, shown by variations in the average number of new edges per\nnode over time, highlight the need for tools better suited for studying\ntransient regimes and ease comparison of real-world networks with minimal\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.10145v1"
    },
    {
        "title": "Orthogonal projections of hypercubes",
        "authors": [
            "Yoshiaki Horiike",
            "Shin Fujishiro"
        ],
        "category": "physics.data-an",
        "published_year": "2025",
        "summary": "  Projections of hypercubes have been applied for visualizing high-dimensional\nbinary state spaces in various fields of sciences. Conventional methods for\nprojecting hypercubes, however, face practical difficulties. Manual methods\nrequire nontrivial adjustments of the projection basis, while optimization\nmethods impose limitations on the interpretation and reproducibility of the\nresulting plots. Here, we propose using principal component analysis (PCA) for\nprojecting hypercubes, which offers an automated, interpretable, and\nreproducible solution. We reveal analytically and numerically that PCA\neffectively captures polarized distributions in the state space. The resulting\ndistribution of projected vertices is shown to be asymptotically the standard\nGaussian, a characteristic of high-dimensional orthogonal projections. This\nmethod is applied to visualize the hypercubic energy landscapes of Ising spin\nsystems, where the biplots reveal the pathways involving correlated spin flips.\nOur work demonstrates the potential of PCA for discovering hidden patterns in\nhigh-dimensional binary data.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.10257v1"
    },
    {
        "title": "A Theory of Measurement Uncertainty Based on Conditional Probability",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "1996",
        "summary": "  A theory of measurement uncertainty is presented, which, since it is based\nexclusively on the Bayesian approach and on the subjective concept of\nconditional probability, is applicable in the most general cases.\n  The recent International Organization for Standardization (ISO)\nrecommendation on measurement uncertainty is reobtained as the limit case in\nwhich linearization is meaningful and one is interested only in the best\nestimates of the quantities and in their variances.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9611016v1"
    },
    {
        "title": "The probability of making a correct decision in hypotheses testing as\n  estimator of quality of planned experiments",
        "authors": [
            "S. I. Bityukov",
            "N. V. Krasnikov"
        ],
        "category": "physics.data-an",
        "published_year": "2003",
        "summary": "  In the report the approach to estimation of quality of planned experiments is\nconsidered. This approach is based on the analysis of uncertainty, which will\ntake place under the future hypotheses testing about the existence of a new\nphenomenon in Nature. The probability of making a correct decision in\nhypotheses testing is proposed as estimator of quality of planned experiments.\nThis estimator allows to take into account systematics and statistical\nuncertainties in determination of signal and background rates.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0309031v3"
    },
    {
        "title": "Maximum Entropy Multivariate Density Estimation: An exact\n  goodness-of-fit approach",
        "authors": [
            "Sabbir Rahman",
            "Mahbub Majumdar"
        ],
        "category": "physics.data-an",
        "published_year": "2004",
        "summary": "  We consider the problem of estimating the population probability distribution\ngiven a finite set of multivariate samples, using the maximum entropy approach.\nIn strict keeping with Jaynes' original definition, our precise formulation of\nthe problem considers contributions only from the smoothness of the estimated\ndistribution (as measured by its entropy) and the loss functional associated\nwith its goodness-of-fit to the sample data, and in particular does not make\nuse of any additional constraints that cannot be justified from the sample data\nalone. By mapping the general multivariate problem to a tractable univariate\none, we are able to write down exact expressions for the goodness-of-fit of an\narbitrary multivariate distribution to any given set of samples using both the\ntraditional likelihood-based approach and a rigorous information-theoretic\napproach, thus solving a long-standing problem. As a corollary we also give an\nexact solution to the `forward problem' of determining the expected\ndistributions of samples taken from a population with known probability\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0406023v5"
    },
    {
        "title": "Nonextensive statistical features of the Polish stock market\n  fluctuations",
        "authors": [
            "R. Rak",
            "S. Drozdz",
            "J. Kwapien"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  The statistics of return distributions on various time scales constitutes one\nof the most informative characteristics of the financial dynamics. Here we\npresent a systematic study of such characteristics for the Polish stock market\nindex WIG20 over the period 04.01.1999 - 31.10.2005 for the time lags ranging\nfrom one minute up to one hour. This market is commonly classified as emerging.\nStill on the shortest time scales studied we find that the tails of the return\ndistributions are consistent with the inverse cubic power-law, as identified\npreviously for majority of the mature markets. Within the time scales studied a\nquick and considerable departure from this law towards a Gaussian can however\nbe traced. Interestingly, all the forms of the distributions observed can be\ncomprised by the single $q$-Gaussians which provide a satisfactory and at the\nsame time compact representation of the distribution of return fluctuations\nover all magnitudes of their variation. The corresponding nonextensivity\nparameter $q$ is found to systematically decrease when increasing the time\nscales.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0603071v3"
    },
    {
        "title": "Spectral properties of empirical covariance matrices for data with\n  power-law tails",
        "authors": [
            "Zdzislaw Burda",
            "Andrzej Goerlich",
            "Bartlomiej Waclaw"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  We present an analytic method for calculating spectral densities of empirical\ncovariance matrices for correlated data. In this approach the data is\nrepresented as a rectangular random matrix whose columns correspond to sampled\nstates of the system. The method is applicable to a class of random matrices\nwith radial measures including those with heavy (power-law) tails in the\nprobability distribution. As an example we apply it to a multivariate Student\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0603186v2"
    },
    {
        "title": "Optimal Data-Based Binning for Histograms",
        "authors": [
            "Kevin H. Knuth"
        ],
        "category": "physics.data-an",
        "published_year": "2006",
        "summary": "  Histograms are convenient non-parametric density estimators, which continue\nto be used ubiquitously. Summary quantities estimated from histogram-based\nprobability density models depend on the choice of the number of bins. We\nintroduce a straightforward data-based method of determining the optimal number\nof bins in a uniform bin-width histogram. By assigning a multinomial likelihood\nand a non-informative prior, we derive the posterior probability for the number\nof bins in a piecewise-constant density model given the data. In addition, we\nestimate the mean and standard deviations of the resulting bin heights, examine\nthe effects of small sample sizes and digitized data, and demonstrate the\napplication to multi-dimensional histograms.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0605197v2"
    },
    {
        "title": "Analysis of the structure of complex networks at different resolution\n  levels",
        "authors": [
            "Alex Arenas",
            "Alberto Fernandez",
            "Sergio Gomez"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Modular structure is ubiquitous in real-world complex networks, and its\ndetection is important because it gives insights in the structure-functionality\nModular structure is ubiquitous in real-world complex networks, and its\ndetection is important because it gives insights in the structure-functionality\nrelationship. The standard approach is based on the optimization of a quality\nfunction, modularity, which is a relative quality measure for a partition of a\nnetwork into modules. Recently some authors [1,2] have pointed out that the\noptimization of modularity has a fundamental drawback: the existence of a\nresolution limit beyond which no modular structure can be detected even though\nthese modules might have own entity. The reason is that several topological\ndescriptions of the network coexist at different scales, which is, in general,\na fingerprint of complex systems. Here we propose a method that allows for\nmultiple resolution screening of the modular structure. The method has been\nvalidated using synthetic networks, discovering the predefined structures at\nall scales. Its application to two real social networks allows to find the\nexact splits reported in the literature, as well as the substructure beyond the\nactual split.\n",
        "pdf_link": "http://arxiv.org/pdf/physics/0703218v2"
    },
    {
        "title": "A Quantitative Measure of Experimental Scientific Merit",
        "authors": [
            "Bruce Knuteson"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  Experimental program review in our field may benefit from a more quantitative\nframework within which to quantitatively discuss the scientific merit of a\nproposed program of research, and to assess the scientific merit of a\nparticular experimental result. This article proposes explicitly such a\nquantitative framework. Examples of the use of this framework in assessing the\nscientific merit of particular avenues of research at the energy frontier in\nmany cases provide results in stark contradiction to accepted wisdom. The\nexperimental scientific figure of merit proposed here has the potential for\ninforming future choices of research direction in our field, and in other\nsubfields of the physical sciences.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.3572v1"
    },
    {
        "title": "Exact corrections for finite-time drift and diffusion coefficients",
        "authors": [
            "C. Anteneodo",
            "R. Riera"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Real data are constrained to finite sampling rates, which calls for a\nsuitable mathematical description of the corrections to the finite-time\nestimations of the dynamic equations. Often in the literature, lower order\ndiscrete time approximations of the modeling diffusion processes are\nconsidered. On the other hand, there is a lack of simple estimating procedures\nbased on higher order approximations. For standard diffusion models, that\ninclude additive and multiplicative noise components, we obtain the exact\ncorrections to the empirical finite-time drift and diffusion coefficients,\nbased on It\\^o-Taylor expansions. These results allow to reconstruct the real\nhidden coefficients from the empirical estimates. We also derive higher-order\nfinite-time expressions for the third and fourth conditional moments, that\nfurnish extra theoretical checks for that class of diffusive models. The\ntheoretical predictions are compared with the numerical outcomes of some\nrepresentative artificial time-series.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.3196v1"
    },
    {
        "title": "Dynamic quantum clustering: a method for visual exploration of\n  structures in data",
        "authors": [
            "Marvin Weinstein",
            "David Horn"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  A given set of data-points in some feature space may be associated with a\nSchrodinger equation whose potential is determined by the data. This is known\nto lead to good clustering solutions. Here we extend this approach into a\nfull-fledged dynamical scheme using a time-dependent Schrodinger equation.\nMoreover, we approximate this Hamiltonian formalism by a truncated calculation\nwithin a set of Gaussian wave functions (coherent states) centered around the\noriginal points. This allows for analytic evaluation of the time evolution of\nall such states, opening up the possibility of exploration of relationships\namong data-points through observation of varying dynamical-distances among\npoints and convergence of points into clusters. This formalism may be further\nsupplemented by preprocessing, such as dimensional reduction through singular\nvalue decomposition or feature filtering.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.2644v1"
    },
    {
        "title": "The performance of modularity maximization in practical contexts",
        "authors": [
            "Benjamin H. Good",
            "Yves-Alexandre de Montjoye",
            "Aaron Clauset"
        ],
        "category": "physics.data-an",
        "published_year": "2009",
        "summary": "  Although widely used in practice, the behavior and accuracy of the popular\nmodule identification technique called modularity maximization is not well\nunderstood in practical contexts. Here, we present a broad characterization of\nits performance in such situations. First, we revisit and clarify the\nresolution limit phenomenon for modularity maximization. Second, we show that\nthe modularity function Q exhibits extreme degeneracies: it typically admits an\nexponential number of distinct high-scoring solutions and typically lacks a\nclear global maximum. Third, we derive the limiting behavior of the maximum\nmodularity Q_max for one model of infinitely modular networks, showing that it\ndepends strongly both on the size of the network and on the number of modules\nit contains. Finally, using three real-world metabolic networks as examples, we\nshow that the degenerate solutions can fundamentally disagree on many, but not\nall, partition properties such as the composition of the largest modules and\nthe distribution of module sizes. These results imply that the output of any\nmodularity maximization procedure should be interpreted cautiously in\nscientific contexts. They also explain why many heuristics are often successful\nat finding high-scoring partitions in practice and why different heuristics can\ndisagree on the modular structure of the same network. We conclude by\ndiscussing avenues for mitigating some of these behaviors, such as combining\ninformation from many degenerate solutions or using generative models.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.0165v2"
    },
    {
        "title": "Machine learning approach to inverse problem and unfolding procedure",
        "authors": [
            "Nikolai Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  A procedure for unfolding the true distribution from experimental data is\npresented. Machine learning methods are applied for simultaneous identification\nof an apparatus function and solving of an inverse problem. A priori\ninformation about the true distribution from theory or previous experiments is\nused for Monte-Carlo simulation of the training sample. The training sample can\nbe used to calculate a transformation from the true distribution to the\nmeasured one. This transformation provides a robust solution for an unfolding\nproblem with minimal biases and statistical errors for the set of distributions\nused to create the training sample. The dimensionality of the solved problem\ncan be arbitrary. A numerical example is presented to illustrate and validate\nthe procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.2006v3"
    },
    {
        "title": "Toy Model for Large Non-Symmetric Random Matrices",
        "authors": [
            "Małgorzata Snarska"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Non-symmetric rectangular correlation matrices occur in many problems in\neconomics. We test the method of extracting statistically meaningful\ncorrelations between input and output variables of large dimensionality and\nbuild a toy model for artificially included correlations in large random time\nseries.The results are then applied to analysis of polish macroeconomic data\nand can be used as an alternative to classical cointegration approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.4522v1"
    },
    {
        "title": "Multifractal Network Generator",
        "authors": [
            "G. Palla",
            "L. Lovasz",
            "T. Vicsek"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  We introduce a new approach to constructing networks with realistic features.\nOur method, in spite of its conceptual simplicity (it has only two parameters)\nis capable of generating a wide variety of network types with prescribed\nstatistical properties, e.g., with degree- or clustering coefficient\ndistributions of various, very different forms. In turn, these graphs can be\nused to test hypotheses, or, as models of actual data. The method is based on a\nmapping between suitably chosen singular measures defined on the unit square\nand sparse infinite networks. Such a mapping has the great potential of\nallowing for graph theoretical results for a variety of network topologies. The\nmain idea of our approach is to go to the infinite limit of the singular\nmeasure and the size of the corresponding graph simultaneously. A very unique\nfeature of this construction is that the complexity of the generated network is\nincreasing with the size. We present analytic expressions derived from the\nparameters of the -- to be iterated-- initial generating measure for such major\ncharacteristics of graphs as their degree, clustering coefficient and\nassortativity coefficient distributions. The optimal parameters of the\ngenerating measure are determined from a simple simulated annealing process.\nThus, the present work provides a tool for researchers from a variety of fields\n(such as biology, computer science, biology, or complex systems) enabling them\nto create a versatile model of their network data.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.5225v1"
    },
    {
        "title": "The Significant Digit Law in Statistical Physics",
        "authors": [
            "Lijing Shao",
            "Bo-Qiang Ma"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  The occurrence of the nonzero leftmost digit, i.e., 1, 2, ..., 9, of numbers\nfrom many real world sources is not uniformly distributed as one might naively\nexpect, but instead, the nature favors smaller ones according to a logarithmic\ndistribution, named Benford's law. We investigate three kinds of widely used\nphysical statistics, i.e., the Boltzmann-Gibbs (BG) distribution, the\nFermi-Dirac (FD) distribution, and the Bose-Einstein (BE) distribution, and\nfind that the BG and FD distributions both fluctuate slightly in a periodic\nmanner around the Benford distribution with respect to the temperature of the\nsystem, while the BE distribution conforms to it exactly whatever the\ntemperature is. Thus the Benford's law seems to present a general pattern for\nphysical statistics and might be even more fundamental and profound in nature.\nFurthermore, various elegant properties of Benford's law, especially the\nmantissa distribution of data sets, are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.0660v1"
    },
    {
        "title": "Evolutionary method for finding communities in bipartite networks",
        "authors": [
            "Zhan WeiHua",
            "Zhang Zhongzhi",
            "Guan Jihong",
            "Zhou Shuigeng"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  An important step in unveiling the relation between network structure and\ndynamics defined on networks is to detect communities, and numerous methods\nhave been developed separately to identify community structure in different\nclasses of networks, such as unipartite networks, bipartite networks, and\ndirected networks. We show that both unipartite and directed networks can be\nrepresented as bipartite networks, and their modularity is completely\nconsistent with that for bipartite networks, the detection of modular structure\non which can be reformulated as modularity maximization. To optimize the\nbipartite modularity, we develop a modified adaptive genetic algorithm (MAGA),\nwhich is shown to be especially efficient for community structure detection.\nThe high efficiency of the MAGA is based on the following three improvements we\nmake. First, we introduce a different measure for the informativeness of a\nlocus instead of the standard deviation, which can exactly determine which loci\nmutate. This measure is the bias between the distribution of a locus over the\ncurrent population and the uniform distribution of the locus, i.e., the\nKullback-Leibler divergence between them. Second, we develop a reassignment\ntechnique for differentiating the informative state a locus has attained from\nthe random state in the initial phase. Third, we present a modified mutation\nrule which by incorporating related operation can guarantee the convergence of\nthe MAGA to the global optimum and can speed up the convergence process.\nExperimental results show that the MAGA outperforms existing methods in terms\nof modularity for both bipartite and unipartite networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.3315v3"
    },
    {
        "title": "Generation of degree-correlated networks using copulas",
        "authors": [
            "Mathias Raschke",
            "Markus Schläpfer",
            "Konstantinos Trantopoulos"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Dynamical processes on complex networks such as information propagation,\ninnovation diffusion, cascading failures or epidemic spreading are highly\naffected by their underlying topologies as characterized by, for instance,\ndegree-degree correlations. Here, we introduce the concept of copulas in order\nto artificially generate random networks with an arbitrary degree distribution\nand a rich a priori degree-degree correlation (or `association') structure. The\naccuracy of the proposed formalism and corresponding algorithm is numerically\nconfirmed. The derived network ensembles can be systematically deployed as\nproper null models, in order to unfold the complex interplay between the\ntopology of real networks and the dynamics on top of them.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.0201v3"
    },
    {
        "title": "Catastrophic Cascade of Failures in Interdependent Networks",
        "authors": [
            "S. Havlin",
            "N. A. M. Araujo",
            "S. V. Buldyrev",
            "C. S. Dias",
            "R. Parshani",
            "G. Paul",
            "H. E. Stanley"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  Modern network-like systems are usually coupled in such a way that failures\nin one network can affect the entire system. In infrastructures, biology,\nsociology, and economy, systems are interconnected and events taking place in\none system can propagate to any other coupled system. Recent studies on such\ncoupled systems show that the coupling increases their vulnerability to random\nfailure. Properties for interdependent networks differ significantly from those\nof single-network systems. In this article, these results are reviewed and the\nmain properties discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.0206v1"
    },
    {
        "title": "High Throughput Software for Powder Diffraction and its Application to\n  Heterogeneous Catalysis",
        "authors": [
            "Taha Sochi"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  In this thesis we investigate high throughput computational methods for\nprocessing large quantities of data collected from synchrotrons and their\napplication to spectral analysis of powder diffraction data. We also present\nthe main product of this PhD programme, specifically a software called 'EasyDD'\ndeveloped by the author. This software was created to meet the increasing\ndemand on data processing and analysis capabilities as required by modern\ndetectors which produce huge quantities of data. Modern detectors coupled with\nthe high intensity X-ray sources available at synchrotrons have led to the\nsituation where datasets can be collected in ever shorter time scales and in\never larger numbers. Such large volumes of datasets pose a data processing\nbottleneck which augments with current and future instrument development.\nEasyDD has achieved its objectives and made significant contributions to\nscientific research. It can also be used as a model for more mature attempts in\nthe future. EasyDD is currently in use by a number of researchers in a number\nof academic and research institutions to process high-energy diffraction data.\nThese include data collected by different techniques such as Energy Dispersive\nDiffraction, Angle Dispersive Diffraction and Computer Aided Tomography. EasyDD\nhas already been used in a number of published studies, and is currently in use\nby the High Energy X-Ray Imaging Technology project. The software was also used\nby the author to process and analyse datasets collected from synchrotron\nradiation facilities. In this regard, the thesis presents novel scientific\nresearch involving the use of EasyDD to handle large diffraction datasets in\nthe study of alumina-supported metal oxide catalyst bodies. These data were\ncollected using Tomographic Energy Dispersive Diffraction Imaging and Computer\nAided Tomography techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.4506v1"
    },
    {
        "title": "Rotated multifractal network generator",
        "authors": [
            "G. Palla",
            "P. Pollner",
            "T. Vicsek"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  The recently introduced multifractal network generator (MFNG), has been shown\nto provide a simple and flexible tool for creating random graphs with very\ndiverse features. The MFNG is based on multifractal measures embedded in 2d,\nleading also to isolated nodes, whose number is relatively low for realistic\ncases, but may become dominant in the limiting case of infinitely large network\nsizes. Here we discuss the relation between this effect and the information\ndimension for the 1d projection of the link probability measure (LPM), and\nargue that the node isolation can be avoided by a simple transformation of the\nLPM based on rotation.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.2338v1"
    },
    {
        "title": "Adaptive Cluster Expansion for Inferring Boltzmann Machines with Noisy\n  Data",
        "authors": [
            "Simona Cocco",
            "Rémi Monasson"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We introduce a procedure to infer the interactions among a set of binary\nvariables, based on their sampled frequencies and pairwise correlations. The\nalgorithm builds the clusters of variables contributing most to the entropy of\nthe inferred Ising model, and rejects the small contributions due to the\nsampling noise. Our procedure successfully recovers benchmark Ising models even\nat criticality and in the low temperature phase, and is applied to\nneurobiological data.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.3260v1"
    },
    {
        "title": "A Paradoxical Property of the Monkey Book",
        "authors": [
            "Sebastian Bernhardsson",
            "Seung Ki Baek",
            "Petter Minnhagen"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  A \"monkey book\" is a book consisting of a random distribution of letters and\nblanks, where a group of letters surrounded by two blanks is defined as a word.\nWe compare the statistics of the word distribution for a monkey book with the\ncorresponding distribution for the general class of random books, where the\nlatter are books for which the words are randomly distributed. It is shown that\nthe word distribution statistics for the monkey book is different and quite\ndistinct from a typical sampled book or real book. In particular the monkey\nbook obeys Heaps' power law to an extraordinary good approximation, in contrast\nto the word distributions for sampled and real books, which deviate from Heaps'\nlaw in a characteristics way. The somewhat counter-intuitive conclusion is that\na \"monkey book\" obeys Heaps' power law precisely because its word-frequency\ndistribution is not a smooth power law, contrary to the expectation based on\nsimple mathematical arguments that if one is a power law, so is the other.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.2681v1"
    },
    {
        "title": "Hidden Variables in Bipartite Networks",
        "authors": [
            "Maksim Kitsak",
            "Dmitri Krioukov"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  We introduce and study random bipartite networks with hidden variables. Nodes\nin these networks are characterized by hidden variables which control the\nappearance of links between node pairs. We derive analytic expressions for the\ndegree distribution, degree correlations, the distribution of the number of\ncommon neighbors, and the bipartite clustering coefficient in these networks.\nWe also establish the relationship between degrees of nodes in original\nbipartite networks and in their unipartite projections. We further demonstrate\nhow hidden variable formalism can be applied to analyze topological properties\nof networks in certain bipartite network models, and verify our analytical\nresults in numerical simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.3184v2"
    },
    {
        "title": "CHIWEI: A code of goodness of fit tests for weighted and unweighted\n  histograms",
        "authors": [
            "Nikolai Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  A self-contained Fortran-77 program for goodness of fit tests for histograms\nwith weighted entries as well as with unweighted entries is presented. The code\ncalculates test statistics for case of histogram with normalized weights of\nevents and in case of unnormalized weights of events.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.3733v3"
    },
    {
        "title": "CHICOM: A code of tests for comparing unweighted and weighted histograms\n  and two weighted histograms",
        "authors": [
            "Nikolai Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  A self-contained Fortran-77 program for calculating test statistics to\ncompare weighted histogram with unweighted histogram and two histograms with\nweighted entries is presented. The code calculates test statistics for cases of\nhistograms with normalized weights of events and unnormalized weights of\nevents.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.1288v3"
    },
    {
        "title": "Extreme value and record statistics in heavy-tailed processes with\n  long-range memory",
        "authors": [
            "Aicko Yves Schumann",
            "Nicholas R. Moloney",
            "Jörn Davidsen"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Extreme events are an important theme in various areas of science because of\ntheir typically devastating effects on society and their scientific\ncomplexities. The latter is particularly true if the underlying dynamics does\nnot lead to independent extreme events as often observed in natural systems.\nHere, we focus on this case and consider stationary stochastic processes that\nare characterized by long-range memory and heavy-tailed distributions, often\ncalled fractional L\\'evy noise. While the size distribution of extreme events\nis not affected by the long-range memory in the asymptotic limit and remains a\nFr\\'echet distribution, there are strong finite-size effects if the memory\nleads to persistence in the underlying dynamics. Moreover, we show that this\npersistence is also present in the extreme events, which allows one to make a\ntime-dependent hazard assessment of future extreme events based on events\nobserved in the past. This has direct applications in the field of space\nweather as we discuss specifically for the case of the solar power influx into\nthe magnetosphere. Finally, we show how the statistics of records, or\nrecord-breaking extreme events, is affected by the presence of long-range\nmemory.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.0294v1"
    },
    {
        "title": "Selectivity in Probabilistic Causality: Where Psychology Runs Into\n  Quantum Physics",
        "authors": [
            "Ehtibar N. Dzhafarov",
            "Janne V. Kujala"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  Given a set of several inputs into a system (e.g., independent variables\ncharacterizing stimuli) and a set of several stochastically non-independent\noutputs (e.g., random variables describing different aspects of responses), how\ncan one determine, for each of the outputs, which of the inputs it is\ninfluenced by? The problem has applications ranging from modeling pairwise\ncomparisons to reconstructing mental processing architectures to conjoint\ntesting. A necessary and sufficient condition for a given pattern of selective\ninfluences is provided by the Joint Distribution Criterion, according to which\nthe problem of \"what influences what\" is equivalent to that of the existence of\na joint distribution for a certain set of random variables. For inputs and\noutputs with finite sets of values this criterion translates into a test of\nconsistency of a certain system of linear equations and inequalities (Linear\nFeasibility Test) which can be performed by means of linear programming. While\nnew in the behavioral context, both this test and the Joint Distribution\nCriterion on which it is based have been previously proposed in quantum\nphysics, in dealing with generalizations of Bell inequalities for the quantum\nentanglement problem. The parallels between this problem and that of selective\ninfluences in behavioral sciences is established by observing that noncommuting\nmeasurements in quantum physics are mutually exclusive and can therefore be\ntreated as different levels of one and the same factor.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.2388v4"
    },
    {
        "title": "Probably a discovery: Bad mathematics means rough scientific\n  communication",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2011",
        "summary": "  According to the media, in spring of this year the experiment CDF at Fermilab\nhas made most likely (\"this result has a 99.7 percent chance of being correct\",\nDiscovery News) a great discovery (\"the most significant in physics in half a\ncentury\", NYT). However, since the very beginning, practically all particle\nphysics experts did not believe that was the case. This is the last of a quite\nlong series of fake claims based on trivial mistakes in the probabilistic\nreasoning. The main purpose of this note is to invite everybody, but especially\njournalists and general public, most times innocent victims of misinformation\nof this kind, to mistrust claims not explicitly reported in terms of how much\nwe should believe something, under well stated conditions and assumptions. (A\nlast minute appendix has been added, with comments on the recent news\nconcerning the Higgs at LHC.)\n",
        "pdf_link": "http://arxiv.org/pdf/1112.3620v2"
    },
    {
        "title": "Estimating the historical and future probabilities of large terrorist\n  events",
        "authors": [
            "Aaron Clauset",
            "Ryan Woodard"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  Quantities with right-skewed distributions are ubiquitous in complex social\nsystems, including political conflict, economics and social networks, and these\nsystems sometimes produce extremely large events. For instance, the 9/11\nterrorist events produced nearly 3000 fatalities, nearly six times more than\nthe next largest event. But, was this enormous loss of life statistically\nunlikely given modern terrorism's historical record? Accurately estimating the\nprobability of such an event is complicated by the large fluctuations in the\nempirical distribution's upper tail. We present a generic statistical algorithm\nfor making such estimates, which combines semi-parametric models of tail\nbehavior and a nonparametric bootstrap. Applied to a global database of\nterrorist events, we estimate the worldwide historical probability of observing\nat least one 9/11-sized or larger event since 1968 to be 11-35%. These results\nare robust to conditioning on global variations in economic development,\ndomestic versus international events, the type of weapon used and a truncated\nhistory that stops at 1998. We then use this procedure to make a data-driven\nstatistical forecast of at least one similar event over the next decade.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.0089v3"
    },
    {
        "title": "Solar Activity and Transformer Failures in the Greek National Electric\n  Grid",
        "authors": [
            "Ioannis P. Zois"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We study both the short term and long term effects of solar activity on the\nlarge transformers (150kV and 400kV) of the Greek national electric grid. We\nuse data analysis and various analytic and statistical methods and models.\nContrary to the common belief in PPC Greece, we see that there are considerable\nboth short term (immediate) and long term effects of solar activity onto large\ntransformers in a mid-latitude country (latitude approx. 35 - 41 degrees North)\nlike Greece. Our results can be summarized as follows: For the short term\neffects: During 1989-2010 there were 43 stormy days (namely days with for\nexample Ap larger or equal to 100) and we had 19 failures occurring during a\nstormy day plus or minus 3 days and 51 failures occurring during a stormy day\nplus or minus 7 days. All these failures can be directly related to\nGeomagnetically Induced Currents (GICs). Explicit cases are presented. For the\nlong term effects we have two main results: The annual transformer failure\nnumber for the period of study 1989-2010 follows the solar activity pattern (11\nyear periodicity, bell-shaped graph). Yet the maximum number of transformer\nfailures occur 3-4 years after the maximum of solar activity. There is\nstatistical correlation between solar activity expressed using various newly\ndefined long term solar activity indices and the annual number of transformer\nfailures. These new long term solar activity indices were defined using both\nlocal (from geomagnetic stations in Greece) and global (planetary averages)\ngeomagnetic data. Applying both linear and non-linear statistical regression we\ncompute the regression equations and the corresponding coefficients of\ndetermination.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.1149v2"
    },
    {
        "title": "Chi-square goodness of fit tests for weighted histograms. Review and\n  improvements",
        "authors": [
            "Nikolai Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Weighted histograms are used for the estimation of probability density\nfunctions. Computer simulation is the main domain of application of this type\nof histogram. A review of chi-square goodness of fit tests for weighted\nhistograms is presented in this paper. Improvements are proposed to these tests\nthat have size more close to its nominal value. Numerical examples are\npresented in this paper for evaluation of tests and to demonstrate various\napplications of tests.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.1582v4"
    },
    {
        "title": "Efficient Monte Carlo and greedy heuristic for the inference of\n  stochastic block models",
        "authors": [
            "Tiago P. Peixoto"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We present an efficient algorithm for the inference of stochastic block\nmodels in large networks. The algorithm can be used as an optimized Markov\nchain Monte Carlo (MCMC) method, with a fast mixing time and a much reduced\nsusceptibility to getting trapped in metastable states, or as a greedy\nagglomerative heuristic, with an almost linear $O(N\\ln^2N)$ complexity, where\n$N$ is the number of nodes in the network, independent on the number of blocks\nbeing inferred. We show that the heuristic is capable of delivering results\nwhich are indistinguishable from the more exact and numerically expensive MCMC\nmethod in many artificial and empirical networks, despite being much faster.\nThe method is entirely unbiased towards any specific mixing pattern, and in\nparticular it does not favor assortative community structures.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.4378v3"
    },
    {
        "title": "Community detection for correlation matrices",
        "authors": [
            "Mel MacMahon",
            "Diego Garlaschelli"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  A challenging problem in the study of complex systems is that of resolving,\nwithout prior information, the emergent, mesoscopic organization determined by\ngroups of units whose dynamical activity is more strongly correlated internally\nthan with the rest of the system. The existing techniques to filter\ncorrelations are not explicitly oriented towards identifying such modules and\ncan suffer from an unavoidable information loss. A promising alternative is\nthat of employing community detection techniques developed in network theory.\nUnfortunately, this approach has focused predominantly on replacing network\ndata with correlation matrices, a procedure that tends to be intrinsically\nbiased due to its inconsistency with the null hypotheses underlying the\nexisting algorithms. Here we introduce, via a consistent redefinition of null\nmodels based on random matrix theory, the appropriate correlation-based\ncounterparts of the most popular community detection techniques. Our methods\ncan filter out both unit-specific noise and system-wide dependencies, and the\nresulting communities are internally correlated and mutually anti-correlated.\nWe also implement multiresolution and multifrequency approaches revealing\nhierarchically nested sub-communities with `hard' cores and `soft' peripheries.\nWe apply our techniques to several financial time series and identify\nmesoscopic groups of stocks which are irreducible to a standard, sectorial\ntaxonomy, detect `soft stocks' that alternate between communities, and discuss\nimplications for portfolio optimization and risk management.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.1924v3"
    },
    {
        "title": "Structural patterns in complex systems using multidendrograms",
        "authors": [
            "Sergio Gomez",
            "Alberto Fernandez",
            "Clara Granell",
            "Alex Arenas"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Complex systems are usually represented as an intricate set of relations\nbetween their components forming a complex graph or network. The understanding\nof their functioning and emergent properties are strongly related to their\nstructural properties. The finding of structural patterns is of utmost\nimportance to reduce the problem of understanding the structure-function\nrelationships. Here we propose the analysis of similarity measures between\nnodes using hierarchical clustering methods. The discrete nature of the\nnetworks usually leads to a small set of different similarity values, making\nstandard hierarchical clustering algorithms ambiguous. We propose the use of\n\"multidendrograms\", an algorithm that computes agglomerative hierarchical\nclusterings implementing a variable-group technique that solves the\nnon-uniqueness problem found in the standard pair-group algorithm. This problem\narises when there are more than two clusters separated by the same maximum\nsimilarity (or minimum distance) during the agglomerative process. Forcing\nbinary trees in this case means breaking ties in some way, thus giving rise to\ndifferent output clusterings depending on the criterion used. Multidendrograms\nsolves this problem grouping more than two clusters at the same time when ties\noccur.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.1236v1"
    },
    {
        "title": "Model selection and hypothesis testing for large-scale network models\n  with overlapping groups",
        "authors": [
            "Tiago P. Peixoto"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The effort to understand network systems in increasing detail has resulted in\na diversity of methods designed to extract their large-scale structure from\ndata. Unfortunately, many of these methods yield diverging descriptions of the\nsame network, making both the comparison and understanding of their results a\ndifficult challenge. A possible solution to this outstanding issue is to shift\nthe focus away from ad hoc methods and move towards more principled approaches\nbased on statistical inference of generative models. As a result, we face\ninstead the more well-defined task of selecting between competing generative\nprocesses, which can be done under a unified probabilistic framework. Here, we\nconsider the comparison between a variety of generative models including\nfeatures such as degree correction, where nodes with arbitrary degrees can\nbelong to the same group, and community overlap, where nodes are allowed to\nbelong to more than one group. Because such model variants possess an\nincreasing number of parameters, they become prone to overfitting. In this\nwork, we present a method of model selection based on the minimum description\nlength criterion and posterior odds ratios that is capable of fully accounting\nfor the increased degrees of freedom of the larger models, and selects the best\none according to the statistical evidence available in the data. In applying\nthis method to many empirical unweighted networks from different fields, we\nobserve that community overlap is very often not supported by statistical\nevidence and is selected as a better model only for a minority of them. On the\nother hand, we find that degree correction tends to be almost universally\nfavored by the available data, implying that intrinsic node proprieties (as\nopposed to group properties) are often an essential ingredient of network\nformation.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.3059v4"
    },
    {
        "title": "Densities mixture unfolding for data obtained from detectors with finite\n  resolution and limited acceptance",
        "authors": [
            "Nikolai Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  A procedure based on a Mixture Density Model for correcting experimental data\nfor distortions due to finite resolution and limited detector acceptance is\npresented. Addressing the case that the solution is known to be non-negative,\nin the approach presented here, the true distribution is estimated by a\nweighted sum of probability density functions with positive weights and with\nthe width of the densities acting as a regularisation parameter responsible for\nthe smoothness of the result. To obtain better smoothing in less populated\nregions, the width parameter is chosen inversely proportional to the square\nroot of the estimated density. Furthermore, the non-negative garrotte method is\nused to find the most economic representation of the solution. Cross-validation\nis employed to determine the optimal values of the resolution and garrotte\nparameters. The proposed approach is directly applicable to multidimensional\nproblems. Numerical examples in one and two dimensions are presented to\nillustrate the procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.1586v2"
    },
    {
        "title": "Signal inference with unknown response: Calibration-uncertainty\n  renormalized estimator",
        "authors": [
            "Sebastian Dorn",
            "Torsten A. Enßlin",
            "Maksim Greiner",
            "Marco Selig",
            "Vanessa Boehm"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  The calibration of a measurement device is crucial for every scientific\nexperiment, where a signal has to be inferred from data. We present CURE, the\ncalibration uncertainty renormalized estimator, to reconstruct a signal and\nsimultaneously the instrument's calibration from the same data without knowing\nthe exact calibration, but its covariance structure. The idea of CURE,\ndeveloped in the framework of information field theory, is starting with an\nassumed calibration to successively include more and more portions of\ncalibration uncertainty into the signal inference equations and to absorb the\nresulting corrections into renormalized signal (and calibration) solutions.\nThereby, the signal inference and calibration problem turns into solving a\nsingle system of ordinary differential equations and can be identified with\ncommon resummation techniques used in field theories. We verify CURE by\napplying it to a simplistic toy example and compare it against existent\nself-calibration schemes, Wiener filter solutions, and Markov Chain Monte Carlo\nsampling. We conclude that the method is able to keep up in accuracy with the\nbest self-calibration methods and serves as a non-iterative alternative to it.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.6289v2"
    },
    {
        "title": "A note on entropy estimation",
        "authors": [
            "Thomas Schürmann"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  We compare an entropy estimator $H_z$ recently discussed in [10] with two\nestimators $H_1$ and $H_2$ introduced in [6][7]. We prove the identity $H_z\n\\equiv H_1$, which has not been taken into account in [10]. Then, we prove that\nthe statistical bias of $H_1$ is less than the bias of the ordinary likelihood\nestimator of entropy. Finally, by numerical simulation we verify that for the\nmost interesting regime of small sample estimation and large event spaces, the\nestimator $H_2$ has a significant smaller statistical error than $H_z$.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.05911v2"
    },
    {
        "title": "A method for comparing non-nested models with application to\n  astrophysical searches for new physics",
        "authors": [
            "Sara Algeri",
            "Jan Conrad",
            "David A. van Dyk"
        ],
        "category": "physics.data-an",
        "published_year": "2015",
        "summary": "  Searches for unknown physics and decisions between competing astrophysical\nmodels to explain data both rely on statistical hypothesis testing. The usual\napproach in searches for new physical phenomena is based on the statistical\nLikelihood Ratio Test (LRT) and its asymptotic properties. In the common\nsituation, when neither of the two models under comparison is a special case of\nthe other i.e., when the hypotheses are non-nested, this test is not\napplicable. In astrophysics, this problem occurs when two models that reside in\ndifferent parameter spaces are to be compared. An important example is the\nrecently reported excess emission in astrophysical $\\gamma$-rays and the\nquestion whether its origin is known astrophysics or dark matter. We develop\nand study a new, simple, generally applicable, frequentist method and validate\nits statistical properties using a suite of simulations studies. We exemplify\nit on realistic simulated data of the Fermi-LAT $\\gamma$-ray satellite, where\nnon-nested hypotheses testing appears in the search for particle dark matter.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01010v3"
    },
    {
        "title": "An Ensemble 4D Seismic History Matching Framework with Sparse\n  Representation Based on Wavelet Multiresolution Analysis",
        "authors": [
            "Xiaodong Luo",
            "Tuhin Bhakta",
            "Morten Jakobsen",
            "Geir Nævdal"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In this work we propose an ensemble 4D seismic history matching framework for\nreservoir characterization. Compared to similar existing frameworks in\nreservoir engineering community, the proposed one consists of some relatively\nnew ingredients, in terms of the type of seismic data in choice, wavelet\nmultiresolution analysis for the chosen seismic data and related data noise\nestimation, and the use of recently developed iterative ensemble history\nmatching algorithms.\n  Typical seismic data used for history matching, such as acoustic impedance,\nare inverted quantities, whereas extra uncertainties may arise during the\ninversion processes. In the proposed framework we avoid such intermediate\ninversion processes. In addition, we also adopt wavelet-based sparse\nrepresentation to reduce data size. Concretely, we use intercept and gradient\nattributes derived from amplitude versus angle (AVA) data, apply multilevel\ndiscrete wavelet transforms (DWT) to attribute data, and estimate noise level\nof resulting wavelet coefficients. We then select the wavelet coefficients\nabove a certain threshold value, and history-match these leading wavelet\ncoefficients using an iterative ensemble smoother.\n  (The rest of the abstract is omitted for exceeding the limit of length)\n",
        "pdf_link": "http://arxiv.org/pdf/1603.04577v1"
    },
    {
        "title": "Model independent analysis of nearly Lévy correlations",
        "authors": [
            "T. Novák",
            "T. Csörgő",
            "H. C. Eggers",
            "M. de Kock"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  A model-independent method for the analysis of the two-particle short-range\ncorrelations is presented, that can be utilized to describe e.g. Bose-Einstein\n(HBT), dynamical (ridge) or other correlation functions, that have a nearly\nL\\'evy or streched exponential shape. For the special case of L\\'evy exponent\nalpha = 1, the earlier Laguerre expansions are recovered, for the alpha = 2\nspecial case, a new expansion method is obtained for nearly Gaussian\ncorrelation functions. Multi-dimensional L\\'evy expansions are also introduced\nand their potential application to analyze rigde correlation data is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.05513v1"
    },
    {
        "title": "Delineating Parameter Unidentifiabilities in Complex Models",
        "authors": [
            "Dhruva V. Raman",
            "James Anderson",
            "Antonis Papachristodoulou"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Scientists use mathematical modelling to understand and predict the\nproperties of complex physical systems. In highly parameterised models there\noften exist relationships between parameters over which model predictions are\nidentical, or nearly so. These are known as structural or practical\nunidentifiabilities, respectively. They are hard to diagnose and make reliable\nparameter estimation from data impossible. They furthermore imply the existence\nof an underlying model simplification. We describe a scalable method for\ndetecting unidentifiabilities, and the functional relations defining them, for\ngeneric models. This allows for model simplification, and appreciation of which\nparameters (or functions thereof) cannot be estimated from data. Our algorithm\ncan identify features such as redundant mechanisms and fast timescale\nsubsystems, as well as the regimes in which such approximations are valid. We\nbase our algorithm on a novel quantification of regional parametric\nsensitivity: multiscale sloppiness. Traditionally, the link between parametric\nsensitivity and the conditioning of the parameter estimation problem is made\nlocally, through the Fisher Information Matrix. This is valid in the regime of\ninfinitesimal measurement uncertainty. We demonstrate the duality between\nmultiscale sloppiness and the geometry of confidence regions surrounding\nparameter estimates made where measurement uncertainty is non-negligible.\nFurther theoretical relationships are provided linking multiscale sloppiness to\nthe Likelihood-ratio test. From this, we show that a local sensitivity analysis\n(as typically done) is insufficient for determining the reliability of\nparameter estimation, even with simple (non)linear systems. Our algorithm\nprovides a tractable alternative. We finally apply our methods to a\nlarge-scale, benchmark Systems Biology model of NF-$\\kappa$B, uncovering\npreviously unknown unidentifiabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.07705v2"
    },
    {
        "title": "Efficient big data assimilation through sparse representation: A 3D\n  benchmark case study in seismic history matching",
        "authors": [
            "Xiaodong Luo",
            "Tuhin Bhakta",
            "Morten Jakobsen",
            "Geir Nævdal"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  In a previous work \\citep{luo2016sparse2d_spej}, the authors proposed an\nensemble-based 4D seismic history matching (SHM) framework, which has some\nrelatively new ingredients, in terms of the type of seismic data in choice, the\nway to handle big seismic data and related data noise estimation, and the use\nof a recently developed iterative ensemble history matching algorithm.\n  In seismic history matching, it is customary to use inverted seismic\nattributes, such as acoustic impedance, as the observed data. In doing so,\nextra uncertainties may arise during the inversion processes. The proposed SHM\nframework avoids such intermediate inversion processes by adopting amplitude\nversus angle (AVA) data. In addition, SHM typically involves assimilating a\nlarge amount of observed seismic attributes into reservoir models. To handle\nthe big-data problem in SHM, the proposed framework adopts the following\nwavelet-based sparse representation procedure: First, a discrete wavelet\ntransform is applied to observed seismic attributes. Then, uncertainty analysis\nis conducted in the wavelet domain to estimate noise in the resulting wavelet\ncoefficients, and to calculate a corresponding threshold value. Wavelet\ncoefficients above the threshold value, called leading wavelet coefficients\nhereafter, are used as the data for history matching. The retained leading\nwavelet coefficients preserve the most salient features of the observed seismic\nattributes, whereas rendering a substantially smaller data size. Finally, an\niterative ensemble smoother is adopted to update reservoir models, in such a\nway that the leading wavelet coefficients of simulated seismic attributes\nbetter match those of observed seismic attributes.\n  (The rest of the abstract was omitted for the length restriction.)\n",
        "pdf_link": "http://arxiv.org/pdf/1609.06842v1"
    },
    {
        "title": "Event Shape Sorting: selecting events with similar evolution",
        "authors": [
            "Boris Tomasik",
            "Renata Kopecna"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  We present novel method for the organisation of events. The method is based\non comparing event-by-event histograms of a chosen quantity Q that is measured\nfor each particle in every event. The events are organised in such a way that\nthose with similar shape of the Q-histograms end-up placed close to each other.\nWe apply the method on histograms of azimuthal angle of the produced hadrons in\nultrarelativsitic nuclear collisions. By selecting events with similar\nazimuthal shape of their hadron distribution one chooses events which are\nlikely that they underwent similar evolution from the initial state to the\nfreeze-out. Such events can more easily be compared to theoretical simulations\nwhere all conditions can be controlled. We illustrate the method on data\nsimulated by the AMPT model.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.08575v1"
    },
    {
        "title": "Density estimation on small datasets",
        "authors": [
            "Wei-Chia Chen",
            "Ammar Tareen",
            "Justin B. Kinney"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  How might a smooth probability distribution be estimated, with accurately\nquantified uncertainty, from a limited amount of sampled data? Here we describe\na field-theoretic approach that addresses this problem remarkably well in one\ndimension, providing an exact nonparametric Bayesian posterior without relying\non tunable parameters or large-data approximations. Strong non-Gaussian\nconstraints, which require a non-perturbative treatment, are found to play a\nmajor role in reducing distribution uncertainty. A software implementation of\nthis method is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.01932v4"
    },
    {
        "title": "An iterative method to estimate the combinatorial background",
        "authors": [
            "Georgy Kornakov",
            "Tetyana Galatyuk"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  The reconstruction of broad resonances is important for understanding the\ndynamics of heavy ion collisions. However, large combinatorial background makes\nthis objective very challenging. In this work an innovative iterative method\nwhich identifies signal and background contributions without input models for\nnormalization constants is presented. This technique is successfully validated\non a simulated thermal cocktail of resonances. This demonstrates that the\niterative procedure is a powerful tool to reconstruct multi-differentially\ninclusive resonant signals in high multiplicity events as produced in heavy ion\ncollisions.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.05466v1"
    },
    {
        "title": "Effect of centrality bin width corrections on two-particle number and\n  transverse momentum differential correlation functions",
        "authors": [
            "Victor Gonzalez",
            "Ana Marin",
            "Pedro Ladron de Guevara",
            "Jinjin Pan",
            "Sumit Basu",
            "Claude Pruneau"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  Two-particle number and transverse momentum differential correlation\nfunctions are powerful tools for unveiling the detailed dynamics and particle\nproduction mechanisms involved in relativistic heavy-ion collisions.\nMeasurements of transverse momentum correlators $P_2$ and $G_2$, in particular,\nprovide added information not readily accessible with better known number\ncorrelation functions $R_2$. However, it is found that the $R_2$ and $G_2$\ncorrelators are somewhat sensitive to the details of the experimental procedure\nused to measure them. They exhibit, in particular, a dependence on the\ncollision centrality bin width, which may have a rather detrimental impact on\ntheir physical interpretation. A technique to correct these correlators for\ncollision centrality bin-width averaging is presented. The technique is based\non the hypothesis that the shape of single- and pair- probability densities\nvary slower with collision centrality than the corresponding integrated yields.\nThe technique is tested with Pb-Pb simulations based on the HIJING and\nultrarelativistic quantum molecular dynamics models and shown to enable a\nprecision better than 1% for particles in the kinematic range $0.2 \\leq p_{\\rm\nT} \\leq 2.0$ GeV/$c$.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.04962v2"
    },
    {
        "title": "Learning representations of irregular particle-detector geometry with\n  distance-weighted graph networks",
        "authors": [
            "Shah Rukh Qasim",
            "Jan Kieseler",
            "Yutaro Iiyama",
            "Maurizio Pierini"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We explore the use of graph networks to deal with irregular-geometry\ndetectors in the context of particle reconstruction. Thanks to their\nrepresentation-learning capabilities, graph networks can exploit the full\ndetector granularity, while natively managing the event sparsity and\narbitrarily complex detector geometries. We introduce two distance-weighted\ngraph network architectures, dubbed GarNet and GravNet layers, and apply them\nto a typical particle reconstruction task. The performance of the new\narchitectures is evaluated on a data set of simulated particle interactions on\na toy model of a highly granular calorimeter, loosely inspired by the endcap\ncalorimeter to be installed in the CMS detector for the High-Luminosity LHC\nphase. We study the clustering of energy depositions, which is the basis for\ncalorimetric particle reconstruction, and provide a quantitative comparison to\nalternative approaches. The proposed algorithms provide an interesting\nalternative to existing methods, offering equally performing or less\nresource-demanding solutions with less underlying assumptions on the detector\ngeometry and, consequently, the possibility to generalize to other detectors.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.07987v2"
    },
    {
        "title": "Detecting new signals under background mismodelling",
        "authors": [
            "Sara Algeri"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Searches for new astrophysical phenomena often involve several sources of\nnon-random uncertainties which can lead to highly misleading results. Among\nthese, model-uncertainty arising from background mismodelling can dramatically\ncompromise the sensitivity of the experiment under study. Specifically,\noverestimating the background distribution in the signal region increases the\nchances of missing new physics. Conversely, underestimating the background\noutside the signal region leads to an artificially enhanced sensitivity and a\nhigher likelihood of claiming false discoveries. The aim of this work is to\nprovide a unified statistical strategy to perform modelling, estimation,\ninference, and signal characterization under background mismodelling. The\nmethod proposed allows to incorporate the (partial) scientific knowledge\navailable on the background distribution and provides a data-updated version of\nit in a purely nonparametric fashion without requiring the specification of\nprior distributions on the parameters. Applications in the context of dark\nmatter searches and radio surveys show how the tools presented in this article\ncan be used to incorporate non-stochastic uncertainty due to instrumental noise\nand to overcome violations of classical distributional assumptions in stacking\nexperiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.06615v3"
    },
    {
        "title": "A new Monte Carlo-based fitting method",
        "authors": [
            "Paolo Pedroni",
            "Stefano Sconfietti"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We present a new fitting technique based on the parametric bootstrap method,\nwhich relies on the idea to produce artificial measurements using the estimated\nprobability distribution of the experimental data. In order to investigate the\nmain properties of this technique, we develop a toy model and we analyze\nseveral fitting conditions with a comparison of our results to the ones\nobtained using both the standard $\\chi^2$ minimization procedure and a Bayesian\napproach. Furthermore, we investigate the effect of the data systematic\nuncertainties both on the probability distribution of the fit parameters and on\nthe shape of the expected goodness-of-fit distribution. Our conclusion is that,\nwhen systematic uncertainties are included in the analysis, only the bootstrap\nprocedure is able to provide reliable confidence intervals and p-values, thus\nimproving the results given by the standard $\\chi^2$ minimization approach. Our\ntechnique is then applied to an actual physics process, the real Compton\nscattering off the proton, thus confirming both the portability and the\nvalidity of the bootstrap-based fit method.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.03885v2"
    },
    {
        "title": "Maximal Relevance and Optimal Learning Machines",
        "authors": [
            "O Duranthon",
            "M Marsili",
            "R Xie"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  We show that the mutual information between the representation of a learning\nmachine and the hidden features that it extracts from data is bounded from\nbelow by the relevance, which is the entropy of the model's energy\ndistribution. Models with maximal relevance -- that we call Optimal Learning\nMachines (OLM) -- are hence expected to extract maximally informative\nrepresentations. We explore this principle in a range of models. For fully\nconnected Ising models and we show that {\\em i)} OLM are characterised by\ninhomogeneous distributions of couplings, and that {\\em ii)} their learning\nperformance is affected by sub-extensive features that are elusive to a\nthermodynamic treatment. On specific learning tasks, we find that likelihood\nmaximisation is achieved by models with maximal relevance. Training of\nRestricted Boltzmann Machines on the MNIST benchmark shows that learning is\nassociated with a broadening of the spectrum of energy levels and that the\ninternal representation of the hidden layer approaches the maximal relevance\nthat can be achieved in a finite dataset. Finally, we discuss a Gaussian\nlearning machine that clarifies that learning hidden features is conceptually\ndifferent from parameter estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.12792v3"
    },
    {
        "title": "Searching for new physics with profile likelihoods: Wilks and beyond",
        "authors": [
            "Sara Algeri",
            "Jelle Aalbers",
            "Knut Dundas Morå",
            "Jan Conrad"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Particle physics experiments use likelihood ratio tests extensively to\ncompare hypotheses and to construct confidence intervals. Often, the null\ndistribution of the likelihood ratio test statistic is approximated by a\n$\\chi^2$ distribution, following a theorem due to Wilks. However, many\ncircumstances relevant to modern experiments can cause this theorem to fail. In\nthis paper, we review how to identify these situations and construct valid\ninference.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.10237v1"
    },
    {
        "title": "DeepRICH: Learning Deeply Cherenkov Detectors",
        "authors": [
            "Cristiano Fanelli",
            "Jary Pomponi"
        ],
        "category": "physics.data-an",
        "published_year": "2019",
        "summary": "  Imaging Cherenkov detectors are largely used for particle identification\n(PID) in nuclear and particle physics experiments, where developing fast\nreconstruction algorithms is becoming of paramount importance to allow for near\nreal time calibration and data quality control, as well as to speed up offline\nanalysis of large amount of data. In this paper we present DeepRICH, a novel\ndeep learning algorithm for fast reconstruction which can be applied to\ndifferent imaging Cherenkov detectors. The core of our architecture is a\ngenerative model which leverages on a custom Variational Auto-encoder (VAE)\ncombined to Maximum Mean Discrepancy (MMD), with a Convolutional Neural Network\n(CNN) extracting features from the space of the latent variables for\nclassification. A thorough comparison with the simulation/reconstruction\npackage FastDIRC is discussed in the text. DeepRICH has the advantage to bypass\nlow-level details needed to build a likelihood, allowing for a sensitive\nimprovement in computation time at potentially the same reconstruction\nperformance of other established reconstruction algorithms. In the conclusions,\nwe address the implications and potentialities of this work, discussing\npossible future extensions and generalization.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.11717v2"
    },
    {
        "title": "Bryan's Maximum Entropy Method -- diagnosis of a flawed argument and its\n  remedy",
        "authors": [
            "Alexander Rothkopf"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  The Maximum Entropy Method (MEM) is a popular data analysis technique based\non Bayesian inference, which has found various applications in the research\nliterature. While the MEM itself is well-grounded in statistics, I argue that\nits state-of-the-art implementation, suggested originally by Bryan,\nartificially restricts its solution space. This restriction leads to a\nsystematic error often unaccounted for in contemporary MEM studies. The goal of\nthis paper is to carefully revisit Bryan's train of thought, point out its flaw\nin applying linear algebra arguments to an inherently nonlinear problem, and\nsuggest possible ways to overcome it.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.09865v3"
    },
    {
        "title": "Towards a Computer Vision Particle Flow",
        "authors": [
            "Francesco Armando Di Bello",
            "Sanmay Ganguly",
            "Eilam Gross",
            "Marumi Kado",
            "Michael Pitt",
            "Lorenzo Santi",
            "Jonathan Shlomi"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  In High Energy Physics experiments Particle Flow (PFlow) algorithms are\ndesigned to provide an optimal reconstruction of the nature and kinematic\nproperties of the particles produced within the detector acceptance during\ncollisions. At the heart of PFlow algorithms is the ability to distinguish the\ncalorimeter energy deposits of neutral particles from those of charged\nparticles, using the complementary measurements of charged particle tracking\ndevices, to provide a superior measurement of the particle content and\nkinematics. In this paper, a computer vision approach to this fundamental\naspect of PFlow algorithms, based on calorimeter images, is proposed. A\ncomparative study of the state of the art deep learning techniques is\nperformed. A significantly improved reconstruction of the neutral particle\ncalorimeter energy deposits is obtained in a context of large overlaps with the\ndeposits from charged particles. Calorimeter images with augmented finer\ngranularity are also obtained using super-resolution techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.08863v3"
    },
    {
        "title": "MatDRAM: A pure-MATLAB Delayed-Rejection Adaptive Metropolis-Hastings\n  Markov Chain Monte Carlo Sampler",
        "authors": [
            "Shashank Kumbhare",
            "Amir Shahmoradi"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Markov Chain Monte Carlo (MCMC) algorithms are widely used for stochastic\noptimization, sampling, and integration of mathematical objective functions, in\nparticular, in the context of Bayesian inverse problems and parameter\nestimation. For decades, the algorithm of choice in MCMC simulations has been\nthe Metropolis-Hastings (MH) algorithm. An advancement over the traditional\nMH-MCMC sampler is the Delayed-Rejection Adaptive Metropolis (DRAM). In this\npaper, we present MatDRAM, a stochastic optimization, sampling, and Monte Carlo\nintegration toolbox in MATLAB which implements a variant of the DRAM algorithm\nfor exploring the mathematical objective functions of arbitrary-dimensions, in\nparticular, the posterior distributions of Bayesian models in data science,\nMachine Learning, and scientific inference. The design goals of MatDRAM include\nnearly-full automation of MCMC simulations, user-friendliness,\nfully-deterministic reproducibility, and the restart functionality of\nsimulations. We also discuss the implementation details of a technique to\nautomatically monitor and ensure the diminishing adaptation of the proposal\ndistribution of the DRAM algorithm and a method of efficiently storing the\nresulting simulated Markov chains. The MatDRAM library is open-source,\nMIT-licensed, and permanently located and maintained as part of the ParaMonte\nlibrary at https://github.com/cdslaborg/paramonte.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.04190v1"
    },
    {
        "title": "Graph Generative Adversarial Networks for Sparse Data Generation in High\n  Energy Physics",
        "authors": [
            "Raghav Kansal",
            "Javier Duarte",
            "Breno Orzari",
            "Thiago Tomei",
            "Maurizio Pierini",
            "Mary Touranakou",
            "Jean-Roch Vlimant",
            "Dimitrios Gunopulos"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  We develop a graph generative adversarial network to generate sparse data\nsets like those produced at the CERN Large Hadron Collider (LHC). We\ndemonstrate this approach by training on and generating sparse representations\nof MNIST handwritten digit images and jets of particles in proton-proton\ncollisions like those at the LHC. We find the model successfully generates\nsparse MNIST digits and particle jet data. We quantify agreement between real\nand generated data with a graph-based Fr\\'echet Inception distance, and the\nparticle and jet feature-level 1-Wasserstein distance for the MNIST and jet\ndatasets respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.00173v4"
    },
    {
        "title": "Image-Based Jet Analysis",
        "authors": [
            "Michael Kagan"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Image-based jet analysis is built upon the jet image representation of jets\nthat enables a direct connection between high energy physics and the fields of\ncomputer vision and deep learning. Through this connection, a wide array of new\njet analysis techniques have emerged. In this text, we survey jet image based\nclassification models, built primarily on the use of convolutional neural\nnetworks, examine the methods to understand what these models have learned and\nwhat is their sensitivity to uncertainties, and review the recent successes in\nmoving these models from phenomenological studies to real world application on\nexperiments at the LHC. Beyond jet classification, several other applications\nof jet image based techniques, including energy estimation, pileup noise\nreduction, data generation, and anomaly detection, are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.09719v2"
    },
    {
        "title": "The Linear Template Fit",
        "authors": [
            "Daniel Britzger"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  The estimation of parameters from data is a common problem in many areas of\nthe physical sciences, and frequently used algorithms rely on sets of simulated\ndata which are fit to data. In this article, an analytic solution for\nsimulation-based parameter estimation problems is presented. The matrix\nformalism, termed the Linear Template Fit, calculates the best estimators for\nthe parameters of interest. It combines a linear regression with the method of\nleast squares. The algorithm uses only predictions calculated for a few values\nof the parameters of interest, which have been made available prior to its\nexecution. The Linear Template Fit is particularly suited for performance\ncritical applications and parameter estimation problems with computationally\nintense simulations, which are otherwise often limited in their usability for\nstatistical inference. Equations for error propagation are discussed in detail\nand are given in closed analytic form. For the solution of problems with a\nnonlinear dependence on the parameters of interest, the Quadratic Template Fit\nis introduced. As an example application, a determination of the strong\ncoupling constant from inclusive jet cross section data at the CERN Large\nHadron Collider is studied and compared with previously published results.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.01548v2"
    },
    {
        "title": "On methods for correcting for the look-elsewhere effect in searches for\n  new physics",
        "authors": [
            "Sara Algeri",
            "David A. van Dyk",
            "Jan Conrad",
            "Brandon Anderson"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The search for new significant peaks over a energy spectrum often involves a\nstatistical multiple hypothesis testing problem. Separate tests of hypothesis\nare conducted at different locations producing an ensemble of local p-values,\nthe smallest of which is reported as evidence for the new resonance.\nUnfortunately, controlling the false detection rate (type I error rate) of such\nprocedures may lead to excessively stringent acceptance criteria. In the recent\nphysics literature, two promising statistical tools have been proposed to\novercome these limitations. In 2005, a method to \"find needles in haystacks\"\nwas introduced by Pilla et al. [1], and a second method was later proposed by\nGross and Vitells [2] in the context of the \"look elsewhere effect\" and trial\nfactors. We show that, for relatively small sample sizes, the former leads to\nan artificial inflation of statistical power that stems from an increase in the\nfalse detection rate, whereas the two methods exhibit similar performance for\nlarge sample sizes. We apply the methods to realistic simulations of the Fermi\nLarge Area Telescope data, in particular the search for dark matter\nannihilation lines. Further, we discuss the counter-intutive scenario where the\nlook-elsewhere corrections are more conservative than much more computationally\nefficient corrections for multiple hypothesis testing. Finally, we provide\ngeneral guidelines for navigating the tradeoffs between statistical and\ncomputational efficiency when selecting a statistical procedure for signal\ndetection.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.03765v5"
    },
    {
        "title": "Unfolding problem clarification and solution validation",
        "authors": [
            "Nikolai Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  The unfolding problem formulation for correcting experimental data\ndistortions due to finite resolution and limited detector acceptance is\ndiscussed. A novel validation of the problem solution is proposed. Attention is\ndrawn to fact that different unfolded distributions may satisfy the validation\ncriteria, in which case a conservative approach using entropy is suggested. The\nimportance of analysis of residuals is demonstrated.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05834v2"
    },
    {
        "title": "Tests for Comparing Weighted Histograms. Review and Improvements",
        "authors": [
            "Nikolai Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  Histograms with weighted entries are used to estimate probability density\nfunctions. Computer simulation is the main application of this type of\nhistograms. A review on chi-square tests for comparing weighted histograms is\npresented in this paper. Improvements to these tests that have a size closer to\nits nominal value are proposed. Numerical examples are presented for evaluation\nand demonstration of various applications of the tests.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.06591v3"
    },
    {
        "title": "Probabilistic treatment of the uncertainty from the finite size of\n  weighted Monte Carlo data",
        "authors": [
            "Thorsten Glüsenkamp"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  Parameter estimation in HEP experiments often involves Monte-Carlo simulation\nto model the experimental response function. A typical application are\nforward-folding likelihood analyses with re-weighting, or time-consuming\nminimization schemes with a new simulation set for each parameter value.\nProblematically, the finite size of such Monte Carlo samples carries intrinsic\nuncertainty that can lead to a substantial bias in parameter estimation if it\nis neglected and the sample size is small. We introduce a probabilistic\ntreatment of this problem by replacing the usual likelihood functions with\nnovel generalized probability distributions that incorporate the finite\nstatistics via suitable marginalization. These new PDFs are analytic, and can\nbe used to replace the Poisson, multinomial, and sample-based unbinned\nlikelihoods, which covers many use cases in high-energy physics. In the limit\nof infinite statistics, they reduce to the respective standard probability\ndistributions. In the general case of arbitrary Monte Carlo weights, the\nexpressions involve the fourth Lauricella function $F_D$, for which we find a\nnew finite-sum representation in a certain parameter setting. The result also\nrepresents an exact form for Carlson's Dirichlet average $R_n$ with $n>0$, and\nthereby an efficient way to calculate the probability generating function of\nthe Dirichlet-multinomial distribution, the extended divided difference of a\nmonomial, or arbitrary moments of univariate B-splines. We demonstrate the bias\nreduction of our approach with a typical toy Monte Carlo problem, estimating\nthe normalization of a peak in a falling energy spectrum, and compare the\nresults with previously published methods from the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.01293v3"
    },
    {
        "title": "Parametric unfolding. Method and restrictions",
        "authors": [
            "Nikolay Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Parametric unfolding of a true distribution distorted due to finite\nresolution and limited efficiency for the registration of individual events is\ndiscussed. Details of the computational algorithm of the unfolding procedure\nare presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12766v1"
    },
    {
        "title": "Supervised learning from noisy observations: Combining machine-learning\n  techniques with data assimilation",
        "authors": [
            "Georg A. Gottwald",
            "Sebastian Reich"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Data-driven prediction and physics-agnostic machine-learning methods have\nattracted increased interest in recent years achieving forecast horizons going\nwell beyond those to be expected for chaotic dynamical systems. In a separate\nstrand of research data-assimilation has been successfully used to optimally\ncombine forecast models and their inherent uncertainty with incoming noisy\nobservations. The key idea in our work here is to achieve increased forecast\ncapabilities by judiciously combining machine-learning algorithms and data\nassimilation. We combine the physics-agnostic data-driven approach of random\nfeature maps as a forecast model within an ensemble Kalman filter data\nassimilation procedure. The machine-learning model is learned sequentially by\nincorporating incoming noisy observations. We show that the obtained forecast\nmodel has remarkably good forecast skill while being computationally cheap once\ntrained. Going beyond the task of forecasting, we show that our method can be\nused to generate reliable ensembles for probabilistic forecasting as well as to\nlearn effective model closure in multi-scale systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.07383v3"
    },
    {
        "title": "The look-elsewhere effect from a unified Bayesian and frequentist\n  perspective",
        "authors": [
            "Adrian E. Bayer",
            "Uros Seljak"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  When searching over a large parameter space for anomalies such as events,\npeaks, objects, or particles, there is a large probability that spurious\nsignals with seemingly high significance will be found. This is known as the\nlook-elsewhere effect and is prevalent throughout cosmology, (astro)particle\nphysics, and beyond. To avoid making false claims of detection, one must\naccount for this effect when assigning the statistical significance of an\nanomaly. This is typically accomplished by considering the trials factor, which\nis generally computed numerically via potentially expensive simulations. In\nthis paper we develop a continuous generalization of the Bonferroni and Sidak\ncorrections by applying the Laplace approximation to evaluate the Bayes factor,\nand in turn relating the trials factor to the prior-to-posterior volume ratio.\nWe use this to define a test statistic whose frequentist properties have a\nsimple interpretation in terms of the global $p$-value, or statistical\nsignificance. We apply this method to various physics-based examples and show\nit to work well for the full range of $p$-values, i.e. in both the asymptotic\nand non-asymptotic regimes. We also show that this method naturally accounts\nfor other model complexities such as additional degrees of freedom,\ngeneralizing Wilks' theorem. This provides a fast way to quantify statistical\nsignificance in light of the look-elsewhere effect, without resorting to\nexpensive simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.13821v1"
    },
    {
        "title": "MLPF: Efficient machine-learned particle-flow reconstruction using graph\n  neural networks",
        "authors": [
            "Joosep Pata",
            "Javier Duarte",
            "Jean-Roch Vlimant",
            "Maurizio Pierini",
            "Maria Spiropulu"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  In general-purpose particle detectors, the particle-flow algorithm may be\nused to reconstruct a comprehensive particle-level view of the event by\ncombining information from the calorimeters and the trackers, significantly\nimproving the detector resolution for jets and the missing transverse momentum.\nIn view of the planned high-luminosity upgrade of the CERN Large Hadron\nCollider (LHC), it is necessary to revisit existing reconstruction algorithms\nand ensure that both the physics and computational performance are sufficient\nin an environment with many simultaneous proton-proton interactions (pileup).\nMachine learning may offer a prospect for computationally efficient event\nreconstruction that is well-suited to heterogeneous computing platforms, while\nsignificantly improving the reconstruction quality over rule-based algorithms\nfor granular detectors. We introduce MLPF, a novel, end-to-end trainable,\nmachine-learned particle-flow algorithm based on parallelizable,\ncomputationally efficient, and scalable graph neural networks optimized using a\nmulti-task objective on simulated events. We report the physics and\ncomputational performance of the MLPF algorithm on a Monte Carlo dataset of top\nquark-antiquark pairs produced in proton-proton collisions in conditions\nsimilar to those expected for the high-luminosity LHC. The MLPF algorithm\nimproves the physics response with respect to a rule-based benchmark algorithm\nand demonstrates computationally scalable particle-flow reconstruction in a\nhigh-pileup environment.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.08578v3"
    },
    {
        "title": "Node metadata can produce predictability transitions in network\n  inference problems",
        "authors": [
            "Oscar Fajardo-Fontiveros",
            "Marta Sales-Pardo",
            "Roger Guimera"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Network inference is the process of learning the properties of complex\nnetworks from data. Besides using information about known links in the network,\nnode attributes and other forms of network metadata can help to solve network\ninference problems. Indeed, several approaches have been proposed to introduce\nmetadata into probabilistic network models and to use them to make better\ninferences. However, we know little about the effect of such metadata in the\ninference process. Here, we investigate this issue. We find that, rather than\naffecting inference gradually, adding metadata causes abrupt transitions in the\ninference process and in our ability to make accurate predictions, from a\nsituation in which metadata does not play any role to a situation in which\nmetadata completely dominates the inference process. When network data and\nmetadata are partly correlated, metadata optimally contributes to the inference\nprocess at the transition between data-dominated and metadata-dominated\nregimes.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.14424v1"
    },
    {
        "title": "Nested sampling for frequentist computation: fast estimation of small\n  $p$-values",
        "authors": [
            "Andrew Fowlie",
            "Sebastian Hoof",
            "Will Handley"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  We propose a novel method for computing $p$-values based on nested sampling\n(NS) applied to the sampling space rather than the parameter space of the\nproblem, in contrast to its usage in Bayesian computation. The computational\ncost of NS scales as $\\log^2{1/p}$, which compares favorably to the $1/p$\nscaling for Monte Carlo (MC) simulations. For significances greater than about\n$4\\sigma$ in both a toy problem and a simplified resonance search, we show that\nNS requires orders of magnitude fewer simulations than ordinary MC estimates.\nThis is particularly relevant for high-energy physics, which adopts a $5\\sigma$\ngold standard for discovery. We conclude with remarks on new connections\nbetween Bayesian and frequentist computation and possibilities for tuning NS\nimplementations for still better performance in this setting.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.13923v2"
    },
    {
        "title": "Optimising experimental design in neutron reflectometry",
        "authors": [
            "James H. Durant",
            "Lucas Wilkins",
            "Joshaniel F. K. Cooper"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Using the Fisher information (FI), the design of neutron reflectometry\nexperiments can be optimised, leading to greater confidence in parameters of\ninterest and better use of experimental time [Durant, Wilkins, Butler, & Cooper\n(2021). J. Appl. Cryst. 54, 1100-1110]. In this work, the FI is utilised in\noptimising the design of a wide range of reflectometry experiments. Two lipid\nbilayer systems are investigated to determine the optimal choice of measurement\nangles and liquid contrasts, in addition to the ratio of the total counting\ntime that should be spent measuring each condition. The reduction in parameter\nuncertainties with the addition of underlayers to these systems is then\nquantified, using the FI, and validated through the use of experiment\nsimulation and Bayesian sampling methods. For a \"one-shot\" measurement of a\ndegrading lipid monolayer, it is shown that the common practice of measuring\nnull-reflecting water is indeed optimal, but that the optimal measurement angle\nis dependent on the deuteration state of the monolayer. Finally, the framework\nis used to demonstrate the feasibility of measuring magnetic signals as small\nas $0.01\\mu_{B}/\\text{atom}$ in layers only $20\\r{A}$ thick, given the\nappropriate experimental design, and that time to reach a given level of\nconfidence in the small magnetic moment is quantifiable.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.05605v2"
    },
    {
        "title": "Pileup Correction on Higher-order Cumulants with Unfolding Approach",
        "authors": [
            "Yu Zhang",
            "Yige Huang",
            "Toshihiro Nonaka",
            "Xiaofeng Luo"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Higher-order cumulants of conserved charge distributions are sensitive\nobservables to probe the critical fluctuations near QCD critical point in\nheavy-ion collisions. Due to high interaction rate, pileup event can be one of\nthe major sources of background in the measurements of higher-order cumulants.\nIn this paper, we studied the effects of pileup events on higher-order\ncumulants of proton multiplicity distributions using UrQMD model. It is found\nthat the proposed pileup correction fails if the correction parameters are\ndetermined by the Glauber fitting of charged particle multiplicities, which is\nusually done in the real heavy-ion experiment. To address this, we propose a\nmodel independent unfolding approach to determine the parameters in the pileup\ncorrection. This approach can be applied in the pileup correction for the\nfuture measurement of higher-order cumulants in heavy-ion collision experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.10134v2"
    },
    {
        "title": "Excess and Deficiency of Extreme Multidimensional Random Fields",
        "authors": [
            "Luk R. Arnaut"
        ],
        "category": "physics.data-an",
        "published_year": "2021",
        "summary": "  Probability distributions and densities are derived for the excess and\ndeficiency of the intensity or instantaneous energy (quasi-static power)\nassociated with a $p$-dimensional random vector field. Explicit expressions for\nthe exact distributions are obtained for arbitrary threshold levels, together\nwith simple approximate functions for relatively high or low thresholds. It is\nshown that precise expressions only require an expansion of order $p-1$ in the\nratio of the excess height to the threshold level. Numerical simulations\nvalidate the analytical results.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.11488v1"
    },
    {
        "title": "Machine Learning for Particle Flow Reconstruction at CMS",
        "authors": [
            "Joosep Pata",
            "Javier Duarte",
            "Farouk Mokhtar",
            "Eric Wulff",
            "Jieun Yoo",
            "Jean-Roch Vlimant",
            "Maurizio Pierini",
            "Maria Girone"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We provide details on the implementation of a machine-learning based particle\nflow algorithm for CMS. The standard particle flow algorithm reconstructs\nstable particles based on calorimeter clusters and tracks to provide a global\nevent reconstruction that exploits the combined information of multiple\ndetector subsystems, leading to strong improvements for quantities such as jets\nand missing transverse energy. We have studied a possible evolution of particle\nflow towards heterogeneous computing platforms such as GPUs using a graph\nneural network. The machine-learned PF model reconstructs particle candidates\nbased on the full list of tracks and calorimeter clusters in the event. For\nvalidation, we determine the physics performance directly in the CMS software\nframework when the proposed algorithm is interfaced with the offline\nreconstruction of jets and missing transverse energy. We also report the\ncomputational performance of the algorithm, which scales approximately linearly\nin runtime and memory usage with the input size.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.00330v1"
    },
    {
        "title": "Low probability states, data statistics, and entropy estimation",
        "authors": [
            "Damián G. Hernández",
            "Ahmed Roman",
            "Ilya Nemenman"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  A fundamental problem in analysis of complex systems is getting a reliable\nestimate of entropy of their probability distributions over the state space.\nThis is difficult because unsampled states can contribute substantially to the\nentropy, while they do not contribute to the Maximum Likelihood estimator of\nentropy, which replaces probabilities by the observed frequencies. Bayesian\nestimators overcome this obstacle by introducing a model of the low-probability\ntail of the probability distribution. Which statistical features of the\nobserved data determine the model of the tail, and hence the output of such\nestimators, remains unclear. Here we show that well-known entropy estimators\nfor probability distributions on discrete state spaces model the structure of\nthe low probability tail based largely on few statistics of the data: the\nsample size, the Maximum Likelihood estimate, the number of coincidences among\nthe samples, the dispersion of the coincidences. We derive approximate\nanalytical entropy estimators for undersampled distributions based on these\nstatistics, and we use the results to propose an intuitive understanding of how\nthe Bayesian entropy estimators work.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.00962v1"
    },
    {
        "title": "Improved particle-flow event reconstruction with scalable neural\n  networks for current and future particle detectors",
        "authors": [
            "Joosep Pata",
            "Eric Wulff",
            "Farouk Mokhtar",
            "David Southwick",
            "Mengke Zhang",
            "Maria Girone",
            "Javier Duarte"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Efficient and accurate algorithms are necessary to reconstruct particles in\nthe highly granular detectors anticipated at the High-Luminosity Large Hadron\nCollider and the Future Circular Collider. We study scalable machine learning\nmodels for event reconstruction in electron-positron collisions based on a full\ndetector simulation. Particle-flow reconstruction can be formulated as a\nsupervised learning task using tracks and calorimeter clusters. We compare a\ngraph neural network and kernel-based transformer and demonstrate that we can\navoid quadratic operations while achieving realistic reconstruction. We show\nthat hyperparameter tuning significantly improves the performance of the\nmodels. The best graph neural network model shows improvement in the jet\ntransverse momentum resolution by up to 50% compared to the rule-based\nalgorithm. The resulting model is portable across Nvidia, AMD and Habana\nhardware. Accurate and fast machine-learning based reconstruction can\nsignificantly improve future measurements at colliders.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.06782v6"
    },
    {
        "title": "Statistical divergences in high-dimensional hypothesis testing and a\n  modern technique for estimating them",
        "authors": [
            "Jeremy J. H. Wilkinson",
            "Christopher G. Lester"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Hypothesis testing in high dimensional data is a notoriously difficult\nproblem without direct access to competing models' likelihood functions. This\npaper argues that statistical divergences can be used to quantify the\ndifference between the population distributions of observed data and competing\nmodels, justifying their use as the basis of a hypothesis test. We go on to\npoint out how modern techniques for functional optimization let us estimate\nmany divergences, without the need for population likelihood functions, using\nsamples from two distributions alone. We use a physics-based example to show\nhow the proposed two-sample test can be implemented in practice, and discuss\nthe necessary steps required to mature the ideas presented into an experimental\nframework. The code used has been made available for others to use.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.06397v3"
    },
    {
        "title": "Identification of the parameters of complex constitutive models: Least\n  squares minimization vs. Bayesian updating",
        "authors": [
            "Thomas Most"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  In this study the common least-squares minimization approach is compared to\nthe Bayesian updating procedure. In the content of material parameter\nidentification the posterior parameter density function is obtained from its\nprior and the likelihood function of the measurements. By using Markov Chain\nMonte Carlo methods, such as the Metropolis-Hastings algorithm\n\\cite{Hastings1970}, the global density function including local peaks can be\ncomputed. Thus this procedure enables an accurate evaluation of the global\nparameter quality. However, the computational effort is remarkable larger\ncompared to the minimization approach. Thus several methodologies for an\nefficient approximation of the likelihood function are discussed in the present\nstudy.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04928v1"
    },
    {
        "title": "Hierarchical Bidirectional Transition Dispersion Entropy-based\n  Lempel-Ziv Complexity and Its Application in Fault-Bearing Diagnosis",
        "authors": [
            "Runze Jiang",
            "Pengjian Shang"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Lempel-Ziv complexity (LZC) is a key measure for detecting the irregularity\nand complexity of nonlinear time series and has seen various improvements in\nrecent decades. However, existing LZC-based metrics, such as Permutation\nLempel-Ziv complexity (PLZC) and Dispersion-Entropy based Lempel-Ziv complexity\n(DELZC), focus mainly on patterns of independent embedding vectors, often\noverlooking the transition patterns within the time series. To address this\ngap, this paper introduces a novel LZC-based method called Bidirectional\nTransition Dispersion Entropy-based Lempel-Ziv complexity (BT-DELZC).\nLeveraging Markov chain theory, this method integrates a bidirectional\ntransition network framework with DELZC to better capture dynamic signal\ninformation. Additionally, an improved hierarchical decomposition algorithm is\nused to extract features from various frequency components of the time series.\nThe proposed BT-DELZC method is first evaluated through four simulated\nexperiments, demonstrating its robustness and effectiveness in characterizing\nnonlinear time series. Additionally, two fault-bearing diagnosis experiments\nare conducted by combining the hierarchical BT-DELZC method with various\nclassifiers from the machine learning domain. The results indicate that\nBT-DELZC achieves the highest accuracy across both datasets, significantly\noutperforming existing methods such as LZC, PLZC, and DELZC in extracting\nfeatures related to fault bearings.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.11123v1"
    },
    {
        "title": "Large Physics Models: Towards a collaborative approach with Large\n  Language Models and Foundation Models",
        "authors": [
            "Kristian G. Barman",
            "Sascha Caron",
            "Emily Sullivan",
            "Henk W. de Regt",
            "Roberto Ruiz de Austri",
            "Mieke Boon",
            "Michael Färber",
            "Stefan Fröse",
            "Faegheh Hasibi",
            "Andreas Ipp",
            "Rukshak Kapoor",
            "Gregor Kasieczka",
            "Daniel Kostić",
            "Michael Krämer",
            "Tobias Golling",
            "Luis G. Lopez",
            "Jesus Marco",
            "Sydney Otten",
            "Pawel Pawlowski",
            "Pietro Vischia",
            "Erik Weber",
            "Christoph Weniger"
        ],
        "category": "physics.data-an",
        "published_year": "2025",
        "summary": "  This paper explores ideas and provides a potential roadmap for the\ndevelopment and evaluation of physics-specific large-scale AI models, which we\ncall Large Physics Models (LPMs). These models, based on foundation models such\nas Large Language Models (LLMs) - trained on broad data - are tailored to\naddress the demands of physics research. LPMs can function independently or as\npart of an integrated framework. This framework can incorporate specialized\ntools, including symbolic reasoning modules for mathematical manipulations,\nframeworks to analyse specific experimental and simulated data, and mechanisms\nfor synthesizing theories and scientific literature. We begin by examining\nwhether the physics community should actively develop and refine dedicated\nmodels, rather than relying solely on commercial LLMs. We then outline how LPMs\ncan be realized through interdisciplinary collaboration among experts in\nphysics, computer science, and philosophy of science. To integrate these models\neffectively, we identify three key pillars: Development, Evaluation, and\nPhilosophical Reflection. Development focuses on constructing models capable of\nprocessing physics texts, mathematical formulations, and diverse physical data.\nEvaluation assesses accuracy and reliability by testing and benchmarking.\nFinally, Philosophical Reflection encompasses the analysis of broader\nimplications of LLMs in physics, including their potential to generate new\nscientific understanding and what novel collaboration dynamics might arise in\nresearch. Inspired by the organizational structure of experimental\ncollaborations in particle physics, we propose a similarly interdisciplinary\nand collaborative approach to building and refining Large Physics Models. This\nroadmap provides specific objectives, defines pathways to achieve them, and\nidentifies challenges that must be addressed to realise physics-specific large\nscale AI models.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.05382v1"
    },
    {
        "title": "Bayesian reasoning versus conventional statistics in High Energy Physics",
        "authors": [
            "G. D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "1998",
        "summary": "  The intuitive reasoning of physicists in conditions of uncertainty is closer\nto the Bayesian approach than to the frequentist ideas taught at University and\nwhich are considered the reference framework for handling statistical problems.\nThe combination of intuition and conventional statistics allows practitioners\nto get results which are very close, both in meaning and in numerical value, to\nthose obtainable by Bayesian methods, at least in simple routine applications.\nThere are, however, cases in which ``arbitrary'' probability inversions produce\nunacceptable or misleading results and in these cases the conscious application\nof Bayesian reasoning becomes crucial. Starting from these considerations, I\nwill finally comment on the often debated question: ``is there any chance that\nall physicists will become Bayesian?''\n",
        "pdf_link": "http://arxiv.org/pdf/physics/9811046v1"
    },
    {
        "title": "Information and Entropy",
        "authors": [
            "Ariel Caticha"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  What is information? Is it physical? We argue that in a Bayesian theory the\nnotion of information must be defined in terms of its effects on the beliefs of\nrational agents. Information is whatever constrains rational beliefs and\ntherefore it is the force that induces us to change our minds. This problem of\nupdating from a prior to a posterior probability distribution is tackled\nthrough an eliminative induction process that singles out the logarithmic\nrelative entropy as the unique tool for inference. The resulting method of\nMaximum relative Entropy (ME), which is designed for updating from arbitrary\npriors given information in the form of arbitrary constraints, includes as\nspecial cases both MaxEnt (which allows arbitrary constraints) and Bayes' rule\n(which allows arbitrary priors). Thus, ME unifies the two themes of these\nworkshops -- the Maximum Entropy and the Bayesian methods -- into a single\ngeneral inference scheme that allows us to handle problems that lie beyond the\nreach of either of the two methods separately. I conclude with a couple of\nsimple illustrative examples.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.1068v1"
    },
    {
        "title": "Parametric fitting of data obtained from detectors with finite\n  resolution and limited acceptance",
        "authors": [
            "N. D. Gagunashvili"
        ],
        "category": "physics.data-an",
        "published_year": "2010",
        "summary": "  A goodness-of-fit test for the fitting of a parametric model to data obtained\nfrom a detector with finite resolution and limited acceptance is proposed. The\nparameters of the model are found by minimization of a statistic that is used\nfor comparing experimental data and simulated reconstructed data. Numerical\nexamples are presented to illustrate and validate the fitting procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.0662v2"
    },
    {
        "title": "Kernel based unfolding of data obtained from detectors with finite\n  resolution and limited acceptance",
        "authors": [
            "N. D. Gagunashvili",
            "M. Schmelling"
        ],
        "category": "physics.data-an",
        "published_year": "2012",
        "summary": "  A kernel based procedure for correcting experimental data for distortions due\nto the finite resolution and limited detector acceptance is presented. The\nunfolding problem is known to be an ill-posed problem that can not be solved\nwithout some a priori information about solution such as, for example,\nsmoothness or positivity. In the approach presented here the true distribution\nis estimated by a weighted sum of kernels, with the width of the kernels acting\nas a regularization parameter responsible for the smoothness of the result.\nCross-validation is used to determine an optimal value for this parameter. A\nnumerical example with a simulation study of systematical and statistical\nerrors is presented to illustrate the procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.3766v1"
    },
    {
        "title": "Hierarchical Block Structures and High-resolution Model Selection in\n  Large Networks",
        "authors": [
            "Tiago P. Peixoto"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  Discovering and characterizing the large-scale topological features in\nempirical networks are crucial steps in understanding how complex systems\nfunction. However, most existing methods used to obtain the modular structure\nof networks suffer from serious problems, such as being oblivious to the\nstatistical evidence supporting the discovered patterns, which results in the\ninability to separate actual structure from noise. In addition to this, one\nalso observes a resolution limit on the size of communities, where smaller but\nwell-defined clusters are not detectable when the network becomes large. This\nphenomenon occurs not only for the very popular approach of modularity\noptimization, which lacks built-in statistical validation, but also for more\nprincipled methods based on statistical inference and model selection, which do\nincorporate statistical validation in a formally correct way. Here we construct\na nested generative model that, through a complete description of the entire\nnetwork hierarchy at multiple scales, is capable of avoiding this limitation,\nand enables the detection of modular structure at levels far beyond those\npossible with current approaches. Even with this increased resolution, the\nmethod is based on the principle of parsimony, and is capable of separating\nsignal from noise, and thus will not lead to the identification of spurious\nmodules even on sparse networks. Furthermore, it fully generalizes other\napproaches in that it is not restricted to purely assortative mixing patterns,\ndirected or undirected graphs, and ad hoc hierarchical structures such as\nbinary trees. Despite its general character, the approach is tractable, and can\nbe combined with advanced techniques of community detection to yield an\nefficient algorithm that scales well for very large networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.4377v6"
    },
    {
        "title": "A path-integral approach to Bayesian inference for inverse problems\n  using the semiclassical approximation",
        "authors": [
            "Joshua C Chang",
            "Van Savage",
            "Tom Chou"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  We demonstrate how path integrals often used in problems of theoretical\nphysics can be adapted to provide a machinery for performing Bayesian inference\nin function spaces. Such inference comes about naturally in the study of\ninverse problems of recovering continuous (infinite dimensional) coefficient\nfunctions from ordinary or partial differential equations (ODE, PDE), a problem\nwhich is typically ill-posed. Regularization of these problems using $L^2$\nfunction spaces (Tikhonov regularization) is equivalent to Bayesian\nprobabilistic inference, using a Gaussian prior. The Bayesian interpretation of\ninverse problem regularization is useful since it allows one to quantify and\ncharacterize error and degree of precision in the solution of inverse problems,\nas well as examine assumptions made in solving the problem -- namely whether\nthe subjective choice of regularization is compatible with prior knowledge.\nUsing path-integral formalism, Bayesian inference can be explored through\nvarious perturbative techniques, such as the semiclassical approximation, which\nwe use in this manuscript. Perturbative path-integral approaches, while\noffering alternatives to computational approaches like Markov-Chain-Monte-Carlo\n(MCMC), also provide natural starting points for MCMC methods that can be used\nto refine approximations.\n  In this manuscript, we illustrate a path-integral formulation for inverse\nproblems and demonstrate it on an inverse problem in membrane biophysics as\nwell as inverse problems in potential theories involving the Poisson equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.2974v4"
    },
    {
        "title": "Rapid and deterministic estimation of probability densities using\n  scale-free field theories",
        "authors": [
            "Justin B. Kinney"
        ],
        "category": "physics.data-an",
        "published_year": "2013",
        "summary": "  The question of how best to estimate a continuous probability density from\nfinite data is an intriguing open problem at the interface of statistics and\nphysics. Previous work has argued that this problem can be addressed in a\nnatural way using methods from statistical field theory. Here I describe new\nresults that allow this field-theoretic approach to be rapidly and\ndeterministically computed in low dimensions, making it practical for use in\nday-to-day data analysis. Importantly, this approach does not impose a\nprivileged length scale for smoothness of the inferred probability density, but\nrather learns a natural length scale from the data due to the tradeoff between\ngoodness-of-fit and an Occam factor. Open source software implementing this\nmethod in one and two dimensions is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.6661v3"
    },
    {
        "title": "Parameter Estimation of Social Forces in Crowd Dynamics Models via a\n  Probabilistic Method",
        "authors": [
            "Alessandro Corbetta",
            "Adrian Muntean",
            "Federico Toschi",
            "Kiamars Vafayi"
        ],
        "category": "physics.data-an",
        "published_year": "2014",
        "summary": "  Focusing on a specific crowd dynamics situation, including real life\nexperiments and measurements, our paper targets a twofold aim: (1) we present a\nBayesian probabilistic method to estimate the value and the uncertainty (in the\nform of a probability density function) of parameters in crowd dynamic models\nfrom the experimental data; and (2) we introduce a fitness measure for the\nmodels to classify a couple of model structures (forces) according to their\nfitness to the experimental data, preparing the stage for a more general\nmodel-selection and validation strategy inspired by probabilistic data\nanalysis. Finally, we review the essential aspects of our experimental setup\nand measurement technique.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.5361v1"
    },
    {
        "title": "The Waves and the Sigmas (To Say Nothing of the 750 GeV Mirage)",
        "authors": [
            "Giulio D'Agostini"
        ],
        "category": "physics.data-an",
        "published_year": "2016",
        "summary": "  This paper shows how p-values do not only create, as well known, wrong\nexpectations in the case of flukes, but they might also dramatically diminish\nthe `significance' of most likely genuine signals. As real life examples, the\n2015 first detections of gravitational waves are discussed. The March 2016\nstatement of the American Statistical Association, warning scientists about\ninterpretation and misuse of p-values, is also reminded and commented. (The\npaper is complemented with some remarks on past, recent and future claims of\ndiscoveries based on sigmas from Particles Physics.)\n",
        "pdf_link": "http://arxiv.org/pdf/1609.01668v1"
    },
    {
        "title": "Maximizing the information learned from finite data selects a simple\n  model",
        "authors": [
            "Henry H. Mattingly",
            "Mark K. Transtrum",
            "Michael C. Abbott",
            "Benjamin B. Machta"
        ],
        "category": "physics.data-an",
        "published_year": "2017",
        "summary": "  We use the language of uninformative Bayesian prior choice to study the\nselection of appropriately simple effective models. We advocate for the prior\nwhich maximizes the mutual information between parameters and predictions,\nlearning as much as possible from limited data. When many parameters are poorly\nconstrained by the available data, we find that this prior puts weight only on\nboundaries of the parameter manifold. Thus it selects a lower-dimensional\neffective theory in a principled way, ignoring irrelevant parameter directions.\nIn the limit where there is sufficient data to tightly constrain any number of\nparameters, this reduces to Jeffreys prior. But we argue that this limit is\npathological when applied to the hyper-ribbon parameter manifolds generic in\nscience, because it leads to dramatic dependence on effects invisible to\nexperiment.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01166v3"
    },
    {
        "title": "Event-by-Event Efficiency Fluctuations and Efficiency Correction for\n  Cumulants of Superposed Multiplicity Distributions in Relativistic Heavy-ion\n  Collision Experiments",
        "authors": [
            "Shu He",
            "Xiaofeng Luo"
        ],
        "category": "physics.data-an",
        "published_year": "2018",
        "summary": "  We performed systematic studies on the effects of event-by-event efficiency\nfluctuations on efficiency correction for cumulant analysis in relativistic\nheavy-ion collision experiments. Experimentally, particle efficiencies of\nevents measured under different experimental conditions should be different.\nFor fluctuation measurements, the final event-by-event multiplicity\ndistributions should be the superposed distributions of various type of events\nmeasured under different conditions. We demonstrate efficiency fluctuation\neffects using numerical simulation, in which we construct an event ensemble\nconsisting of events with two different efficiencies. By using the mean\nparticle efficiencies, we find that the efficiency corrected cumulants show\nlarge deviations from the original inputs when the discrepancy between the two\nefficiencies is large. We further studied the effects of efficiency\nfluctuations for the cumulants of net-proton distributions by implementing the\nUrQMD events of Au+Au collisions at $\\sqrt{s_{NN}}=7.7$ GeV in a realistic STAR\ndetector acceptance. We consider the unequal efficiency in two sides of the\nTime Projection Chamber (TPC), multiplicity dependent efficiency, and the\nevent-by-event variations of the collision vertex position along the\nlongitudinal direction ($V_\\mathrm{z}$). When the efficiencies fluctuate\ndramatically within the studied event sample, the effects of efficiency\nfluctuations have significant impacts on the efficiency corrections of\ncumulants with the mean efficiency. We find that this effect can be effectively\nsuppressed by binning the entire event ensemble into various sub-event samples,\nin which the efficiency variations are relatively small. The final efficiency\ncorrected cumulants can be calculated from the weighted average of the\ncorrected factorial moments of the sub-event samples with the mean efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.02911v2"
    },
    {
        "title": "Application of Adaptive Multilevel Splitting to High-Dimensional\n  Dynamical Systems",
        "authors": [
            "S. Baars",
            "D. Castellana",
            "F. W. Wubs",
            "H. A. Dijkstra"
        ],
        "category": "physics.data-an",
        "published_year": "2020",
        "summary": "  Stochastic nonlinear dynamical systems can undergo rapid transitions relative\nto the change in their forcing, for example due to the occurrence of multiple\nequilibrium solutions for a specific interval of parameters. In this paper, we\nmodify one of the methods developed to compute probabilities of such\ntransitions, Trajectory-Adaptive Multilevel Sampling (TAMS), to be able to\napply it to high-dimensional systems. The key innovation is a projected\ntime-stepping approach, which leads to a strong reduction in computational\ncosts, in particular memory usage. The performance of this new implementation\nof TAMS is studied through an example of the collapse of the Atlantic Ocean\nCirculation.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.05745v1"
    },
    {
        "title": "Lectures on Probability, Entropy, and Statistical Physics",
        "authors": [
            "Ariel Caticha"
        ],
        "category": "physics.data-an",
        "published_year": "2008",
        "summary": "  These lectures deal with the problem of inductive inference, that is, the\nproblem of reasoning under conditions of incomplete information. Is there a\ngeneral method for handling uncertainty? Or, at least, are there rules that\ncould in principle be followed by an ideally rational mind when discussing\nscientific matters? What makes one statement more plausible than another? How\nmuch more plausible? And then, when new information is acquired how do we\nchange our minds? Or, to put it differently, are there rules for learning? Are\nthere rules for processing information that are objective and consistent? Are\nthey unique? And, come to think of it, what, after all, is information? It is\nclear that data contains or conveys information, but what does this precisely\nmean? Can information be conveyed in other ways? Is information physical? Can\nwe measure amounts of information? Do we need to? Our goal is to develop the\nmain tools for inductive inference--probability and entropy--from a thoroughly\nBayesian point of view and to illustrate their use in physics with examples\nborrowed from the foundations of classical statistical physics.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.0012v1"
    },
    {
        "title": "Scale Dependencies and Self-Similar Models with Wavelet Scattering\n  Spectra",
        "authors": [
            "Rudy Morel",
            "Gaspar Rochette",
            "Roberto Leonarduzzi",
            "Jean-Philippe Bouchaud",
            "Stéphane Mallat"
        ],
        "category": "physics.data-an",
        "published_year": "2022",
        "summary": "  We introduce the wavelet scattering spectra which provide non-Gaussian models\nof time-series having stationary increments. A complex wavelet transform\ncomputes signal variations at each scale. Dependencies across scales are\ncaptured by the joint correlation across time and scales of wavelet\ncoefficients and their modulus. This correlation matrix is nearly diagonalized\nby a second wavelet transform, which defines the scattering spectra. We show\nthat this vector of moments characterizes a wide range of non-Gaussian\nproperties of multi-scale processes. We prove that self-similar processes have\nscattering spectra which are scale invariant. This property can be tested\nstatistically on a single realization and defines a class of wide-sense\nself-similar processes. We build maximum entropy models conditioned by\nscattering spectra coefficients, and generate new time-series with a\nmicrocanonical sampling algorithm. Applications are shown for highly\nnon-Gaussian financial and turbulence time-series.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.10177v2"
    },
    {
        "title": "Geometric path augmentation for inference of sparsely observed\n  stochastic nonlinear systems",
        "authors": [
            "Dimitra Maoutsa"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Stochastic evolution equations describing the dynamics of systems under the\ninfluence of both deterministic and stochastic forces are prevalent in all\nfields of science. Yet, identifying these systems from sparse-in-time\nobservations remains still a challenging endeavour. Existing approaches focus\neither on the temporal structure of the observations by relying on conditional\nexpectations, discarding thereby information ingrained in the geometry of the\nsystem's invariant density; or employ geometric approximations of the invariant\ndensity, which are nevertheless restricted to systems with conservative forces.\nHere we propose a method that reconciles these two paradigms. We introduce a\nnew data-driven path augmentation scheme that takes the local observation\ngeometry into account. By employing non-parametric inference on the augmented\npaths, we can efficiently identify the deterministic driving forces of the\nunderlying system for systems observed at low sampling rates.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.08102v1"
    },
    {
        "title": "Inferential Moments of Uncertain Multivariable Systems",
        "authors": [
            "Kevin Vanslette"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  This article expands the framework of Bayesian inference and provides direct\nprobabilistic methods for approaching inference tasks that are typically\nhandled with information theory. We treat Bayesian probability updating as a\nrandom process and uncover intrinsic quantitative features of joint probability\ndistributions called inferential moments. Inferential moments quantify shape\ninformation about how a prior distribution is expected to update in response to\nyet to be obtained information. Further, we quantify the unique probability\ndistribution whose statistical moments are the inferential moments in question.\nWe find a power series expansion of the mutual information in terms of\ninferential moments, which implies a connection between inferential theoretic\nlogic and elements of information theory. Of particular interest is the\ninferential deviation, which is the expected variation of the probability of\none variable in response to an inferential update of another. We explore two\napplications that analyze the inferential deviations of a Bayesian network to\nimprove decision-making. We implement simple greedy algorithms for exploring\nsensor tasking using inferential deviations that generally outperform similar\ngreedy mutual information algorithms in terms of root mean squared error\nbetween epistemic probability estimates and the ground truth probabilities they\nare estimating.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.01841v2"
    },
    {
        "title": "Towards effective information content assessment: analytical derivation\n  of information loss in the reconstruction of random fields with model\n  uncertainty",
        "authors": [
            "Aleksei Cherkasov",
            "Kirill M. Gerke",
            "Aleksey Khlyupin"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  Structures are abundant in both natural and human-made environments and\nusually studied in the form of images or scattering patterns. To characterize\nstructures a huge variety of descriptors is available spanning from porosity to\nradial and correlation functions. In addition to morphological structural\nanalysis, such descriptors are necessary for stochastic reconstructions,\nstationarity and representativity analysis. The most important characteristic\nof any such descriptor is its information content - or its ability to describe\nthe structure at hand. For example, from crystallography it is well known that\nexperimentally measurable $S_2$ correlation function lacks necessary\ninformation content to describe majority of structures. The information content\nof this function can be assessed using Monte-Carlo methods only for very small\n2D images due to computational expenses. Some indirect quantitative approaches\nfor this and other correlation function were also proposed. Yet, to date no\nmethodology to obtain information content for arbitrary 2D or 3D image is\navailable. In this work, we make a step toward developing a general framework\nto perform such computations analytically. We show, that one can assess the\nentropy of a perturbed random field and that stochastic perturbation of fields\ncorrelation function decreases its information content. In addition to\nanalytical expression, we demonstrate that different regions of correlation\nfunction are in different extent informative and sensitive for perturbation.\nProposed model bridges the gap between descriptor-based heterogeneous media\nreconstruction and information theory and opens way for computationally\neffective way to compute information content of any descriptor as applied to\narbitrary structure.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.13870v1"
    },
    {
        "title": "Sorting ECGs by lag irreversibility",
        "authors": [
            "Nazul Merino Negrete",
            "Cesar Maldonado",
            "Raúl Salgado-García"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  In this work we introduce the lag irreversibility function as a method to\nassess time-irreversibility in discrete time series. It quantifies the degree\nof time-asymmetry for the joint probability function of the state variable\nunder study and the state variable lagged in time. We test its performance in a\ntime-irreversible Markov chain model for which theoretical results are known.\nMoreover, we use our approach to analyze electrocardiographic recordings of\nfour groups of subjects: healthy young individuals, healthy elderly\nindividuals, and persons with two different disease conditions, namely,\ncongestive heart failure and atrial fibrillation. We find that by studying\njointly the variability of the amplitudes of the different waves in the\nelectrocardiographic signals, one can obtain an efficient method to\ndiscriminate between the groups already mentioned. Finally, we test the\naccuracy of our method using the ROC analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.15998v1"
    },
    {
        "title": "Bayesian questions with frequentist answers",
        "authors": [
            "Alan H. Guth",
            "Mohammad Hossein Namjoo"
        ],
        "category": "physics.data-an",
        "published_year": "2023",
        "summary": "  The two statistical methods, namely the frequentist and the Bayesian methods,\nare both commonly used for probabilistic inference in many scientific\nsituations. However, it is not straightforward to interpret the result of one\napproach in terms of the concepts of the other. In this paper we explore the\npossibility of finding a Bayesian significance for the frequentist's main\nobject of interest, the $p$-value, which is the probability assigned to the\nproposition -- which we call the {\\it extremity proposition} -- that a\nmeasurement will result in a value that is at least as extreme as the value\nthat was actually obtained. To make contact with the frequentist language, the\nBayesian can choose to update probabilities based on the {\\it extremity\nproposition}, which is weaker than the standard Bayesian update proposition,\nwhich uses the actual observed value. We then show that the posterior\nprobability (or probability density) of a theory is equal to the prior\nprobability (or probability density) multiplied by the ratio of the $p$-value\nfor the data obtained, given that theory, to the mean $p$-value -- averaged\nover all theories weighted by their prior probabilities. Thus, we provide\nfrequentist answers to Bayesian questions. Our result is generic -- it does not\nrely on restrictive assumptions about the situation under consideration or\nspecific properties of the likelihoods or the priors.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.16252v1"
    },
    {
        "title": "The Emergence of the Normal Distribution in Deterministic Chaotic Maps",
        "authors": [
            "Damián H. Zanette",
            "Inés Samengo"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  The Central Limit Theorem states that, in the limit of a large number of\nterms, an appropriately scaled sum of independent random variables yields\nanother random variable whose probability distribution tends to a stable\ndistribution. The condition of independence, however, only holds in real\nsystems as an approximation. To extend the theorem to more general situations,\nprevious studies have derived a version of the Central Limit Theorem that also\nholds for variables that are not independent. Here, we present numerical\nresults that characterize how convergence is attained when the variables being\nsummed are deterministically related to one another by the recurrent\napplication of an ergodic mapping. In all the explored cases, the convergence\nto the limit distribution is slower than for random sampling. Yet, the speed at\nwhich convergence is attained varies substantially from system to system, and\nthese variations imply differences in the way information about the\ndeterministic nature of the dynamics is progressively lost as the number of\nsummands increases. Some of the identified factors in shaping the convergence\nprocess are the strength of mixing induced by the mapping and the shape of the\nmarginal distribution of each variable, most particularly, the presence of\ndivergences or fat tails.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.03808v1"
    },
    {
        "title": "Modelling parametric uncertainty in PDEs models via Physics-Informed\n  Neural Networks",
        "authors": [
            "Milad Panahi",
            "Giovanni Michele Porta",
            "Monica Riva",
            "Alberto Guadagnini"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  We provide an approach enabling one to employ physics-informed neural\nnetworks (PINNs) for uncertainty quantification. Our approach is applicable to\nsystems where observations are scarce (or even lacking), these being typical\nsituations associated with subsurface water bodies. Our novel physics-informed\nneural network under uncertainty (PINN-UU) integrates the space-time domain\nacross which processes take place and uncertain parameter spaces within a\nunique computational domain. PINN-UU is then trained to satisfy the relevant\nphysical principles (e.g., mass conservation) in the defined input domain. We\nemploy a stage training approach via transfer learning to accommodate\nhigh-dimensional solution spaces. We demonstrate the effectiveness of PINN-UU\nin a scenario associated with reactive transport in porous media, showcasing\nits reliability, efficiency, and applicability to sensitivity analysis. PINN-UU\nemerges as a promising tool for robust uncertainty quantification, with broad\napplicability to groundwater systems. As such, it can be considered as a\nvaluable alternative to traditional methods such as multi-realization Monte\nCarlo simulations based on direct solvers or black-box surrogate models.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04690v1"
    },
    {
        "title": "Bayesian forecasting with information theory",
        "authors": [
            "Mohammad Hossein Namjoo"
        ],
        "category": "physics.data-an",
        "published_year": "2024",
        "summary": "  Forecasting techniques for assessing the power of future experiments to\ndiscriminate between theories or discover new laws of nature are of great\ninterest in many areas of science. In this paper, we introduce a Bayesian\nforecasting method using information theory. We argue that mutual information\nis a suitable quantity to study in this context. Besides being Bayesian, this\nproposal has the advantage of not relying on the choice of fiducial parameters,\ndescribing the \"true\" theory (which is a priori unknown), and is applicable to\nany probability distribution. We demonstrate that the proposed method can be\nused for parameter estimation and model selection, both of which are of\ninterest concerning future experiments. We argue that mutual information has\nplausible interpretation in both situations. In addition, we state a number of\npropositions that offer information-theoretic meaning to some of the Bayesian\npractices such as performing multiple experiments, combining different\ndatasets, and marginalization.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.13802v1"
    },
    {
        "title": "Updating Probabilities with Data and Moments",
        "authors": [
            "Adom Giffin",
            "Ariel Caticha"
        ],
        "category": "physics.data-an",
        "published_year": "2007",
        "summary": "  We use the method of Maximum (relative) Entropy to process information in the\nform of observed data and moment constraints. The generic \"canonical\" form of\nthe posterior distribution for the problem of simultaneous updating with data\nand moments is obtained. We discuss the general problem of non-commuting\nconstraints, when they should be processed sequentially and when\nsimultaneously. As an illustration, the multinomial example of die tosses is\nsolved in detail for two superficially similar but actually very different\nproblems.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.1593v2"
    }
]